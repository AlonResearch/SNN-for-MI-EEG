{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as da\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io as scio\n",
        "!pip install spikingjelly\n",
        "from spikingjelly.activation_based import ann2snn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "xy9K5bJ9aB7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717b332f-067d-4a07-dae2-0b7a633bd154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spikingjelly in /usr/local/lib/python3.11/dist-packages (0.0.0.0.14)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from spikingjelly) (2.6.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from spikingjelly) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from spikingjelly) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from spikingjelly) (4.67.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from spikingjelly) (0.21.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from spikingjelly) (1.15.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->spikingjelly) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->spikingjelly) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->spikingjelly) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->spikingjelly) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lAEV1BHaBp9",
        "outputId": "111b0289-9c89-4185-ce76-32259af92ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Thu May  1 22:02:06 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   78C    P0             35W /   70W |     496MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYNshZR5ZhDy"
      },
      "outputs": [],
      "source": [
        "#Defining functions\n",
        "\n",
        "def data_loader(data, label, batch=64, shuffle=True, drop=False):\n",
        "    \"\"\"\n",
        "    Preprocess the data to fit model.\n",
        "    Feed data into data_loader.\n",
        "    input:\n",
        "        data (float): samples*length*ch (samples*ch*length).\n",
        "        label (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        batch (int): batch size\n",
        "        shuffle (bool): shuffle data before input into decoder\n",
        "        drop (bool): drop the last samples if True\n",
        "    output:\n",
        "        data loader\n",
        "    \"\"\"\n",
        "    label = torch.LongTensor(label.flatten()).to(device)\n",
        "    if data.shape[1] >= data.shape[2]:\n",
        "        data = torch.tensor(data.swapaxes(1, 2))\n",
        "    data = torch.unsqueeze(data, dim=1).type('torch.FloatTensor').to(device)\n",
        "    data = da.TensorDataset(data, label)\n",
        "    loader = da.DataLoader(dataset=data, batch_size=batch, shuffle=shuffle, drop_last=drop)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def val_snn(Dec, test_loader, T=None):\n",
        "    Dec.eval().to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if T is not None:\n",
        "        corrects = np.zeros(T)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            if T is None:\n",
        "                outputs = Dec(inputs)\n",
        "                correct += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            else:\n",
        "                for m in Dec.modules():\n",
        "                    if hasattr(m, 'reset'):\n",
        "                        m.reset()\n",
        "                for t in range(T):\n",
        "                    if t == 0:\n",
        "                        outputs = Dec(inputs)\n",
        "                    else:\n",
        "                        outputs += Dec(inputs)\n",
        "                    corrects[t] += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            total += targets.shape[0]\n",
        "    return correct / total if T is None else corrects / total\n",
        "\n",
        "\n",
        "def anntosnn(ann_model, train_x, train_y, test_x, test_y, batch=64, T=None):\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    print('---------------------------------------------')\n",
        "    print('Converting using MaxNorm')\n",
        "    model_converter = ann2snn.Converter(mode='max', dataloader=train_loader)\n",
        "    snn_model = model_converter(ann_model)\n",
        "    mode_max_accs = val_snn(snn_model, test_loader, T=T)\n",
        "\n",
        "    return mode_max_accs\n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def train_ann(ann_model, train_x, train_y, test_x, test_y, ep=500, batch=64):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        train_x, test_x (float): samples*length*ch (samples*ch*length).\n",
        "        train_y, test_y (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        ep (int): total train and test epoch\n",
        "        batch (int): batch size\n",
        "    output:\n",
        "        train acc, test acc, weight_file\n",
        "    \"\"\"\n",
        "    # Define training configuration\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(ann_model.parameters(), lr=0.01)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ep)\n",
        "\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "    for epoch in range(ep):\n",
        "        # Train ANN\n",
        "        ann_model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        print('\\n')\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = ann_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            print(batch_idx, len(train_loader), 'Epoch: %d | ANN: trainLoss: %.4f | trainAcc: %.4f%% (%d/%d)'\n",
        "                  % (epoch, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        train_acc.append(round(correct / total, 4))\n",
        "\n",
        "        # Test ANN\n",
        "        ann_model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "                outputs = ann_model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "                print(batch_idx, len(test_loader), 'Epoch: %d | ANN: testLoss: %.4f | testAcc: %.4f%% (%d/%d)'\n",
        "                      % (epoch, val_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        test_acc.append(round(correct / total, 4))\n",
        "\n",
        "    train_acc = np.asarray(train_acc[-1])\n",
        "    test_acc = np.asarray(test_acc[-1])\n",
        "    return train_acc, test_acc,ann_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2a\n",
        "\n",
        "class LENet(nn.Module):\n",
        "    \"\"\"\n",
        "        LENet Model\n",
        "    input:\n",
        "         data shape as: batch_size*1*channel*length (64*1*22*1000) BCI IV-2a\n",
        "         batch_size：64\n",
        "         channel：22\n",
        "         length：1000\n",
        "    output:\n",
        "        classes_num\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classes_num=4):\n",
        "        super(LENet, self).__init__()\n",
        "        self.drop_out = 0.5\n",
        "\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,64) #\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,32) #\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,16) #\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            # Temporal Convolution block fusion kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            # Spatial Convolution block kernel_size (channel,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(22, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            # Feature Fusion Convolution block kernel_size (1,16) and (1,1) #\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),  #\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.CCB = nn.Sequential(\n",
        "            # Classification Convolution block kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=classes_num,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "        x = self.CCB(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LENet_FCL(nn.Module):\n",
        "    def __init__(self, classes_num=4):\n",
        "        super(LENet_FCL, self).__init__()\n",
        "        self.drop_out = 0.5\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(22, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),  #\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FCL = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16 * 31, classes_num)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "        x = self.FCL(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EC8xjyPDaM6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "An example for the pipeline pf ANN to SNN Conversion Framework.\n",
        "\"\"\"\n",
        "\n",
        "def framework_pipeline(train_x, train_y, test_x, test_y, epoch=200, batch=64, T=100):\n",
        "    \"\"\"\n",
        "        ANN to SNN Conversion framework\n",
        "    input:\n",
        "        train_x, test_x (float): Train and test data, shape as: samples*length*ch (samples*ch*length).\n",
        "        train_y, test_y (int): Train and test label, shape as: samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        epoch (int): Total train and test epoch\n",
        "        batch (int): Batch size\n",
        "        T (int): Time step for SNN\n",
        "    output:\n",
        "        None\n",
        "    \"\"\"\n",
        "    ann_model = LENet(classes_num=4).to(device)\n",
        "    ann_model.apply(initialize_weights)\n",
        "\n",
        "    train_acc, test_acc, model_trained = train_ann(ann_model, train_x, train_y, test_x, test_y, ep=epoch, batch=batch)\n",
        "    max_norm_acc = anntosnn(model_trained, train_x, train_y, test_x, test_y, batch=batch, T=T)\n",
        "    snn_model = ann2snn.Converter(mode='max', dataloader=data_loader(train_x, train_y, batch=batch))(model_trained)\n",
        "\n",
        "    # Get ANN predictions\n",
        "    ann_model.eval()\n",
        "    ann_predictions = []\n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader(test_x, test_y, batch=batch, shuffle=False, drop=False):\n",
        "            outputs = ann_model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            ann_predictions.extend(predicted.cpu().numpy())\n",
        "            true_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    # Get SNN predictions\n",
        "    snn_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader(test_x, test_y, batch=batch, shuffle=False, drop=False):\n",
        "          for m in snn_model.modules():\n",
        "              if hasattr(m, 'reset'):\n",
        "                  m.reset()\n",
        "          for t in range(T):\n",
        "              if t == 0:\n",
        "                  outputs = snn_model(inputs)\n",
        "              else:\n",
        "                  outputs += snn_model(inputs)\n",
        "          _, predicted = outputs.max(1)\n",
        "          snn_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "\n",
        "    # Confusion Matrix for ANN\n",
        "    cm_ann = confusion_matrix(true_labels, ann_predictions)\n",
        "    disp_ann = ConfusionMatrixDisplay(confusion_matrix=cm_ann)\n",
        "    disp_ann.plot()\n",
        "    plt.title('Confusion Matrix - ANN')\n",
        "    plt.show()\n",
        "\n",
        "    # Confusion Matrix for SNN\n",
        "    cm_snn = confusion_matrix(true_labels, snn_predictions)\n",
        "    disp_snn = ConfusionMatrixDisplay(confusion_matrix=cm_snn)\n",
        "    disp_snn.plot()\n",
        "    plt.title('Confusion Matrix - SNN')\n",
        "    plt.show()\n",
        "\n",
        "    print('\\n')\n",
        "    print('ANN accuracy: Test: %.4f%%' % (test_acc * 100))\n",
        "    print('SNN accuracy: max_norm: %.4f%%' % (max_norm_acc[-1] * 100))\n",
        "\n",
        "\n",
        "# Getting real samples\n",
        "file = scio.loadmat('/content/A01T.mat')\n",
        "all_data = file['all_data']\n",
        "all_label = file['all_label']\n",
        "\n",
        "datasetX = torch.tensor(all_data, dtype=torch.float32)\n",
        "datasetY = torch.tensor(all_label, dtype=torch.int64)\n",
        "train_data, test_data, train_label, test_label = train_test_split(datasetX, datasetY, test_size=0.4, shuffle=True,\n",
        "                                                                  random_state=0)\n",
        "\n",
        "# ANN to SNN Conversion\n",
        "framework_pipeline(train_data, train_label, test_data, test_label, epoch=100, batch=64, T=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T0hmMue2aMqA",
        "outputId": "6fb20033-d2d7-41fd-e575-645347f9d428",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "0 3 Epoch: 0 | ANN: trainLoss: 3.3046 | trainAcc: 15.6250% (10/64)\n",
            "1 3 Epoch: 0 | ANN: trainLoss: 2.9337 | trainAcc: 21.8750% (28/128)\n",
            "2 3 Epoch: 0 | ANN: trainLoss: 2.7178 | trainAcc: 24.4186% (42/172)\n",
            "0 2 Epoch: 0 | ANN: testLoss: 1.5379 | testAcc: 26.5625% (17/64)\n",
            "1 2 Epoch: 0 | ANN: testLoss: 1.5312 | testAcc: 26.7241% (31/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 1 | ANN: trainLoss: 2.5957 | trainAcc: 20.3125% (13/64)\n",
            "1 3 Epoch: 1 | ANN: trainLoss: 2.3503 | trainAcc: 24.2188% (31/128)\n",
            "2 3 Epoch: 1 | ANN: trainLoss: 2.2624 | trainAcc: 27.3256% (47/172)\n",
            "0 2 Epoch: 1 | ANN: testLoss: 1.5455 | testAcc: 28.1250% (18/64)\n",
            "1 2 Epoch: 1 | ANN: testLoss: 1.6524 | testAcc: 26.7241% (31/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 2 | ANN: trainLoss: 1.8001 | trainAcc: 21.8750% (14/64)\n",
            "1 3 Epoch: 2 | ANN: trainLoss: 1.9835 | trainAcc: 23.4375% (30/128)\n",
            "2 3 Epoch: 2 | ANN: trainLoss: 1.8461 | trainAcc: 23.8372% (41/172)\n",
            "0 2 Epoch: 2 | ANN: testLoss: 1.6573 | testAcc: 28.1250% (18/64)\n",
            "1 2 Epoch: 2 | ANN: testLoss: 1.6320 | testAcc: 30.1724% (35/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 3 | ANN: trainLoss: 1.8242 | trainAcc: 23.4375% (15/64)\n",
            "1 3 Epoch: 3 | ANN: trainLoss: 1.7111 | trainAcc: 24.2188% (31/128)\n",
            "2 3 Epoch: 3 | ANN: trainLoss: 1.6486 | trainAcc: 25.0000% (43/172)\n",
            "0 2 Epoch: 3 | ANN: testLoss: 1.8012 | testAcc: 21.8750% (14/64)\n",
            "1 2 Epoch: 3 | ANN: testLoss: 1.6847 | testAcc: 25.8621% (30/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 4 | ANN: trainLoss: 1.5981 | trainAcc: 20.3125% (13/64)\n",
            "1 3 Epoch: 4 | ANN: trainLoss: 1.5246 | trainAcc: 28.1250% (36/128)\n",
            "2 3 Epoch: 4 | ANN: trainLoss: 1.5004 | trainAcc: 26.1628% (45/172)\n",
            "0 2 Epoch: 4 | ANN: testLoss: 1.9104 | testAcc: 21.8750% (14/64)\n",
            "1 2 Epoch: 4 | ANN: testLoss: 1.8869 | testAcc: 25.8621% (30/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 5 | ANN: trainLoss: 1.3315 | trainAcc: 40.6250% (26/64)\n",
            "1 3 Epoch: 5 | ANN: trainLoss: 1.4047 | trainAcc: 34.3750% (44/128)\n",
            "2 3 Epoch: 5 | ANN: trainLoss: 1.4276 | trainAcc: 34.3023% (59/172)\n",
            "0 2 Epoch: 5 | ANN: testLoss: 1.5672 | testAcc: 29.6875% (19/64)\n",
            "1 2 Epoch: 5 | ANN: testLoss: 1.8288 | testAcc: 25.8621% (30/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 6 | ANN: trainLoss: 1.3202 | trainAcc: 34.3750% (22/64)\n",
            "1 3 Epoch: 6 | ANN: trainLoss: 1.3149 | trainAcc: 34.3750% (44/128)\n",
            "2 3 Epoch: 6 | ANN: trainLoss: 1.2841 | trainAcc: 36.6279% (63/172)\n",
            "0 2 Epoch: 6 | ANN: testLoss: 1.6278 | testAcc: 32.8125% (21/64)\n",
            "1 2 Epoch: 6 | ANN: testLoss: 1.6496 | testAcc: 26.7241% (31/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 7 | ANN: trainLoss: 1.2211 | trainAcc: 45.3125% (29/64)\n",
            "1 3 Epoch: 7 | ANN: trainLoss: 1.2922 | trainAcc: 40.6250% (52/128)\n",
            "2 3 Epoch: 7 | ANN: trainLoss: 1.2685 | trainAcc: 40.6977% (70/172)\n",
            "0 2 Epoch: 7 | ANN: testLoss: 1.4430 | testAcc: 29.6875% (19/64)\n",
            "1 2 Epoch: 7 | ANN: testLoss: 1.5152 | testAcc: 27.5862% (32/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 8 | ANN: trainLoss: 1.2885 | trainAcc: 35.9375% (23/64)\n",
            "1 3 Epoch: 8 | ANN: trainLoss: 1.2375 | trainAcc: 40.6250% (52/128)\n",
            "2 3 Epoch: 8 | ANN: trainLoss: 1.1969 | trainAcc: 40.1163% (69/172)\n",
            "0 2 Epoch: 8 | ANN: testLoss: 1.3946 | testAcc: 34.3750% (22/64)\n",
            "1 2 Epoch: 8 | ANN: testLoss: 1.3917 | testAcc: 35.3448% (41/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 9 | ANN: trainLoss: 1.1659 | trainAcc: 50.0000% (32/64)\n",
            "1 3 Epoch: 9 | ANN: trainLoss: 1.1958 | trainAcc: 39.0625% (50/128)\n",
            "2 3 Epoch: 9 | ANN: trainLoss: 1.1831 | trainAcc: 40.1163% (69/172)\n",
            "0 2 Epoch: 9 | ANN: testLoss: 1.3575 | testAcc: 39.0625% (25/64)\n",
            "1 2 Epoch: 9 | ANN: testLoss: 1.3132 | testAcc: 38.7931% (45/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 10 | ANN: trainLoss: 1.1111 | trainAcc: 50.0000% (32/64)\n",
            "1 3 Epoch: 10 | ANN: trainLoss: 1.1424 | trainAcc: 46.8750% (60/128)\n",
            "2 3 Epoch: 10 | ANN: trainLoss: 1.1770 | trainAcc: 45.9302% (79/172)\n",
            "0 2 Epoch: 10 | ANN: testLoss: 1.2722 | testAcc: 39.0625% (25/64)\n",
            "1 2 Epoch: 10 | ANN: testLoss: 1.3026 | testAcc: 37.9310% (44/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 11 | ANN: trainLoss: 1.1938 | trainAcc: 50.0000% (32/64)\n",
            "1 3 Epoch: 11 | ANN: trainLoss: 1.1654 | trainAcc: 51.5625% (66/128)\n",
            "2 3 Epoch: 11 | ANN: trainLoss: 1.1871 | trainAcc: 46.5116% (80/172)\n",
            "0 2 Epoch: 11 | ANN: testLoss: 1.2768 | testAcc: 39.0625% (25/64)\n",
            "1 2 Epoch: 11 | ANN: testLoss: 1.3060 | testAcc: 36.2069% (42/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 12 | ANN: trainLoss: 1.0866 | trainAcc: 51.5625% (33/64)\n",
            "1 3 Epoch: 12 | ANN: trainLoss: 1.0810 | trainAcc: 55.4688% (71/128)\n",
            "2 3 Epoch: 12 | ANN: trainLoss: 1.1080 | trainAcc: 52.9070% (91/172)\n",
            "0 2 Epoch: 12 | ANN: testLoss: 1.4015 | testAcc: 32.8125% (21/64)\n",
            "1 2 Epoch: 12 | ANN: testLoss: 1.2732 | testAcc: 37.0690% (43/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 13 | ANN: trainLoss: 1.1349 | trainAcc: 48.4375% (31/64)\n",
            "1 3 Epoch: 13 | ANN: trainLoss: 1.1018 | trainAcc: 47.6562% (61/128)\n",
            "2 3 Epoch: 13 | ANN: trainLoss: 1.1253 | trainAcc: 45.9302% (79/172)\n",
            "0 2 Epoch: 13 | ANN: testLoss: 1.2111 | testAcc: 45.3125% (29/64)\n",
            "1 2 Epoch: 13 | ANN: testLoss: 1.2910 | testAcc: 37.9310% (44/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 14 | ANN: trainLoss: 1.2179 | trainAcc: 42.1875% (27/64)\n",
            "1 3 Epoch: 14 | ANN: trainLoss: 1.1480 | trainAcc: 44.5312% (57/128)\n",
            "2 3 Epoch: 14 | ANN: trainLoss: 1.1487 | trainAcc: 43.6047% (75/172)\n",
            "0 2 Epoch: 14 | ANN: testLoss: 1.3322 | testAcc: 35.9375% (23/64)\n",
            "1 2 Epoch: 14 | ANN: testLoss: 1.2509 | testAcc: 41.3793% (48/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 15 | ANN: trainLoss: 1.0167 | trainAcc: 48.4375% (31/64)\n",
            "1 3 Epoch: 15 | ANN: trainLoss: 1.0675 | trainAcc: 47.6562% (61/128)\n",
            "2 3 Epoch: 15 | ANN: trainLoss: 1.0685 | trainAcc: 48.8372% (84/172)\n",
            "0 2 Epoch: 15 | ANN: testLoss: 1.1935 | testAcc: 50.0000% (32/64)\n",
            "1 2 Epoch: 15 | ANN: testLoss: 1.2506 | testAcc: 42.2414% (49/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 16 | ANN: trainLoss: 1.0765 | trainAcc: 50.0000% (32/64)\n",
            "1 3 Epoch: 16 | ANN: trainLoss: 1.0621 | trainAcc: 53.1250% (68/128)\n",
            "2 3 Epoch: 16 | ANN: trainLoss: 1.0943 | trainAcc: 50.0000% (86/172)\n",
            "0 2 Epoch: 16 | ANN: testLoss: 1.2397 | testAcc: 42.1875% (27/64)\n",
            "1 2 Epoch: 16 | ANN: testLoss: 1.2109 | testAcc: 41.3793% (48/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 17 | ANN: trainLoss: 1.1443 | trainAcc: 48.4375% (31/64)\n",
            "1 3 Epoch: 17 | ANN: trainLoss: 1.0840 | trainAcc: 45.3125% (58/128)\n",
            "2 3 Epoch: 17 | ANN: trainLoss: 1.0928 | trainAcc: 44.1860% (76/172)\n",
            "0 2 Epoch: 17 | ANN: testLoss: 1.2524 | testAcc: 35.9375% (23/64)\n",
            "1 2 Epoch: 17 | ANN: testLoss: 1.2119 | testAcc: 38.7931% (45/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 18 | ANN: trainLoss: 0.9645 | trainAcc: 62.5000% (40/64)\n",
            "1 3 Epoch: 18 | ANN: trainLoss: 0.9687 | trainAcc: 62.5000% (80/128)\n",
            "2 3 Epoch: 18 | ANN: trainLoss: 1.0372 | trainAcc: 58.7209% (101/172)\n",
            "0 2 Epoch: 18 | ANN: testLoss: 1.2593 | testAcc: 39.0625% (25/64)\n",
            "1 2 Epoch: 18 | ANN: testLoss: 1.1986 | testAcc: 40.5172% (47/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 19 | ANN: trainLoss: 1.0144 | trainAcc: 48.4375% (31/64)\n",
            "1 3 Epoch: 19 | ANN: trainLoss: 0.9934 | trainAcc: 53.9062% (69/128)\n",
            "2 3 Epoch: 19 | ANN: trainLoss: 1.0148 | trainAcc: 54.0698% (93/172)\n",
            "0 2 Epoch: 19 | ANN: testLoss: 1.1594 | testAcc: 40.6250% (26/64)\n",
            "1 2 Epoch: 19 | ANN: testLoss: 1.1845 | testAcc: 42.2414% (49/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 20 | ANN: trainLoss: 0.9929 | trainAcc: 60.9375% (39/64)\n",
            "1 3 Epoch: 20 | ANN: trainLoss: 1.0327 | trainAcc: 57.8125% (74/128)\n",
            "2 3 Epoch: 20 | ANN: trainLoss: 1.0144 | trainAcc: 56.3953% (97/172)\n",
            "0 2 Epoch: 20 | ANN: testLoss: 1.1715 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 20 | ANN: testLoss: 1.1806 | testAcc: 42.2414% (49/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 21 | ANN: trainLoss: 0.9947 | trainAcc: 59.3750% (38/64)\n",
            "1 3 Epoch: 21 | ANN: trainLoss: 0.9990 | trainAcc: 56.2500% (72/128)\n",
            "2 3 Epoch: 21 | ANN: trainLoss: 1.0625 | trainAcc: 52.9070% (91/172)\n",
            "0 2 Epoch: 21 | ANN: testLoss: 1.1926 | testAcc: 40.6250% (26/64)\n",
            "1 2 Epoch: 21 | ANN: testLoss: 1.1926 | testAcc: 38.7931% (45/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 22 | ANN: trainLoss: 1.0215 | trainAcc: 57.8125% (37/64)\n",
            "1 3 Epoch: 22 | ANN: trainLoss: 1.0127 | trainAcc: 56.2500% (72/128)\n",
            "2 3 Epoch: 22 | ANN: trainLoss: 1.0038 | trainAcc: 55.8140% (96/172)\n",
            "0 2 Epoch: 22 | ANN: testLoss: 1.1515 | testAcc: 39.0625% (25/64)\n",
            "1 2 Epoch: 22 | ANN: testLoss: 1.2049 | testAcc: 38.7931% (45/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 23 | ANN: trainLoss: 0.8851 | trainAcc: 65.6250% (42/64)\n",
            "1 3 Epoch: 23 | ANN: trainLoss: 0.9325 | trainAcc: 61.7188% (79/128)\n",
            "2 3 Epoch: 23 | ANN: trainLoss: 0.9215 | trainAcc: 62.2093% (107/172)\n",
            "0 2 Epoch: 23 | ANN: testLoss: 1.2268 | testAcc: 39.0625% (25/64)\n",
            "1 2 Epoch: 23 | ANN: testLoss: 1.1833 | testAcc: 43.1034% (50/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 24 | ANN: trainLoss: 0.9876 | trainAcc: 53.1250% (34/64)\n",
            "1 3 Epoch: 24 | ANN: trainLoss: 0.9761 | trainAcc: 59.3750% (76/128)\n",
            "2 3 Epoch: 24 | ANN: trainLoss: 0.9594 | trainAcc: 58.7209% (101/172)\n",
            "0 2 Epoch: 24 | ANN: testLoss: 1.1068 | testAcc: 46.8750% (30/64)\n",
            "1 2 Epoch: 24 | ANN: testLoss: 1.1934 | testAcc: 43.9655% (51/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 25 | ANN: trainLoss: 0.8945 | trainAcc: 59.3750% (38/64)\n",
            "1 3 Epoch: 25 | ANN: trainLoss: 0.9280 | trainAcc: 60.1562% (77/128)\n",
            "2 3 Epoch: 25 | ANN: trainLoss: 0.9743 | trainAcc: 58.7209% (101/172)\n",
            "0 2 Epoch: 25 | ANN: testLoss: 1.1150 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 25 | ANN: testLoss: 1.1580 | testAcc: 48.2759% (56/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 26 | ANN: trainLoss: 1.0277 | trainAcc: 51.5625% (33/64)\n",
            "1 3 Epoch: 26 | ANN: trainLoss: 1.0000 | trainAcc: 53.9062% (69/128)\n",
            "2 3 Epoch: 26 | ANN: trainLoss: 1.0190 | trainAcc: 53.4884% (92/172)\n",
            "0 2 Epoch: 26 | ANN: testLoss: 1.0519 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 26 | ANN: testLoss: 1.1463 | testAcc: 50.8621% (59/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 27 | ANN: trainLoss: 0.9729 | trainAcc: 56.2500% (36/64)\n",
            "1 3 Epoch: 27 | ANN: trainLoss: 0.9790 | trainAcc: 56.2500% (72/128)\n",
            "2 3 Epoch: 27 | ANN: trainLoss: 0.9538 | trainAcc: 56.9767% (98/172)\n",
            "0 2 Epoch: 27 | ANN: testLoss: 1.1660 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 27 | ANN: testLoss: 1.1415 | testAcc: 49.1379% (57/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 28 | ANN: trainLoss: 0.9199 | trainAcc: 64.0625% (41/64)\n",
            "1 3 Epoch: 28 | ANN: trainLoss: 0.9393 | trainAcc: 57.8125% (74/128)\n",
            "2 3 Epoch: 28 | ANN: trainLoss: 0.9018 | trainAcc: 62.2093% (107/172)\n",
            "0 2 Epoch: 28 | ANN: testLoss: 1.0875 | testAcc: 50.0000% (32/64)\n",
            "1 2 Epoch: 28 | ANN: testLoss: 1.1858 | testAcc: 43.9655% (51/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 29 | ANN: trainLoss: 1.0261 | trainAcc: 48.4375% (31/64)\n",
            "1 3 Epoch: 29 | ANN: trainLoss: 0.9492 | trainAcc: 53.1250% (68/128)\n",
            "2 3 Epoch: 29 | ANN: trainLoss: 0.9022 | trainAcc: 57.5581% (99/172)\n",
            "0 2 Epoch: 29 | ANN: testLoss: 1.1466 | testAcc: 43.7500% (28/64)\n",
            "1 2 Epoch: 29 | ANN: testLoss: 1.2178 | testAcc: 43.9655% (51/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 30 | ANN: trainLoss: 0.9093 | trainAcc: 60.9375% (39/64)\n",
            "1 3 Epoch: 30 | ANN: trainLoss: 0.8912 | trainAcc: 64.8438% (83/128)\n",
            "2 3 Epoch: 30 | ANN: trainLoss: 0.8696 | trainAcc: 63.9535% (110/172)\n",
            "0 2 Epoch: 30 | ANN: testLoss: 1.1264 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 30 | ANN: testLoss: 1.2124 | testAcc: 46.5517% (54/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 31 | ANN: trainLoss: 0.8873 | trainAcc: 53.1250% (34/64)\n",
            "1 3 Epoch: 31 | ANN: trainLoss: 0.8774 | trainAcc: 58.5938% (75/128)\n",
            "2 3 Epoch: 31 | ANN: trainLoss: 0.8970 | trainAcc: 58.1395% (100/172)\n",
            "0 2 Epoch: 31 | ANN: testLoss: 1.2030 | testAcc: 42.1875% (27/64)\n",
            "1 2 Epoch: 31 | ANN: testLoss: 1.1798 | testAcc: 46.5517% (54/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 32 | ANN: trainLoss: 0.8197 | trainAcc: 70.3125% (45/64)\n",
            "1 3 Epoch: 32 | ANN: trainLoss: 0.8457 | trainAcc: 67.1875% (86/128)\n",
            "2 3 Epoch: 32 | ANN: trainLoss: 0.8817 | trainAcc: 65.6977% (113/172)\n",
            "0 2 Epoch: 32 | ANN: testLoss: 1.1912 | testAcc: 42.1875% (27/64)\n",
            "1 2 Epoch: 32 | ANN: testLoss: 1.1708 | testAcc: 42.2414% (49/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 33 | ANN: trainLoss: 0.7893 | trainAcc: 71.8750% (46/64)\n",
            "1 3 Epoch: 33 | ANN: trainLoss: 0.8113 | trainAcc: 66.4062% (85/128)\n",
            "2 3 Epoch: 33 | ANN: trainLoss: 0.8580 | trainAcc: 63.3721% (109/172)\n",
            "0 2 Epoch: 33 | ANN: testLoss: 1.2102 | testAcc: 42.1875% (27/64)\n",
            "1 2 Epoch: 33 | ANN: testLoss: 1.1863 | testAcc: 46.5517% (54/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 34 | ANN: trainLoss: 0.8182 | trainAcc: 68.7500% (44/64)\n",
            "1 3 Epoch: 34 | ANN: trainLoss: 0.8139 | trainAcc: 67.9688% (87/128)\n",
            "2 3 Epoch: 34 | ANN: trainLoss: 0.8094 | trainAcc: 67.4419% (116/172)\n",
            "0 2 Epoch: 34 | ANN: testLoss: 1.3085 | testAcc: 45.3125% (29/64)\n",
            "1 2 Epoch: 34 | ANN: testLoss: 1.1709 | testAcc: 47.4138% (55/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 35 | ANN: trainLoss: 0.7553 | trainAcc: 73.4375% (47/64)\n",
            "1 3 Epoch: 35 | ANN: trainLoss: 0.7930 | trainAcc: 68.7500% (88/128)\n",
            "2 3 Epoch: 35 | ANN: trainLoss: 0.8447 | trainAcc: 68.6047% (118/172)\n",
            "0 2 Epoch: 35 | ANN: testLoss: 1.3110 | testAcc: 39.0625% (25/64)\n",
            "1 2 Epoch: 35 | ANN: testLoss: 1.1931 | testAcc: 43.1034% (50/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 36 | ANN: trainLoss: 0.7897 | trainAcc: 67.1875% (43/64)\n",
            "1 3 Epoch: 36 | ANN: trainLoss: 0.8261 | trainAcc: 68.7500% (88/128)\n",
            "2 3 Epoch: 36 | ANN: trainLoss: 0.8205 | trainAcc: 68.6047% (118/172)\n",
            "0 2 Epoch: 36 | ANN: testLoss: 1.2862 | testAcc: 40.6250% (26/64)\n",
            "1 2 Epoch: 36 | ANN: testLoss: 1.2520 | testAcc: 41.3793% (48/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 37 | ANN: trainLoss: 0.8454 | trainAcc: 62.5000% (40/64)\n",
            "1 3 Epoch: 37 | ANN: trainLoss: 0.8726 | trainAcc: 62.5000% (80/128)\n",
            "2 3 Epoch: 37 | ANN: trainLoss: 0.8292 | trainAcc: 64.5349% (111/172)\n",
            "0 2 Epoch: 37 | ANN: testLoss: 1.2507 | testAcc: 39.0625% (25/64)\n",
            "1 2 Epoch: 37 | ANN: testLoss: 1.2334 | testAcc: 44.8276% (52/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 38 | ANN: trainLoss: 0.7988 | trainAcc: 67.1875% (43/64)\n",
            "1 3 Epoch: 38 | ANN: trainLoss: 0.7743 | trainAcc: 67.1875% (86/128)\n",
            "2 3 Epoch: 38 | ANN: trainLoss: 0.8238 | trainAcc: 67.4419% (116/172)\n",
            "0 2 Epoch: 38 | ANN: testLoss: 1.1066 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 38 | ANN: testLoss: 1.1888 | testAcc: 45.6897% (53/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 39 | ANN: trainLoss: 0.7295 | trainAcc: 73.4375% (47/64)\n",
            "1 3 Epoch: 39 | ANN: trainLoss: 0.8024 | trainAcc: 70.3125% (90/128)\n",
            "2 3 Epoch: 39 | ANN: trainLoss: 0.7539 | trainAcc: 70.3488% (121/172)\n",
            "0 2 Epoch: 39 | ANN: testLoss: 1.1374 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 39 | ANN: testLoss: 1.1636 | testAcc: 46.5517% (54/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 40 | ANN: trainLoss: 0.7784 | trainAcc: 70.3125% (45/64)\n",
            "1 3 Epoch: 40 | ANN: trainLoss: 0.7676 | trainAcc: 70.3125% (90/128)\n",
            "2 3 Epoch: 40 | ANN: trainLoss: 0.7885 | trainAcc: 66.2791% (114/172)\n",
            "0 2 Epoch: 40 | ANN: testLoss: 1.1630 | testAcc: 46.8750% (30/64)\n",
            "1 2 Epoch: 40 | ANN: testLoss: 1.1774 | testAcc: 45.6897% (53/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 41 | ANN: trainLoss: 0.8222 | trainAcc: 68.7500% (44/64)\n",
            "1 3 Epoch: 41 | ANN: trainLoss: 0.7637 | trainAcc: 69.5312% (89/128)\n",
            "2 3 Epoch: 41 | ANN: trainLoss: 0.7513 | trainAcc: 68.0233% (117/172)\n",
            "0 2 Epoch: 41 | ANN: testLoss: 1.1235 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 41 | ANN: testLoss: 1.1505 | testAcc: 46.5517% (54/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 42 | ANN: trainLoss: 0.7754 | trainAcc: 73.4375% (47/64)\n",
            "1 3 Epoch: 42 | ANN: trainLoss: 0.7381 | trainAcc: 73.4375% (94/128)\n",
            "2 3 Epoch: 42 | ANN: trainLoss: 0.7732 | trainAcc: 71.5116% (123/172)\n",
            "0 2 Epoch: 42 | ANN: testLoss: 1.1381 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 42 | ANN: testLoss: 1.1287 | testAcc: 48.2759% (56/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 43 | ANN: trainLoss: 0.7227 | trainAcc: 73.4375% (47/64)\n",
            "1 3 Epoch: 43 | ANN: trainLoss: 0.7225 | trainAcc: 74.2188% (95/128)\n",
            "2 3 Epoch: 43 | ANN: trainLoss: 0.6916 | trainAcc: 74.4186% (128/172)\n",
            "0 2 Epoch: 43 | ANN: testLoss: 1.0649 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 43 | ANN: testLoss: 1.1407 | testAcc: 49.1379% (57/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 44 | ANN: trainLoss: 0.6844 | trainAcc: 73.4375% (47/64)\n",
            "1 3 Epoch: 44 | ANN: trainLoss: 0.6680 | trainAcc: 75.7812% (97/128)\n",
            "2 3 Epoch: 44 | ANN: trainLoss: 0.6710 | trainAcc: 74.4186% (128/172)\n",
            "0 2 Epoch: 44 | ANN: testLoss: 1.0952 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 44 | ANN: testLoss: 1.1358 | testAcc: 48.2759% (56/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 45 | ANN: trainLoss: 0.7202 | trainAcc: 67.1875% (43/64)\n",
            "1 3 Epoch: 45 | ANN: trainLoss: 0.7174 | trainAcc: 67.1875% (86/128)\n",
            "2 3 Epoch: 45 | ANN: trainLoss: 0.6914 | trainAcc: 68.0233% (117/172)\n",
            "0 2 Epoch: 45 | ANN: testLoss: 1.1219 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 45 | ANN: testLoss: 1.1305 | testAcc: 49.1379% (57/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 46 | ANN: trainLoss: 0.7979 | trainAcc: 60.9375% (39/64)\n",
            "1 3 Epoch: 46 | ANN: trainLoss: 0.6995 | trainAcc: 71.8750% (92/128)\n",
            "2 3 Epoch: 46 | ANN: trainLoss: 0.6336 | trainAcc: 73.2558% (126/172)\n",
            "0 2 Epoch: 46 | ANN: testLoss: 1.2328 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 46 | ANN: testLoss: 1.0912 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 47 | ANN: trainLoss: 0.6697 | trainAcc: 76.5625% (49/64)\n",
            "1 3 Epoch: 47 | ANN: trainLoss: 0.6093 | trainAcc: 78.1250% (100/128)\n",
            "2 3 Epoch: 47 | ANN: trainLoss: 0.6138 | trainAcc: 78.4884% (135/172)\n",
            "0 2 Epoch: 47 | ANN: testLoss: 0.9831 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 47 | ANN: testLoss: 1.0553 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 48 | ANN: trainLoss: 0.5856 | trainAcc: 76.5625% (49/64)\n",
            "1 3 Epoch: 48 | ANN: trainLoss: 0.6505 | trainAcc: 73.4375% (94/128)\n",
            "2 3 Epoch: 48 | ANN: trainLoss: 0.6478 | trainAcc: 72.0930% (124/172)\n",
            "0 2 Epoch: 48 | ANN: testLoss: 1.0881 | testAcc: 45.3125% (29/64)\n",
            "1 2 Epoch: 48 | ANN: testLoss: 1.0708 | testAcc: 49.1379% (57/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 49 | ANN: trainLoss: 0.5818 | trainAcc: 79.6875% (51/64)\n",
            "1 3 Epoch: 49 | ANN: trainLoss: 0.6495 | trainAcc: 74.2188% (95/128)\n",
            "2 3 Epoch: 49 | ANN: trainLoss: 0.6446 | trainAcc: 74.4186% (128/172)\n",
            "0 2 Epoch: 49 | ANN: testLoss: 1.1567 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 49 | ANN: testLoss: 1.0474 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 50 | ANN: trainLoss: 0.5561 | trainAcc: 81.2500% (52/64)\n",
            "1 3 Epoch: 50 | ANN: trainLoss: 0.6459 | trainAcc: 73.4375% (94/128)\n",
            "2 3 Epoch: 50 | ANN: trainLoss: 0.6158 | trainAcc: 75.0000% (129/172)\n",
            "0 2 Epoch: 50 | ANN: testLoss: 0.9980 | testAcc: 57.8125% (37/64)\n",
            "1 2 Epoch: 50 | ANN: testLoss: 1.0384 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 51 | ANN: trainLoss: 0.5353 | trainAcc: 82.8125% (53/64)\n",
            "1 3 Epoch: 51 | ANN: trainLoss: 0.6195 | trainAcc: 76.5625% (98/128)\n",
            "2 3 Epoch: 51 | ANN: trainLoss: 0.6411 | trainAcc: 77.9070% (134/172)\n",
            "0 2 Epoch: 51 | ANN: testLoss: 1.0276 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 51 | ANN: testLoss: 1.0168 | testAcc: 57.7586% (67/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 52 | ANN: trainLoss: 0.6175 | trainAcc: 79.6875% (51/64)\n",
            "1 3 Epoch: 52 | ANN: trainLoss: 0.5761 | trainAcc: 82.0312% (105/128)\n",
            "2 3 Epoch: 52 | ANN: trainLoss: 0.6624 | trainAcc: 77.9070% (134/172)\n",
            "0 2 Epoch: 52 | ANN: testLoss: 1.1358 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 52 | ANN: testLoss: 1.0311 | testAcc: 51.7241% (60/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 53 | ANN: trainLoss: 0.6236 | trainAcc: 81.2500% (52/64)\n",
            "1 3 Epoch: 53 | ANN: trainLoss: 0.6156 | trainAcc: 78.9062% (101/128)\n",
            "2 3 Epoch: 53 | ANN: trainLoss: 0.6595 | trainAcc: 76.7442% (132/172)\n",
            "0 2 Epoch: 53 | ANN: testLoss: 0.8834 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 53 | ANN: testLoss: 1.0237 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 54 | ANN: trainLoss: 0.5209 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 54 | ANN: trainLoss: 0.6760 | trainAcc: 75.7812% (97/128)\n",
            "2 3 Epoch: 54 | ANN: trainLoss: 0.6611 | trainAcc: 74.4186% (128/172)\n",
            "0 2 Epoch: 54 | ANN: testLoss: 0.9000 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 54 | ANN: testLoss: 0.9957 | testAcc: 52.5862% (61/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 55 | ANN: trainLoss: 0.4884 | trainAcc: 79.6875% (51/64)\n",
            "1 3 Epoch: 55 | ANN: trainLoss: 0.5522 | trainAcc: 75.0000% (96/128)\n",
            "2 3 Epoch: 55 | ANN: trainLoss: 0.5064 | trainAcc: 77.9070% (134/172)\n",
            "0 2 Epoch: 55 | ANN: testLoss: 1.0193 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 55 | ANN: testLoss: 1.0248 | testAcc: 50.8621% (59/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 56 | ANN: trainLoss: 0.6202 | trainAcc: 78.1250% (50/64)\n",
            "1 3 Epoch: 56 | ANN: trainLoss: 0.5610 | trainAcc: 81.2500% (104/128)\n",
            "2 3 Epoch: 56 | ANN: trainLoss: 0.5537 | trainAcc: 80.2326% (138/172)\n",
            "0 2 Epoch: 56 | ANN: testLoss: 1.0451 | testAcc: 46.8750% (30/64)\n",
            "1 2 Epoch: 56 | ANN: testLoss: 1.0086 | testAcc: 49.1379% (57/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 57 | ANN: trainLoss: 0.5613 | trainAcc: 81.2500% (52/64)\n",
            "1 3 Epoch: 57 | ANN: trainLoss: 0.5185 | trainAcc: 84.3750% (108/128)\n",
            "2 3 Epoch: 57 | ANN: trainLoss: 0.5259 | trainAcc: 81.9767% (141/172)\n",
            "0 2 Epoch: 57 | ANN: testLoss: 1.0415 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 57 | ANN: testLoss: 1.0247 | testAcc: 50.8621% (59/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 58 | ANN: trainLoss: 0.4547 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 58 | ANN: trainLoss: 0.5301 | trainAcc: 80.4688% (103/128)\n",
            "2 3 Epoch: 58 | ANN: trainLoss: 0.5098 | trainAcc: 80.2326% (138/172)\n",
            "0 2 Epoch: 58 | ANN: testLoss: 1.0472 | testAcc: 57.8125% (37/64)\n",
            "1 2 Epoch: 58 | ANN: testLoss: 1.0626 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 59 | ANN: trainLoss: 0.5722 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 59 | ANN: trainLoss: 0.5718 | trainAcc: 81.2500% (104/128)\n",
            "2 3 Epoch: 59 | ANN: trainLoss: 0.5670 | trainAcc: 82.5581% (142/172)\n",
            "0 2 Epoch: 59 | ANN: testLoss: 1.0297 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 59 | ANN: testLoss: 1.0614 | testAcc: 51.7241% (60/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 60 | ANN: trainLoss: 0.4324 | trainAcc: 87.5000% (56/64)\n",
            "1 3 Epoch: 60 | ANN: trainLoss: 0.4430 | trainAcc: 86.7188% (111/128)\n",
            "2 3 Epoch: 60 | ANN: trainLoss: 0.4469 | trainAcc: 87.2093% (150/172)\n",
            "0 2 Epoch: 60 | ANN: testLoss: 1.1482 | testAcc: 42.1875% (27/64)\n",
            "1 2 Epoch: 60 | ANN: testLoss: 1.1067 | testAcc: 43.9655% (51/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 61 | ANN: trainLoss: 0.5607 | trainAcc: 79.6875% (51/64)\n",
            "1 3 Epoch: 61 | ANN: trainLoss: 0.6066 | trainAcc: 75.0000% (96/128)\n",
            "2 3 Epoch: 61 | ANN: trainLoss: 0.5733 | trainAcc: 76.1628% (131/172)\n",
            "0 2 Epoch: 61 | ANN: testLoss: 1.0981 | testAcc: 46.8750% (30/64)\n",
            "1 2 Epoch: 61 | ANN: testLoss: 1.0886 | testAcc: 45.6897% (53/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 62 | ANN: trainLoss: 0.5532 | trainAcc: 78.1250% (50/64)\n",
            "1 3 Epoch: 62 | ANN: trainLoss: 0.4910 | trainAcc: 79.6875% (102/128)\n",
            "2 3 Epoch: 62 | ANN: trainLoss: 0.5116 | trainAcc: 80.2326% (138/172)\n",
            "0 2 Epoch: 62 | ANN: testLoss: 0.9916 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 62 | ANN: testLoss: 1.0492 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 63 | ANN: trainLoss: 0.4956 | trainAcc: 82.8125% (53/64)\n",
            "1 3 Epoch: 63 | ANN: trainLoss: 0.5078 | trainAcc: 77.3438% (99/128)\n",
            "2 3 Epoch: 63 | ANN: trainLoss: 0.5110 | trainAcc: 76.7442% (132/172)\n",
            "0 2 Epoch: 63 | ANN: testLoss: 1.0645 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 63 | ANN: testLoss: 1.0395 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 64 | ANN: trainLoss: 0.4364 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 64 | ANN: trainLoss: 0.4728 | trainAcc: 81.2500% (104/128)\n",
            "2 3 Epoch: 64 | ANN: trainLoss: 0.5044 | trainAcc: 81.9767% (141/172)\n",
            "0 2 Epoch: 64 | ANN: testLoss: 1.0819 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 64 | ANN: testLoss: 1.0368 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 65 | ANN: trainLoss: 0.4742 | trainAcc: 89.0625% (57/64)\n",
            "1 3 Epoch: 65 | ANN: trainLoss: 0.5036 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 65 | ANN: trainLoss: 0.5173 | trainAcc: 83.1395% (143/172)\n",
            "0 2 Epoch: 65 | ANN: testLoss: 1.0077 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 65 | ANN: testLoss: 1.0532 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 66 | ANN: trainLoss: 0.4263 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 66 | ANN: trainLoss: 0.4172 | trainAcc: 82.0312% (105/128)\n",
            "2 3 Epoch: 66 | ANN: trainLoss: 0.4295 | trainAcc: 83.7209% (144/172)\n",
            "0 2 Epoch: 66 | ANN: testLoss: 1.1188 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 66 | ANN: testLoss: 1.0268 | testAcc: 52.5862% (61/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 67 | ANN: trainLoss: 0.5280 | trainAcc: 82.8125% (53/64)\n",
            "1 3 Epoch: 67 | ANN: trainLoss: 0.5111 | trainAcc: 79.6875% (102/128)\n",
            "2 3 Epoch: 67 | ANN: trainLoss: 0.5980 | trainAcc: 77.9070% (134/172)\n",
            "0 2 Epoch: 67 | ANN: testLoss: 1.0386 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 67 | ANN: testLoss: 1.0069 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 68 | ANN: trainLoss: 0.3939 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 68 | ANN: trainLoss: 0.4218 | trainAcc: 83.5938% (107/128)\n",
            "2 3 Epoch: 68 | ANN: trainLoss: 0.4957 | trainAcc: 81.9767% (141/172)\n",
            "0 2 Epoch: 68 | ANN: testLoss: 0.9772 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 68 | ANN: testLoss: 0.9849 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 69 | ANN: trainLoss: 0.5361 | trainAcc: 75.0000% (48/64)\n",
            "1 3 Epoch: 69 | ANN: trainLoss: 0.5284 | trainAcc: 79.6875% (102/128)\n",
            "2 3 Epoch: 69 | ANN: trainLoss: 0.4828 | trainAcc: 81.9767% (141/172)\n",
            "0 2 Epoch: 69 | ANN: testLoss: 1.0158 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 69 | ANN: testLoss: 0.9477 | testAcc: 56.8966% (66/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 70 | ANN: trainLoss: 0.4120 | trainAcc: 85.9375% (55/64)\n",
            "1 3 Epoch: 70 | ANN: trainLoss: 0.4204 | trainAcc: 86.7188% (111/128)\n",
            "2 3 Epoch: 70 | ANN: trainLoss: 0.4072 | trainAcc: 87.7907% (151/172)\n",
            "0 2 Epoch: 70 | ANN: testLoss: 0.9405 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 70 | ANN: testLoss: 0.9476 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 71 | ANN: trainLoss: 0.5941 | trainAcc: 73.4375% (47/64)\n",
            "1 3 Epoch: 71 | ANN: trainLoss: 0.5335 | trainAcc: 78.9062% (101/128)\n",
            "2 3 Epoch: 71 | ANN: trainLoss: 0.4908 | trainAcc: 81.3953% (140/172)\n",
            "0 2 Epoch: 71 | ANN: testLoss: 1.0615 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 71 | ANN: testLoss: 0.9400 | testAcc: 56.8966% (66/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 72 | ANN: trainLoss: 0.4068 | trainAcc: 85.9375% (55/64)\n",
            "1 3 Epoch: 72 | ANN: trainLoss: 0.4505 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 72 | ANN: trainLoss: 0.4455 | trainAcc: 85.4651% (147/172)\n",
            "0 2 Epoch: 72 | ANN: testLoss: 1.0179 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 72 | ANN: testLoss: 0.9462 | testAcc: 57.7586% (67/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 73 | ANN: trainLoss: 0.3923 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 73 | ANN: trainLoss: 0.4241 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 73 | ANN: trainLoss: 0.4482 | trainAcc: 85.4651% (147/172)\n",
            "0 2 Epoch: 73 | ANN: testLoss: 0.9165 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 73 | ANN: testLoss: 0.9576 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 74 | ANN: trainLoss: 0.3996 | trainAcc: 89.0625% (57/64)\n",
            "1 3 Epoch: 74 | ANN: trainLoss: 0.4569 | trainAcc: 85.1562% (109/128)\n",
            "2 3 Epoch: 74 | ANN: trainLoss: 0.4410 | trainAcc: 86.6279% (149/172)\n",
            "0 2 Epoch: 74 | ANN: testLoss: 0.9181 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 74 | ANN: testLoss: 0.9503 | testAcc: 57.7586% (67/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 75 | ANN: trainLoss: 0.5150 | trainAcc: 82.8125% (53/64)\n",
            "1 3 Epoch: 75 | ANN: trainLoss: 0.4186 | trainAcc: 88.2812% (113/128)\n",
            "2 3 Epoch: 75 | ANN: trainLoss: 0.4365 | trainAcc: 87.7907% (151/172)\n",
            "0 2 Epoch: 75 | ANN: testLoss: 0.8833 | testAcc: 67.1875% (43/64)\n",
            "1 2 Epoch: 75 | ANN: testLoss: 0.9536 | testAcc: 58.6207% (68/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 76 | ANN: trainLoss: 0.4019 | trainAcc: 92.1875% (59/64)\n",
            "1 3 Epoch: 76 | ANN: trainLoss: 0.3786 | trainAcc: 91.4062% (117/128)\n",
            "2 3 Epoch: 76 | ANN: trainLoss: 0.4074 | trainAcc: 90.1163% (155/172)\n",
            "0 2 Epoch: 76 | ANN: testLoss: 1.0091 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 76 | ANN: testLoss: 0.9348 | testAcc: 57.7586% (67/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 77 | ANN: trainLoss: 0.4026 | trainAcc: 82.8125% (53/64)\n",
            "1 3 Epoch: 77 | ANN: trainLoss: 0.4143 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 77 | ANN: trainLoss: 0.4452 | trainAcc: 84.3023% (145/172)\n",
            "0 2 Epoch: 77 | ANN: testLoss: 1.0119 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 77 | ANN: testLoss: 0.9268 | testAcc: 56.8966% (66/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 78 | ANN: trainLoss: 0.3915 | trainAcc: 90.6250% (58/64)\n",
            "1 3 Epoch: 78 | ANN: trainLoss: 0.3871 | trainAcc: 89.0625% (114/128)\n",
            "2 3 Epoch: 78 | ANN: trainLoss: 0.3866 | trainAcc: 89.5349% (154/172)\n",
            "0 2 Epoch: 78 | ANN: testLoss: 0.8697 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 78 | ANN: testLoss: 0.9378 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 79 | ANN: trainLoss: 0.4903 | trainAcc: 82.8125% (53/64)\n",
            "1 3 Epoch: 79 | ANN: trainLoss: 0.4335 | trainAcc: 85.1562% (109/128)\n",
            "2 3 Epoch: 79 | ANN: trainLoss: 0.4322 | trainAcc: 86.6279% (149/172)\n",
            "0 2 Epoch: 79 | ANN: testLoss: 0.8743 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 79 | ANN: testLoss: 0.9320 | testAcc: 57.7586% (67/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 80 | ANN: trainLoss: 0.4460 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 80 | ANN: trainLoss: 0.4516 | trainAcc: 82.0312% (105/128)\n",
            "2 3 Epoch: 80 | ANN: trainLoss: 0.4780 | trainAcc: 80.8140% (139/172)\n",
            "0 2 Epoch: 80 | ANN: testLoss: 0.9466 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 80 | ANN: testLoss: 0.9260 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 81 | ANN: trainLoss: 0.3874 | trainAcc: 87.5000% (56/64)\n",
            "1 3 Epoch: 81 | ANN: trainLoss: 0.4054 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 81 | ANN: trainLoss: 0.3826 | trainAcc: 87.7907% (151/172)\n",
            "0 2 Epoch: 81 | ANN: testLoss: 0.8675 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 81 | ANN: testLoss: 0.9402 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 82 | ANN: trainLoss: 0.4246 | trainAcc: 85.9375% (55/64)\n",
            "1 3 Epoch: 82 | ANN: trainLoss: 0.3888 | trainAcc: 88.2812% (113/128)\n",
            "2 3 Epoch: 82 | ANN: trainLoss: 0.3549 | trainAcc: 90.1163% (155/172)\n",
            "0 2 Epoch: 82 | ANN: testLoss: 1.0045 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 82 | ANN: testLoss: 0.9334 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 83 | ANN: trainLoss: 0.3952 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 83 | ANN: trainLoss: 0.3951 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 83 | ANN: trainLoss: 0.4275 | trainAcc: 85.4651% (147/172)\n",
            "0 2 Epoch: 83 | ANN: testLoss: 1.0712 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 83 | ANN: testLoss: 0.9281 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 84 | ANN: trainLoss: 0.3353 | trainAcc: 92.1875% (59/64)\n",
            "1 3 Epoch: 84 | ANN: trainLoss: 0.3496 | trainAcc: 89.8438% (115/128)\n",
            "2 3 Epoch: 84 | ANN: trainLoss: 0.3724 | trainAcc: 89.5349% (154/172)\n",
            "0 2 Epoch: 84 | ANN: testLoss: 1.0494 | testAcc: 46.8750% (30/64)\n",
            "1 2 Epoch: 84 | ANN: testLoss: 0.9381 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 85 | ANN: trainLoss: 0.5060 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 85 | ANN: trainLoss: 0.3872 | trainAcc: 89.0625% (114/128)\n",
            "2 3 Epoch: 85 | ANN: trainLoss: 0.3797 | trainAcc: 90.1163% (155/172)\n",
            "0 2 Epoch: 85 | ANN: testLoss: 1.0337 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 85 | ANN: testLoss: 0.9501 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 86 | ANN: trainLoss: 0.4967 | trainAcc: 85.9375% (55/64)\n",
            "1 3 Epoch: 86 | ANN: trainLoss: 0.4490 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 86 | ANN: trainLoss: 0.4229 | trainAcc: 86.6279% (149/172)\n",
            "0 2 Epoch: 86 | ANN: testLoss: 1.0175 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 86 | ANN: testLoss: 0.9594 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 87 | ANN: trainLoss: 0.3900 | trainAcc: 87.5000% (56/64)\n",
            "1 3 Epoch: 87 | ANN: trainLoss: 0.3943 | trainAcc: 86.7188% (111/128)\n",
            "2 3 Epoch: 87 | ANN: trainLoss: 0.4069 | trainAcc: 85.4651% (147/172)\n",
            "0 2 Epoch: 87 | ANN: testLoss: 0.8725 | testAcc: 57.8125% (37/64)\n",
            "1 2 Epoch: 87 | ANN: testLoss: 0.9785 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 88 | ANN: trainLoss: 0.4108 | trainAcc: 87.5000% (56/64)\n",
            "1 3 Epoch: 88 | ANN: trainLoss: 0.4237 | trainAcc: 85.1562% (109/128)\n",
            "2 3 Epoch: 88 | ANN: trainLoss: 0.4351 | trainAcc: 84.3023% (145/172)\n",
            "0 2 Epoch: 88 | ANN: testLoss: 1.0268 | testAcc: 50.0000% (32/64)\n",
            "1 2 Epoch: 88 | ANN: testLoss: 0.9545 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 89 | ANN: trainLoss: 0.3824 | trainAcc: 90.6250% (58/64)\n",
            "1 3 Epoch: 89 | ANN: trainLoss: 0.4043 | trainAcc: 87.5000% (112/128)\n",
            "2 3 Epoch: 89 | ANN: trainLoss: 0.3815 | trainAcc: 88.9535% (153/172)\n",
            "0 2 Epoch: 89 | ANN: testLoss: 0.9226 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 89 | ANN: testLoss: 0.9640 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 90 | ANN: trainLoss: 0.3440 | trainAcc: 89.0625% (57/64)\n",
            "1 3 Epoch: 90 | ANN: trainLoss: 0.3931 | trainAcc: 86.7188% (111/128)\n",
            "2 3 Epoch: 90 | ANN: trainLoss: 0.3924 | trainAcc: 87.2093% (150/172)\n",
            "0 2 Epoch: 90 | ANN: testLoss: 0.9942 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 90 | ANN: testLoss: 0.9503 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 91 | ANN: trainLoss: 0.3462 | trainAcc: 90.6250% (58/64)\n",
            "1 3 Epoch: 91 | ANN: trainLoss: 0.3675 | trainAcc: 89.0625% (114/128)\n",
            "2 3 Epoch: 91 | ANN: trainLoss: 0.3659 | trainAcc: 88.9535% (153/172)\n",
            "0 2 Epoch: 91 | ANN: testLoss: 0.9453 | testAcc: 57.8125% (37/64)\n",
            "1 2 Epoch: 91 | ANN: testLoss: 0.9480 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 92 | ANN: trainLoss: 0.5092 | trainAcc: 81.2500% (52/64)\n",
            "1 3 Epoch: 92 | ANN: trainLoss: 0.4255 | trainAcc: 85.1562% (109/128)\n",
            "2 3 Epoch: 92 | ANN: trainLoss: 0.4065 | trainAcc: 85.4651% (147/172)\n",
            "0 2 Epoch: 92 | ANN: testLoss: 0.9406 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 92 | ANN: testLoss: 0.9456 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 93 | ANN: trainLoss: 0.3669 | trainAcc: 89.0625% (57/64)\n",
            "1 3 Epoch: 93 | ANN: trainLoss: 0.3990 | trainAcc: 89.0625% (114/128)\n",
            "2 3 Epoch: 93 | ANN: trainLoss: 0.4195 | trainAcc: 87.7907% (151/172)\n",
            "0 2 Epoch: 93 | ANN: testLoss: 0.8825 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 93 | ANN: testLoss: 0.9526 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 94 | ANN: trainLoss: 0.4223 | trainAcc: 87.5000% (56/64)\n",
            "1 3 Epoch: 94 | ANN: trainLoss: 0.3762 | trainAcc: 89.0625% (114/128)\n",
            "2 3 Epoch: 94 | ANN: trainLoss: 0.3493 | trainAcc: 89.5349% (154/172)\n",
            "0 2 Epoch: 94 | ANN: testLoss: 0.9831 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 94 | ANN: testLoss: 0.9383 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 95 | ANN: trainLoss: 0.4013 | trainAcc: 89.0625% (57/64)\n",
            "1 3 Epoch: 95 | ANN: trainLoss: 0.4028 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 95 | ANN: trainLoss: 0.4481 | trainAcc: 84.3023% (145/172)\n",
            "0 2 Epoch: 95 | ANN: testLoss: 0.9936 | testAcc: 50.0000% (32/64)\n",
            "1 2 Epoch: 95 | ANN: testLoss: 0.9362 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 96 | ANN: trainLoss: 0.3546 | trainAcc: 92.1875% (59/64)\n",
            "1 3 Epoch: 96 | ANN: trainLoss: 0.4142 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 96 | ANN: trainLoss: 0.4332 | trainAcc: 86.0465% (148/172)\n",
            "0 2 Epoch: 96 | ANN: testLoss: 1.0411 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 96 | ANN: testLoss: 0.9279 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 97 | ANN: trainLoss: 0.3216 | trainAcc: 96.8750% (62/64)\n",
            "1 3 Epoch: 97 | ANN: trainLoss: 0.3705 | trainAcc: 89.8438% (115/128)\n",
            "2 3 Epoch: 97 | ANN: trainLoss: 0.3864 | trainAcc: 88.3721% (152/172)\n",
            "0 2 Epoch: 97 | ANN: testLoss: 0.9514 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 97 | ANN: testLoss: 0.9384 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 98 | ANN: trainLoss: 0.3263 | trainAcc: 92.1875% (59/64)\n",
            "1 3 Epoch: 98 | ANN: trainLoss: 0.3483 | trainAcc: 90.6250% (116/128)\n",
            "2 3 Epoch: 98 | ANN: trainLoss: 0.3824 | trainAcc: 88.9535% (153/172)\n",
            "0 2 Epoch: 98 | ANN: testLoss: 0.9578 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 98 | ANN: testLoss: 0.9346 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 99 | ANN: trainLoss: 0.3218 | trainAcc: 90.6250% (58/64)\n",
            "1 3 Epoch: 99 | ANN: trainLoss: 0.3311 | trainAcc: 91.4062% (117/128)\n",
            "2 3 Epoch: 99 | ANN: trainLoss: 0.3247 | trainAcc: 91.8605% (158/172)\n",
            "0 2 Epoch: 99 | ANN: testLoss: 0.9087 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 99 | ANN: testLoss: 0.9399 | testAcc: 55.1724% (64/116)\n",
            "---------------------------------------------\n",
            "Converting using MaxNorm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 263.15it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 287.83it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ANN accuracy: Test: 55.1700%\n",
            "SNN accuracy: max_norm: 58.6207%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7o4VT4r0r-xf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}