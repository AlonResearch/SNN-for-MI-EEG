{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as da\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io as scio\n",
        "!pip install spikingjelly\n",
        "from spikingjelly.activation_based import ann2snn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "xy9K5bJ9aB7D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lAEV1BHaBp9",
        "outputId": "a6567916-7ce6-4437-d535-1ecce2c89cbd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Thu May  1 20:42:42 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8             12W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CYNshZR5ZhDy"
      },
      "outputs": [],
      "source": [
        "#Defining functions\n",
        "\n",
        "def data_loader(data, label, batch=64, shuffle=True, drop=False):\n",
        "    \"\"\"\n",
        "    Preprocess the data to fit model.\n",
        "    Feed data into data_loader.\n",
        "    input:\n",
        "        data (float): samples*length*ch (samples*ch*length).\n",
        "        label (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        batch (int): batch size\n",
        "        shuffle (bool): shuffle data before input into decoder\n",
        "        drop (bool): drop the last samples if True\n",
        "    output:\n",
        "        data loader\n",
        "    \"\"\"\n",
        "    label = torch.LongTensor(label.flatten()).to(device)\n",
        "    if data.shape[1] >= data.shape[2]:\n",
        "        data = torch.tensor(data.swapaxes(1, 2))\n",
        "    data = torch.unsqueeze(data, dim=1).type('torch.FloatTensor').to(device)\n",
        "    data = da.TensorDataset(data, label)\n",
        "    loader = da.DataLoader(dataset=data, batch_size=batch, shuffle=shuffle, drop_last=drop)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def val_snn(Dec, test_loader, T=None):\n",
        "    Dec.eval().to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if T is not None:\n",
        "        corrects = np.zeros(T)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            if T is None:\n",
        "                outputs = Dec(inputs)\n",
        "                correct += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            else:\n",
        "                for m in Dec.modules():\n",
        "                    if hasattr(m, 'reset'):\n",
        "                        m.reset()\n",
        "                for t in range(T):\n",
        "                    if t == 0:\n",
        "                        outputs = Dec(inputs)\n",
        "                    else:\n",
        "                        outputs += Dec(inputs)\n",
        "                    corrects[t] += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            total += targets.shape[0]\n",
        "    return correct / total if T is None else corrects / total\n",
        "\n",
        "\n",
        "def anntosnn(ann_model, train_x, train_y, test_x, test_y, batch=64, T=None):\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    print('---------------------------------------------')\n",
        "    print('Converting using MaxNorm')\n",
        "    model_converter = ann2snn.Converter(mode='max', dataloader=train_loader)\n",
        "    snn_model = model_converter(ann_model)\n",
        "    mode_max_accs = val_snn(snn_model, test_loader, T=T)\n",
        "\n",
        "    return mode_max_accs\n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def train_ann(ann_model, train_x, train_y, test_x, test_y, ep=500, batch=64):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        train_x, test_x (float): samples*length*ch (samples*ch*length).\n",
        "        train_y, test_y (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        ep (int): total train and test epoch\n",
        "        batch (int): batch size\n",
        "    output:\n",
        "        train acc, test acc, weight_file\n",
        "    \"\"\"\n",
        "    # Define training configuration\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(ann_model.parameters(), lr=0.01)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ep)\n",
        "\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "    for epoch in range(ep):\n",
        "        # Train ANN\n",
        "        ann_model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        print('\\n')\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = ann_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            print(batch_idx, len(train_loader), 'Epoch: %d | ANN: trainLoss: %.4f | trainAcc: %.4f%% (%d/%d)'\n",
        "                  % (epoch, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        train_acc.append(round(correct / total, 4))\n",
        "\n",
        "        # Test ANN\n",
        "        ann_model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "                outputs = ann_model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "                print(batch_idx, len(test_loader), 'Epoch: %d | ANN: testLoss: %.4f | testAcc: %.4f%% (%d/%d)'\n",
        "                      % (epoch, val_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        test_acc.append(round(correct / total, 4))\n",
        "\n",
        "    train_acc = np.asarray(train_acc[-1])\n",
        "    test_acc = np.asarray(test_acc[-1])\n",
        "    return train_acc, test_acc,ann_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2a\n",
        "\n",
        "class LENet(nn.Module):\n",
        "    \"\"\"\n",
        "        LENet Model\n",
        "    input:\n",
        "         data shape as: batch_size*1*channel*length (64*1*22*1000) BCI IV-2a\n",
        "         batch_size：64\n",
        "         channel：22\n",
        "         length：1000\n",
        "    output:\n",
        "        classes_num\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classes_num=4):\n",
        "        super(LENet, self).__init__()\n",
        "        self.drop_out = 0.5\n",
        "\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,64) #\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,32) #\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,16) #\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            # Temporal Convolution block fusion kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            # Spatial Convolution block kernel_size (channel,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(22, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            # Feature Fusion Convolution block kernel_size (1,16) and (1,1) #\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),  #\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.CCB = nn.Sequential(\n",
        "            # Classification Convolution block kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=classes_num,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "        x = self.CCB(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LENet_FCL(nn.Module):\n",
        "    def __init__(self, classes_num=4):\n",
        "        super(LENet_FCL, self).__init__()\n",
        "        self.drop_out = 0.5\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(22, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),  #\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FCL = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16 * 31, classes_num)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "        x = self.FCL(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EC8xjyPDaM6G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "An example for the pipeline pf ANN to SNN Conversion Framework.\n",
        "\"\"\"\n",
        "\n",
        "def framework_pipeline(train_x, train_y, test_x, test_y, epoch=200, batch=64, T=100):\n",
        "    \"\"\"\n",
        "        ANN to SNN Conversion framework\n",
        "    input:\n",
        "        train_x, test_x (float): Train and test data, shape as: samples*length*ch (samples*ch*length).\n",
        "        train_y, test_y (int): Train and test label, shape as: samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        epoch (int): Total train and test epoch\n",
        "        batch (int): Batch size\n",
        "        T (int): Time step for SNN\n",
        "    output:\n",
        "        None\n",
        "    \"\"\"\n",
        "    ann_model = LENet(classes_num=4).to(device)\n",
        "    ann_model.apply(initialize_weights)\n",
        "\n",
        "    train_acc, test_acc, model_trained = train_ann(ann_model, train_x, train_y, test_x, test_y, ep=epoch, batch=batch)\n",
        "    max_norm_acc = anntosnn(model_trained, train_x, train_y, test_x, test_y, batch=batch, T=T)\n",
        "\n",
        "    print('\\n')\n",
        "    print('ANN accuracy: Test: %.4f%%' % (test_acc * 100))\n",
        "    print('SNN accuracy: max_norm: %.4f%%' % (max_norm_acc[-1] * 100))\n",
        "\n",
        "\n",
        "# Getting real samples\n",
        "file = scio.loadmat('/content/A01T.mat')\n",
        "all_data = file['all_data']\n",
        "all_label = file['all_label']\n",
        "\n",
        "datasetX = torch.tensor(all_data, dtype=torch.float32)\n",
        "datasetY = torch.tensor(all_label, dtype=torch.int64)\n",
        "train_data, test_data, train_label, test_label = train_test_split(datasetX, datasetY, test_size=0.2, shuffle=True,\n",
        "                                                                  random_state=0)\n",
        "\n",
        "# ANN to SNN Conversion\n",
        "framework_pipeline(train_data, train_label, test_data, test_label, epoch=100, batch=64, T=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T0hmMue2aMqA",
        "outputId": "224a2a9f-6c95-40c7-bc81-762f27e49ff4",
        "collapsed": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "0 3 Epoch: 0 | ANN: trainLoss: 1.7517 | trainAcc: 21.8750% (14/64)\n",
            "1 3 Epoch: 0 | ANN: trainLoss: 1.6244 | trainAcc: 26.5625% (34/128)\n",
            "2 3 Epoch: 0 | ANN: trainLoss: 1.5995 | trainAcc: 27.3256% (47/172)\n",
            "0 2 Epoch: 0 | ANN: testLoss: 1.5091 | testAcc: 23.4375% (15/64)\n",
            "1 2 Epoch: 0 | ANN: testLoss: 1.5002 | testAcc: 26.7241% (31/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 1 | ANN: trainLoss: 1.3873 | trainAcc: 37.5000% (24/64)\n",
            "1 3 Epoch: 1 | ANN: trainLoss: 1.3699 | trainAcc: 35.9375% (46/128)\n",
            "2 3 Epoch: 1 | ANN: trainLoss: 1.3606 | trainAcc: 36.6279% (63/172)\n",
            "0 2 Epoch: 1 | ANN: testLoss: 1.8808 | testAcc: 26.5625% (17/64)\n",
            "1 2 Epoch: 1 | ANN: testLoss: 1.7959 | testAcc: 26.7241% (31/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 2 | ANN: trainLoss: 1.3135 | trainAcc: 40.6250% (26/64)\n",
            "1 3 Epoch: 2 | ANN: trainLoss: 1.3142 | trainAcc: 39.0625% (50/128)\n",
            "2 3 Epoch: 2 | ANN: trainLoss: 1.3828 | trainAcc: 32.5581% (56/172)\n",
            "0 2 Epoch: 2 | ANN: testLoss: 1.9841 | testAcc: 25.0000% (16/64)\n",
            "1 2 Epoch: 2 | ANN: testLoss: 1.9064 | testAcc: 25.8621% (30/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 3 | ANN: trainLoss: 1.3728 | trainAcc: 40.6250% (26/64)\n",
            "1 3 Epoch: 3 | ANN: trainLoss: 1.3543 | trainAcc: 35.9375% (46/128)\n",
            "2 3 Epoch: 3 | ANN: trainLoss: 1.3341 | trainAcc: 33.1395% (57/172)\n",
            "0 2 Epoch: 3 | ANN: testLoss: 1.6176 | testAcc: 28.1250% (18/64)\n",
            "1 2 Epoch: 3 | ANN: testLoss: 1.7302 | testAcc: 23.2759% (27/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 4 | ANN: trainLoss: 1.3087 | trainAcc: 40.6250% (26/64)\n",
            "1 3 Epoch: 4 | ANN: trainLoss: 1.2575 | trainAcc: 40.6250% (52/128)\n",
            "2 3 Epoch: 4 | ANN: trainLoss: 1.2650 | trainAcc: 40.6977% (70/172)\n",
            "0 2 Epoch: 4 | ANN: testLoss: 1.5790 | testAcc: 28.1250% (18/64)\n",
            "1 2 Epoch: 4 | ANN: testLoss: 1.7344 | testAcc: 26.7241% (31/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 5 | ANN: trainLoss: 1.4233 | trainAcc: 34.3750% (22/64)\n",
            "1 3 Epoch: 5 | ANN: trainLoss: 1.2994 | trainAcc: 39.8438% (51/128)\n",
            "2 3 Epoch: 5 | ANN: trainLoss: 1.2562 | trainAcc: 40.1163% (69/172)\n",
            "0 2 Epoch: 5 | ANN: testLoss: 1.9561 | testAcc: 29.6875% (19/64)\n",
            "1 2 Epoch: 5 | ANN: testLoss: 1.7063 | testAcc: 29.3103% (34/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 6 | ANN: trainLoss: 1.2869 | trainAcc: 31.2500% (20/64)\n",
            "1 3 Epoch: 6 | ANN: trainLoss: 1.1999 | trainAcc: 39.0625% (50/128)\n",
            "2 3 Epoch: 6 | ANN: trainLoss: 1.2101 | trainAcc: 38.9535% (67/172)\n",
            "0 2 Epoch: 6 | ANN: testLoss: 1.9009 | testAcc: 31.2500% (20/64)\n",
            "1 2 Epoch: 6 | ANN: testLoss: 1.7227 | testAcc: 30.1724% (35/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 7 | ANN: trainLoss: 1.2075 | trainAcc: 46.8750% (30/64)\n",
            "1 3 Epoch: 7 | ANN: trainLoss: 1.2210 | trainAcc: 41.4062% (53/128)\n",
            "2 3 Epoch: 7 | ANN: trainLoss: 1.2146 | trainAcc: 38.9535% (67/172)\n",
            "0 2 Epoch: 7 | ANN: testLoss: 1.8297 | testAcc: 31.2500% (20/64)\n",
            "1 2 Epoch: 7 | ANN: testLoss: 1.7228 | testAcc: 28.4483% (33/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 8 | ANN: trainLoss: 1.1634 | trainAcc: 48.4375% (31/64)\n",
            "1 3 Epoch: 8 | ANN: trainLoss: 1.1492 | trainAcc: 46.0938% (59/128)\n",
            "2 3 Epoch: 8 | ANN: trainLoss: 1.2193 | trainAcc: 40.6977% (70/172)\n",
            "0 2 Epoch: 8 | ANN: testLoss: 1.6636 | testAcc: 26.5625% (17/64)\n",
            "1 2 Epoch: 8 | ANN: testLoss: 1.6171 | testAcc: 29.3103% (34/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 9 | ANN: trainLoss: 1.1180 | trainAcc: 50.0000% (32/64)\n",
            "1 3 Epoch: 9 | ANN: trainLoss: 1.1473 | trainAcc: 44.5312% (57/128)\n",
            "2 3 Epoch: 9 | ANN: trainLoss: 1.1811 | trainAcc: 43.6047% (75/172)\n",
            "0 2 Epoch: 9 | ANN: testLoss: 1.5102 | testAcc: 31.2500% (20/64)\n",
            "1 2 Epoch: 9 | ANN: testLoss: 1.4307 | testAcc: 33.6207% (39/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 10 | ANN: trainLoss: 1.1718 | trainAcc: 39.0625% (25/64)\n",
            "1 3 Epoch: 10 | ANN: trainLoss: 1.1444 | trainAcc: 42.1875% (54/128)\n",
            "2 3 Epoch: 10 | ANN: trainLoss: 1.1271 | trainAcc: 43.6047% (75/172)\n",
            "0 2 Epoch: 10 | ANN: testLoss: 1.3647 | testAcc: 28.1250% (18/64)\n",
            "1 2 Epoch: 10 | ANN: testLoss: 1.3249 | testAcc: 35.3448% (41/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 11 | ANN: trainLoss: 1.1263 | trainAcc: 48.4375% (31/64)\n",
            "1 3 Epoch: 11 | ANN: trainLoss: 1.0999 | trainAcc: 47.6562% (61/128)\n",
            "2 3 Epoch: 11 | ANN: trainLoss: 1.0826 | trainAcc: 50.0000% (86/172)\n",
            "0 2 Epoch: 11 | ANN: testLoss: 1.3562 | testAcc: 35.9375% (23/64)\n",
            "1 2 Epoch: 11 | ANN: testLoss: 1.3311 | testAcc: 40.5172% (47/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 12 | ANN: trainLoss: 1.0628 | trainAcc: 50.0000% (32/64)\n",
            "1 3 Epoch: 12 | ANN: trainLoss: 1.0518 | trainAcc: 52.3438% (67/128)\n",
            "2 3 Epoch: 12 | ANN: trainLoss: 1.0954 | trainAcc: 48.2558% (83/172)\n",
            "0 2 Epoch: 12 | ANN: testLoss: 1.2631 | testAcc: 43.7500% (28/64)\n",
            "1 2 Epoch: 12 | ANN: testLoss: 1.3226 | testAcc: 43.1034% (50/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 13 | ANN: trainLoss: 1.1400 | trainAcc: 56.2500% (36/64)\n",
            "1 3 Epoch: 13 | ANN: trainLoss: 1.0813 | trainAcc: 55.4688% (71/128)\n",
            "2 3 Epoch: 13 | ANN: trainLoss: 1.0289 | trainAcc: 56.9767% (98/172)\n",
            "0 2 Epoch: 13 | ANN: testLoss: 1.4618 | testAcc: 34.3750% (22/64)\n",
            "1 2 Epoch: 13 | ANN: testLoss: 1.3009 | testAcc: 39.6552% (46/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 14 | ANN: trainLoss: 1.1005 | trainAcc: 45.3125% (29/64)\n",
            "1 3 Epoch: 14 | ANN: trainLoss: 1.0559 | trainAcc: 48.4375% (62/128)\n",
            "2 3 Epoch: 14 | ANN: trainLoss: 1.0337 | trainAcc: 50.0000% (86/172)\n",
            "0 2 Epoch: 14 | ANN: testLoss: 1.2485 | testAcc: 46.8750% (30/64)\n",
            "1 2 Epoch: 14 | ANN: testLoss: 1.3308 | testAcc: 44.8276% (52/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 15 | ANN: trainLoss: 0.8994 | trainAcc: 60.9375% (39/64)\n",
            "1 3 Epoch: 15 | ANN: trainLoss: 0.9288 | trainAcc: 57.8125% (74/128)\n",
            "2 3 Epoch: 15 | ANN: trainLoss: 0.9867 | trainAcc: 55.2326% (95/172)\n",
            "0 2 Epoch: 15 | ANN: testLoss: 1.2623 | testAcc: 45.3125% (29/64)\n",
            "1 2 Epoch: 15 | ANN: testLoss: 1.2929 | testAcc: 47.4138% (55/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 16 | ANN: trainLoss: 1.0589 | trainAcc: 43.7500% (28/64)\n",
            "1 3 Epoch: 16 | ANN: trainLoss: 1.0960 | trainAcc: 45.3125% (58/128)\n",
            "2 3 Epoch: 16 | ANN: trainLoss: 1.0485 | trainAcc: 47.0930% (81/172)\n",
            "0 2 Epoch: 16 | ANN: testLoss: 1.2880 | testAcc: 43.7500% (28/64)\n",
            "1 2 Epoch: 16 | ANN: testLoss: 1.2283 | testAcc: 44.8276% (52/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 17 | ANN: trainLoss: 0.9285 | trainAcc: 62.5000% (40/64)\n",
            "1 3 Epoch: 17 | ANN: trainLoss: 0.9296 | trainAcc: 60.1562% (77/128)\n",
            "2 3 Epoch: 17 | ANN: trainLoss: 0.9533 | trainAcc: 57.5581% (99/172)\n",
            "0 2 Epoch: 17 | ANN: testLoss: 1.0607 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 17 | ANN: testLoss: 1.1620 | testAcc: 48.2759% (56/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 18 | ANN: trainLoss: 0.9875 | trainAcc: 53.1250% (34/64)\n",
            "1 3 Epoch: 18 | ANN: trainLoss: 0.9832 | trainAcc: 52.3438% (67/128)\n",
            "2 3 Epoch: 18 | ANN: trainLoss: 0.9546 | trainAcc: 52.9070% (91/172)\n",
            "0 2 Epoch: 18 | ANN: testLoss: 1.0550 | testAcc: 50.0000% (32/64)\n",
            "1 2 Epoch: 18 | ANN: testLoss: 1.1121 | testAcc: 51.7241% (60/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 19 | ANN: trainLoss: 0.9614 | trainAcc: 56.2500% (36/64)\n",
            "1 3 Epoch: 19 | ANN: trainLoss: 0.9491 | trainAcc: 56.2500% (72/128)\n",
            "2 3 Epoch: 19 | ANN: trainLoss: 0.9831 | trainAcc: 51.7442% (89/172)\n",
            "0 2 Epoch: 19 | ANN: testLoss: 1.1398 | testAcc: 46.8750% (30/64)\n",
            "1 2 Epoch: 19 | ANN: testLoss: 1.0931 | testAcc: 47.4138% (55/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 20 | ANN: trainLoss: 0.9862 | trainAcc: 46.8750% (30/64)\n",
            "1 3 Epoch: 20 | ANN: trainLoss: 0.9192 | trainAcc: 54.6875% (70/128)\n",
            "2 3 Epoch: 20 | ANN: trainLoss: 0.9125 | trainAcc: 57.5581% (99/172)\n",
            "0 2 Epoch: 20 | ANN: testLoss: 1.0589 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 20 | ANN: testLoss: 1.0997 | testAcc: 48.2759% (56/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 21 | ANN: trainLoss: 0.8795 | trainAcc: 60.9375% (39/64)\n",
            "1 3 Epoch: 21 | ANN: trainLoss: 0.9682 | trainAcc: 58.5938% (75/128)\n",
            "2 3 Epoch: 21 | ANN: trainLoss: 0.9702 | trainAcc: 58.1395% (100/172)\n",
            "0 2 Epoch: 21 | ANN: testLoss: 1.0573 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 21 | ANN: testLoss: 1.0885 | testAcc: 51.7241% (60/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 22 | ANN: trainLoss: 0.8708 | trainAcc: 59.3750% (38/64)\n",
            "1 3 Epoch: 22 | ANN: trainLoss: 0.8954 | trainAcc: 55.4688% (71/128)\n",
            "2 3 Epoch: 22 | ANN: trainLoss: 0.8753 | trainAcc: 56.9767% (98/172)\n",
            "0 2 Epoch: 22 | ANN: testLoss: 1.0454 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 22 | ANN: testLoss: 1.0754 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 23 | ANN: trainLoss: 0.9373 | trainAcc: 59.3750% (38/64)\n",
            "1 3 Epoch: 23 | ANN: trainLoss: 0.9115 | trainAcc: 60.9375% (78/128)\n",
            "2 3 Epoch: 23 | ANN: trainLoss: 0.8804 | trainAcc: 59.8837% (103/172)\n",
            "0 2 Epoch: 23 | ANN: testLoss: 1.0469 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 23 | ANN: testLoss: 1.0658 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 24 | ANN: trainLoss: 0.7704 | trainAcc: 59.3750% (38/64)\n",
            "1 3 Epoch: 24 | ANN: trainLoss: 0.7719 | trainAcc: 66.4062% (85/128)\n",
            "2 3 Epoch: 24 | ANN: trainLoss: 0.8490 | trainAcc: 65.1163% (112/172)\n",
            "0 2 Epoch: 24 | ANN: testLoss: 1.1458 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 24 | ANN: testLoss: 1.0648 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 25 | ANN: trainLoss: 0.8383 | trainAcc: 64.0625% (41/64)\n",
            "1 3 Epoch: 25 | ANN: trainLoss: 0.8261 | trainAcc: 64.8438% (83/128)\n",
            "2 3 Epoch: 25 | ANN: trainLoss: 0.8348 | trainAcc: 62.2093% (107/172)\n",
            "0 2 Epoch: 25 | ANN: testLoss: 1.0526 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 25 | ANN: testLoss: 1.0879 | testAcc: 49.1379% (57/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 26 | ANN: trainLoss: 0.7857 | trainAcc: 68.7500% (44/64)\n",
            "1 3 Epoch: 26 | ANN: trainLoss: 0.8272 | trainAcc: 64.8438% (83/128)\n",
            "2 3 Epoch: 26 | ANN: trainLoss: 0.8259 | trainAcc: 64.5349% (111/172)\n",
            "0 2 Epoch: 26 | ANN: testLoss: 1.0793 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 26 | ANN: testLoss: 1.1101 | testAcc: 45.6897% (53/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 27 | ANN: trainLoss: 0.6955 | trainAcc: 71.8750% (46/64)\n",
            "1 3 Epoch: 27 | ANN: trainLoss: 0.7363 | trainAcc: 71.8750% (92/128)\n",
            "2 3 Epoch: 27 | ANN: trainLoss: 0.8190 | trainAcc: 68.6047% (118/172)\n",
            "0 2 Epoch: 27 | ANN: testLoss: 1.1142 | testAcc: 45.3125% (29/64)\n",
            "1 2 Epoch: 27 | ANN: testLoss: 1.1179 | testAcc: 45.6897% (53/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 28 | ANN: trainLoss: 0.7492 | trainAcc: 60.9375% (39/64)\n",
            "1 3 Epoch: 28 | ANN: trainLoss: 0.7908 | trainAcc: 61.7188% (79/128)\n",
            "2 3 Epoch: 28 | ANN: trainLoss: 0.8075 | trainAcc: 61.6279% (106/172)\n",
            "0 2 Epoch: 28 | ANN: testLoss: 1.1458 | testAcc: 45.3125% (29/64)\n",
            "1 2 Epoch: 28 | ANN: testLoss: 1.0631 | testAcc: 48.2759% (56/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 29 | ANN: trainLoss: 0.9028 | trainAcc: 60.9375% (39/64)\n",
            "1 3 Epoch: 29 | ANN: trainLoss: 0.8075 | trainAcc: 64.0625% (82/128)\n",
            "2 3 Epoch: 29 | ANN: trainLoss: 0.7748 | trainAcc: 65.6977% (113/172)\n",
            "0 2 Epoch: 29 | ANN: testLoss: 1.0781 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 29 | ANN: testLoss: 1.0429 | testAcc: 50.8621% (59/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 30 | ANN: trainLoss: 0.8323 | trainAcc: 62.5000% (40/64)\n",
            "1 3 Epoch: 30 | ANN: trainLoss: 0.8348 | trainAcc: 64.0625% (82/128)\n",
            "2 3 Epoch: 30 | ANN: trainLoss: 0.7789 | trainAcc: 68.0233% (117/172)\n",
            "0 2 Epoch: 30 | ANN: testLoss: 0.9874 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 30 | ANN: testLoss: 1.0491 | testAcc: 49.1379% (57/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 31 | ANN: trainLoss: 0.7197 | trainAcc: 71.8750% (46/64)\n",
            "1 3 Epoch: 31 | ANN: trainLoss: 0.6670 | trainAcc: 75.0000% (96/128)\n",
            "2 3 Epoch: 31 | ANN: trainLoss: 0.7155 | trainAcc: 71.5116% (123/172)\n",
            "0 2 Epoch: 31 | ANN: testLoss: 0.9954 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 31 | ANN: testLoss: 1.0796 | testAcc: 46.5517% (54/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 32 | ANN: trainLoss: 0.6620 | trainAcc: 75.0000% (48/64)\n",
            "1 3 Epoch: 32 | ANN: trainLoss: 0.7883 | trainAcc: 67.1875% (86/128)\n",
            "2 3 Epoch: 32 | ANN: trainLoss: 0.7809 | trainAcc: 67.4419% (116/172)\n",
            "0 2 Epoch: 32 | ANN: testLoss: 1.0095 | testAcc: 43.7500% (28/64)\n",
            "1 2 Epoch: 32 | ANN: testLoss: 1.0944 | testAcc: 44.8276% (52/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 33 | ANN: trainLoss: 0.7945 | trainAcc: 64.0625% (41/64)\n",
            "1 3 Epoch: 33 | ANN: trainLoss: 0.7344 | trainAcc: 67.9688% (87/128)\n",
            "2 3 Epoch: 33 | ANN: trainLoss: 0.7001 | trainAcc: 69.7674% (120/172)\n",
            "0 2 Epoch: 33 | ANN: testLoss: 0.9502 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 33 | ANN: testLoss: 1.0417 | testAcc: 50.0000% (58/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 34 | ANN: trainLoss: 0.7058 | trainAcc: 73.4375% (47/64)\n",
            "1 3 Epoch: 34 | ANN: trainLoss: 0.6943 | trainAcc: 71.0938% (91/128)\n",
            "2 3 Epoch: 34 | ANN: trainLoss: 0.7685 | trainAcc: 68.6047% (118/172)\n",
            "0 2 Epoch: 34 | ANN: testLoss: 0.9677 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 34 | ANN: testLoss: 1.0125 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 35 | ANN: trainLoss: 0.7687 | trainAcc: 73.4375% (47/64)\n",
            "1 3 Epoch: 35 | ANN: trainLoss: 0.7479 | trainAcc: 69.5312% (89/128)\n",
            "2 3 Epoch: 35 | ANN: trainLoss: 0.7408 | trainAcc: 69.1860% (119/172)\n",
            "0 2 Epoch: 35 | ANN: testLoss: 1.0999 | testAcc: 37.5000% (24/64)\n",
            "1 2 Epoch: 35 | ANN: testLoss: 1.0345 | testAcc: 45.6897% (53/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 36 | ANN: trainLoss: 0.7891 | trainAcc: 70.3125% (45/64)\n",
            "1 3 Epoch: 36 | ANN: trainLoss: 0.6966 | trainAcc: 68.7500% (88/128)\n",
            "2 3 Epoch: 36 | ANN: trainLoss: 0.6609 | trainAcc: 70.9302% (122/172)\n",
            "0 2 Epoch: 36 | ANN: testLoss: 1.1874 | testAcc: 42.1875% (27/64)\n",
            "1 2 Epoch: 36 | ANN: testLoss: 1.0576 | testAcc: 48.2759% (56/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 37 | ANN: trainLoss: 0.6434 | trainAcc: 73.4375% (47/64)\n",
            "1 3 Epoch: 37 | ANN: trainLoss: 0.6762 | trainAcc: 71.0938% (91/128)\n",
            "2 3 Epoch: 37 | ANN: trainLoss: 0.6619 | trainAcc: 71.5116% (123/172)\n",
            "0 2 Epoch: 37 | ANN: testLoss: 0.9092 | testAcc: 50.0000% (32/64)\n",
            "1 2 Epoch: 37 | ANN: testLoss: 1.0420 | testAcc: 44.8276% (52/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 38 | ANN: trainLoss: 0.6314 | trainAcc: 73.4375% (47/64)\n",
            "1 3 Epoch: 38 | ANN: trainLoss: 0.6515 | trainAcc: 73.4375% (94/128)\n",
            "2 3 Epoch: 38 | ANN: trainLoss: 0.6489 | trainAcc: 72.6744% (125/172)\n",
            "0 2 Epoch: 38 | ANN: testLoss: 1.0098 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 38 | ANN: testLoss: 1.0071 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 39 | ANN: trainLoss: 0.6006 | trainAcc: 75.0000% (48/64)\n",
            "1 3 Epoch: 39 | ANN: trainLoss: 0.5990 | trainAcc: 75.0000% (96/128)\n",
            "2 3 Epoch: 39 | ANN: trainLoss: 0.5848 | trainAcc: 77.3256% (133/172)\n",
            "0 2 Epoch: 39 | ANN: testLoss: 0.9661 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 39 | ANN: testLoss: 1.0497 | testAcc: 50.0000% (58/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 40 | ANN: trainLoss: 0.7108 | trainAcc: 70.3125% (45/64)\n",
            "1 3 Epoch: 40 | ANN: trainLoss: 0.6548 | trainAcc: 73.4375% (94/128)\n",
            "2 3 Epoch: 40 | ANN: trainLoss: 0.6519 | trainAcc: 73.8372% (127/172)\n",
            "0 2 Epoch: 40 | ANN: testLoss: 1.0691 | testAcc: 50.0000% (32/64)\n",
            "1 2 Epoch: 40 | ANN: testLoss: 1.1116 | testAcc: 48.2759% (56/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 41 | ANN: trainLoss: 0.6328 | trainAcc: 75.0000% (48/64)\n",
            "1 3 Epoch: 41 | ANN: trainLoss: 0.6055 | trainAcc: 75.0000% (96/128)\n",
            "2 3 Epoch: 41 | ANN: trainLoss: 0.6167 | trainAcc: 75.5814% (130/172)\n",
            "0 2 Epoch: 41 | ANN: testLoss: 1.0489 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 41 | ANN: testLoss: 1.0908 | testAcc: 51.7241% (60/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 42 | ANN: trainLoss: 0.5035 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 42 | ANN: trainLoss: 0.5967 | trainAcc: 80.4688% (103/128)\n",
            "2 3 Epoch: 42 | ANN: trainLoss: 0.6024 | trainAcc: 77.9070% (134/172)\n",
            "0 2 Epoch: 42 | ANN: testLoss: 0.9818 | testAcc: 57.8125% (37/64)\n",
            "1 2 Epoch: 42 | ANN: testLoss: 1.0083 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 43 | ANN: trainLoss: 0.5517 | trainAcc: 79.6875% (51/64)\n",
            "1 3 Epoch: 43 | ANN: trainLoss: 0.5709 | trainAcc: 79.6875% (102/128)\n",
            "2 3 Epoch: 43 | ANN: trainLoss: 0.5428 | trainAcc: 80.2326% (138/172)\n",
            "0 2 Epoch: 43 | ANN: testLoss: 0.8754 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 43 | ANN: testLoss: 0.9574 | testAcc: 51.7241% (60/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 44 | ANN: trainLoss: 0.5024 | trainAcc: 76.5625% (49/64)\n",
            "1 3 Epoch: 44 | ANN: trainLoss: 0.5448 | trainAcc: 76.5625% (98/128)\n",
            "2 3 Epoch: 44 | ANN: trainLoss: 0.5642 | trainAcc: 75.0000% (129/172)\n",
            "0 2 Epoch: 44 | ANN: testLoss: 0.8949 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 44 | ANN: testLoss: 0.9616 | testAcc: 47.4138% (55/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 45 | ANN: trainLoss: 0.5091 | trainAcc: 76.5625% (49/64)\n",
            "1 3 Epoch: 45 | ANN: trainLoss: 0.5043 | trainAcc: 79.6875% (102/128)\n",
            "2 3 Epoch: 45 | ANN: trainLoss: 0.5473 | trainAcc: 77.9070% (134/172)\n",
            "0 2 Epoch: 45 | ANN: testLoss: 1.0554 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 45 | ANN: testLoss: 0.9866 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 46 | ANN: trainLoss: 0.5856 | trainAcc: 79.6875% (51/64)\n",
            "1 3 Epoch: 46 | ANN: trainLoss: 0.5636 | trainAcc: 79.6875% (102/128)\n",
            "2 3 Epoch: 46 | ANN: trainLoss: 0.5615 | trainAcc: 79.0698% (136/172)\n",
            "0 2 Epoch: 46 | ANN: testLoss: 0.8357 | testAcc: 64.0625% (41/64)\n",
            "1 2 Epoch: 46 | ANN: testLoss: 1.0175 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 47 | ANN: trainLoss: 0.4914 | trainAcc: 85.9375% (55/64)\n",
            "1 3 Epoch: 47 | ANN: trainLoss: 0.4921 | trainAcc: 85.1562% (109/128)\n",
            "2 3 Epoch: 47 | ANN: trainLoss: 0.4644 | trainAcc: 85.4651% (147/172)\n",
            "0 2 Epoch: 47 | ANN: testLoss: 0.9795 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 47 | ANN: testLoss: 1.0012 | testAcc: 51.7241% (60/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 48 | ANN: trainLoss: 0.4747 | trainAcc: 89.0625% (57/64)\n",
            "1 3 Epoch: 48 | ANN: trainLoss: 0.5162 | trainAcc: 82.0312% (105/128)\n",
            "2 3 Epoch: 48 | ANN: trainLoss: 0.5034 | trainAcc: 82.5581% (142/172)\n",
            "0 2 Epoch: 48 | ANN: testLoss: 1.0366 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 48 | ANN: testLoss: 1.0481 | testAcc: 50.0000% (58/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 49 | ANN: trainLoss: 0.7072 | trainAcc: 70.3125% (45/64)\n",
            "1 3 Epoch: 49 | ANN: trainLoss: 0.6408 | trainAcc: 71.8750% (92/128)\n",
            "2 3 Epoch: 49 | ANN: trainLoss: 0.5637 | trainAcc: 75.0000% (129/172)\n",
            "0 2 Epoch: 49 | ANN: testLoss: 1.1733 | testAcc: 46.8750% (30/64)\n",
            "1 2 Epoch: 49 | ANN: testLoss: 1.1636 | testAcc: 48.2759% (56/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 50 | ANN: trainLoss: 0.5038 | trainAcc: 81.2500% (52/64)\n",
            "1 3 Epoch: 50 | ANN: trainLoss: 0.5306 | trainAcc: 79.6875% (102/128)\n",
            "2 3 Epoch: 50 | ANN: trainLoss: 0.5212 | trainAcc: 80.8140% (139/172)\n",
            "0 2 Epoch: 50 | ANN: testLoss: 1.2633 | testAcc: 45.3125% (29/64)\n",
            "1 2 Epoch: 50 | ANN: testLoss: 1.1924 | testAcc: 46.5517% (54/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 51 | ANN: trainLoss: 0.4968 | trainAcc: 82.8125% (53/64)\n",
            "1 3 Epoch: 51 | ANN: trainLoss: 0.5001 | trainAcc: 82.0312% (105/128)\n",
            "2 3 Epoch: 51 | ANN: trainLoss: 0.4760 | trainAcc: 81.9767% (141/172)\n",
            "0 2 Epoch: 51 | ANN: testLoss: 1.1610 | testAcc: 46.8750% (30/64)\n",
            "1 2 Epoch: 51 | ANN: testLoss: 1.0755 | testAcc: 48.2759% (56/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 52 | ANN: trainLoss: 0.4269 | trainAcc: 82.8125% (53/64)\n",
            "1 3 Epoch: 52 | ANN: trainLoss: 0.4305 | trainAcc: 82.0312% (105/128)\n",
            "2 3 Epoch: 52 | ANN: trainLoss: 0.4809 | trainAcc: 81.3953% (140/172)\n",
            "0 2 Epoch: 52 | ANN: testLoss: 1.0565 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 52 | ANN: testLoss: 1.0074 | testAcc: 51.7241% (60/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 53 | ANN: trainLoss: 0.4222 | trainAcc: 82.8125% (53/64)\n",
            "1 3 Epoch: 53 | ANN: trainLoss: 0.4483 | trainAcc: 81.2500% (104/128)\n",
            "2 3 Epoch: 53 | ANN: trainLoss: 0.4242 | trainAcc: 81.9767% (141/172)\n",
            "0 2 Epoch: 53 | ANN: testLoss: 0.8770 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 53 | ANN: testLoss: 0.9931 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 54 | ANN: trainLoss: 0.4117 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 54 | ANN: trainLoss: 0.4597 | trainAcc: 82.0312% (105/128)\n",
            "2 3 Epoch: 54 | ANN: trainLoss: 0.4633 | trainAcc: 82.5581% (142/172)\n",
            "0 2 Epoch: 54 | ANN: testLoss: 0.9941 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 54 | ANN: testLoss: 0.9955 | testAcc: 50.0000% (58/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 55 | ANN: trainLoss: 0.4822 | trainAcc: 79.6875% (51/64)\n",
            "1 3 Epoch: 55 | ANN: trainLoss: 0.4841 | trainAcc: 81.2500% (104/128)\n",
            "2 3 Epoch: 55 | ANN: trainLoss: 0.4530 | trainAcc: 83.1395% (143/172)\n",
            "0 2 Epoch: 55 | ANN: testLoss: 1.0217 | testAcc: 43.7500% (28/64)\n",
            "1 2 Epoch: 55 | ANN: testLoss: 1.0403 | testAcc: 49.1379% (57/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 56 | ANN: trainLoss: 0.4996 | trainAcc: 76.5625% (49/64)\n",
            "1 3 Epoch: 56 | ANN: trainLoss: 0.4918 | trainAcc: 78.9062% (101/128)\n",
            "2 3 Epoch: 56 | ANN: trainLoss: 0.5005 | trainAcc: 79.6512% (137/172)\n",
            "0 2 Epoch: 56 | ANN: testLoss: 1.1309 | testAcc: 50.0000% (32/64)\n",
            "1 2 Epoch: 56 | ANN: testLoss: 1.1580 | testAcc: 46.5517% (54/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 57 | ANN: trainLoss: 0.3597 | trainAcc: 90.6250% (58/64)\n",
            "1 3 Epoch: 57 | ANN: trainLoss: 0.3793 | trainAcc: 89.8438% (115/128)\n",
            "2 3 Epoch: 57 | ANN: trainLoss: 0.3903 | trainAcc: 88.3721% (152/172)\n",
            "0 2 Epoch: 57 | ANN: testLoss: 1.1005 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 57 | ANN: testLoss: 1.2295 | testAcc: 46.5517% (54/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 58 | ANN: trainLoss: 0.4506 | trainAcc: 82.8125% (53/64)\n",
            "1 3 Epoch: 58 | ANN: trainLoss: 0.3956 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 58 | ANN: trainLoss: 0.4306 | trainAcc: 85.4651% (147/172)\n",
            "0 2 Epoch: 58 | ANN: testLoss: 1.1218 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 58 | ANN: testLoss: 1.0680 | testAcc: 47.4138% (55/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 59 | ANN: trainLoss: 0.4652 | trainAcc: 79.6875% (51/64)\n",
            "1 3 Epoch: 59 | ANN: trainLoss: 0.4186 | trainAcc: 82.0312% (105/128)\n",
            "2 3 Epoch: 59 | ANN: trainLoss: 0.4155 | trainAcc: 83.1395% (143/172)\n",
            "0 2 Epoch: 59 | ANN: testLoss: 0.9015 | testAcc: 57.8125% (37/64)\n",
            "1 2 Epoch: 59 | ANN: testLoss: 1.0600 | testAcc: 50.0000% (58/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 60 | ANN: trainLoss: 0.3711 | trainAcc: 90.6250% (58/64)\n",
            "1 3 Epoch: 60 | ANN: trainLoss: 0.4597 | trainAcc: 84.3750% (108/128)\n",
            "2 3 Epoch: 60 | ANN: trainLoss: 0.4752 | trainAcc: 84.3023% (145/172)\n",
            "0 2 Epoch: 60 | ANN: testLoss: 1.1615 | testAcc: 46.8750% (30/64)\n",
            "1 2 Epoch: 60 | ANN: testLoss: 1.0737 | testAcc: 47.4138% (55/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 61 | ANN: trainLoss: 0.4309 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 61 | ANN: trainLoss: 0.4376 | trainAcc: 84.3750% (108/128)\n",
            "2 3 Epoch: 61 | ANN: trainLoss: 0.4442 | trainAcc: 85.4651% (147/172)\n",
            "0 2 Epoch: 61 | ANN: testLoss: 1.1485 | testAcc: 50.0000% (32/64)\n",
            "1 2 Epoch: 61 | ANN: testLoss: 1.0882 | testAcc: 49.1379% (57/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 62 | ANN: trainLoss: 0.4165 | trainAcc: 87.5000% (56/64)\n",
            "1 3 Epoch: 62 | ANN: trainLoss: 0.4117 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 62 | ANN: trainLoss: 0.4344 | trainAcc: 83.7209% (144/172)\n",
            "0 2 Epoch: 62 | ANN: testLoss: 1.0593 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 62 | ANN: testLoss: 1.1439 | testAcc: 49.1379% (57/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 63 | ANN: trainLoss: 0.5300 | trainAcc: 82.8125% (53/64)\n",
            "1 3 Epoch: 63 | ANN: trainLoss: 0.4446 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 63 | ANN: trainLoss: 0.4281 | trainAcc: 84.8837% (146/172)\n",
            "0 2 Epoch: 63 | ANN: testLoss: 1.1436 | testAcc: 50.0000% (32/64)\n",
            "1 2 Epoch: 63 | ANN: testLoss: 1.1177 | testAcc: 49.1379% (57/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 64 | ANN: trainLoss: 0.4761 | trainAcc: 85.9375% (55/64)\n",
            "1 3 Epoch: 64 | ANN: trainLoss: 0.4428 | trainAcc: 85.1562% (109/128)\n",
            "2 3 Epoch: 64 | ANN: trainLoss: 0.4493 | trainAcc: 84.3023% (145/172)\n",
            "0 2 Epoch: 64 | ANN: testLoss: 1.0512 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 64 | ANN: testLoss: 1.1068 | testAcc: 52.5862% (61/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 65 | ANN: trainLoss: 0.4540 | trainAcc: 78.1250% (50/64)\n",
            "1 3 Epoch: 65 | ANN: trainLoss: 0.4027 | trainAcc: 84.3750% (108/128)\n",
            "2 3 Epoch: 65 | ANN: trainLoss: 0.3738 | trainAcc: 85.4651% (147/172)\n",
            "0 2 Epoch: 65 | ANN: testLoss: 1.1460 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 65 | ANN: testLoss: 1.1173 | testAcc: 50.8621% (59/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 66 | ANN: trainLoss: 0.4282 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 66 | ANN: trainLoss: 0.4012 | trainAcc: 86.7188% (111/128)\n",
            "2 3 Epoch: 66 | ANN: trainLoss: 0.3797 | trainAcc: 88.3721% (152/172)\n",
            "0 2 Epoch: 66 | ANN: testLoss: 1.1736 | testAcc: 46.8750% (30/64)\n",
            "1 2 Epoch: 66 | ANN: testLoss: 1.1223 | testAcc: 50.8621% (59/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 67 | ANN: trainLoss: 0.4357 | trainAcc: 85.9375% (55/64)\n",
            "1 3 Epoch: 67 | ANN: trainLoss: 0.4265 | trainAcc: 85.1562% (109/128)\n",
            "2 3 Epoch: 67 | ANN: trainLoss: 0.3815 | trainAcc: 86.6279% (149/172)\n",
            "0 2 Epoch: 67 | ANN: testLoss: 0.9696 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 67 | ANN: testLoss: 1.1393 | testAcc: 50.8621% (59/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 68 | ANN: trainLoss: 0.3347 | trainAcc: 89.0625% (57/64)\n",
            "1 3 Epoch: 68 | ANN: trainLoss: 0.3517 | trainAcc: 89.8438% (115/128)\n",
            "2 3 Epoch: 68 | ANN: trainLoss: 0.4009 | trainAcc: 86.6279% (149/172)\n",
            "0 2 Epoch: 68 | ANN: testLoss: 1.1928 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 68 | ANN: testLoss: 1.1229 | testAcc: 50.0000% (58/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 69 | ANN: trainLoss: 0.2682 | trainAcc: 90.6250% (58/64)\n",
            "1 3 Epoch: 69 | ANN: trainLoss: 0.3187 | trainAcc: 88.2812% (113/128)\n",
            "2 3 Epoch: 69 | ANN: trainLoss: 0.3225 | trainAcc: 87.7907% (151/172)\n",
            "0 2 Epoch: 69 | ANN: testLoss: 1.0301 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 69 | ANN: testLoss: 1.0776 | testAcc: 52.5862% (61/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 70 | ANN: trainLoss: 0.3160 | trainAcc: 90.6250% (58/64)\n",
            "1 3 Epoch: 70 | ANN: trainLoss: 0.3550 | trainAcc: 87.5000% (112/128)\n",
            "2 3 Epoch: 70 | ANN: trainLoss: 0.3309 | trainAcc: 89.5349% (154/172)\n",
            "0 2 Epoch: 70 | ANN: testLoss: 1.0841 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 70 | ANN: testLoss: 1.0207 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 71 | ANN: trainLoss: 0.4047 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 71 | ANN: trainLoss: 0.3905 | trainAcc: 83.5938% (107/128)\n",
            "2 3 Epoch: 71 | ANN: trainLoss: 0.3618 | trainAcc: 86.6279% (149/172)\n",
            "0 2 Epoch: 71 | ANN: testLoss: 0.9593 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 71 | ANN: testLoss: 0.9917 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 72 | ANN: trainLoss: 0.4647 | trainAcc: 82.8125% (53/64)\n",
            "1 3 Epoch: 72 | ANN: trainLoss: 0.4343 | trainAcc: 83.5938% (107/128)\n",
            "2 3 Epoch: 72 | ANN: trainLoss: 0.3829 | trainAcc: 85.4651% (147/172)\n",
            "0 2 Epoch: 72 | ANN: testLoss: 0.9855 | testAcc: 57.8125% (37/64)\n",
            "1 2 Epoch: 72 | ANN: testLoss: 0.9766 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 73 | ANN: trainLoss: 0.2949 | trainAcc: 93.7500% (60/64)\n",
            "1 3 Epoch: 73 | ANN: trainLoss: 0.3215 | trainAcc: 92.9688% (119/128)\n",
            "2 3 Epoch: 73 | ANN: trainLoss: 0.3474 | trainAcc: 90.1163% (155/172)\n",
            "0 2 Epoch: 73 | ANN: testLoss: 0.9714 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 73 | ANN: testLoss: 0.9900 | testAcc: 58.6207% (68/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 74 | ANN: trainLoss: 0.3583 | trainAcc: 87.5000% (56/64)\n",
            "1 3 Epoch: 74 | ANN: trainLoss: 0.3676 | trainAcc: 89.0625% (114/128)\n",
            "2 3 Epoch: 74 | ANN: trainLoss: 0.3550 | trainAcc: 88.3721% (152/172)\n",
            "0 2 Epoch: 74 | ANN: testLoss: 1.0083 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 74 | ANN: testLoss: 1.0037 | testAcc: 60.3448% (70/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 75 | ANN: trainLoss: 0.3700 | trainAcc: 87.5000% (56/64)\n",
            "1 3 Epoch: 75 | ANN: trainLoss: 0.3460 | trainAcc: 88.2812% (113/128)\n",
            "2 3 Epoch: 75 | ANN: trainLoss: 0.3470 | trainAcc: 87.7907% (151/172)\n",
            "0 2 Epoch: 75 | ANN: testLoss: 1.1292 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 75 | ANN: testLoss: 1.0036 | testAcc: 56.8966% (66/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 76 | ANN: trainLoss: 0.3258 | trainAcc: 89.0625% (57/64)\n",
            "1 3 Epoch: 76 | ANN: trainLoss: 0.3372 | trainAcc: 89.0625% (114/128)\n",
            "2 3 Epoch: 76 | ANN: trainLoss: 0.3592 | trainAcc: 88.3721% (152/172)\n",
            "0 2 Epoch: 76 | ANN: testLoss: 0.8777 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 76 | ANN: testLoss: 1.0571 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 77 | ANN: trainLoss: 0.2761 | trainAcc: 93.7500% (60/64)\n",
            "1 3 Epoch: 77 | ANN: trainLoss: 0.3000 | trainAcc: 90.6250% (116/128)\n",
            "2 3 Epoch: 77 | ANN: trainLoss: 0.3212 | trainAcc: 91.2791% (157/172)\n",
            "0 2 Epoch: 77 | ANN: testLoss: 0.9810 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 77 | ANN: testLoss: 1.0431 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 78 | ANN: trainLoss: 0.3704 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 78 | ANN: trainLoss: 0.3379 | trainAcc: 86.7188% (111/128)\n",
            "2 3 Epoch: 78 | ANN: trainLoss: 0.3649 | trainAcc: 87.2093% (150/172)\n",
            "0 2 Epoch: 78 | ANN: testLoss: 1.0241 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 78 | ANN: testLoss: 1.0457 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 79 | ANN: trainLoss: 0.3662 | trainAcc: 87.5000% (56/64)\n",
            "1 3 Epoch: 79 | ANN: trainLoss: 0.3718 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 79 | ANN: trainLoss: 0.4083 | trainAcc: 84.3023% (145/172)\n",
            "0 2 Epoch: 79 | ANN: testLoss: 1.0178 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 79 | ANN: testLoss: 1.0584 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 80 | ANN: trainLoss: 0.3850 | trainAcc: 87.5000% (56/64)\n",
            "1 3 Epoch: 80 | ANN: trainLoss: 0.3961 | trainAcc: 85.1562% (109/128)\n",
            "2 3 Epoch: 80 | ANN: trainLoss: 0.3736 | trainAcc: 85.4651% (147/172)\n",
            "0 2 Epoch: 80 | ANN: testLoss: 0.9974 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 80 | ANN: testLoss: 1.0804 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 81 | ANN: trainLoss: 0.2759 | trainAcc: 93.7500% (60/64)\n",
            "1 3 Epoch: 81 | ANN: trainLoss: 0.2519 | trainAcc: 93.7500% (120/128)\n",
            "2 3 Epoch: 81 | ANN: trainLoss: 0.2678 | trainAcc: 93.0233% (160/172)\n",
            "0 2 Epoch: 81 | ANN: testLoss: 1.1905 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 81 | ANN: testLoss: 1.0712 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 82 | ANN: trainLoss: 0.3733 | trainAcc: 87.5000% (56/64)\n",
            "1 3 Epoch: 82 | ANN: trainLoss: 0.3448 | trainAcc: 90.6250% (116/128)\n",
            "2 3 Epoch: 82 | ANN: trainLoss: 0.3379 | trainAcc: 89.5349% (154/172)\n",
            "0 2 Epoch: 82 | ANN: testLoss: 1.1739 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 82 | ANN: testLoss: 1.0663 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 83 | ANN: trainLoss: 0.3054 | trainAcc: 87.5000% (56/64)\n",
            "1 3 Epoch: 83 | ANN: trainLoss: 0.3193 | trainAcc: 87.5000% (112/128)\n",
            "2 3 Epoch: 83 | ANN: trainLoss: 0.3367 | trainAcc: 87.2093% (150/172)\n",
            "0 2 Epoch: 83 | ANN: testLoss: 1.1648 | testAcc: 57.8125% (37/64)\n",
            "1 2 Epoch: 83 | ANN: testLoss: 1.0699 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 84 | ANN: trainLoss: 0.2893 | trainAcc: 90.6250% (58/64)\n",
            "1 3 Epoch: 84 | ANN: trainLoss: 0.3319 | trainAcc: 89.8438% (115/128)\n",
            "2 3 Epoch: 84 | ANN: trainLoss: 0.3722 | trainAcc: 87.2093% (150/172)\n",
            "0 2 Epoch: 84 | ANN: testLoss: 0.9437 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 84 | ANN: testLoss: 1.0891 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 85 | ANN: trainLoss: 0.2677 | trainAcc: 90.6250% (58/64)\n",
            "1 3 Epoch: 85 | ANN: trainLoss: 0.3074 | trainAcc: 89.8438% (115/128)\n",
            "2 3 Epoch: 85 | ANN: trainLoss: 0.2948 | trainAcc: 90.1163% (155/172)\n",
            "0 2 Epoch: 85 | ANN: testLoss: 0.9516 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 85 | ANN: testLoss: 1.0827 | testAcc: 56.0345% (65/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 86 | ANN: trainLoss: 0.3988 | trainAcc: 85.9375% (55/64)\n",
            "1 3 Epoch: 86 | ANN: trainLoss: 0.3768 | trainAcc: 87.5000% (112/128)\n",
            "2 3 Epoch: 86 | ANN: trainLoss: 0.3771 | trainAcc: 87.2093% (150/172)\n",
            "0 2 Epoch: 86 | ANN: testLoss: 1.0346 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 86 | ANN: testLoss: 1.0698 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 87 | ANN: trainLoss: 0.2972 | trainAcc: 90.6250% (58/64)\n",
            "1 3 Epoch: 87 | ANN: trainLoss: 0.3032 | trainAcc: 89.8438% (115/128)\n",
            "2 3 Epoch: 87 | ANN: trainLoss: 0.3106 | trainAcc: 89.5349% (154/172)\n",
            "0 2 Epoch: 87 | ANN: testLoss: 1.0748 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 87 | ANN: testLoss: 1.0643 | testAcc: 52.5862% (61/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 88 | ANN: trainLoss: 0.2715 | trainAcc: 93.7500% (60/64)\n",
            "1 3 Epoch: 88 | ANN: trainLoss: 0.3128 | trainAcc: 91.4062% (117/128)\n",
            "2 3 Epoch: 88 | ANN: trainLoss: 0.3420 | trainAcc: 91.2791% (157/172)\n",
            "0 2 Epoch: 88 | ANN: testLoss: 0.9375 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 88 | ANN: testLoss: 1.0835 | testAcc: 54.3103% (63/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 89 | ANN: trainLoss: 0.3684 | trainAcc: 89.0625% (57/64)\n",
            "1 3 Epoch: 89 | ANN: trainLoss: 0.3618 | trainAcc: 89.0625% (114/128)\n",
            "2 3 Epoch: 89 | ANN: trainLoss: 0.3104 | trainAcc: 91.8605% (158/172)\n",
            "0 2 Epoch: 89 | ANN: testLoss: 1.1160 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 89 | ANN: testLoss: 1.0656 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 90 | ANN: trainLoss: 0.5529 | trainAcc: 79.6875% (51/64)\n",
            "1 3 Epoch: 90 | ANN: trainLoss: 0.4209 | trainAcc: 85.9375% (110/128)\n",
            "2 3 Epoch: 90 | ANN: trainLoss: 0.4001 | trainAcc: 86.0465% (148/172)\n",
            "0 2 Epoch: 90 | ANN: testLoss: 1.0787 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 90 | ANN: testLoss: 1.0675 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 91 | ANN: trainLoss: 0.2682 | trainAcc: 90.6250% (58/64)\n",
            "1 3 Epoch: 91 | ANN: trainLoss: 0.2742 | trainAcc: 89.0625% (114/128)\n",
            "2 3 Epoch: 91 | ANN: trainLoss: 0.3011 | trainAcc: 88.9535% (153/172)\n",
            "0 2 Epoch: 91 | ANN: testLoss: 1.0139 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 91 | ANN: testLoss: 1.0732 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 92 | ANN: trainLoss: 0.3060 | trainAcc: 90.6250% (58/64)\n",
            "1 3 Epoch: 92 | ANN: trainLoss: 0.2893 | trainAcc: 92.9688% (119/128)\n",
            "2 3 Epoch: 92 | ANN: trainLoss: 0.2765 | trainAcc: 93.0233% (160/172)\n",
            "0 2 Epoch: 92 | ANN: testLoss: 0.9638 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 92 | ANN: testLoss: 1.0772 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 93 | ANN: trainLoss: 0.4074 | trainAcc: 89.0625% (57/64)\n",
            "1 3 Epoch: 93 | ANN: trainLoss: 0.3658 | trainAcc: 89.0625% (114/128)\n",
            "2 3 Epoch: 93 | ANN: trainLoss: 0.3629 | trainAcc: 88.9535% (153/172)\n",
            "0 2 Epoch: 93 | ANN: testLoss: 1.2972 | testAcc: 43.7500% (28/64)\n",
            "1 2 Epoch: 93 | ANN: testLoss: 1.0358 | testAcc: 52.5862% (61/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 94 | ANN: trainLoss: 0.2617 | trainAcc: 95.3125% (61/64)\n",
            "1 3 Epoch: 94 | ANN: trainLoss: 0.2992 | trainAcc: 91.4062% (117/128)\n",
            "2 3 Epoch: 94 | ANN: trainLoss: 0.3460 | trainAcc: 88.3721% (152/172)\n",
            "0 2 Epoch: 94 | ANN: testLoss: 0.9671 | testAcc: 57.8125% (37/64)\n",
            "1 2 Epoch: 94 | ANN: testLoss: 1.0799 | testAcc: 52.5862% (61/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 95 | ANN: trainLoss: 0.3955 | trainAcc: 84.3750% (54/64)\n",
            "1 3 Epoch: 95 | ANN: trainLoss: 0.4775 | trainAcc: 82.8125% (106/128)\n",
            "2 3 Epoch: 95 | ANN: trainLoss: 0.3929 | trainAcc: 87.2093% (150/172)\n",
            "0 2 Epoch: 95 | ANN: testLoss: 1.1065 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 95 | ANN: testLoss: 1.0595 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 96 | ANN: trainLoss: 0.2715 | trainAcc: 87.5000% (56/64)\n",
            "1 3 Epoch: 96 | ANN: trainLoss: 0.2598 | trainAcc: 89.8438% (115/128)\n",
            "2 3 Epoch: 96 | ANN: trainLoss: 0.2611 | trainAcc: 90.6977% (156/172)\n",
            "0 2 Epoch: 96 | ANN: testLoss: 0.9995 | testAcc: 64.0625% (41/64)\n",
            "1 2 Epoch: 96 | ANN: testLoss: 1.0707 | testAcc: 53.4483% (62/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 97 | ANN: trainLoss: 0.4495 | trainAcc: 79.6875% (51/64)\n",
            "1 3 Epoch: 97 | ANN: trainLoss: 0.4090 | trainAcc: 83.5938% (107/128)\n",
            "2 3 Epoch: 97 | ANN: trainLoss: 0.3751 | trainAcc: 85.4651% (147/172)\n",
            "0 2 Epoch: 97 | ANN: testLoss: 0.9622 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 97 | ANN: testLoss: 1.0809 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 98 | ANN: trainLoss: 0.2588 | trainAcc: 93.7500% (60/64)\n",
            "1 3 Epoch: 98 | ANN: trainLoss: 0.2804 | trainAcc: 92.9688% (119/128)\n",
            "2 3 Epoch: 98 | ANN: trainLoss: 0.2941 | trainAcc: 91.2791% (157/172)\n",
            "0 2 Epoch: 98 | ANN: testLoss: 1.0098 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 98 | ANN: testLoss: 1.0743 | testAcc: 55.1724% (64/116)\n",
            "\n",
            "\n",
            "0 3 Epoch: 99 | ANN: trainLoss: 0.3831 | trainAcc: 85.9375% (55/64)\n",
            "1 3 Epoch: 99 | ANN: trainLoss: 0.3254 | trainAcc: 90.6250% (116/128)\n",
            "2 3 Epoch: 99 | ANN: trainLoss: 0.3205 | trainAcc: 90.6977% (156/172)\n",
            "0 2 Epoch: 99 | ANN: testLoss: 1.1049 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 99 | ANN: testLoss: 1.0586 | testAcc: 55.1724% (64/116)\n",
            "---------------------------------------------\n",
            "Converting using MaxNorm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 381.66it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 352.10it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ANN accuracy: Test: 55.1700%\n",
            "SNN accuracy: max_norm: 52.5862%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7o4VT4r0r-xf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}