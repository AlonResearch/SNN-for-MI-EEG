{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlonResearch/SNN-for-MI-EEG/blob/main/MI3_SNNforMIeeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xy9K5bJ9aB7D",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as da\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import scipy.io as scio\n",
        "!pip install spikingjelly -q\n",
        "from spikingjelly.activation_based import ann2snn\n",
        "#!apt install -y graphviz graphviz-dev -q\n",
        "#!pip install nnviz -q\n",
        "#from nnviz import drawing\n",
        "#import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lAEV1BHaBp9",
        "outputId": "053ccbda-01e0-4024-c361-45f1dfa9bc88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wed May  7 12:22:02 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8             10W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CYNshZR5ZhDy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Data loader and other functions\n",
        "#Defining functions\n",
        "\n",
        "def data_loader(data, label, batch=64, shuffle=True, drop=False):\n",
        "    \"\"\"\n",
        "    Preprocess the data to fit model.\n",
        "    Feed data into data_loader.\n",
        "    input:\n",
        "        data (float): samples*length*ch (samples*ch*length).\n",
        "        label (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        batch (int): batch size\n",
        "        shuffle (bool): shuffle data before input into decoder\n",
        "        drop (bool): drop the last samples if True\n",
        "    output:\n",
        "        data loader\n",
        "    \"\"\"\n",
        "    label = torch.LongTensor(label.flatten()).to(device)\n",
        "    if data.shape[1] >= data.shape[2]:\n",
        "        data = torch.tensor(data.swapaxes(1, 2))\n",
        "    data = torch.unsqueeze(data, dim=1).type('torch.FloatTensor').to(device)\n",
        "    data = da.TensorDataset(data, label)\n",
        "    loader = da.DataLoader(dataset=data, batch_size=batch, shuffle=shuffle, drop_last=drop)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def val_snn(Dec, test_loader, T=None):\n",
        "    Dec.eval().to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if T is not None:\n",
        "        corrects = np.zeros(T)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            if T is None:\n",
        "                outputs = Dec(inputs)\n",
        "                correct += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            else:\n",
        "                for m in Dec.modules():\n",
        "                    if hasattr(m, 'reset'):\n",
        "                        m.reset()\n",
        "                for t in range(T):\n",
        "                    if t == 0:\n",
        "                        outputs = Dec(inputs)\n",
        "                    else:\n",
        "                        outputs += Dec(inputs)\n",
        "                    corrects[t] += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            total += targets.shape[0]\n",
        "    return correct / total if T is None else corrects / total\n",
        "\n",
        "\n",
        "def anntosnn(cnn_model, train_x, train_y, test_x, test_y, batch=64, T=None):\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    print('---------------------------------------------')\n",
        "    print('Converting using MaxNorm')\n",
        "    model_converter = ann2snn.Converter(mode='max', dataloader=train_loader)\n",
        "    snn_model = model_converter(cnn_model)\n",
        "    mode_max_accs = val_snn(snn_model, test_loader, T=T)\n",
        "\n",
        "    return mode_max_accs\n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def train_ann(cnn_model, train_x, train_y, test_x, test_y, ep=500, batch=64):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        train_x, test_x (float): samples*length*ch (samples*ch*length).\n",
        "        train_y, test_y (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        ep (int): total train and test epoch\n",
        "        batch (int): batch size\n",
        "    output:\n",
        "        train acc, test acc, weight_file\n",
        "    \"\"\"\n",
        "    # Define training configuration\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.01)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ep)\n",
        "\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "    for epoch in range(ep):\n",
        "        # Train ANN\n",
        "        cnn_model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        print('\\n')\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = cnn_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            print(batch_idx, len(train_loader), 'Epoch: %d | ANN: trainLoss: %.4f | trainAcc: %.4f%% (%d/%d)'\n",
        "                  % (epoch, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        train_acc.append(round(correct / total, 4))\n",
        "\n",
        "        # Test ANN\n",
        "        cnn_model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "                outputs = cnn_model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "                print(batch_idx, len(test_loader), 'Epoch: %d | ANN: testLoss: %.4f | testAcc: %.4f%% (%d/%d)'\n",
        "                      % (epoch, val_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        test_acc.append(round(correct / total, 4))\n",
        "\n",
        "    train_acc = np.asarray(train_acc[-1])\n",
        "    test_acc = np.asarray(test_acc[-1])\n",
        "    return train_acc, test_acc,cnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EC8xjyPDaM6G",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Model definitions\n",
        "# Model 2a\n",
        "\n",
        "class LENet(nn.Module):\n",
        "    \"\"\"\n",
        "        LENet Model\n",
        "    input:\n",
        "         data shape as: batch_size*1*channel*length (64*1*22*1000) BCI IV-2a\n",
        "         batch_size：64\n",
        "         channel：22\n",
        "         length：1000\n",
        "    output:\n",
        "        classes_num\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classes_num=3, channel_count=22, drop_out = 0.5):\n",
        "        super(LENet, self).__init__()\n",
        "        self.drop_out = drop_out\n",
        "\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,64) #\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,32) #\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,16) #\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            # Temporal Convolution block fusion kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            # Spatial Convolution block kernel_size (channel,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(channel_count, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            # Feature Fusion Convolution block kernel_size (1,16) and (1,1) #\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),  #\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.CCB = nn.Sequential(\n",
        "            # Classification Convolution block kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=classes_num,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "        x = self.CCB(x)\n",
        "        return x\n",
        "\n",
        "class LENet_FCL(nn.Module):\n",
        "    def __init__(self, classes_num=3, channel_count=60, drop_out=0.5):\n",
        "        super(LENet_FCL, self).__init__()\n",
        "        self.drop_out = drop_out\n",
        "\n",
        "        # Keep all the convolutional layers the same\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(channel_count, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        # We'll determine the size of the FC layer in the forward pass\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = None\n",
        "        self.classes_num = classes_num\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Create the FC layer on first forward pass if it doesn't exist\n",
        "        if self.fc is None:\n",
        "            in_features = x.shape[1]\n",
        "            self.fc = nn.Linear(in_features, self.classes_num).to(x.device)\n",
        "            # Initialize weights for the new layer\n",
        "            nn.init.kaiming_normal_(self.fc.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            if self.fc.bias is not None:\n",
        "                nn.init.constant_(self.fc.bias, 0)\n",
        "\n",
        "        # Apply the FC layer\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T0hmMue2aMqA",
        "outputId": "63f087b4-078e-474b-d3e0-31f3454a83d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channel count: 62\n",
            "Data shape: (965, 62, 360)\n",
            "Label shape: (965, 1)\n",
            "Class distribution: Rest: 365, Elbow: 300, Hand: 300\n",
            "Balanced class distribution: Rest: 365, Elbow: 300, Hand: 300\n",
            "Balanced data shape: (965, 62, 360)\n",
            "Balanced label shape: (965, 1)\n"
          ]
        }
      ],
      "source": [
        "# @title Loading the data\n",
        "\"\"\"\n",
        "Loading the data\n",
        "\"\"\"\n",
        "\n",
        "# Getting real samples\n",
        "#Locally load the dataset\n",
        "#file = scio.loadmat('Datasets\\BCICIV_2a_gdf\\Derivatives\\A01T.mat')\n",
        "\n",
        "# Google Colab load the dataset\n",
        "file = scio.loadmat('/content/sub-011_eeg90hz_7-35.mat')\n",
        "\n",
        "\n",
        "all_data = file['all_data']\n",
        "all_label = file['all_label']\n",
        "REDUCE_REST = 1\n",
        "\n",
        "# Print data information\n",
        "channel_count = all_data.shape[1]\n",
        "num_classes = len(np.unique(all_label.flatten()))\n",
        "data_length = all_data.shape[2] # Assuming data is samples*ch*length\n",
        "print(f\"Channel count: {channel_count}\")\n",
        "print(f\"Data shape: {all_data.shape}\")\n",
        "print(f\"Label shape: {all_label.shape}\")\n",
        "print(f\"Class distribution: Rest: {np.sum(all_label == 0)}, Elbow: {np.sum(all_label == 1)}, Hand: {np.sum(all_label == 2)}\")\n",
        "\n",
        "# Address class imbalance for 'Rest' class (label 0)\n",
        "label_0_indices = np.where(all_label.flatten() == 0)[0]\n",
        "label_other_indices = np.where(all_label.flatten() != 0)[0]\n",
        "\n",
        "# Randomly select \"REDUCE_REST\"% of label 0 indices\n",
        "num_label_0_to_keep = int(len(label_0_indices) * REDUCE_REST)\n",
        "#np.random.seed(42) # for reproducibility\n",
        "selected_label_0_indices = np.random.choice(label_0_indices, size=num_label_0_to_keep, replace=False)\n",
        "\n",
        "# Combine selected label 0 indices with all other labels\n",
        "balanced_indices = np.concatenate((selected_label_0_indices, label_other_indices))\n",
        "np.random.shuffle(balanced_indices) # Shuffle the combined dataset\n",
        "\n",
        "all_data_balanced = all_data[balanced_indices]\n",
        "all_label_balanced = all_label[balanced_indices]\n",
        "\n",
        "print(f\"Balanced class distribution: Rest: {np.sum(all_label_balanced == 0)}, Elbow: {np.sum(all_label_balanced == 1)}, Hand: {np.sum(all_label_balanced == 2)}\")\n",
        "print(f\"Balanced data shape: {all_data_balanced.shape}\")\n",
        "print(f\"Balanced label shape: {all_label_balanced.shape}\")\n",
        "\n",
        "datasetX = torch.tensor(all_data_balanced, dtype=torch.float32)\n",
        "datasetY = torch.tensor(all_label_balanced, dtype=torch.int64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LENet to SNN Conversion Framework execution\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "TIME_STEPS = 100  # T for SNN\n",
        "TEST_SIZE = 0.2\n",
        "DROP_OUT = 0.25\n",
        "\n",
        "\n",
        "# Split the data\n",
        "print(f\"{100 - (TEST_SIZE * 100)}% of the dataset is used for training and {TEST_SIZE * 100}% is used for testing.\")\n",
        "train_data, test_data, train_label, test_label = train_test_split(datasetX, datasetY, test_size=TEST_SIZE, shuffle=True,\n",
        "                                                                  random_state=0)\n",
        "\n",
        "# Initialize model\n",
        "cnn_model_lenet = LENet(classes_num=3, channel_count=channel_count, drop_out = DROP_OUT).to(device)\n",
        "cnn_model_lenet.apply(initialize_weights)\n",
        "\n",
        "# Train CNN model\n",
        "train_acc, test_acc,  cnn_model_lenet = train_ann(cnn_model_lenet, train_data, train_label, test_data, test_label,\n",
        "                                              ep=EPOCHS, batch=BATCH_SIZE)\n",
        "max_norm_acc = anntosnn( cnn_model_lenet, train_data, train_label, test_data, test_label,\n",
        "                        batch=BATCH_SIZE, T=TIME_STEPS)\n",
        "snn_model_lenet = ann2snn.Converter(mode='max', dataloader=data_loader(train_data, train_label, batch=BATCH_SIZE))( cnn_model_lenet)\n",
        "\n",
        "print('\\n')\n",
        "print('ANN accuracy: Test: %.4f%%' % (test_acc * 100))\n",
        "print('SNN accuracy: max_norm: %.4f%%' % (max_norm_acc[-1] * 100))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JiMa-ylsC2bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae3a6ed-a79a-4de4-d891-49b1fadd2b47"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80.0% of the dataset is used for training and 20.0% is used for testing.\n",
            "\n",
            "\n",
            "0 13 Epoch: 0 | ANN: trainLoss: 1.9497 | trainAcc: 43.7500% (28/64)\n",
            "1 13 Epoch: 0 | ANN: trainLoss: 1.8127 | trainAcc: 38.2812% (49/128)\n",
            "2 13 Epoch: 0 | ANN: trainLoss: 1.6671 | trainAcc: 38.5417% (74/192)\n",
            "3 13 Epoch: 0 | ANN: trainLoss: 1.5789 | trainAcc: 38.2812% (98/256)\n",
            "4 13 Epoch: 0 | ANN: trainLoss: 1.4707 | trainAcc: 40.9375% (131/320)\n",
            "5 13 Epoch: 0 | ANN: trainLoss: 1.4148 | trainAcc: 39.8438% (153/384)\n",
            "6 13 Epoch: 0 | ANN: trainLoss: 1.3749 | trainAcc: 40.1786% (180/448)\n",
            "7 13 Epoch: 0 | ANN: trainLoss: 1.3219 | trainAcc: 42.3828% (217/512)\n",
            "8 13 Epoch: 0 | ANN: trainLoss: 1.2933 | trainAcc: 43.7500% (252/576)\n",
            "9 13 Epoch: 0 | ANN: trainLoss: 1.2668 | trainAcc: 44.3750% (284/640)\n",
            "10 13 Epoch: 0 | ANN: trainLoss: 1.2439 | trainAcc: 45.0284% (317/704)\n",
            "11 13 Epoch: 0 | ANN: trainLoss: 1.2241 | trainAcc: 45.5729% (350/768)\n",
            "12 13 Epoch: 0 | ANN: trainLoss: 1.2243 | trainAcc: 45.4663% (351/772)\n",
            "0 4 Epoch: 0 | ANN: testLoss: 1.2158 | testAcc: 34.3750% (22/64)\n",
            "1 4 Epoch: 0 | ANN: testLoss: 1.1440 | testAcc: 36.7188% (47/128)\n",
            "2 4 Epoch: 0 | ANN: testLoss: 1.1724 | testAcc: 36.9792% (71/192)\n",
            "3 4 Epoch: 0 | ANN: testLoss: 0.9517 | testAcc: 37.3057% (72/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 1 | ANN: trainLoss: 1.1049 | trainAcc: 45.3125% (29/64)\n",
            "1 13 Epoch: 1 | ANN: trainLoss: 1.1034 | trainAcc: 47.6562% (61/128)\n",
            "2 13 Epoch: 1 | ANN: trainLoss: 0.9956 | trainAcc: 53.6458% (103/192)\n",
            "3 13 Epoch: 1 | ANN: trainLoss: 0.9818 | trainAcc: 53.5156% (137/256)\n",
            "4 13 Epoch: 1 | ANN: trainLoss: 0.9668 | trainAcc: 52.8125% (169/320)\n",
            "5 13 Epoch: 1 | ANN: trainLoss: 0.9439 | trainAcc: 54.9479% (211/384)\n",
            "6 13 Epoch: 1 | ANN: trainLoss: 0.9413 | trainAcc: 54.4643% (244/448)\n",
            "7 13 Epoch: 1 | ANN: trainLoss: 0.9436 | trainAcc: 53.9062% (276/512)\n",
            "8 13 Epoch: 1 | ANN: trainLoss: 0.9517 | trainAcc: 52.7778% (304/576)\n",
            "9 13 Epoch: 1 | ANN: trainLoss: 0.9436 | trainAcc: 52.8125% (338/640)\n",
            "10 13 Epoch: 1 | ANN: trainLoss: 0.9325 | trainAcc: 54.1193% (381/704)\n",
            "11 13 Epoch: 1 | ANN: trainLoss: 0.9245 | trainAcc: 54.9479% (422/768)\n",
            "12 13 Epoch: 1 | ANN: trainLoss: 0.9037 | trainAcc: 54.9223% (424/772)\n",
            "0 4 Epoch: 1 | ANN: testLoss: 1.0614 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 1 | ANN: testLoss: 1.0325 | testAcc: 54.6875% (70/128)\n",
            "2 4 Epoch: 1 | ANN: testLoss: 1.0115 | testAcc: 55.7292% (107/192)\n",
            "3 4 Epoch: 1 | ANN: testLoss: 0.8118 | testAcc: 55.9585% (108/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 2 | ANN: trainLoss: 0.8625 | trainAcc: 59.3750% (38/64)\n",
            "1 13 Epoch: 2 | ANN: trainLoss: 0.9209 | trainAcc: 57.8125% (74/128)\n",
            "2 13 Epoch: 2 | ANN: trainLoss: 0.8835 | trainAcc: 58.8542% (113/192)\n",
            "3 13 Epoch: 2 | ANN: trainLoss: 0.8610 | trainAcc: 58.5938% (150/256)\n",
            "4 13 Epoch: 2 | ANN: trainLoss: 0.8743 | trainAcc: 57.1875% (183/320)\n",
            "5 13 Epoch: 2 | ANN: trainLoss: 0.8720 | trainAcc: 57.2917% (220/384)\n",
            "6 13 Epoch: 2 | ANN: trainLoss: 0.8655 | trainAcc: 58.2589% (261/448)\n",
            "7 13 Epoch: 2 | ANN: trainLoss: 0.8471 | trainAcc: 60.1562% (308/512)\n",
            "8 13 Epoch: 2 | ANN: trainLoss: 0.8258 | trainAcc: 61.6319% (355/576)\n",
            "9 13 Epoch: 2 | ANN: trainLoss: 0.8207 | trainAcc: 62.1875% (398/640)\n",
            "10 13 Epoch: 2 | ANN: trainLoss: 0.8122 | trainAcc: 63.4943% (447/704)\n",
            "11 13 Epoch: 2 | ANN: trainLoss: 0.8108 | trainAcc: 63.0208% (484/768)\n",
            "12 13 Epoch: 2 | ANN: trainLoss: 0.8488 | trainAcc: 62.8238% (485/772)\n",
            "0 4 Epoch: 2 | ANN: testLoss: 0.7939 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 2 | ANN: testLoss: 0.8256 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 2 | ANN: testLoss: 0.8209 | testAcc: 63.5417% (122/192)\n",
            "3 4 Epoch: 2 | ANN: testLoss: 0.8019 | testAcc: 63.7306% (123/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 3 | ANN: trainLoss: 0.8380 | trainAcc: 53.1250% (34/64)\n",
            "1 13 Epoch: 3 | ANN: trainLoss: 0.7931 | trainAcc: 60.9375% (78/128)\n",
            "2 13 Epoch: 3 | ANN: trainLoss: 0.7913 | trainAcc: 60.4167% (116/192)\n",
            "3 13 Epoch: 3 | ANN: trainLoss: 0.7587 | trainAcc: 63.6719% (163/256)\n",
            "4 13 Epoch: 3 | ANN: trainLoss: 0.7502 | trainAcc: 63.7500% (204/320)\n",
            "5 13 Epoch: 3 | ANN: trainLoss: 0.7594 | trainAcc: 61.7188% (237/384)\n",
            "6 13 Epoch: 3 | ANN: trainLoss: 0.7477 | trainAcc: 62.7232% (281/448)\n",
            "7 13 Epoch: 3 | ANN: trainLoss: 0.7379 | trainAcc: 63.2812% (324/512)\n",
            "8 13 Epoch: 3 | ANN: trainLoss: 0.7408 | trainAcc: 63.5417% (366/576)\n",
            "9 13 Epoch: 3 | ANN: trainLoss: 0.7393 | trainAcc: 63.9062% (409/640)\n",
            "10 13 Epoch: 3 | ANN: trainLoss: 0.7435 | trainAcc: 63.6364% (448/704)\n",
            "11 13 Epoch: 3 | ANN: trainLoss: 0.7352 | trainAcc: 63.9323% (491/768)\n",
            "12 13 Epoch: 3 | ANN: trainLoss: 0.7264 | trainAcc: 63.8601% (493/772)\n",
            "0 4 Epoch: 3 | ANN: testLoss: 0.7581 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 3 | ANN: testLoss: 0.7220 | testAcc: 60.1562% (77/128)\n",
            "2 4 Epoch: 3 | ANN: testLoss: 0.7493 | testAcc: 59.3750% (114/192)\n",
            "3 4 Epoch: 3 | ANN: testLoss: 0.9609 | testAcc: 59.0674% (114/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 4 | ANN: trainLoss: 0.5962 | trainAcc: 62.5000% (40/64)\n",
            "1 13 Epoch: 4 | ANN: trainLoss: 0.7033 | trainAcc: 63.2812% (81/128)\n",
            "2 13 Epoch: 4 | ANN: trainLoss: 0.6924 | trainAcc: 61.9792% (119/192)\n",
            "3 13 Epoch: 4 | ANN: trainLoss: 0.6832 | trainAcc: 63.6719% (163/256)\n",
            "4 13 Epoch: 4 | ANN: trainLoss: 0.6718 | trainAcc: 63.4375% (203/320)\n",
            "5 13 Epoch: 4 | ANN: trainLoss: 0.6989 | trainAcc: 62.7604% (241/384)\n",
            "6 13 Epoch: 4 | ANN: trainLoss: 0.6978 | trainAcc: 63.3929% (284/448)\n",
            "7 13 Epoch: 4 | ANN: trainLoss: 0.6996 | trainAcc: 63.0859% (323/512)\n",
            "8 13 Epoch: 4 | ANN: trainLoss: 0.6845 | trainAcc: 64.7569% (373/576)\n",
            "9 13 Epoch: 4 | ANN: trainLoss: 0.6851 | trainAcc: 64.5312% (413/640)\n",
            "10 13 Epoch: 4 | ANN: trainLoss: 0.6752 | trainAcc: 65.7670% (463/704)\n",
            "11 13 Epoch: 4 | ANN: trainLoss: 0.6729 | trainAcc: 65.7552% (505/768)\n",
            "12 13 Epoch: 4 | ANN: trainLoss: 0.6963 | trainAcc: 65.6736% (507/772)\n",
            "0 4 Epoch: 4 | ANN: testLoss: 0.6752 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 4 | ANN: testLoss: 0.6622 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 4 | ANN: testLoss: 0.6425 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 4 | ANN: testLoss: 0.5657 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 5 | ANN: trainLoss: 0.5638 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 5 | ANN: trainLoss: 0.5522 | trainAcc: 74.2188% (95/128)\n",
            "2 13 Epoch: 5 | ANN: trainLoss: 0.6352 | trainAcc: 68.2292% (131/192)\n",
            "3 13 Epoch: 5 | ANN: trainLoss: 0.6522 | trainAcc: 66.7969% (171/256)\n",
            "4 13 Epoch: 5 | ANN: trainLoss: 0.6460 | trainAcc: 68.4375% (219/320)\n",
            "5 13 Epoch: 5 | ANN: trainLoss: 0.6408 | trainAcc: 67.4479% (259/384)\n",
            "6 13 Epoch: 5 | ANN: trainLoss: 0.6388 | trainAcc: 67.6339% (303/448)\n",
            "7 13 Epoch: 5 | ANN: trainLoss: 0.6435 | trainAcc: 68.1641% (349/512)\n",
            "8 13 Epoch: 5 | ANN: trainLoss: 0.6381 | trainAcc: 68.2292% (393/576)\n",
            "9 13 Epoch: 5 | ANN: trainLoss: 0.6283 | trainAcc: 69.0625% (442/640)\n",
            "10 13 Epoch: 5 | ANN: trainLoss: 0.6279 | trainAcc: 68.6080% (483/704)\n",
            "11 13 Epoch: 5 | ANN: trainLoss: 0.6286 | trainAcc: 68.8802% (529/768)\n",
            "12 13 Epoch: 5 | ANN: trainLoss: 0.6265 | trainAcc: 69.0415% (533/772)\n",
            "0 4 Epoch: 5 | ANN: testLoss: 0.5647 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 5 | ANN: testLoss: 0.5754 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 5 | ANN: testLoss: 0.5974 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 5 | ANN: testLoss: 0.6763 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 6 | ANN: trainLoss: 0.5906 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 6 | ANN: trainLoss: 0.5655 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 6 | ANN: trainLoss: 0.5574 | trainAcc: 74.4792% (143/192)\n",
            "3 13 Epoch: 6 | ANN: trainLoss: 0.5704 | trainAcc: 74.6094% (191/256)\n",
            "4 13 Epoch: 6 | ANN: trainLoss: 0.5714 | trainAcc: 73.4375% (235/320)\n",
            "5 13 Epoch: 6 | ANN: trainLoss: 0.5888 | trainAcc: 71.3542% (274/384)\n",
            "6 13 Epoch: 6 | ANN: trainLoss: 0.6037 | trainAcc: 70.7589% (317/448)\n",
            "7 13 Epoch: 6 | ANN: trainLoss: 0.5991 | trainAcc: 70.7031% (362/512)\n",
            "8 13 Epoch: 6 | ANN: trainLoss: 0.5972 | trainAcc: 69.9653% (403/576)\n",
            "9 13 Epoch: 6 | ANN: trainLoss: 0.6278 | trainAcc: 69.0625% (442/640)\n",
            "10 13 Epoch: 6 | ANN: trainLoss: 0.6157 | trainAcc: 69.8864% (492/704)\n",
            "11 13 Epoch: 6 | ANN: trainLoss: 0.6174 | trainAcc: 69.6615% (535/768)\n",
            "12 13 Epoch: 6 | ANN: trainLoss: 0.6230 | trainAcc: 69.6891% (538/772)\n",
            "0 4 Epoch: 6 | ANN: testLoss: 0.5145 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 6 | ANN: testLoss: 0.5525 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 6 | ANN: testLoss: 0.5789 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 6 | ANN: testLoss: 0.4983 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 7 | ANN: trainLoss: 0.5277 | trainAcc: 68.7500% (44/64)\n",
            "1 13 Epoch: 7 | ANN: trainLoss: 0.5856 | trainAcc: 68.7500% (88/128)\n",
            "2 13 Epoch: 7 | ANN: trainLoss: 0.5655 | trainAcc: 69.2708% (133/192)\n",
            "3 13 Epoch: 7 | ANN: trainLoss: 0.5932 | trainAcc: 67.9688% (174/256)\n",
            "4 13 Epoch: 7 | ANN: trainLoss: 0.5785 | trainAcc: 67.8125% (217/320)\n",
            "5 13 Epoch: 7 | ANN: trainLoss: 0.5726 | trainAcc: 69.2708% (266/384)\n",
            "6 13 Epoch: 7 | ANN: trainLoss: 0.5627 | trainAcc: 70.7589% (317/448)\n",
            "7 13 Epoch: 7 | ANN: trainLoss: 0.5650 | trainAcc: 71.8750% (368/512)\n",
            "8 13 Epoch: 7 | ANN: trainLoss: 0.5665 | trainAcc: 72.0486% (415/576)\n",
            "9 13 Epoch: 7 | ANN: trainLoss: 0.5640 | trainAcc: 71.7188% (459/640)\n",
            "10 13 Epoch: 7 | ANN: trainLoss: 0.5610 | trainAcc: 71.8750% (506/704)\n",
            "11 13 Epoch: 7 | ANN: trainLoss: 0.5690 | trainAcc: 71.4844% (549/768)\n",
            "12 13 Epoch: 7 | ANN: trainLoss: 0.5657 | trainAcc: 71.5026% (552/772)\n",
            "0 4 Epoch: 7 | ANN: testLoss: 0.6389 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 7 | ANN: testLoss: 0.6377 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 7 | ANN: testLoss: 0.6023 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 7 | ANN: testLoss: 0.6509 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 8 | ANN: trainLoss: 0.6473 | trainAcc: 64.0625% (41/64)\n",
            "1 13 Epoch: 8 | ANN: trainLoss: 0.5956 | trainAcc: 70.3125% (90/128)\n",
            "2 13 Epoch: 8 | ANN: trainLoss: 0.5651 | trainAcc: 73.9583% (142/192)\n",
            "3 13 Epoch: 8 | ANN: trainLoss: 0.5523 | trainAcc: 75.3906% (193/256)\n",
            "4 13 Epoch: 8 | ANN: trainLoss: 0.5872 | trainAcc: 73.1250% (234/320)\n",
            "5 13 Epoch: 8 | ANN: trainLoss: 0.5859 | trainAcc: 71.8750% (276/384)\n",
            "6 13 Epoch: 8 | ANN: trainLoss: 0.5844 | trainAcc: 71.8750% (322/448)\n",
            "7 13 Epoch: 8 | ANN: trainLoss: 0.5899 | trainAcc: 71.6797% (367/512)\n",
            "8 13 Epoch: 8 | ANN: trainLoss: 0.5856 | trainAcc: 71.3542% (411/576)\n",
            "9 13 Epoch: 8 | ANN: trainLoss: 0.5813 | trainAcc: 71.0938% (455/640)\n",
            "10 13 Epoch: 8 | ANN: trainLoss: 0.5738 | trainAcc: 71.1648% (501/704)\n",
            "11 13 Epoch: 8 | ANN: trainLoss: 0.5697 | trainAcc: 71.0938% (546/768)\n",
            "12 13 Epoch: 8 | ANN: trainLoss: 0.6667 | trainAcc: 70.9845% (548/772)\n",
            "0 4 Epoch: 8 | ANN: testLoss: 0.6501 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 8 | ANN: testLoss: 0.6155 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 8 | ANN: testLoss: 0.6216 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 8 | ANN: testLoss: 0.7294 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 9 | ANN: trainLoss: 0.6023 | trainAcc: 75.0000% (48/64)\n",
            "1 13 Epoch: 9 | ANN: trainLoss: 0.6017 | trainAcc: 73.4375% (94/128)\n",
            "2 13 Epoch: 9 | ANN: trainLoss: 0.5824 | trainAcc: 71.3542% (137/192)\n",
            "3 13 Epoch: 9 | ANN: trainLoss: 0.6152 | trainAcc: 69.9219% (179/256)\n",
            "4 13 Epoch: 9 | ANN: trainLoss: 0.5807 | trainAcc: 71.8750% (230/320)\n",
            "5 13 Epoch: 9 | ANN: trainLoss: 0.5800 | trainAcc: 72.3958% (278/384)\n",
            "6 13 Epoch: 9 | ANN: trainLoss: 0.5949 | trainAcc: 71.6518% (321/448)\n",
            "7 13 Epoch: 9 | ANN: trainLoss: 0.6061 | trainAcc: 71.0938% (364/512)\n",
            "8 13 Epoch: 9 | ANN: trainLoss: 0.5944 | trainAcc: 72.2222% (416/576)\n",
            "9 13 Epoch: 9 | ANN: trainLoss: 0.5949 | trainAcc: 71.5625% (458/640)\n",
            "10 13 Epoch: 9 | ANN: trainLoss: 0.5874 | trainAcc: 72.0170% (507/704)\n",
            "11 13 Epoch: 9 | ANN: trainLoss: 0.5820 | trainAcc: 72.3958% (556/768)\n",
            "12 13 Epoch: 9 | ANN: trainLoss: 0.5800 | trainAcc: 72.4093% (559/772)\n",
            "0 4 Epoch: 9 | ANN: testLoss: 0.5496 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 9 | ANN: testLoss: 0.6188 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 9 | ANN: testLoss: 0.6351 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 9 | ANN: testLoss: 0.4765 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 10 | ANN: trainLoss: 0.5351 | trainAcc: 75.0000% (48/64)\n",
            "1 13 Epoch: 10 | ANN: trainLoss: 0.5292 | trainAcc: 73.4375% (94/128)\n",
            "2 13 Epoch: 10 | ANN: trainLoss: 0.5513 | trainAcc: 71.3542% (137/192)\n",
            "3 13 Epoch: 10 | ANN: trainLoss: 0.5853 | trainAcc: 68.7500% (176/256)\n",
            "4 13 Epoch: 10 | ANN: trainLoss: 0.5998 | trainAcc: 69.3750% (222/320)\n",
            "5 13 Epoch: 10 | ANN: trainLoss: 0.5809 | trainAcc: 70.8333% (272/384)\n",
            "6 13 Epoch: 10 | ANN: trainLoss: 0.5910 | trainAcc: 70.0893% (314/448)\n",
            "7 13 Epoch: 10 | ANN: trainLoss: 0.5854 | trainAcc: 70.3125% (360/512)\n",
            "8 13 Epoch: 10 | ANN: trainLoss: 0.5800 | trainAcc: 70.3125% (405/576)\n",
            "9 13 Epoch: 10 | ANN: trainLoss: 0.5718 | trainAcc: 70.1562% (449/640)\n",
            "10 13 Epoch: 10 | ANN: trainLoss: 0.5616 | trainAcc: 70.7386% (498/704)\n",
            "11 13 Epoch: 10 | ANN: trainLoss: 0.5571 | trainAcc: 70.9635% (545/768)\n",
            "12 13 Epoch: 10 | ANN: trainLoss: 0.5440 | trainAcc: 70.9845% (548/772)\n",
            "0 4 Epoch: 10 | ANN: testLoss: 0.5902 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 10 | ANN: testLoss: 0.5585 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 10 | ANN: testLoss: 0.5518 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 10 | ANN: testLoss: 0.5572 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 11 | ANN: trainLoss: 0.4831 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 11 | ANN: trainLoss: 0.4953 | trainAcc: 74.2188% (95/128)\n",
            "2 13 Epoch: 11 | ANN: trainLoss: 0.4715 | trainAcc: 77.0833% (148/192)\n",
            "3 13 Epoch: 11 | ANN: trainLoss: 0.4665 | trainAcc: 76.9531% (197/256)\n",
            "4 13 Epoch: 11 | ANN: trainLoss: 0.4530 | trainAcc: 77.5000% (248/320)\n",
            "5 13 Epoch: 11 | ANN: trainLoss: 0.4589 | trainAcc: 77.0833% (296/384)\n",
            "6 13 Epoch: 11 | ANN: trainLoss: 0.4570 | trainAcc: 77.2321% (346/448)\n",
            "7 13 Epoch: 11 | ANN: trainLoss: 0.4687 | trainAcc: 75.9766% (389/512)\n",
            "8 13 Epoch: 11 | ANN: trainLoss: 0.4751 | trainAcc: 75.5208% (435/576)\n",
            "9 13 Epoch: 11 | ANN: trainLoss: 0.4811 | trainAcc: 75.0000% (480/640)\n",
            "10 13 Epoch: 11 | ANN: trainLoss: 0.4949 | trainAcc: 73.8636% (520/704)\n",
            "11 13 Epoch: 11 | ANN: trainLoss: 0.4996 | trainAcc: 73.6979% (566/768)\n",
            "12 13 Epoch: 11 | ANN: trainLoss: 0.5048 | trainAcc: 73.5751% (568/772)\n",
            "0 4 Epoch: 11 | ANN: testLoss: 0.5772 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 11 | ANN: testLoss: 0.5471 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 11 | ANN: testLoss: 0.5592 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 11 | ANN: testLoss: 0.4219 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 12 | ANN: trainLoss: 0.4387 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 12 | ANN: trainLoss: 0.4754 | trainAcc: 78.1250% (100/128)\n",
            "2 13 Epoch: 12 | ANN: trainLoss: 0.4835 | trainAcc: 76.5625% (147/192)\n",
            "3 13 Epoch: 12 | ANN: trainLoss: 0.4661 | trainAcc: 78.9062% (202/256)\n",
            "4 13 Epoch: 12 | ANN: trainLoss: 0.4756 | trainAcc: 78.7500% (252/320)\n",
            "5 13 Epoch: 12 | ANN: trainLoss: 0.4860 | trainAcc: 77.6042% (298/384)\n",
            "6 13 Epoch: 12 | ANN: trainLoss: 0.4942 | trainAcc: 77.2321% (346/448)\n",
            "7 13 Epoch: 12 | ANN: trainLoss: 0.5041 | trainAcc: 76.1719% (390/512)\n",
            "8 13 Epoch: 12 | ANN: trainLoss: 0.5016 | trainAcc: 75.8681% (437/576)\n",
            "9 13 Epoch: 12 | ANN: trainLoss: 0.4965 | trainAcc: 75.6250% (484/640)\n",
            "10 13 Epoch: 12 | ANN: trainLoss: 0.4981 | trainAcc: 74.8580% (527/704)\n",
            "11 13 Epoch: 12 | ANN: trainLoss: 0.4907 | trainAcc: 75.3906% (579/768)\n",
            "12 13 Epoch: 12 | ANN: trainLoss: 0.4828 | trainAcc: 75.5181% (583/772)\n",
            "0 4 Epoch: 12 | ANN: testLoss: 0.6966 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 12 | ANN: testLoss: 0.7485 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 12 | ANN: testLoss: 0.7708 | testAcc: 64.0625% (123/192)\n",
            "3 4 Epoch: 12 | ANN: testLoss: 1.0127 | testAcc: 63.7306% (123/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 13 | ANN: trainLoss: 0.4114 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 13 | ANN: trainLoss: 0.4877 | trainAcc: 75.7812% (97/128)\n",
            "2 13 Epoch: 13 | ANN: trainLoss: 0.5643 | trainAcc: 71.8750% (138/192)\n",
            "3 13 Epoch: 13 | ANN: trainLoss: 0.5274 | trainAcc: 75.0000% (192/256)\n",
            "4 13 Epoch: 13 | ANN: trainLoss: 0.5182 | trainAcc: 74.3750% (238/320)\n",
            "5 13 Epoch: 13 | ANN: trainLoss: 0.5074 | trainAcc: 73.9583% (284/384)\n",
            "6 13 Epoch: 13 | ANN: trainLoss: 0.5075 | trainAcc: 72.9911% (327/448)\n",
            "7 13 Epoch: 13 | ANN: trainLoss: 0.5016 | trainAcc: 73.8281% (378/512)\n",
            "8 13 Epoch: 13 | ANN: trainLoss: 0.5040 | trainAcc: 74.3056% (428/576)\n",
            "9 13 Epoch: 13 | ANN: trainLoss: 0.5030 | trainAcc: 74.0625% (474/640)\n",
            "10 13 Epoch: 13 | ANN: trainLoss: 0.5078 | trainAcc: 73.8636% (520/704)\n",
            "11 13 Epoch: 13 | ANN: trainLoss: 0.5047 | trainAcc: 74.0885% (569/768)\n",
            "12 13 Epoch: 13 | ANN: trainLoss: 0.5195 | trainAcc: 73.9637% (571/772)\n",
            "0 4 Epoch: 13 | ANN: testLoss: 0.5299 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 13 | ANN: testLoss: 0.5277 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 13 | ANN: testLoss: 0.5144 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 13 | ANN: testLoss: 0.4977 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 14 | ANN: trainLoss: 0.3980 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 14 | ANN: trainLoss: 0.4600 | trainAcc: 77.3438% (99/128)\n",
            "2 13 Epoch: 14 | ANN: trainLoss: 0.4557 | trainAcc: 77.0833% (148/192)\n",
            "3 13 Epoch: 14 | ANN: trainLoss: 0.5043 | trainAcc: 73.8281% (189/256)\n",
            "4 13 Epoch: 14 | ANN: trainLoss: 0.4941 | trainAcc: 75.6250% (242/320)\n",
            "5 13 Epoch: 14 | ANN: trainLoss: 0.5049 | trainAcc: 75.0000% (288/384)\n",
            "6 13 Epoch: 14 | ANN: trainLoss: 0.5054 | trainAcc: 74.3304% (333/448)\n",
            "7 13 Epoch: 14 | ANN: trainLoss: 0.5395 | trainAcc: 72.8516% (373/512)\n",
            "8 13 Epoch: 14 | ANN: trainLoss: 0.5430 | trainAcc: 73.4375% (423/576)\n",
            "9 13 Epoch: 14 | ANN: trainLoss: 0.5339 | trainAcc: 73.9062% (473/640)\n",
            "10 13 Epoch: 14 | ANN: trainLoss: 0.5342 | trainAcc: 73.2955% (516/704)\n",
            "11 13 Epoch: 14 | ANN: trainLoss: 0.5398 | trainAcc: 72.7865% (559/768)\n",
            "12 13 Epoch: 14 | ANN: trainLoss: 0.5284 | trainAcc: 72.7979% (562/772)\n",
            "0 4 Epoch: 14 | ANN: testLoss: 0.5169 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 14 | ANN: testLoss: 0.5392 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 14 | ANN: testLoss: 0.5450 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 14 | ANN: testLoss: 0.7432 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 15 | ANN: trainLoss: 0.7881 | trainAcc: 65.6250% (42/64)\n",
            "1 13 Epoch: 15 | ANN: trainLoss: 0.6032 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 15 | ANN: trainLoss: 0.5555 | trainAcc: 77.6042% (149/192)\n",
            "3 13 Epoch: 15 | ANN: trainLoss: 0.5350 | trainAcc: 77.3438% (198/256)\n",
            "4 13 Epoch: 15 | ANN: trainLoss: 0.5292 | trainAcc: 76.5625% (245/320)\n",
            "5 13 Epoch: 15 | ANN: trainLoss: 0.5337 | trainAcc: 75.2604% (289/384)\n",
            "6 13 Epoch: 15 | ANN: trainLoss: 0.5319 | trainAcc: 74.5536% (334/448)\n",
            "7 13 Epoch: 15 | ANN: trainLoss: 0.5152 | trainAcc: 75.1953% (385/512)\n",
            "8 13 Epoch: 15 | ANN: trainLoss: 0.5130 | trainAcc: 75.1736% (433/576)\n",
            "9 13 Epoch: 15 | ANN: trainLoss: 0.5106 | trainAcc: 75.4688% (483/640)\n",
            "10 13 Epoch: 15 | ANN: trainLoss: 0.5016 | trainAcc: 76.5625% (539/704)\n",
            "11 13 Epoch: 15 | ANN: trainLoss: 0.5014 | trainAcc: 75.7812% (582/768)\n",
            "12 13 Epoch: 15 | ANN: trainLoss: 0.4987 | trainAcc: 75.7772% (585/772)\n",
            "0 4 Epoch: 15 | ANN: testLoss: 0.5351 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 15 | ANN: testLoss: 0.5722 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 15 | ANN: testLoss: 0.5414 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 15 | ANN: testLoss: 0.5215 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 16 | ANN: trainLoss: 0.4050 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 16 | ANN: trainLoss: 0.3741 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 16 | ANN: trainLoss: 0.4931 | trainAcc: 77.0833% (148/192)\n",
            "3 13 Epoch: 16 | ANN: trainLoss: 0.5025 | trainAcc: 74.6094% (191/256)\n",
            "4 13 Epoch: 16 | ANN: trainLoss: 0.5202 | trainAcc: 74.3750% (238/320)\n",
            "5 13 Epoch: 16 | ANN: trainLoss: 0.4937 | trainAcc: 76.0417% (292/384)\n",
            "6 13 Epoch: 16 | ANN: trainLoss: 0.5018 | trainAcc: 75.4464% (338/448)\n",
            "7 13 Epoch: 16 | ANN: trainLoss: 0.4882 | trainAcc: 75.9766% (389/512)\n",
            "8 13 Epoch: 16 | ANN: trainLoss: 0.4770 | trainAcc: 76.9097% (443/576)\n",
            "9 13 Epoch: 16 | ANN: trainLoss: 0.4714 | trainAcc: 77.5000% (496/640)\n",
            "10 13 Epoch: 16 | ANN: trainLoss: 0.4773 | trainAcc: 76.9886% (542/704)\n",
            "11 13 Epoch: 16 | ANN: trainLoss: 0.4764 | trainAcc: 76.3021% (586/768)\n",
            "12 13 Epoch: 16 | ANN: trainLoss: 0.5032 | trainAcc: 76.1658% (588/772)\n",
            "0 4 Epoch: 16 | ANN: testLoss: 0.7954 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 16 | ANN: testLoss: 0.6673 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 16 | ANN: testLoss: 0.6480 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 16 | ANN: testLoss: 0.5256 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 17 | ANN: trainLoss: 0.4107 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 17 | ANN: trainLoss: 0.4686 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 17 | ANN: trainLoss: 0.5403 | trainAcc: 75.0000% (144/192)\n",
            "3 13 Epoch: 17 | ANN: trainLoss: 0.5215 | trainAcc: 76.1719% (195/256)\n",
            "4 13 Epoch: 17 | ANN: trainLoss: 0.5115 | trainAcc: 76.8750% (246/320)\n",
            "5 13 Epoch: 17 | ANN: trainLoss: 0.5488 | trainAcc: 75.2604% (289/384)\n",
            "6 13 Epoch: 17 | ANN: trainLoss: 0.5610 | trainAcc: 73.2143% (328/448)\n",
            "7 13 Epoch: 17 | ANN: trainLoss: 0.5426 | trainAcc: 74.2188% (380/512)\n",
            "8 13 Epoch: 17 | ANN: trainLoss: 0.5339 | trainAcc: 73.6111% (424/576)\n",
            "9 13 Epoch: 17 | ANN: trainLoss: 0.5273 | trainAcc: 74.2188% (475/640)\n",
            "10 13 Epoch: 17 | ANN: trainLoss: 0.5293 | trainAcc: 74.0057% (521/704)\n",
            "11 13 Epoch: 17 | ANN: trainLoss: 0.5178 | trainAcc: 73.6979% (566/768)\n",
            "12 13 Epoch: 17 | ANN: trainLoss: 0.5401 | trainAcc: 73.7047% (569/772)\n",
            "0 4 Epoch: 17 | ANN: testLoss: 0.5616 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 17 | ANN: testLoss: 0.5349 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 17 | ANN: testLoss: 0.5299 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 17 | ANN: testLoss: 0.4000 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 18 | ANN: trainLoss: 0.5213 | trainAcc: 73.4375% (47/64)\n",
            "1 13 Epoch: 18 | ANN: trainLoss: 0.4851 | trainAcc: 76.5625% (98/128)\n",
            "2 13 Epoch: 18 | ANN: trainLoss: 0.4816 | trainAcc: 77.0833% (148/192)\n",
            "3 13 Epoch: 18 | ANN: trainLoss: 0.4758 | trainAcc: 78.1250% (200/256)\n",
            "4 13 Epoch: 18 | ANN: trainLoss: 0.4697 | trainAcc: 77.8125% (249/320)\n",
            "5 13 Epoch: 18 | ANN: trainLoss: 0.4727 | trainAcc: 77.3438% (297/384)\n",
            "6 13 Epoch: 18 | ANN: trainLoss: 0.4666 | trainAcc: 77.9018% (349/448)\n",
            "7 13 Epoch: 18 | ANN: trainLoss: 0.4699 | trainAcc: 77.9297% (399/512)\n",
            "8 13 Epoch: 18 | ANN: trainLoss: 0.4740 | trainAcc: 77.4306% (446/576)\n",
            "9 13 Epoch: 18 | ANN: trainLoss: 0.4887 | trainAcc: 77.0312% (493/640)\n",
            "10 13 Epoch: 18 | ANN: trainLoss: 0.4870 | trainAcc: 76.8466% (541/704)\n",
            "11 13 Epoch: 18 | ANN: trainLoss: 0.4918 | trainAcc: 76.3021% (586/768)\n",
            "12 13 Epoch: 18 | ANN: trainLoss: 0.4811 | trainAcc: 76.2953% (589/772)\n",
            "0 4 Epoch: 18 | ANN: testLoss: 0.6182 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 18 | ANN: testLoss: 0.7579 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 18 | ANN: testLoss: 0.6671 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 18 | ANN: testLoss: 0.5004 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 19 | ANN: trainLoss: 0.4444 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 19 | ANN: trainLoss: 0.4547 | trainAcc: 77.3438% (99/128)\n",
            "2 13 Epoch: 19 | ANN: trainLoss: 0.4504 | trainAcc: 76.5625% (147/192)\n",
            "3 13 Epoch: 19 | ANN: trainLoss: 0.4704 | trainAcc: 75.0000% (192/256)\n",
            "4 13 Epoch: 19 | ANN: trainLoss: 0.4668 | trainAcc: 76.2500% (244/320)\n",
            "5 13 Epoch: 19 | ANN: trainLoss: 0.4641 | trainAcc: 76.8229% (295/384)\n",
            "6 13 Epoch: 19 | ANN: trainLoss: 0.4583 | trainAcc: 77.6786% (348/448)\n",
            "7 13 Epoch: 19 | ANN: trainLoss: 0.4512 | trainAcc: 77.7344% (398/512)\n",
            "8 13 Epoch: 19 | ANN: trainLoss: 0.4461 | trainAcc: 77.9514% (449/576)\n",
            "9 13 Epoch: 19 | ANN: trainLoss: 0.4458 | trainAcc: 78.2812% (501/640)\n",
            "10 13 Epoch: 19 | ANN: trainLoss: 0.4446 | trainAcc: 78.2670% (551/704)\n",
            "11 13 Epoch: 19 | ANN: trainLoss: 0.4494 | trainAcc: 78.1250% (600/768)\n",
            "12 13 Epoch: 19 | ANN: trainLoss: 0.6439 | trainAcc: 77.8497% (601/772)\n",
            "0 4 Epoch: 19 | ANN: testLoss: 0.4553 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 19 | ANN: testLoss: 0.5393 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 19 | ANN: testLoss: 0.5073 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 19 | ANN: testLoss: 0.3971 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 20 | ANN: trainLoss: 0.3937 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 20 | ANN: trainLoss: 0.4642 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 20 | ANN: trainLoss: 0.4551 | trainAcc: 76.5625% (147/192)\n",
            "3 13 Epoch: 20 | ANN: trainLoss: 0.4649 | trainAcc: 76.5625% (196/256)\n",
            "4 13 Epoch: 20 | ANN: trainLoss: 0.4604 | trainAcc: 77.8125% (249/320)\n",
            "5 13 Epoch: 20 | ANN: trainLoss: 0.4771 | trainAcc: 78.3854% (301/384)\n",
            "6 13 Epoch: 20 | ANN: trainLoss: 0.4910 | trainAcc: 77.4554% (347/448)\n",
            "7 13 Epoch: 20 | ANN: trainLoss: 0.5013 | trainAcc: 77.1484% (395/512)\n",
            "8 13 Epoch: 20 | ANN: trainLoss: 0.5033 | trainAcc: 77.0833% (444/576)\n",
            "9 13 Epoch: 20 | ANN: trainLoss: 0.5161 | trainAcc: 76.8750% (492/640)\n",
            "10 13 Epoch: 20 | ANN: trainLoss: 0.5179 | trainAcc: 76.1364% (536/704)\n",
            "11 13 Epoch: 20 | ANN: trainLoss: 0.5170 | trainAcc: 76.4323% (587/768)\n",
            "12 13 Epoch: 20 | ANN: trainLoss: 0.5195 | trainAcc: 76.4249% (590/772)\n",
            "0 4 Epoch: 20 | ANN: testLoss: 0.5611 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 20 | ANN: testLoss: 0.5324 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 20 | ANN: testLoss: 0.5182 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 20 | ANN: testLoss: 0.4820 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 21 | ANN: trainLoss: 0.5038 | trainAcc: 71.8750% (46/64)\n",
            "1 13 Epoch: 21 | ANN: trainLoss: 0.5327 | trainAcc: 74.2188% (95/128)\n",
            "2 13 Epoch: 21 | ANN: trainLoss: 0.5454 | trainAcc: 74.4792% (143/192)\n",
            "3 13 Epoch: 21 | ANN: trainLoss: 0.5255 | trainAcc: 76.1719% (195/256)\n",
            "4 13 Epoch: 21 | ANN: trainLoss: 0.5382 | trainAcc: 75.9375% (243/320)\n",
            "5 13 Epoch: 21 | ANN: trainLoss: 0.5358 | trainAcc: 76.5625% (294/384)\n",
            "6 13 Epoch: 21 | ANN: trainLoss: 0.5182 | trainAcc: 76.7857% (344/448)\n",
            "7 13 Epoch: 21 | ANN: trainLoss: 0.5276 | trainAcc: 76.1719% (390/512)\n",
            "8 13 Epoch: 21 | ANN: trainLoss: 0.5248 | trainAcc: 76.7361% (442/576)\n",
            "9 13 Epoch: 21 | ANN: trainLoss: 0.5171 | trainAcc: 76.7188% (491/640)\n",
            "10 13 Epoch: 21 | ANN: trainLoss: 0.5150 | trainAcc: 76.7045% (540/704)\n",
            "11 13 Epoch: 21 | ANN: trainLoss: 0.5124 | trainAcc: 76.5625% (588/768)\n",
            "12 13 Epoch: 21 | ANN: trainLoss: 0.4989 | trainAcc: 76.6839% (592/772)\n",
            "0 4 Epoch: 21 | ANN: testLoss: 0.4851 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 21 | ANN: testLoss: 0.5122 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 21 | ANN: testLoss: 0.5229 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 21 | ANN: testLoss: 0.4065 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 22 | ANN: trainLoss: 0.4556 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 22 | ANN: trainLoss: 0.4280 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 22 | ANN: trainLoss: 0.4248 | trainAcc: 80.7292% (155/192)\n",
            "3 13 Epoch: 22 | ANN: trainLoss: 0.4133 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 22 | ANN: trainLoss: 0.3993 | trainAcc: 83.7500% (268/320)\n",
            "5 13 Epoch: 22 | ANN: trainLoss: 0.4174 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 22 | ANN: trainLoss: 0.4105 | trainAcc: 82.5893% (370/448)\n",
            "7 13 Epoch: 22 | ANN: trainLoss: 0.4170 | trainAcc: 81.2500% (416/512)\n",
            "8 13 Epoch: 22 | ANN: trainLoss: 0.4119 | trainAcc: 81.7708% (471/576)\n",
            "9 13 Epoch: 22 | ANN: trainLoss: 0.4181 | trainAcc: 81.7188% (523/640)\n",
            "10 13 Epoch: 22 | ANN: trainLoss: 0.4226 | trainAcc: 81.3920% (573/704)\n",
            "11 13 Epoch: 22 | ANN: trainLoss: 0.4324 | trainAcc: 80.3385% (617/768)\n",
            "12 13 Epoch: 22 | ANN: trainLoss: 0.4303 | trainAcc: 80.3109% (620/772)\n",
            "0 4 Epoch: 22 | ANN: testLoss: 0.6141 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 22 | ANN: testLoss: 0.6272 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 22 | ANN: testLoss: 0.6163 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 22 | ANN: testLoss: 0.5310 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 23 | ANN: trainLoss: 0.4382 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 23 | ANN: trainLoss: 0.3963 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 23 | ANN: trainLoss: 0.4006 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 23 | ANN: trainLoss: 0.4023 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 23 | ANN: trainLoss: 0.4023 | trainAcc: 82.5000% (264/320)\n",
            "5 13 Epoch: 23 | ANN: trainLoss: 0.3944 | trainAcc: 83.3333% (320/384)\n",
            "6 13 Epoch: 23 | ANN: trainLoss: 0.3913 | trainAcc: 82.8125% (371/448)\n",
            "7 13 Epoch: 23 | ANN: trainLoss: 0.3924 | trainAcc: 82.6172% (423/512)\n",
            "8 13 Epoch: 23 | ANN: trainLoss: 0.4108 | trainAcc: 81.9444% (472/576)\n",
            "9 13 Epoch: 23 | ANN: trainLoss: 0.4133 | trainAcc: 81.0938% (519/640)\n",
            "10 13 Epoch: 23 | ANN: trainLoss: 0.4141 | trainAcc: 81.1080% (571/704)\n",
            "11 13 Epoch: 23 | ANN: trainLoss: 0.4222 | trainAcc: 80.3385% (617/768)\n",
            "12 13 Epoch: 23 | ANN: trainLoss: 0.4290 | trainAcc: 80.3109% (620/772)\n",
            "0 4 Epoch: 23 | ANN: testLoss: 0.5579 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 23 | ANN: testLoss: 0.5059 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 23 | ANN: testLoss: 0.5244 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 23 | ANN: testLoss: 0.3957 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 24 | ANN: trainLoss: 0.3577 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 24 | ANN: trainLoss: 0.4182 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 24 | ANN: trainLoss: 0.3936 | trainAcc: 83.3333% (160/192)\n",
            "3 13 Epoch: 24 | ANN: trainLoss: 0.3824 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 24 | ANN: trainLoss: 0.3907 | trainAcc: 83.4375% (267/320)\n",
            "5 13 Epoch: 24 | ANN: trainLoss: 0.4019 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 24 | ANN: trainLoss: 0.4007 | trainAcc: 82.1429% (368/448)\n",
            "7 13 Epoch: 24 | ANN: trainLoss: 0.3995 | trainAcc: 82.8125% (424/512)\n",
            "8 13 Epoch: 24 | ANN: trainLoss: 0.4064 | trainAcc: 82.4653% (475/576)\n",
            "9 13 Epoch: 24 | ANN: trainLoss: 0.4055 | trainAcc: 82.1875% (526/640)\n",
            "10 13 Epoch: 24 | ANN: trainLoss: 0.4185 | trainAcc: 80.9659% (570/704)\n",
            "11 13 Epoch: 24 | ANN: trainLoss: 0.4176 | trainAcc: 80.9896% (622/768)\n",
            "12 13 Epoch: 24 | ANN: trainLoss: 0.4127 | trainAcc: 81.0881% (626/772)\n",
            "0 4 Epoch: 24 | ANN: testLoss: 0.5843 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 24 | ANN: testLoss: 0.4891 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 24 | ANN: testLoss: 0.4991 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 24 | ANN: testLoss: 0.5796 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 25 | ANN: trainLoss: 0.3438 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 25 | ANN: trainLoss: 0.3840 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 25 | ANN: trainLoss: 0.3681 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 25 | ANN: trainLoss: 0.3976 | trainAcc: 79.6875% (204/256)\n",
            "4 13 Epoch: 25 | ANN: trainLoss: 0.4086 | trainAcc: 79.6875% (255/320)\n",
            "5 13 Epoch: 25 | ANN: trainLoss: 0.4156 | trainAcc: 80.2083% (308/384)\n",
            "6 13 Epoch: 25 | ANN: trainLoss: 0.4129 | trainAcc: 79.9107% (358/448)\n",
            "7 13 Epoch: 25 | ANN: trainLoss: 0.4066 | trainAcc: 79.8828% (409/512)\n",
            "8 13 Epoch: 25 | ANN: trainLoss: 0.4074 | trainAcc: 79.8611% (460/576)\n",
            "9 13 Epoch: 25 | ANN: trainLoss: 0.4157 | trainAcc: 79.5312% (509/640)\n",
            "10 13 Epoch: 25 | ANN: trainLoss: 0.4072 | trainAcc: 80.1136% (564/704)\n",
            "11 13 Epoch: 25 | ANN: trainLoss: 0.4127 | trainAcc: 79.5573% (611/768)\n",
            "12 13 Epoch: 25 | ANN: trainLoss: 0.4106 | trainAcc: 79.5337% (614/772)\n",
            "0 4 Epoch: 25 | ANN: testLoss: 0.4494 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 25 | ANN: testLoss: 0.4982 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 25 | ANN: testLoss: 0.5094 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 25 | ANN: testLoss: 0.7962 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 26 | ANN: trainLoss: 0.3707 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 26 | ANN: trainLoss: 0.3972 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 26 | ANN: trainLoss: 0.4152 | trainAcc: 80.7292% (155/192)\n",
            "3 13 Epoch: 26 | ANN: trainLoss: 0.4046 | trainAcc: 82.0312% (210/256)\n",
            "4 13 Epoch: 26 | ANN: trainLoss: 0.4157 | trainAcc: 81.5625% (261/320)\n",
            "5 13 Epoch: 26 | ANN: trainLoss: 0.4122 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 26 | ANN: trainLoss: 0.4051 | trainAcc: 82.5893% (370/448)\n",
            "7 13 Epoch: 26 | ANN: trainLoss: 0.3990 | trainAcc: 82.2266% (421/512)\n",
            "8 13 Epoch: 26 | ANN: trainLoss: 0.4015 | trainAcc: 81.5972% (470/576)\n",
            "9 13 Epoch: 26 | ANN: trainLoss: 0.3951 | trainAcc: 82.1875% (526/640)\n",
            "10 13 Epoch: 26 | ANN: trainLoss: 0.4029 | trainAcc: 82.1023% (578/704)\n",
            "11 13 Epoch: 26 | ANN: trainLoss: 0.4012 | trainAcc: 81.9010% (629/768)\n",
            "12 13 Epoch: 26 | ANN: trainLoss: 0.4841 | trainAcc: 81.7358% (631/772)\n",
            "0 4 Epoch: 26 | ANN: testLoss: 0.5967 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 26 | ANN: testLoss: 0.6437 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 26 | ANN: testLoss: 0.5955 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 26 | ANN: testLoss: 0.7556 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 27 | ANN: trainLoss: 0.3617 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 27 | ANN: trainLoss: 0.4049 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 27 | ANN: trainLoss: 0.4666 | trainAcc: 79.1667% (152/192)\n",
            "3 13 Epoch: 27 | ANN: trainLoss: 0.4748 | trainAcc: 80.4688% (206/256)\n",
            "4 13 Epoch: 27 | ANN: trainLoss: 0.4964 | trainAcc: 79.0625% (253/320)\n",
            "5 13 Epoch: 27 | ANN: trainLoss: 0.4987 | trainAcc: 78.6458% (302/384)\n",
            "6 13 Epoch: 27 | ANN: trainLoss: 0.5083 | trainAcc: 78.1250% (350/448)\n",
            "7 13 Epoch: 27 | ANN: trainLoss: 0.5068 | trainAcc: 77.5391% (397/512)\n",
            "8 13 Epoch: 27 | ANN: trainLoss: 0.4963 | trainAcc: 77.4306% (446/576)\n",
            "9 13 Epoch: 27 | ANN: trainLoss: 0.4873 | trainAcc: 78.1250% (500/640)\n",
            "10 13 Epoch: 27 | ANN: trainLoss: 0.4768 | trainAcc: 78.6932% (554/704)\n",
            "11 13 Epoch: 27 | ANN: trainLoss: 0.4746 | trainAcc: 79.0365% (607/768)\n",
            "12 13 Epoch: 27 | ANN: trainLoss: 0.4967 | trainAcc: 78.8860% (609/772)\n",
            "0 4 Epoch: 27 | ANN: testLoss: 0.6333 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 27 | ANN: testLoss: 0.6998 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 27 | ANN: testLoss: 0.7468 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 27 | ANN: testLoss: 1.0313 | testAcc: 65.2850% (126/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 28 | ANN: trainLoss: 0.4595 | trainAcc: 75.0000% (48/64)\n",
            "1 13 Epoch: 28 | ANN: trainLoss: 0.4695 | trainAcc: 73.4375% (94/128)\n",
            "2 13 Epoch: 28 | ANN: trainLoss: 0.4407 | trainAcc: 76.0417% (146/192)\n",
            "3 13 Epoch: 28 | ANN: trainLoss: 0.4637 | trainAcc: 76.5625% (196/256)\n",
            "4 13 Epoch: 28 | ANN: trainLoss: 0.4541 | trainAcc: 77.5000% (248/320)\n",
            "5 13 Epoch: 28 | ANN: trainLoss: 0.4644 | trainAcc: 76.8229% (295/384)\n",
            "6 13 Epoch: 28 | ANN: trainLoss: 0.4562 | trainAcc: 77.6786% (348/448)\n",
            "7 13 Epoch: 28 | ANN: trainLoss: 0.4536 | trainAcc: 77.5391% (397/512)\n",
            "8 13 Epoch: 28 | ANN: trainLoss: 0.4701 | trainAcc: 77.0833% (444/576)\n",
            "9 13 Epoch: 28 | ANN: trainLoss: 0.4609 | trainAcc: 77.6562% (497/640)\n",
            "10 13 Epoch: 28 | ANN: trainLoss: 0.4635 | trainAcc: 77.4148% (545/704)\n",
            "11 13 Epoch: 28 | ANN: trainLoss: 0.4560 | trainAcc: 78.1250% (600/768)\n",
            "12 13 Epoch: 28 | ANN: trainLoss: 0.4950 | trainAcc: 78.1088% (603/772)\n",
            "0 4 Epoch: 28 | ANN: testLoss: 0.5266 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 28 | ANN: testLoss: 0.5338 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 28 | ANN: testLoss: 0.5316 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 28 | ANN: testLoss: 0.5159 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 29 | ANN: trainLoss: 0.4054 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 29 | ANN: trainLoss: 0.4720 | trainAcc: 80.4688% (103/128)\n",
            "2 13 Epoch: 29 | ANN: trainLoss: 0.4798 | trainAcc: 79.6875% (153/192)\n",
            "3 13 Epoch: 29 | ANN: trainLoss: 0.4737 | trainAcc: 78.9062% (202/256)\n",
            "4 13 Epoch: 29 | ANN: trainLoss: 0.4686 | trainAcc: 79.0625% (253/320)\n",
            "5 13 Epoch: 29 | ANN: trainLoss: 0.4714 | trainAcc: 78.6458% (302/384)\n",
            "6 13 Epoch: 29 | ANN: trainLoss: 0.4569 | trainAcc: 79.4643% (356/448)\n",
            "7 13 Epoch: 29 | ANN: trainLoss: 0.4449 | trainAcc: 79.8828% (409/512)\n",
            "8 13 Epoch: 29 | ANN: trainLoss: 0.4537 | trainAcc: 79.6875% (459/576)\n",
            "9 13 Epoch: 29 | ANN: trainLoss: 0.4459 | trainAcc: 80.1562% (513/640)\n",
            "10 13 Epoch: 29 | ANN: trainLoss: 0.4445 | trainAcc: 80.2557% (565/704)\n",
            "11 13 Epoch: 29 | ANN: trainLoss: 0.4349 | trainAcc: 81.2500% (624/768)\n",
            "12 13 Epoch: 29 | ANN: trainLoss: 0.4350 | trainAcc: 81.2176% (627/772)\n",
            "0 4 Epoch: 29 | ANN: testLoss: 0.3663 | testAcc: 90.6250% (58/64)\n",
            "1 4 Epoch: 29 | ANN: testLoss: 0.5072 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 29 | ANN: testLoss: 0.5401 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 29 | ANN: testLoss: 0.7119 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 30 | ANN: trainLoss: 0.3811 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 30 | ANN: trainLoss: 0.3910 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 30 | ANN: trainLoss: 0.4005 | trainAcc: 81.7708% (157/192)\n",
            "3 13 Epoch: 30 | ANN: trainLoss: 0.3662 | trainAcc: 83.5938% (214/256)\n",
            "4 13 Epoch: 30 | ANN: trainLoss: 0.3593 | trainAcc: 83.4375% (267/320)\n",
            "5 13 Epoch: 30 | ANN: trainLoss: 0.3703 | trainAcc: 81.7708% (314/384)\n",
            "6 13 Epoch: 30 | ANN: trainLoss: 0.4141 | trainAcc: 80.8036% (362/448)\n",
            "7 13 Epoch: 30 | ANN: trainLoss: 0.4142 | trainAcc: 81.0547% (415/512)\n",
            "8 13 Epoch: 30 | ANN: trainLoss: 0.4181 | trainAcc: 80.9028% (466/576)\n",
            "9 13 Epoch: 30 | ANN: trainLoss: 0.4280 | trainAcc: 80.3125% (514/640)\n",
            "10 13 Epoch: 30 | ANN: trainLoss: 0.4255 | trainAcc: 80.3977% (566/704)\n",
            "11 13 Epoch: 30 | ANN: trainLoss: 0.4314 | trainAcc: 79.6875% (612/768)\n",
            "12 13 Epoch: 30 | ANN: trainLoss: 0.4412 | trainAcc: 79.6632% (615/772)\n",
            "0 4 Epoch: 30 | ANN: testLoss: 0.5329 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 30 | ANN: testLoss: 0.6049 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 30 | ANN: testLoss: 0.6043 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 30 | ANN: testLoss: 1.0623 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 31 | ANN: trainLoss: 0.3532 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 31 | ANN: trainLoss: 0.3700 | trainAcc: 80.4688% (103/128)\n",
            "2 13 Epoch: 31 | ANN: trainLoss: 0.3470 | trainAcc: 81.7708% (157/192)\n",
            "3 13 Epoch: 31 | ANN: trainLoss: 0.3985 | trainAcc: 78.5156% (201/256)\n",
            "4 13 Epoch: 31 | ANN: trainLoss: 0.3919 | trainAcc: 79.0625% (253/320)\n",
            "5 13 Epoch: 31 | ANN: trainLoss: 0.4136 | trainAcc: 78.1250% (300/384)\n",
            "6 13 Epoch: 31 | ANN: trainLoss: 0.3934 | trainAcc: 79.9107% (358/448)\n",
            "7 13 Epoch: 31 | ANN: trainLoss: 0.3913 | trainAcc: 80.2734% (411/512)\n",
            "8 13 Epoch: 31 | ANN: trainLoss: 0.3908 | trainAcc: 80.2083% (462/576)\n",
            "9 13 Epoch: 31 | ANN: trainLoss: 0.3837 | trainAcc: 80.1562% (513/640)\n",
            "10 13 Epoch: 31 | ANN: trainLoss: 0.3887 | trainAcc: 80.2557% (565/704)\n",
            "11 13 Epoch: 31 | ANN: trainLoss: 0.3817 | trainAcc: 80.5990% (619/768)\n",
            "12 13 Epoch: 31 | ANN: trainLoss: 0.4113 | trainAcc: 80.4404% (621/772)\n",
            "0 4 Epoch: 31 | ANN: testLoss: 0.7259 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 31 | ANN: testLoss: 0.7385 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 31 | ANN: testLoss: 0.7462 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 31 | ANN: testLoss: 1.2124 | testAcc: 65.2850% (126/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 32 | ANN: trainLoss: 0.3688 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 32 | ANN: trainLoss: 0.3776 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 32 | ANN: trainLoss: 0.4158 | trainAcc: 79.6875% (153/192)\n",
            "3 13 Epoch: 32 | ANN: trainLoss: 0.4431 | trainAcc: 77.3438% (198/256)\n",
            "4 13 Epoch: 32 | ANN: trainLoss: 0.4365 | trainAcc: 76.8750% (246/320)\n",
            "5 13 Epoch: 32 | ANN: trainLoss: 0.4251 | trainAcc: 77.0833% (296/384)\n",
            "6 13 Epoch: 32 | ANN: trainLoss: 0.4238 | trainAcc: 77.2321% (346/448)\n",
            "7 13 Epoch: 32 | ANN: trainLoss: 0.4332 | trainAcc: 76.9531% (394/512)\n",
            "8 13 Epoch: 32 | ANN: trainLoss: 0.4193 | trainAcc: 78.4722% (452/576)\n",
            "9 13 Epoch: 32 | ANN: trainLoss: 0.4192 | trainAcc: 78.4375% (502/640)\n",
            "10 13 Epoch: 32 | ANN: trainLoss: 0.4234 | trainAcc: 78.2670% (551/704)\n",
            "11 13 Epoch: 32 | ANN: trainLoss: 0.4155 | trainAcc: 79.1667% (608/768)\n",
            "12 13 Epoch: 32 | ANN: trainLoss: 0.4065 | trainAcc: 79.1451% (611/772)\n",
            "0 4 Epoch: 32 | ANN: testLoss: 0.4950 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 32 | ANN: testLoss: 0.5152 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 32 | ANN: testLoss: 0.5084 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 32 | ANN: testLoss: 0.3815 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 33 | ANN: trainLoss: 0.3896 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 33 | ANN: trainLoss: 0.3731 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 33 | ANN: trainLoss: 0.3655 | trainAcc: 83.3333% (160/192)\n",
            "3 13 Epoch: 33 | ANN: trainLoss: 0.3590 | trainAcc: 82.8125% (212/256)\n",
            "4 13 Epoch: 33 | ANN: trainLoss: 0.3578 | trainAcc: 82.8125% (265/320)\n",
            "5 13 Epoch: 33 | ANN: trainLoss: 0.3482 | trainAcc: 83.8542% (322/384)\n",
            "6 13 Epoch: 33 | ANN: trainLoss: 0.3635 | trainAcc: 83.2589% (373/448)\n",
            "7 13 Epoch: 33 | ANN: trainLoss: 0.3540 | trainAcc: 84.1797% (431/512)\n",
            "8 13 Epoch: 33 | ANN: trainLoss: 0.3580 | trainAcc: 83.3333% (480/576)\n",
            "9 13 Epoch: 33 | ANN: trainLoss: 0.3586 | trainAcc: 82.6562% (529/640)\n",
            "10 13 Epoch: 33 | ANN: trainLoss: 0.3732 | trainAcc: 81.9602% (577/704)\n",
            "11 13 Epoch: 33 | ANN: trainLoss: 0.3699 | trainAcc: 82.0312% (630/768)\n",
            "12 13 Epoch: 33 | ANN: trainLoss: 0.5299 | trainAcc: 81.7358% (631/772)\n",
            "0 4 Epoch: 33 | ANN: testLoss: 0.4957 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 33 | ANN: testLoss: 0.4642 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 33 | ANN: testLoss: 0.5038 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 33 | ANN: testLoss: 0.5352 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 34 | ANN: trainLoss: 0.4587 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 34 | ANN: trainLoss: 0.3567 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 34 | ANN: trainLoss: 0.3769 | trainAcc: 83.3333% (160/192)\n",
            "3 13 Epoch: 34 | ANN: trainLoss: 0.3533 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 34 | ANN: trainLoss: 0.3597 | trainAcc: 83.4375% (267/320)\n",
            "5 13 Epoch: 34 | ANN: trainLoss: 0.3653 | trainAcc: 82.5521% (317/384)\n",
            "6 13 Epoch: 34 | ANN: trainLoss: 0.3625 | trainAcc: 82.3661% (369/448)\n",
            "7 13 Epoch: 34 | ANN: trainLoss: 0.3692 | trainAcc: 82.2266% (421/512)\n",
            "8 13 Epoch: 34 | ANN: trainLoss: 0.3632 | trainAcc: 82.4653% (475/576)\n",
            "9 13 Epoch: 34 | ANN: trainLoss: 0.3724 | trainAcc: 81.7188% (523/640)\n",
            "10 13 Epoch: 34 | ANN: trainLoss: 0.3677 | trainAcc: 82.1023% (578/704)\n",
            "11 13 Epoch: 34 | ANN: trainLoss: 0.3658 | trainAcc: 82.4219% (633/768)\n",
            "12 13 Epoch: 34 | ANN: trainLoss: 0.3567 | trainAcc: 82.5130% (637/772)\n",
            "0 4 Epoch: 34 | ANN: testLoss: 0.4819 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 34 | ANN: testLoss: 0.4474 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 34 | ANN: testLoss: 0.4991 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 34 | ANN: testLoss: 0.4216 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 35 | ANN: trainLoss: 0.4563 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 35 | ANN: trainLoss: 0.4203 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 35 | ANN: trainLoss: 0.3977 | trainAcc: 81.7708% (157/192)\n",
            "3 13 Epoch: 35 | ANN: trainLoss: 0.3803 | trainAcc: 82.8125% (212/256)\n",
            "4 13 Epoch: 35 | ANN: trainLoss: 0.3720 | trainAcc: 83.1250% (266/320)\n",
            "5 13 Epoch: 35 | ANN: trainLoss: 0.3553 | trainAcc: 84.6354% (325/384)\n",
            "6 13 Epoch: 35 | ANN: trainLoss: 0.3423 | trainAcc: 85.2679% (382/448)\n",
            "7 13 Epoch: 35 | ANN: trainLoss: 0.3429 | trainAcc: 85.1562% (436/512)\n",
            "8 13 Epoch: 35 | ANN: trainLoss: 0.3430 | trainAcc: 85.0694% (490/576)\n",
            "9 13 Epoch: 35 | ANN: trainLoss: 0.3468 | trainAcc: 84.5312% (541/640)\n",
            "10 13 Epoch: 35 | ANN: trainLoss: 0.3442 | trainAcc: 84.8011% (597/704)\n",
            "11 13 Epoch: 35 | ANN: trainLoss: 0.3498 | trainAcc: 84.3750% (648/768)\n",
            "12 13 Epoch: 35 | ANN: trainLoss: 0.5507 | trainAcc: 84.0674% (649/772)\n",
            "0 4 Epoch: 35 | ANN: testLoss: 0.5826 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 35 | ANN: testLoss: 0.6172 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 35 | ANN: testLoss: 0.6203 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 35 | ANN: testLoss: 0.4807 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 36 | ANN: trainLoss: 0.3238 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 36 | ANN: trainLoss: 0.3132 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 36 | ANN: trainLoss: 0.3571 | trainAcc: 81.7708% (157/192)\n",
            "3 13 Epoch: 36 | ANN: trainLoss: 0.3682 | trainAcc: 81.6406% (209/256)\n",
            "4 13 Epoch: 36 | ANN: trainLoss: 0.3558 | trainAcc: 82.5000% (264/320)\n",
            "5 13 Epoch: 36 | ANN: trainLoss: 0.3638 | trainAcc: 83.0729% (319/384)\n",
            "6 13 Epoch: 36 | ANN: trainLoss: 0.3629 | trainAcc: 83.0357% (372/448)\n",
            "7 13 Epoch: 36 | ANN: trainLoss: 0.3639 | trainAcc: 83.9844% (430/512)\n",
            "8 13 Epoch: 36 | ANN: trainLoss: 0.3671 | trainAcc: 83.6806% (482/576)\n",
            "9 13 Epoch: 36 | ANN: trainLoss: 0.3825 | trainAcc: 82.1875% (526/640)\n",
            "10 13 Epoch: 36 | ANN: trainLoss: 0.3836 | trainAcc: 82.5284% (581/704)\n",
            "11 13 Epoch: 36 | ANN: trainLoss: 0.3826 | trainAcc: 82.5521% (634/768)\n",
            "12 13 Epoch: 36 | ANN: trainLoss: 0.3599 | trainAcc: 82.6425% (638/772)\n",
            "0 4 Epoch: 36 | ANN: testLoss: 0.5219 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 36 | ANN: testLoss: 0.5947 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 36 | ANN: testLoss: 0.5589 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 36 | ANN: testLoss: 0.6402 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 37 | ANN: trainLoss: 0.3768 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 37 | ANN: trainLoss: 0.4162 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 37 | ANN: trainLoss: 0.3715 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 37 | ANN: trainLoss: 0.3521 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 37 | ANN: trainLoss: 0.3468 | trainAcc: 84.3750% (270/320)\n",
            "5 13 Epoch: 37 | ANN: trainLoss: 0.3447 | trainAcc: 84.3750% (324/384)\n",
            "6 13 Epoch: 37 | ANN: trainLoss: 0.3480 | trainAcc: 84.3750% (378/448)\n",
            "7 13 Epoch: 37 | ANN: trainLoss: 0.3676 | trainAcc: 83.0078% (425/512)\n",
            "8 13 Epoch: 37 | ANN: trainLoss: 0.3648 | trainAcc: 83.5069% (481/576)\n",
            "9 13 Epoch: 37 | ANN: trainLoss: 0.3600 | trainAcc: 84.0625% (538/640)\n",
            "10 13 Epoch: 37 | ANN: trainLoss: 0.3625 | trainAcc: 84.0909% (592/704)\n",
            "11 13 Epoch: 37 | ANN: trainLoss: 0.3642 | trainAcc: 83.9844% (645/768)\n",
            "12 13 Epoch: 37 | ANN: trainLoss: 0.3968 | trainAcc: 83.9378% (648/772)\n",
            "0 4 Epoch: 37 | ANN: testLoss: 0.4603 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 37 | ANN: testLoss: 0.5098 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 37 | ANN: testLoss: 0.5169 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 37 | ANN: testLoss: 0.3930 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 38 | ANN: trainLoss: 0.3538 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 38 | ANN: trainLoss: 0.3123 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 38 | ANN: trainLoss: 0.2745 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 38 | ANN: trainLoss: 0.3030 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 38 | ANN: trainLoss: 0.3216 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 38 | ANN: trainLoss: 0.3480 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 38 | ANN: trainLoss: 0.3534 | trainAcc: 84.8214% (380/448)\n",
            "7 13 Epoch: 38 | ANN: trainLoss: 0.3502 | trainAcc: 85.1562% (436/512)\n",
            "8 13 Epoch: 38 | ANN: trainLoss: 0.3402 | trainAcc: 86.1111% (496/576)\n",
            "9 13 Epoch: 38 | ANN: trainLoss: 0.3425 | trainAcc: 85.9375% (550/640)\n",
            "10 13 Epoch: 38 | ANN: trainLoss: 0.3490 | trainAcc: 85.9375% (605/704)\n",
            "11 13 Epoch: 38 | ANN: trainLoss: 0.3483 | trainAcc: 85.8073% (659/768)\n",
            "12 13 Epoch: 38 | ANN: trainLoss: 0.4935 | trainAcc: 85.6218% (661/772)\n",
            "0 4 Epoch: 38 | ANN: testLoss: 0.5006 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 38 | ANN: testLoss: 0.5268 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 38 | ANN: testLoss: 0.5001 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 38 | ANN: testLoss: 0.6648 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 39 | ANN: trainLoss: 0.2797 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 39 | ANN: trainLoss: 0.3766 | trainAcc: 79.6875% (102/128)\n",
            "2 13 Epoch: 39 | ANN: trainLoss: 0.3419 | trainAcc: 82.8125% (159/192)\n",
            "3 13 Epoch: 39 | ANN: trainLoss: 0.3414 | trainAcc: 83.2031% (213/256)\n",
            "4 13 Epoch: 39 | ANN: trainLoss: 0.3500 | trainAcc: 82.1875% (263/320)\n",
            "5 13 Epoch: 39 | ANN: trainLoss: 0.3629 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 39 | ANN: trainLoss: 0.3650 | trainAcc: 81.9196% (367/448)\n",
            "7 13 Epoch: 39 | ANN: trainLoss: 0.3694 | trainAcc: 82.2266% (421/512)\n",
            "8 13 Epoch: 39 | ANN: trainLoss: 0.3804 | trainAcc: 82.4653% (475/576)\n",
            "9 13 Epoch: 39 | ANN: trainLoss: 0.3761 | trainAcc: 82.8125% (530/640)\n",
            "10 13 Epoch: 39 | ANN: trainLoss: 0.3758 | trainAcc: 82.9545% (584/704)\n",
            "11 13 Epoch: 39 | ANN: trainLoss: 0.3825 | trainAcc: 82.2917% (632/768)\n",
            "12 13 Epoch: 39 | ANN: trainLoss: 0.3971 | trainAcc: 82.1244% (634/772)\n",
            "0 4 Epoch: 39 | ANN: testLoss: 0.6242 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 39 | ANN: testLoss: 0.6700 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 39 | ANN: testLoss: 0.6531 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 39 | ANN: testLoss: 0.5107 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 40 | ANN: trainLoss: 0.3938 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 40 | ANN: trainLoss: 0.3342 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 40 | ANN: trainLoss: 0.3402 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 40 | ANN: trainLoss: 0.3451 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 40 | ANN: trainLoss: 0.3417 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 40 | ANN: trainLoss: 0.3582 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 40 | ANN: trainLoss: 0.3692 | trainAcc: 85.2679% (382/448)\n",
            "7 13 Epoch: 40 | ANN: trainLoss: 0.3721 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 40 | ANN: trainLoss: 0.3725 | trainAcc: 85.2431% (491/576)\n",
            "9 13 Epoch: 40 | ANN: trainLoss: 0.3704 | trainAcc: 85.4688% (547/640)\n",
            "10 13 Epoch: 40 | ANN: trainLoss: 0.3666 | trainAcc: 85.5114% (602/704)\n",
            "11 13 Epoch: 40 | ANN: trainLoss: 0.3628 | trainAcc: 85.6771% (658/768)\n",
            "12 13 Epoch: 40 | ANN: trainLoss: 0.3589 | trainAcc: 85.7513% (662/772)\n",
            "0 4 Epoch: 40 | ANN: testLoss: 0.4972 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 40 | ANN: testLoss: 0.4614 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 40 | ANN: testLoss: 0.5122 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 40 | ANN: testLoss: 0.5280 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 41 | ANN: trainLoss: 0.4470 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 41 | ANN: trainLoss: 0.4084 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 41 | ANN: trainLoss: 0.3879 | trainAcc: 82.8125% (159/192)\n",
            "3 13 Epoch: 41 | ANN: trainLoss: 0.3456 | trainAcc: 84.7656% (217/256)\n",
            "4 13 Epoch: 41 | ANN: trainLoss: 0.3329 | trainAcc: 85.3125% (273/320)\n",
            "5 13 Epoch: 41 | ANN: trainLoss: 0.3228 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 41 | ANN: trainLoss: 0.3153 | trainAcc: 85.9375% (385/448)\n",
            "7 13 Epoch: 41 | ANN: trainLoss: 0.3231 | trainAcc: 84.9609% (435/512)\n",
            "8 13 Epoch: 41 | ANN: trainLoss: 0.3247 | trainAcc: 85.0694% (490/576)\n",
            "9 13 Epoch: 41 | ANN: trainLoss: 0.3340 | trainAcc: 84.8438% (543/640)\n",
            "10 13 Epoch: 41 | ANN: trainLoss: 0.3270 | trainAcc: 85.6534% (603/704)\n",
            "11 13 Epoch: 41 | ANN: trainLoss: 0.3322 | trainAcc: 85.2865% (655/768)\n",
            "12 13 Epoch: 41 | ANN: trainLoss: 0.3321 | trainAcc: 85.2332% (658/772)\n",
            "0 4 Epoch: 41 | ANN: testLoss: 0.4356 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 41 | ANN: testLoss: 0.5090 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 41 | ANN: testLoss: 0.5492 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 41 | ANN: testLoss: 0.4120 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 42 | ANN: trainLoss: 0.2005 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 42 | ANN: trainLoss: 0.2410 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 42 | ANN: trainLoss: 0.2683 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 42 | ANN: trainLoss: 0.2835 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 42 | ANN: trainLoss: 0.2881 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 42 | ANN: trainLoss: 0.2848 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 42 | ANN: trainLoss: 0.3027 | trainAcc: 86.6071% (388/448)\n",
            "7 13 Epoch: 42 | ANN: trainLoss: 0.2946 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 42 | ANN: trainLoss: 0.2990 | trainAcc: 86.9792% (501/576)\n",
            "9 13 Epoch: 42 | ANN: trainLoss: 0.2983 | trainAcc: 86.8750% (556/640)\n",
            "10 13 Epoch: 42 | ANN: trainLoss: 0.3069 | trainAcc: 86.2216% (607/704)\n",
            "11 13 Epoch: 42 | ANN: trainLoss: 0.3035 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 42 | ANN: trainLoss: 0.3074 | trainAcc: 86.6580% (669/772)\n",
            "0 4 Epoch: 42 | ANN: testLoss: 0.5445 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 42 | ANN: testLoss: 0.6105 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 42 | ANN: testLoss: 0.5951 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 42 | ANN: testLoss: 0.6042 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 43 | ANN: trainLoss: 0.2199 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 43 | ANN: trainLoss: 0.2359 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 43 | ANN: trainLoss: 0.2836 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 43 | ANN: trainLoss: 0.2825 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 43 | ANN: trainLoss: 0.2900 | trainAcc: 85.3125% (273/320)\n",
            "5 13 Epoch: 43 | ANN: trainLoss: 0.2822 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 43 | ANN: trainLoss: 0.2855 | trainAcc: 85.9375% (385/448)\n",
            "7 13 Epoch: 43 | ANN: trainLoss: 0.2885 | trainAcc: 85.7422% (439/512)\n",
            "8 13 Epoch: 43 | ANN: trainLoss: 0.2975 | trainAcc: 84.3750% (486/576)\n",
            "9 13 Epoch: 43 | ANN: trainLoss: 0.2925 | trainAcc: 85.1562% (545/640)\n",
            "10 13 Epoch: 43 | ANN: trainLoss: 0.2911 | trainAcc: 85.5114% (602/704)\n",
            "11 13 Epoch: 43 | ANN: trainLoss: 0.2997 | trainAcc: 85.1562% (654/768)\n",
            "12 13 Epoch: 43 | ANN: trainLoss: 0.3335 | trainAcc: 85.1036% (657/772)\n",
            "0 4 Epoch: 43 | ANN: testLoss: 0.6719 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 43 | ANN: testLoss: 0.6084 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 43 | ANN: testLoss: 0.6051 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 43 | ANN: testLoss: 0.4832 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 44 | ANN: trainLoss: 0.3382 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 44 | ANN: trainLoss: 0.3183 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 44 | ANN: trainLoss: 0.3112 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 44 | ANN: trainLoss: 0.3352 | trainAcc: 84.7656% (217/256)\n",
            "4 13 Epoch: 44 | ANN: trainLoss: 0.3360 | trainAcc: 85.0000% (272/320)\n",
            "5 13 Epoch: 44 | ANN: trainLoss: 0.3429 | trainAcc: 84.6354% (325/384)\n",
            "6 13 Epoch: 44 | ANN: trainLoss: 0.3439 | trainAcc: 84.5982% (379/448)\n",
            "7 13 Epoch: 44 | ANN: trainLoss: 0.3344 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 44 | ANN: trainLoss: 0.3391 | trainAcc: 85.7639% (494/576)\n",
            "9 13 Epoch: 44 | ANN: trainLoss: 0.3361 | trainAcc: 85.7812% (549/640)\n",
            "10 13 Epoch: 44 | ANN: trainLoss: 0.3421 | trainAcc: 85.3693% (601/704)\n",
            "11 13 Epoch: 44 | ANN: trainLoss: 0.3396 | trainAcc: 85.4167% (656/768)\n",
            "12 13 Epoch: 44 | ANN: trainLoss: 0.3427 | trainAcc: 85.3627% (659/772)\n",
            "0 4 Epoch: 44 | ANN: testLoss: 0.6190 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 44 | ANN: testLoss: 0.5680 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 44 | ANN: testLoss: 0.5903 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 44 | ANN: testLoss: 0.4427 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 45 | ANN: trainLoss: 0.3307 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 45 | ANN: trainLoss: 0.2597 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 45 | ANN: trainLoss: 0.2822 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 45 | ANN: trainLoss: 0.2945 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 45 | ANN: trainLoss: 0.3016 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 45 | ANN: trainLoss: 0.3083 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 45 | ANN: trainLoss: 0.3343 | trainAcc: 84.8214% (380/448)\n",
            "7 13 Epoch: 45 | ANN: trainLoss: 0.3299 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 45 | ANN: trainLoss: 0.3223 | trainAcc: 85.5903% (493/576)\n",
            "9 13 Epoch: 45 | ANN: trainLoss: 0.3223 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 45 | ANN: trainLoss: 0.3175 | trainAcc: 86.3636% (608/704)\n",
            "11 13 Epoch: 45 | ANN: trainLoss: 0.3125 | trainAcc: 86.4583% (664/768)\n",
            "12 13 Epoch: 45 | ANN: trainLoss: 0.3066 | trainAcc: 86.3990% (667/772)\n",
            "0 4 Epoch: 45 | ANN: testLoss: 0.3733 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 45 | ANN: testLoss: 0.5794 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 45 | ANN: testLoss: 0.5412 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 45 | ANN: testLoss: 0.8019 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 46 | ANN: trainLoss: 0.3842 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 46 | ANN: trainLoss: 0.3399 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 46 | ANN: trainLoss: 0.3294 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 46 | ANN: trainLoss: 0.3228 | trainAcc: 85.1562% (218/256)\n",
            "4 13 Epoch: 46 | ANN: trainLoss: 0.3047 | trainAcc: 85.6250% (274/320)\n",
            "5 13 Epoch: 46 | ANN: trainLoss: 0.3016 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 46 | ANN: trainLoss: 0.3078 | trainAcc: 86.1607% (386/448)\n",
            "7 13 Epoch: 46 | ANN: trainLoss: 0.3062 | trainAcc: 85.5469% (438/512)\n",
            "8 13 Epoch: 46 | ANN: trainLoss: 0.3055 | trainAcc: 85.9375% (495/576)\n",
            "9 13 Epoch: 46 | ANN: trainLoss: 0.3096 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 46 | ANN: trainLoss: 0.3065 | trainAcc: 86.2216% (607/704)\n",
            "11 13 Epoch: 46 | ANN: trainLoss: 0.3132 | trainAcc: 86.1979% (662/768)\n",
            "12 13 Epoch: 46 | ANN: trainLoss: 0.3162 | trainAcc: 86.2694% (666/772)\n",
            "0 4 Epoch: 46 | ANN: testLoss: 0.7327 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 46 | ANN: testLoss: 0.6968 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 46 | ANN: testLoss: 0.6586 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 46 | ANN: testLoss: 0.8160 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 47 | ANN: trainLoss: 0.3105 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 47 | ANN: trainLoss: 0.3011 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 47 | ANN: trainLoss: 0.3175 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 47 | ANN: trainLoss: 0.3100 | trainAcc: 82.4219% (211/256)\n",
            "4 13 Epoch: 47 | ANN: trainLoss: 0.3036 | trainAcc: 83.1250% (266/320)\n",
            "5 13 Epoch: 47 | ANN: trainLoss: 0.3110 | trainAcc: 83.0729% (319/384)\n",
            "6 13 Epoch: 47 | ANN: trainLoss: 0.3159 | trainAcc: 83.4821% (374/448)\n",
            "7 13 Epoch: 47 | ANN: trainLoss: 0.3200 | trainAcc: 83.9844% (430/512)\n",
            "8 13 Epoch: 47 | ANN: trainLoss: 0.3189 | trainAcc: 83.8542% (483/576)\n",
            "9 13 Epoch: 47 | ANN: trainLoss: 0.3106 | trainAcc: 84.5312% (541/640)\n",
            "10 13 Epoch: 47 | ANN: trainLoss: 0.3098 | trainAcc: 84.3750% (594/704)\n",
            "11 13 Epoch: 47 | ANN: trainLoss: 0.3038 | trainAcc: 84.7656% (651/768)\n",
            "12 13 Epoch: 47 | ANN: trainLoss: 0.3126 | trainAcc: 84.5855% (653/772)\n",
            "0 4 Epoch: 47 | ANN: testLoss: 0.5880 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 47 | ANN: testLoss: 0.5877 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 47 | ANN: testLoss: 0.6364 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 47 | ANN: testLoss: 1.0097 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 48 | ANN: trainLoss: 0.2272 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 48 | ANN: trainLoss: 0.2246 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 48 | ANN: trainLoss: 0.2612 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 48 | ANN: trainLoss: 0.2487 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 48 | ANN: trainLoss: 0.2603 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 48 | ANN: trainLoss: 0.2713 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 48 | ANN: trainLoss: 0.2601 | trainAcc: 88.3929% (396/448)\n",
            "7 13 Epoch: 48 | ANN: trainLoss: 0.2701 | trainAcc: 88.0859% (451/512)\n",
            "8 13 Epoch: 48 | ANN: trainLoss: 0.2662 | trainAcc: 88.3681% (509/576)\n",
            "9 13 Epoch: 48 | ANN: trainLoss: 0.2698 | trainAcc: 88.1250% (564/640)\n",
            "10 13 Epoch: 48 | ANN: trainLoss: 0.2730 | trainAcc: 87.7841% (618/704)\n",
            "11 13 Epoch: 48 | ANN: trainLoss: 0.2692 | trainAcc: 88.0208% (676/768)\n",
            "12 13 Epoch: 48 | ANN: trainLoss: 0.2684 | trainAcc: 88.0829% (680/772)\n",
            "0 4 Epoch: 48 | ANN: testLoss: 0.5892 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 48 | ANN: testLoss: 0.5984 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 48 | ANN: testLoss: 0.5801 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 48 | ANN: testLoss: 0.5810 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 49 | ANN: trainLoss: 0.2200 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 49 | ANN: trainLoss: 0.1989 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 49 | ANN: trainLoss: 0.1999 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 49 | ANN: trainLoss: 0.2084 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 49 | ANN: trainLoss: 0.2331 | trainAcc: 90.0000% (288/320)\n",
            "5 13 Epoch: 49 | ANN: trainLoss: 0.2244 | trainAcc: 90.6250% (348/384)\n",
            "6 13 Epoch: 49 | ANN: trainLoss: 0.2272 | trainAcc: 90.6250% (406/448)\n",
            "7 13 Epoch: 49 | ANN: trainLoss: 0.2228 | trainAcc: 91.2109% (467/512)\n",
            "8 13 Epoch: 49 | ANN: trainLoss: 0.2455 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 49 | ANN: trainLoss: 0.2431 | trainAcc: 89.6875% (574/640)\n",
            "10 13 Epoch: 49 | ANN: trainLoss: 0.2390 | trainAcc: 89.9148% (633/704)\n",
            "11 13 Epoch: 49 | ANN: trainLoss: 0.2398 | trainAcc: 89.4531% (687/768)\n",
            "12 13 Epoch: 49 | ANN: trainLoss: 0.2398 | trainAcc: 89.3782% (690/772)\n",
            "0 4 Epoch: 49 | ANN: testLoss: 0.5112 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 49 | ANN: testLoss: 0.6649 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 49 | ANN: testLoss: 0.5798 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 49 | ANN: testLoss: 0.4351 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 50 | ANN: trainLoss: 0.2994 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 50 | ANN: trainLoss: 0.2381 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 50 | ANN: trainLoss: 0.2358 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 50 | ANN: trainLoss: 0.2281 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 50 | ANN: trainLoss: 0.2453 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 50 | ANN: trainLoss: 0.2457 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 50 | ANN: trainLoss: 0.2521 | trainAcc: 89.7321% (402/448)\n",
            "7 13 Epoch: 50 | ANN: trainLoss: 0.2427 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 50 | ANN: trainLoss: 0.2437 | trainAcc: 90.4514% (521/576)\n",
            "9 13 Epoch: 50 | ANN: trainLoss: 0.2421 | trainAcc: 90.1562% (577/640)\n",
            "10 13 Epoch: 50 | ANN: trainLoss: 0.2411 | trainAcc: 90.1989% (635/704)\n",
            "11 13 Epoch: 50 | ANN: trainLoss: 0.2408 | trainAcc: 90.1042% (692/768)\n",
            "12 13 Epoch: 50 | ANN: trainLoss: 0.3416 | trainAcc: 89.8964% (694/772)\n",
            "0 4 Epoch: 50 | ANN: testLoss: 0.5513 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 50 | ANN: testLoss: 0.6298 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 50 | ANN: testLoss: 0.5853 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 50 | ANN: testLoss: 0.4390 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 51 | ANN: trainLoss: 0.2548 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 51 | ANN: trainLoss: 0.2418 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 51 | ANN: trainLoss: 0.2443 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 51 | ANN: trainLoss: 0.2381 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 51 | ANN: trainLoss: 0.2452 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 51 | ANN: trainLoss: 0.2535 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 51 | ANN: trainLoss: 0.2519 | trainAcc: 89.0625% (399/448)\n",
            "7 13 Epoch: 51 | ANN: trainLoss: 0.2477 | trainAcc: 89.4531% (458/512)\n",
            "8 13 Epoch: 51 | ANN: trainLoss: 0.2487 | trainAcc: 89.4097% (515/576)\n",
            "9 13 Epoch: 51 | ANN: trainLoss: 0.2599 | trainAcc: 88.9062% (569/640)\n",
            "10 13 Epoch: 51 | ANN: trainLoss: 0.2605 | trainAcc: 88.6364% (624/704)\n",
            "11 13 Epoch: 51 | ANN: trainLoss: 0.2621 | trainAcc: 88.6719% (681/768)\n",
            "12 13 Epoch: 51 | ANN: trainLoss: 0.3423 | trainAcc: 88.4715% (683/772)\n",
            "0 4 Epoch: 51 | ANN: testLoss: 0.6191 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 51 | ANN: testLoss: 0.6126 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 51 | ANN: testLoss: 0.5668 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 51 | ANN: testLoss: 0.5069 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 52 | ANN: trainLoss: 0.2594 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 52 | ANN: trainLoss: 0.2386 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 52 | ANN: trainLoss: 0.2287 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 52 | ANN: trainLoss: 0.2316 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 52 | ANN: trainLoss: 0.2589 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 52 | ANN: trainLoss: 0.2556 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 52 | ANN: trainLoss: 0.2583 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 52 | ANN: trainLoss: 0.2928 | trainAcc: 88.6719% (454/512)\n",
            "8 13 Epoch: 52 | ANN: trainLoss: 0.2868 | trainAcc: 88.7153% (511/576)\n",
            "9 13 Epoch: 52 | ANN: trainLoss: 0.2884 | trainAcc: 88.5938% (567/640)\n",
            "10 13 Epoch: 52 | ANN: trainLoss: 0.2794 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 52 | ANN: trainLoss: 0.2740 | trainAcc: 89.5833% (688/768)\n",
            "12 13 Epoch: 52 | ANN: trainLoss: 0.2785 | trainAcc: 89.5078% (691/772)\n",
            "0 4 Epoch: 52 | ANN: testLoss: 0.5716 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 52 | ANN: testLoss: 0.6190 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 52 | ANN: testLoss: 0.6041 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 52 | ANN: testLoss: 0.5071 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 53 | ANN: trainLoss: 0.2138 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 53 | ANN: trainLoss: 0.1764 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 53 | ANN: trainLoss: 0.2235 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 53 | ANN: trainLoss: 0.2460 | trainAcc: 90.2344% (231/256)\n",
            "4 13 Epoch: 53 | ANN: trainLoss: 0.2499 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 53 | ANN: trainLoss: 0.2517 | trainAcc: 90.3646% (347/384)\n",
            "6 13 Epoch: 53 | ANN: trainLoss: 0.2547 | trainAcc: 90.1786% (404/448)\n",
            "7 13 Epoch: 53 | ANN: trainLoss: 0.2475 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 53 | ANN: trainLoss: 0.2427 | trainAcc: 90.7986% (523/576)\n",
            "9 13 Epoch: 53 | ANN: trainLoss: 0.2445 | trainAcc: 90.4688% (579/640)\n",
            "10 13 Epoch: 53 | ANN: trainLoss: 0.2463 | trainAcc: 90.4830% (637/704)\n",
            "11 13 Epoch: 53 | ANN: trainLoss: 0.2441 | trainAcc: 90.4948% (695/768)\n",
            "12 13 Epoch: 53 | ANN: trainLoss: 0.2388 | trainAcc: 90.5440% (699/772)\n",
            "0 4 Epoch: 53 | ANN: testLoss: 0.5617 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 53 | ANN: testLoss: 0.5784 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 53 | ANN: testLoss: 0.5845 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 53 | ANN: testLoss: 0.6189 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 54 | ANN: trainLoss: 0.2231 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 54 | ANN: trainLoss: 0.2411 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 54 | ANN: trainLoss: 0.2600 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 54 | ANN: trainLoss: 0.2393 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 54 | ANN: trainLoss: 0.2448 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 54 | ANN: trainLoss: 0.2478 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 54 | ANN: trainLoss: 0.2449 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 54 | ANN: trainLoss: 0.2470 | trainAcc: 89.0625% (456/512)\n",
            "8 13 Epoch: 54 | ANN: trainLoss: 0.2380 | trainAcc: 89.9306% (518/576)\n",
            "9 13 Epoch: 54 | ANN: trainLoss: 0.2379 | trainAcc: 89.8438% (575/640)\n",
            "10 13 Epoch: 54 | ANN: trainLoss: 0.2317 | trainAcc: 90.6250% (638/704)\n",
            "11 13 Epoch: 54 | ANN: trainLoss: 0.2313 | trainAcc: 90.7552% (697/768)\n",
            "12 13 Epoch: 54 | ANN: trainLoss: 0.2614 | trainAcc: 90.6736% (700/772)\n",
            "0 4 Epoch: 54 | ANN: testLoss: 0.7496 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 54 | ANN: testLoss: 0.6384 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 54 | ANN: testLoss: 0.6092 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 54 | ANN: testLoss: 0.8216 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 55 | ANN: trainLoss: 0.2166 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 55 | ANN: trainLoss: 0.2592 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 55 | ANN: trainLoss: 0.2612 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 55 | ANN: trainLoss: 0.2346 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 55 | ANN: trainLoss: 0.2568 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 55 | ANN: trainLoss: 0.2512 | trainAcc: 89.5833% (344/384)\n",
            "6 13 Epoch: 55 | ANN: trainLoss: 0.2513 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 55 | ANN: trainLoss: 0.2574 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 55 | ANN: trainLoss: 0.2494 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 55 | ANN: trainLoss: 0.2513 | trainAcc: 90.0000% (576/640)\n",
            "10 13 Epoch: 55 | ANN: trainLoss: 0.2442 | trainAcc: 90.3409% (636/704)\n",
            "11 13 Epoch: 55 | ANN: trainLoss: 0.2385 | trainAcc: 90.7552% (697/768)\n",
            "12 13 Epoch: 55 | ANN: trainLoss: 0.2533 | trainAcc: 90.8031% (701/772)\n",
            "0 4 Epoch: 55 | ANN: testLoss: 0.7844 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 55 | ANN: testLoss: 0.6467 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 55 | ANN: testLoss: 0.5972 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 55 | ANN: testLoss: 0.5651 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 56 | ANN: trainLoss: 0.1729 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 56 | ANN: trainLoss: 0.1806 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 56 | ANN: trainLoss: 0.1737 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 56 | ANN: trainLoss: 0.1817 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 56 | ANN: trainLoss: 0.1898 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 56 | ANN: trainLoss: 0.1810 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 56 | ANN: trainLoss: 0.1824 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 56 | ANN: trainLoss: 0.1958 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 56 | ANN: trainLoss: 0.1977 | trainAcc: 92.5347% (533/576)\n",
            "9 13 Epoch: 56 | ANN: trainLoss: 0.1981 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 56 | ANN: trainLoss: 0.2066 | trainAcc: 92.0455% (648/704)\n",
            "11 13 Epoch: 56 | ANN: trainLoss: 0.2068 | trainAcc: 92.3177% (709/768)\n",
            "12 13 Epoch: 56 | ANN: trainLoss: 0.1982 | trainAcc: 92.3575% (713/772)\n",
            "0 4 Epoch: 56 | ANN: testLoss: 0.5437 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 56 | ANN: testLoss: 0.5796 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 56 | ANN: testLoss: 0.5896 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 56 | ANN: testLoss: 0.4862 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 57 | ANN: trainLoss: 0.2116 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 57 | ANN: trainLoss: 0.2403 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 57 | ANN: trainLoss: 0.2138 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 57 | ANN: trainLoss: 0.2131 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 57 | ANN: trainLoss: 0.2080 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 57 | ANN: trainLoss: 0.2166 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 57 | ANN: trainLoss: 0.2232 | trainAcc: 90.8482% (407/448)\n",
            "7 13 Epoch: 57 | ANN: trainLoss: 0.2244 | trainAcc: 90.8203% (465/512)\n",
            "8 13 Epoch: 57 | ANN: trainLoss: 0.2260 | trainAcc: 90.7986% (523/576)\n",
            "9 13 Epoch: 57 | ANN: trainLoss: 0.2227 | trainAcc: 91.2500% (584/640)\n",
            "10 13 Epoch: 57 | ANN: trainLoss: 0.2185 | trainAcc: 91.6193% (645/704)\n",
            "11 13 Epoch: 57 | ANN: trainLoss: 0.2232 | trainAcc: 91.4062% (702/768)\n",
            "12 13 Epoch: 57 | ANN: trainLoss: 0.2534 | trainAcc: 91.3212% (705/772)\n",
            "0 4 Epoch: 57 | ANN: testLoss: 0.6400 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 57 | ANN: testLoss: 0.6695 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 57 | ANN: testLoss: 0.5992 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 57 | ANN: testLoss: 0.5046 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 58 | ANN: trainLoss: 0.1721 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 58 | ANN: trainLoss: 0.1973 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 58 | ANN: trainLoss: 0.2176 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 58 | ANN: trainLoss: 0.2254 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 58 | ANN: trainLoss: 0.2299 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 58 | ANN: trainLoss: 0.2250 | trainAcc: 90.8854% (349/384)\n",
            "6 13 Epoch: 58 | ANN: trainLoss: 0.2240 | trainAcc: 90.6250% (406/448)\n",
            "7 13 Epoch: 58 | ANN: trainLoss: 0.2274 | trainAcc: 90.4297% (463/512)\n",
            "8 13 Epoch: 58 | ANN: trainLoss: 0.2268 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 58 | ANN: trainLoss: 0.2213 | trainAcc: 90.6250% (580/640)\n",
            "10 13 Epoch: 58 | ANN: trainLoss: 0.2270 | trainAcc: 90.1989% (635/704)\n",
            "11 13 Epoch: 58 | ANN: trainLoss: 0.2280 | trainAcc: 90.3646% (694/768)\n",
            "12 13 Epoch: 58 | ANN: trainLoss: 0.2441 | trainAcc: 90.2850% (697/772)\n",
            "0 4 Epoch: 58 | ANN: testLoss: 0.9579 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 58 | ANN: testLoss: 0.8863 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 58 | ANN: testLoss: 0.8276 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 58 | ANN: testLoss: 0.7227 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 59 | ANN: trainLoss: 0.3118 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 59 | ANN: trainLoss: 0.2935 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 59 | ANN: trainLoss: 0.2714 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 59 | ANN: trainLoss: 0.2514 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 59 | ANN: trainLoss: 0.2436 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 59 | ANN: trainLoss: 0.2533 | trainAcc: 87.5000% (336/384)\n",
            "6 13 Epoch: 59 | ANN: trainLoss: 0.2558 | trainAcc: 87.2768% (391/448)\n",
            "7 13 Epoch: 59 | ANN: trainLoss: 0.2457 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 59 | ANN: trainLoss: 0.2464 | trainAcc: 88.0208% (507/576)\n",
            "9 13 Epoch: 59 | ANN: trainLoss: 0.2533 | trainAcc: 87.6562% (561/640)\n",
            "10 13 Epoch: 59 | ANN: trainLoss: 0.2527 | trainAcc: 87.9261% (619/704)\n",
            "11 13 Epoch: 59 | ANN: trainLoss: 0.2484 | trainAcc: 88.1510% (677/768)\n",
            "12 13 Epoch: 59 | ANN: trainLoss: 0.2344 | trainAcc: 88.2124% (681/772)\n",
            "0 4 Epoch: 59 | ANN: testLoss: 0.6163 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 59 | ANN: testLoss: 0.6655 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 59 | ANN: testLoss: 0.6454 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 59 | ANN: testLoss: 0.4842 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 60 | ANN: trainLoss: 0.1798 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 60 | ANN: trainLoss: 0.1830 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 60 | ANN: trainLoss: 0.1792 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 60 | ANN: trainLoss: 0.1970 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 60 | ANN: trainLoss: 0.1944 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 60 | ANN: trainLoss: 0.2157 | trainAcc: 90.3646% (347/384)\n",
            "6 13 Epoch: 60 | ANN: trainLoss: 0.2107 | trainAcc: 90.4018% (405/448)\n",
            "7 13 Epoch: 60 | ANN: trainLoss: 0.2167 | trainAcc: 90.2344% (462/512)\n",
            "8 13 Epoch: 60 | ANN: trainLoss: 0.2223 | trainAcc: 90.2778% (520/576)\n",
            "9 13 Epoch: 60 | ANN: trainLoss: 0.2152 | trainAcc: 90.9375% (582/640)\n",
            "10 13 Epoch: 60 | ANN: trainLoss: 0.2207 | trainAcc: 90.4830% (637/704)\n",
            "11 13 Epoch: 60 | ANN: trainLoss: 0.2184 | trainAcc: 90.8854% (698/768)\n",
            "12 13 Epoch: 60 | ANN: trainLoss: 0.2352 | trainAcc: 90.8031% (701/772)\n",
            "0 4 Epoch: 60 | ANN: testLoss: 0.5437 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 60 | ANN: testLoss: 0.5508 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 60 | ANN: testLoss: 0.5816 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 60 | ANN: testLoss: 0.5419 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 61 | ANN: trainLoss: 0.1324 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 61 | ANN: trainLoss: 0.1529 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 61 | ANN: trainLoss: 0.1454 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 61 | ANN: trainLoss: 0.1548 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 61 | ANN: trainLoss: 0.1553 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 61 | ANN: trainLoss: 0.1613 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 61 | ANN: trainLoss: 0.1851 | trainAcc: 92.4107% (414/448)\n",
            "7 13 Epoch: 61 | ANN: trainLoss: 0.1792 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 61 | ANN: trainLoss: 0.1891 | trainAcc: 92.5347% (533/576)\n",
            "9 13 Epoch: 61 | ANN: trainLoss: 0.1936 | trainAcc: 92.0312% (589/640)\n",
            "10 13 Epoch: 61 | ANN: trainLoss: 0.1961 | trainAcc: 92.1875% (649/704)\n",
            "11 13 Epoch: 61 | ANN: trainLoss: 0.1986 | trainAcc: 92.3177% (709/768)\n",
            "12 13 Epoch: 61 | ANN: trainLoss: 0.1861 | trainAcc: 92.3575% (713/772)\n",
            "0 4 Epoch: 61 | ANN: testLoss: 0.5213 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 61 | ANN: testLoss: 0.5066 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 61 | ANN: testLoss: 0.5930 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 61 | ANN: testLoss: 0.6431 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 62 | ANN: trainLoss: 0.1726 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 62 | ANN: trainLoss: 0.1665 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 62 | ANN: trainLoss: 0.1654 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 62 | ANN: trainLoss: 0.1713 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 62 | ANN: trainLoss: 0.1724 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 62 | ANN: trainLoss: 0.1821 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 62 | ANN: trainLoss: 0.1796 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 62 | ANN: trainLoss: 0.1825 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 62 | ANN: trainLoss: 0.1790 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 62 | ANN: trainLoss: 0.1802 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 62 | ANN: trainLoss: 0.1752 | trainAcc: 94.3182% (664/704)\n",
            "11 13 Epoch: 62 | ANN: trainLoss: 0.1756 | trainAcc: 94.2708% (724/768)\n",
            "12 13 Epoch: 62 | ANN: trainLoss: 0.1822 | trainAcc: 94.1710% (727/772)\n",
            "0 4 Epoch: 62 | ANN: testLoss: 0.5394 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 62 | ANN: testLoss: 0.5989 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 62 | ANN: testLoss: 0.6275 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 62 | ANN: testLoss: 0.5416 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 63 | ANN: trainLoss: 0.1381 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 63 | ANN: trainLoss: 0.1571 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 63 | ANN: trainLoss: 0.1522 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 63 | ANN: trainLoss: 0.1841 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 63 | ANN: trainLoss: 0.1730 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 63 | ANN: trainLoss: 0.1740 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 63 | ANN: trainLoss: 0.1733 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 63 | ANN: trainLoss: 0.1736 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 63 | ANN: trainLoss: 0.1770 | trainAcc: 93.5764% (539/576)\n",
            "9 13 Epoch: 63 | ANN: trainLoss: 0.1768 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 63 | ANN: trainLoss: 0.1760 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 63 | ANN: trainLoss: 0.1800 | trainAcc: 93.0990% (715/768)\n",
            "12 13 Epoch: 63 | ANN: trainLoss: 0.1833 | trainAcc: 93.0052% (718/772)\n",
            "0 4 Epoch: 63 | ANN: testLoss: 0.5995 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 63 | ANN: testLoss: 0.6834 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 63 | ANN: testLoss: 0.6376 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 63 | ANN: testLoss: 0.4894 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 64 | ANN: trainLoss: 0.1565 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 64 | ANN: trainLoss: 0.1561 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 64 | ANN: trainLoss: 0.1522 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 64 | ANN: trainLoss: 0.1533 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 64 | ANN: trainLoss: 0.1630 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 64 | ANN: trainLoss: 0.1626 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 64 | ANN: trainLoss: 0.1726 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 64 | ANN: trainLoss: 0.1736 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 64 | ANN: trainLoss: 0.1756 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 64 | ANN: trainLoss: 0.1804 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 64 | ANN: trainLoss: 0.1789 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 64 | ANN: trainLoss: 0.1750 | trainAcc: 94.2708% (724/768)\n",
            "12 13 Epoch: 64 | ANN: trainLoss: 0.1638 | trainAcc: 94.3005% (728/772)\n",
            "0 4 Epoch: 64 | ANN: testLoss: 0.9208 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 64 | ANN: testLoss: 0.7478 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 64 | ANN: testLoss: 0.6796 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 64 | ANN: testLoss: 0.5191 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 65 | ANN: trainLoss: 0.1549 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 65 | ANN: trainLoss: 0.1501 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 65 | ANN: trainLoss: 0.1684 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 65 | ANN: trainLoss: 0.1753 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 65 | ANN: trainLoss: 0.1770 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 65 | ANN: trainLoss: 0.1746 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 65 | ANN: trainLoss: 0.1741 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 65 | ANN: trainLoss: 0.1732 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 65 | ANN: trainLoss: 0.1635 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 65 | ANN: trainLoss: 0.1616 | trainAcc: 95.0000% (608/640)\n",
            "10 13 Epoch: 65 | ANN: trainLoss: 0.1605 | trainAcc: 95.0284% (669/704)\n",
            "11 13 Epoch: 65 | ANN: trainLoss: 0.1725 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 65 | ANN: trainLoss: 0.2575 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 65 | ANN: testLoss: 0.8091 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 65 | ANN: testLoss: 0.6984 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 65 | ANN: testLoss: 0.6812 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 65 | ANN: testLoss: 1.0699 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 66 | ANN: trainLoss: 0.1514 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 66 | ANN: trainLoss: 0.2762 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 66 | ANN: trainLoss: 0.2335 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 66 | ANN: trainLoss: 0.2122 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 66 | ANN: trainLoss: 0.2168 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 66 | ANN: trainLoss: 0.2116 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 66 | ANN: trainLoss: 0.2075 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 66 | ANN: trainLoss: 0.2275 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 66 | ANN: trainLoss: 0.2305 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 66 | ANN: trainLoss: 0.2433 | trainAcc: 92.5000% (592/640)\n",
            "10 13 Epoch: 66 | ANN: trainLoss: 0.2404 | trainAcc: 92.4716% (651/704)\n",
            "11 13 Epoch: 66 | ANN: trainLoss: 0.2363 | trainAcc: 92.7083% (712/768)\n",
            "12 13 Epoch: 66 | ANN: trainLoss: 0.2684 | trainAcc: 92.4870% (714/772)\n",
            "0 4 Epoch: 66 | ANN: testLoss: 0.4185 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 66 | ANN: testLoss: 0.7205 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 66 | ANN: testLoss: 0.7056 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 66 | ANN: testLoss: 0.7937 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 67 | ANN: trainLoss: 0.2107 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 67 | ANN: trainLoss: 0.2029 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 67 | ANN: trainLoss: 0.2099 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 67 | ANN: trainLoss: 0.2168 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 67 | ANN: trainLoss: 0.2204 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 67 | ANN: trainLoss: 0.2069 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 67 | ANN: trainLoss: 0.2254 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 67 | ANN: trainLoss: 0.2332 | trainAcc: 89.6484% (459/512)\n",
            "8 13 Epoch: 67 | ANN: trainLoss: 0.2313 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 67 | ANN: trainLoss: 0.2250 | trainAcc: 90.4688% (579/640)\n",
            "10 13 Epoch: 67 | ANN: trainLoss: 0.2319 | trainAcc: 90.0568% (634/704)\n",
            "11 13 Epoch: 67 | ANN: trainLoss: 0.2289 | trainAcc: 90.4948% (695/768)\n",
            "12 13 Epoch: 67 | ANN: trainLoss: 0.2304 | trainAcc: 90.5440% (699/772)\n",
            "0 4 Epoch: 67 | ANN: testLoss: 1.0593 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 67 | ANN: testLoss: 0.9398 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 67 | ANN: testLoss: 0.7420 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 67 | ANN: testLoss: 0.5590 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 68 | ANN: trainLoss: 0.2013 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 68 | ANN: trainLoss: 0.2271 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 68 | ANN: trainLoss: 0.2226 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 68 | ANN: trainLoss: 0.2233 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 68 | ANN: trainLoss: 0.2110 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 68 | ANN: trainLoss: 0.2000 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 68 | ANN: trainLoss: 0.1931 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 68 | ANN: trainLoss: 0.1878 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 68 | ANN: trainLoss: 0.1911 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 68 | ANN: trainLoss: 0.1937 | trainAcc: 92.3438% (591/640)\n",
            "10 13 Epoch: 68 | ANN: trainLoss: 0.1864 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 68 | ANN: trainLoss: 0.1896 | trainAcc: 93.0990% (715/768)\n",
            "12 13 Epoch: 68 | ANN: trainLoss: 0.1777 | trainAcc: 93.1347% (719/772)\n",
            "0 4 Epoch: 68 | ANN: testLoss: 0.5441 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 68 | ANN: testLoss: 0.6133 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 68 | ANN: testLoss: 0.6222 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 68 | ANN: testLoss: 0.7336 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 69 | ANN: trainLoss: 0.1039 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 69 | ANN: trainLoss: 0.1586 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 69 | ANN: trainLoss: 0.1633 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 69 | ANN: trainLoss: 0.1519 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 69 | ANN: trainLoss: 0.1586 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 69 | ANN: trainLoss: 0.1735 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 69 | ANN: trainLoss: 0.1684 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 69 | ANN: trainLoss: 0.1732 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 69 | ANN: trainLoss: 0.1728 | trainAcc: 92.8819% (535/576)\n",
            "9 13 Epoch: 69 | ANN: trainLoss: 0.1761 | trainAcc: 92.5000% (592/640)\n",
            "10 13 Epoch: 69 | ANN: trainLoss: 0.1742 | trainAcc: 92.6136% (652/704)\n",
            "11 13 Epoch: 69 | ANN: trainLoss: 0.1751 | trainAcc: 92.7083% (712/768)\n",
            "12 13 Epoch: 69 | ANN: trainLoss: 0.1660 | trainAcc: 92.7461% (716/772)\n",
            "0 4 Epoch: 69 | ANN: testLoss: 0.6839 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 69 | ANN: testLoss: 0.5781 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 69 | ANN: testLoss: 0.6219 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 69 | ANN: testLoss: 0.7753 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 70 | ANN: trainLoss: 0.1512 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 70 | ANN: trainLoss: 0.1435 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 70 | ANN: trainLoss: 0.1518 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 70 | ANN: trainLoss: 0.1411 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 70 | ANN: trainLoss: 0.1402 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 70 | ANN: trainLoss: 0.1526 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 70 | ANN: trainLoss: 0.1582 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 70 | ANN: trainLoss: 0.1572 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 70 | ANN: trainLoss: 0.1624 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 70 | ANN: trainLoss: 0.1647 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 70 | ANN: trainLoss: 0.1630 | trainAcc: 94.3182% (664/704)\n",
            "11 13 Epoch: 70 | ANN: trainLoss: 0.1595 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 70 | ANN: trainLoss: 0.1506 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 70 | ANN: testLoss: 0.5832 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 70 | ANN: testLoss: 0.6006 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 70 | ANN: testLoss: 0.6265 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 70 | ANN: testLoss: 0.7538 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 71 | ANN: trainLoss: 0.1218 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 71 | ANN: trainLoss: 0.1181 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 71 | ANN: trainLoss: 0.1491 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 71 | ANN: trainLoss: 0.1453 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 71 | ANN: trainLoss: 0.1418 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 71 | ANN: trainLoss: 0.1425 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 71 | ANN: trainLoss: 0.1514 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 71 | ANN: trainLoss: 0.1490 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 71 | ANN: trainLoss: 0.1483 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 71 | ANN: trainLoss: 0.1550 | trainAcc: 95.4688% (611/640)\n",
            "10 13 Epoch: 71 | ANN: trainLoss: 0.1523 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 71 | ANN: trainLoss: 0.1521 | trainAcc: 95.7031% (735/768)\n",
            "12 13 Epoch: 71 | ANN: trainLoss: 0.1767 | trainAcc: 95.7254% (739/772)\n",
            "0 4 Epoch: 71 | ANN: testLoss: 0.6010 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 71 | ANN: testLoss: 0.6548 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 71 | ANN: testLoss: 0.6454 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 71 | ANN: testLoss: 0.4969 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 72 | ANN: trainLoss: 0.2104 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 72 | ANN: trainLoss: 0.2119 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 72 | ANN: trainLoss: 0.1936 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 72 | ANN: trainLoss: 0.1890 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 72 | ANN: trainLoss: 0.1899 | trainAcc: 92.5000% (296/320)\n",
            "5 13 Epoch: 72 | ANN: trainLoss: 0.1778 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 72 | ANN: trainLoss: 0.1765 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 72 | ANN: trainLoss: 0.1666 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 72 | ANN: trainLoss: 0.1616 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 72 | ANN: trainLoss: 0.1528 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 72 | ANN: trainLoss: 0.1498 | trainAcc: 95.3125% (671/704)\n",
            "11 13 Epoch: 72 | ANN: trainLoss: 0.1529 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 72 | ANN: trainLoss: 0.1424 | trainAcc: 95.2073% (735/772)\n",
            "0 4 Epoch: 72 | ANN: testLoss: 0.6355 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 72 | ANN: testLoss: 0.6527 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 72 | ANN: testLoss: 0.6451 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 72 | ANN: testLoss: 0.4845 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 73 | ANN: trainLoss: 0.1090 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 73 | ANN: trainLoss: 0.1485 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 73 | ANN: trainLoss: 0.1458 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 73 | ANN: trainLoss: 0.1524 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 73 | ANN: trainLoss: 0.1454 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 73 | ANN: trainLoss: 0.1449 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 73 | ANN: trainLoss: 0.1527 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 73 | ANN: trainLoss: 0.1538 | trainAcc: 95.3125% (488/512)\n",
            "8 13 Epoch: 73 | ANN: trainLoss: 0.1561 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 73 | ANN: trainLoss: 0.1527 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 73 | ANN: trainLoss: 0.1568 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 73 | ANN: trainLoss: 0.1558 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 73 | ANN: trainLoss: 0.1560 | trainAcc: 94.8187% (732/772)\n",
            "0 4 Epoch: 73 | ANN: testLoss: 0.7570 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 73 | ANN: testLoss: 0.6870 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 73 | ANN: testLoss: 0.6628 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 73 | ANN: testLoss: 0.7051 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 74 | ANN: trainLoss: 0.1922 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 74 | ANN: trainLoss: 0.1703 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 74 | ANN: trainLoss: 0.1764 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 74 | ANN: trainLoss: 0.1551 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 74 | ANN: trainLoss: 0.1460 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 74 | ANN: trainLoss: 0.1470 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 74 | ANN: trainLoss: 0.1462 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 74 | ANN: trainLoss: 0.1434 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 74 | ANN: trainLoss: 0.1358 | trainAcc: 95.6597% (551/576)\n",
            "9 13 Epoch: 74 | ANN: trainLoss: 0.1430 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 74 | ANN: trainLoss: 0.1393 | trainAcc: 95.3125% (671/704)\n",
            "11 13 Epoch: 74 | ANN: trainLoss: 0.1408 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 74 | ANN: trainLoss: 0.1351 | trainAcc: 95.2073% (735/772)\n",
            "0 4 Epoch: 74 | ANN: testLoss: 0.5618 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 74 | ANN: testLoss: 0.5252 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 74 | ANN: testLoss: 0.6447 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 74 | ANN: testLoss: 0.4836 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 75 | ANN: trainLoss: 0.1741 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 75 | ANN: trainLoss: 0.1532 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 75 | ANN: trainLoss: 0.1353 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 75 | ANN: trainLoss: 0.1386 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 75 | ANN: trainLoss: 0.1373 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 75 | ANN: trainLoss: 0.1307 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 75 | ANN: trainLoss: 0.1332 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 75 | ANN: trainLoss: 0.1356 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 75 | ANN: trainLoss: 0.1362 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 75 | ANN: trainLoss: 0.1346 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 75 | ANN: trainLoss: 0.1455 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 75 | ANN: trainLoss: 0.1437 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 75 | ANN: trainLoss: 0.1605 | trainAcc: 94.4301% (729/772)\n",
            "0 4 Epoch: 75 | ANN: testLoss: 0.5097 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 75 | ANN: testLoss: 0.6027 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 75 | ANN: testLoss: 0.6416 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 75 | ANN: testLoss: 1.4303 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 76 | ANN: trainLoss: 0.1504 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 76 | ANN: trainLoss: 0.1495 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 76 | ANN: trainLoss: 0.1721 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 76 | ANN: trainLoss: 0.1673 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 76 | ANN: trainLoss: 0.1608 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 76 | ANN: trainLoss: 0.1587 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 76 | ANN: trainLoss: 0.1714 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 76 | ANN: trainLoss: 0.1709 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 76 | ANN: trainLoss: 0.1720 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 76 | ANN: trainLoss: 0.1684 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 76 | ANN: trainLoss: 0.1614 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 76 | ANN: trainLoss: 0.1592 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 76 | ANN: trainLoss: 0.1515 | trainAcc: 93.6528% (723/772)\n",
            "0 4 Epoch: 76 | ANN: testLoss: 0.7360 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 76 | ANN: testLoss: 0.6188 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 76 | ANN: testLoss: 0.6641 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 76 | ANN: testLoss: 0.4989 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 77 | ANN: trainLoss: 0.0968 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 77 | ANN: trainLoss: 0.1284 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 77 | ANN: trainLoss: 0.1308 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 77 | ANN: trainLoss: 0.1344 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 77 | ANN: trainLoss: 0.1242 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 77 | ANN: trainLoss: 0.1258 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 77 | ANN: trainLoss: 0.1350 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 77 | ANN: trainLoss: 0.1415 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 77 | ANN: trainLoss: 0.1396 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 77 | ANN: trainLoss: 0.1350 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 77 | ANN: trainLoss: 0.1336 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 77 | ANN: trainLoss: 0.1355 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 77 | ANN: trainLoss: 0.1722 | trainAcc: 95.0777% (734/772)\n",
            "0 4 Epoch: 77 | ANN: testLoss: 0.4428 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 77 | ANN: testLoss: 0.6886 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 77 | ANN: testLoss: 0.6487 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 77 | ANN: testLoss: 0.4867 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 78 | ANN: trainLoss: 0.0840 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 78 | ANN: trainLoss: 0.1580 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 78 | ANN: trainLoss: 0.1471 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 78 | ANN: trainLoss: 0.1756 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 78 | ANN: trainLoss: 0.1702 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 78 | ANN: trainLoss: 0.1675 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 78 | ANN: trainLoss: 0.1547 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 78 | ANN: trainLoss: 0.1526 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 78 | ANN: trainLoss: 0.1564 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 78 | ANN: trainLoss: 0.1643 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 78 | ANN: trainLoss: 0.1622 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 78 | ANN: trainLoss: 0.1667 | trainAcc: 93.4896% (718/768)\n",
            "12 13 Epoch: 78 | ANN: trainLoss: 0.1564 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 78 | ANN: testLoss: 0.7068 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 78 | ANN: testLoss: 0.6705 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 78 | ANN: testLoss: 0.6831 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 78 | ANN: testLoss: 0.5123 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 79 | ANN: trainLoss: 0.1509 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 79 | ANN: trainLoss: 0.1441 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 79 | ANN: trainLoss: 0.1652 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 79 | ANN: trainLoss: 0.1653 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 79 | ANN: trainLoss: 0.1555 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 79 | ANN: trainLoss: 0.1510 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 79 | ANN: trainLoss: 0.1489 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 79 | ANN: trainLoss: 0.1453 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 79 | ANN: trainLoss: 0.1451 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 79 | ANN: trainLoss: 0.1463 | trainAcc: 95.0000% (608/640)\n",
            "10 13 Epoch: 79 | ANN: trainLoss: 0.1489 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 79 | ANN: trainLoss: 0.1516 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 79 | ANN: trainLoss: 0.1517 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 79 | ANN: testLoss: 0.6806 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 79 | ANN: testLoss: 0.5640 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 79 | ANN: testLoss: 0.6489 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 79 | ANN: testLoss: 0.6112 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 80 | ANN: trainLoss: 0.1762 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 80 | ANN: trainLoss: 0.1560 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 80 | ANN: trainLoss: 0.1840 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 80 | ANN: trainLoss: 0.1958 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 80 | ANN: trainLoss: 0.1830 | trainAcc: 92.5000% (296/320)\n",
            "5 13 Epoch: 80 | ANN: trainLoss: 0.1816 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 80 | ANN: trainLoss: 0.1815 | trainAcc: 92.1875% (413/448)\n",
            "7 13 Epoch: 80 | ANN: trainLoss: 0.1718 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 80 | ANN: trainLoss: 0.1657 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 80 | ANN: trainLoss: 0.1574 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 80 | ANN: trainLoss: 0.1613 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 80 | ANN: trainLoss: 0.1597 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 80 | ANN: trainLoss: 0.1685 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 80 | ANN: testLoss: 0.4507 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 80 | ANN: testLoss: 0.5632 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 80 | ANN: testLoss: 0.6532 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 80 | ANN: testLoss: 0.5420 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 81 | ANN: trainLoss: 0.1347 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 81 | ANN: trainLoss: 0.1341 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 81 | ANN: trainLoss: 0.1439 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 81 | ANN: trainLoss: 0.1408 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 81 | ANN: trainLoss: 0.1294 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 81 | ANN: trainLoss: 0.1425 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 81 | ANN: trainLoss: 0.1488 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 81 | ANN: trainLoss: 0.1451 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 81 | ANN: trainLoss: 0.1429 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 81 | ANN: trainLoss: 0.1419 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 81 | ANN: trainLoss: 0.1364 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 81 | ANN: trainLoss: 0.1329 | trainAcc: 94.9219% (729/768)\n",
            "12 13 Epoch: 81 | ANN: trainLoss: 0.1888 | trainAcc: 94.8187% (732/772)\n",
            "0 4 Epoch: 81 | ANN: testLoss: 0.6120 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 81 | ANN: testLoss: 0.6717 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 81 | ANN: testLoss: 0.6564 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 81 | ANN: testLoss: 0.6694 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 82 | ANN: trainLoss: 0.1769 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 82 | ANN: trainLoss: 0.1581 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 82 | ANN: trainLoss: 0.1495 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 82 | ANN: trainLoss: 0.1405 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 82 | ANN: trainLoss: 0.1332 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 82 | ANN: trainLoss: 0.1413 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 82 | ANN: trainLoss: 0.1430 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 82 | ANN: trainLoss: 0.1555 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 82 | ANN: trainLoss: 0.1448 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 82 | ANN: trainLoss: 0.1450 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 82 | ANN: trainLoss: 0.1463 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 82 | ANN: trainLoss: 0.1503 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 82 | ANN: trainLoss: 0.2893 | trainAcc: 94.5596% (730/772)\n",
            "0 4 Epoch: 82 | ANN: testLoss: 0.5829 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 82 | ANN: testLoss: 0.7903 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 82 | ANN: testLoss: 0.6570 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 82 | ANN: testLoss: 0.4929 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 83 | ANN: trainLoss: 0.0888 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 83 | ANN: trainLoss: 0.1300 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 83 | ANN: trainLoss: 0.1397 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 83 | ANN: trainLoss: 0.1379 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 83 | ANN: trainLoss: 0.1352 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 83 | ANN: trainLoss: 0.1456 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 83 | ANN: trainLoss: 0.1394 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 83 | ANN: trainLoss: 0.1339 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 83 | ANN: trainLoss: 0.1315 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 83 | ANN: trainLoss: 0.1295 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 83 | ANN: trainLoss: 0.1346 | trainAcc: 95.5966% (673/704)\n",
            "11 13 Epoch: 83 | ANN: trainLoss: 0.1317 | trainAcc: 95.7031% (735/768)\n",
            "12 13 Epoch: 83 | ANN: trainLoss: 0.1771 | trainAcc: 95.5959% (738/772)\n",
            "0 4 Epoch: 83 | ANN: testLoss: 0.7315 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 83 | ANN: testLoss: 0.6939 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 83 | ANN: testLoss: 0.6545 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 83 | ANN: testLoss: 0.5082 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 84 | ANN: trainLoss: 0.1104 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 84 | ANN: trainLoss: 0.1376 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 84 | ANN: trainLoss: 0.1425 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 84 | ANN: trainLoss: 0.1500 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 84 | ANN: trainLoss: 0.1469 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 84 | ANN: trainLoss: 0.1498 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 84 | ANN: trainLoss: 0.1502 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 84 | ANN: trainLoss: 0.1585 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 84 | ANN: trainLoss: 0.1611 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 84 | ANN: trainLoss: 0.1574 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 84 | ANN: trainLoss: 0.1541 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 84 | ANN: trainLoss: 0.1528 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 84 | ANN: trainLoss: 0.1443 | trainAcc: 94.1710% (727/772)\n",
            "0 4 Epoch: 84 | ANN: testLoss: 0.8041 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 84 | ANN: testLoss: 0.6972 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 84 | ANN: testLoss: 0.6684 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 84 | ANN: testLoss: 0.5015 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 85 | ANN: trainLoss: 0.0981 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 85 | ANN: trainLoss: 0.1038 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 85 | ANN: trainLoss: 0.1146 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 85 | ANN: trainLoss: 0.1307 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 85 | ANN: trainLoss: 0.1344 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 85 | ANN: trainLoss: 0.1548 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 85 | ANN: trainLoss: 0.1519 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 85 | ANN: trainLoss: 0.1483 | trainAcc: 95.3125% (488/512)\n",
            "8 13 Epoch: 85 | ANN: trainLoss: 0.1484 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 85 | ANN: trainLoss: 0.1479 | trainAcc: 95.0000% (608/640)\n",
            "10 13 Epoch: 85 | ANN: trainLoss: 0.1441 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 85 | ANN: trainLoss: 0.1408 | trainAcc: 95.4427% (733/768)\n",
            "12 13 Epoch: 85 | ANN: trainLoss: 0.1475 | trainAcc: 95.4663% (737/772)\n",
            "0 4 Epoch: 85 | ANN: testLoss: 0.7006 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 85 | ANN: testLoss: 0.7411 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 85 | ANN: testLoss: 0.6632 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 85 | ANN: testLoss: 0.4974 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 86 | ANN: trainLoss: 0.1547 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 86 | ANN: trainLoss: 0.1228 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 86 | ANN: trainLoss: 0.1497 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 86 | ANN: trainLoss: 0.1455 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 86 | ANN: trainLoss: 0.1392 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 86 | ANN: trainLoss: 0.1329 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 86 | ANN: trainLoss: 0.1257 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 86 | ANN: trainLoss: 0.1397 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 86 | ANN: trainLoss: 0.1399 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 86 | ANN: trainLoss: 0.1366 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 86 | ANN: trainLoss: 0.1378 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 86 | ANN: trainLoss: 0.1366 | trainAcc: 95.8333% (736/768)\n",
            "12 13 Epoch: 86 | ANN: trainLoss: 0.2052 | trainAcc: 95.7254% (739/772)\n",
            "0 4 Epoch: 86 | ANN: testLoss: 0.5034 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 86 | ANN: testLoss: 0.7217 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 86 | ANN: testLoss: 0.6512 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 86 | ANN: testLoss: 0.5051 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 87 | ANN: trainLoss: 0.1497 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 87 | ANN: trainLoss: 0.1336 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 87 | ANN: trainLoss: 0.1345 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 87 | ANN: trainLoss: 0.1332 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 87 | ANN: trainLoss: 0.1346 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 87 | ANN: trainLoss: 0.1266 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 87 | ANN: trainLoss: 0.1257 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 87 | ANN: trainLoss: 0.1202 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 87 | ANN: trainLoss: 0.1272 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 87 | ANN: trainLoss: 0.1277 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 87 | ANN: trainLoss: 0.1304 | trainAcc: 95.8807% (675/704)\n",
            "11 13 Epoch: 87 | ANN: trainLoss: 0.1301 | trainAcc: 95.7031% (735/768)\n",
            "12 13 Epoch: 87 | ANN: trainLoss: 0.1402 | trainAcc: 95.7254% (739/772)\n",
            "0 4 Epoch: 87 | ANN: testLoss: 0.5292 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 87 | ANN: testLoss: 0.6153 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 87 | ANN: testLoss: 0.6509 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 87 | ANN: testLoss: 0.4881 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 88 | ANN: trainLoss: 0.1463 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 88 | ANN: trainLoss: 0.1591 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 88 | ANN: trainLoss: 0.1462 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 88 | ANN: trainLoss: 0.1315 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 88 | ANN: trainLoss: 0.1452 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 88 | ANN: trainLoss: 0.1395 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 88 | ANN: trainLoss: 0.1349 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 88 | ANN: trainLoss: 0.1402 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 88 | ANN: trainLoss: 0.1454 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 88 | ANN: trainLoss: 0.1423 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 88 | ANN: trainLoss: 0.1411 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 88 | ANN: trainLoss: 0.1419 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 88 | ANN: trainLoss: 0.4157 | trainAcc: 94.4301% (729/772)\n",
            "0 4 Epoch: 88 | ANN: testLoss: 0.5856 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 88 | ANN: testLoss: 0.6302 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 88 | ANN: testLoss: 0.6268 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 88 | ANN: testLoss: 1.9717 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 89 | ANN: trainLoss: 0.1373 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 89 | ANN: trainLoss: 0.1133 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 89 | ANN: trainLoss: 0.1143 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 89 | ANN: trainLoss: 0.1094 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 89 | ANN: trainLoss: 0.1119 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 89 | ANN: trainLoss: 0.1158 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 89 | ANN: trainLoss: 0.1207 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 89 | ANN: trainLoss: 0.1243 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 89 | ANN: trainLoss: 0.1226 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 89 | ANN: trainLoss: 0.1207 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 89 | ANN: trainLoss: 0.1232 | trainAcc: 96.0227% (676/704)\n",
            "11 13 Epoch: 89 | ANN: trainLoss: 0.1210 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 89 | ANN: trainLoss: 0.2234 | trainAcc: 95.5959% (738/772)\n",
            "0 4 Epoch: 89 | ANN: testLoss: 0.5510 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 89 | ANN: testLoss: 0.6421 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 89 | ANN: testLoss: 0.6598 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 89 | ANN: testLoss: 0.4950 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 90 | ANN: trainLoss: 0.1908 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 90 | ANN: trainLoss: 0.1688 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 90 | ANN: trainLoss: 0.1595 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 90 | ANN: trainLoss: 0.1393 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 90 | ANN: trainLoss: 0.1327 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 90 | ANN: trainLoss: 0.1309 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 90 | ANN: trainLoss: 0.1401 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 90 | ANN: trainLoss: 0.1427 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 90 | ANN: trainLoss: 0.1368 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 90 | ANN: trainLoss: 0.1404 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 90 | ANN: trainLoss: 0.1391 | trainAcc: 95.5966% (673/704)\n",
            "11 13 Epoch: 90 | ANN: trainLoss: 0.1375 | trainAcc: 95.5729% (734/768)\n",
            "12 13 Epoch: 90 | ANN: trainLoss: 0.1351 | trainAcc: 95.5959% (738/772)\n",
            "0 4 Epoch: 90 | ANN: testLoss: 0.6555 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 90 | ANN: testLoss: 0.7304 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 90 | ANN: testLoss: 0.6510 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 90 | ANN: testLoss: 0.5537 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 91 | ANN: trainLoss: 0.1298 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 91 | ANN: trainLoss: 0.1848 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 91 | ANN: trainLoss: 0.1570 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 91 | ANN: trainLoss: 0.1396 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 91 | ANN: trainLoss: 0.1401 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 91 | ANN: trainLoss: 0.1495 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 91 | ANN: trainLoss: 0.1511 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 91 | ANN: trainLoss: 0.1530 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 91 | ANN: trainLoss: 0.1512 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 91 | ANN: trainLoss: 0.1579 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 91 | ANN: trainLoss: 0.1558 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 91 | ANN: trainLoss: 0.1546 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 91 | ANN: trainLoss: 0.1474 | trainAcc: 94.5596% (730/772)\n",
            "0 4 Epoch: 91 | ANN: testLoss: 0.4456 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 91 | ANN: testLoss: 0.6439 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 91 | ANN: testLoss: 0.6598 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 91 | ANN: testLoss: 0.5139 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 92 | ANN: trainLoss: 0.1387 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 92 | ANN: trainLoss: 0.1903 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 92 | ANN: trainLoss: 0.1875 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 92 | ANN: trainLoss: 0.1802 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 92 | ANN: trainLoss: 0.1635 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 92 | ANN: trainLoss: 0.1511 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 92 | ANN: trainLoss: 0.1534 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 92 | ANN: trainLoss: 0.1507 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 92 | ANN: trainLoss: 0.1441 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 92 | ANN: trainLoss: 0.1420 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 92 | ANN: trainLoss: 0.1490 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 92 | ANN: trainLoss: 0.1530 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 92 | ANN: trainLoss: 0.1944 | trainAcc: 93.6528% (723/772)\n",
            "0 4 Epoch: 92 | ANN: testLoss: 0.4610 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 92 | ANN: testLoss: 0.5719 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 92 | ANN: testLoss: 0.6410 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 92 | ANN: testLoss: 1.5804 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 93 | ANN: trainLoss: 0.1309 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 93 | ANN: trainLoss: 0.1402 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 93 | ANN: trainLoss: 0.1625 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 93 | ANN: trainLoss: 0.1714 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 93 | ANN: trainLoss: 0.1583 | trainAcc: 92.5000% (296/320)\n",
            "5 13 Epoch: 93 | ANN: trainLoss: 0.1507 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 93 | ANN: trainLoss: 0.1463 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 93 | ANN: trainLoss: 0.1481 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 93 | ANN: trainLoss: 0.1489 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 93 | ANN: trainLoss: 0.1454 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 93 | ANN: trainLoss: 0.1466 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 93 | ANN: trainLoss: 0.1469 | trainAcc: 94.4010% (725/768)\n",
            "12 13 Epoch: 93 | ANN: trainLoss: 0.2374 | trainAcc: 94.1710% (727/772)\n",
            "0 4 Epoch: 93 | ANN: testLoss: 0.7885 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 93 | ANN: testLoss: 0.6640 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 93 | ANN: testLoss: 0.6665 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 93 | ANN: testLoss: 0.5132 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 94 | ANN: trainLoss: 0.0770 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 94 | ANN: trainLoss: 0.0902 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 94 | ANN: trainLoss: 0.1192 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 94 | ANN: trainLoss: 0.1155 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 94 | ANN: trainLoss: 0.1280 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 94 | ANN: trainLoss: 0.1257 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 94 | ANN: trainLoss: 0.1276 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 94 | ANN: trainLoss: 0.1345 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 94 | ANN: trainLoss: 0.1350 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 94 | ANN: trainLoss: 0.1397 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 94 | ANN: trainLoss: 0.1401 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 94 | ANN: trainLoss: 0.1376 | trainAcc: 95.3125% (732/768)\n",
            "12 13 Epoch: 94 | ANN: trainLoss: 0.1356 | trainAcc: 95.3368% (736/772)\n",
            "0 4 Epoch: 94 | ANN: testLoss: 0.7303 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 94 | ANN: testLoss: 0.7130 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 94 | ANN: testLoss: 0.6556 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 94 | ANN: testLoss: 0.4921 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 95 | ANN: trainLoss: 0.0890 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 95 | ANN: trainLoss: 0.0930 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 95 | ANN: trainLoss: 0.1190 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 95 | ANN: trainLoss: 0.1127 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 95 | ANN: trainLoss: 0.1035 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 95 | ANN: trainLoss: 0.1080 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 95 | ANN: trainLoss: 0.1190 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 95 | ANN: trainLoss: 0.1248 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 95 | ANN: trainLoss: 0.1248 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 95 | ANN: trainLoss: 0.1260 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 95 | ANN: trainLoss: 0.1281 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 95 | ANN: trainLoss: 0.1287 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 95 | ANN: trainLoss: 0.1256 | trainAcc: 96.2435% (743/772)\n",
            "0 4 Epoch: 95 | ANN: testLoss: 0.8122 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 95 | ANN: testLoss: 0.7566 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 95 | ANN: testLoss: 0.6678 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 95 | ANN: testLoss: 0.5009 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 96 | ANN: trainLoss: 0.1408 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 96 | ANN: trainLoss: 0.1311 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 96 | ANN: trainLoss: 0.1290 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 96 | ANN: trainLoss: 0.1272 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 96 | ANN: trainLoss: 0.1458 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 96 | ANN: trainLoss: 0.1543 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 96 | ANN: trainLoss: 0.1581 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 96 | ANN: trainLoss: 0.1539 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 96 | ANN: trainLoss: 0.1521 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 96 | ANN: trainLoss: 0.1486 | trainAcc: 95.0000% (608/640)\n",
            "10 13 Epoch: 96 | ANN: trainLoss: 0.1436 | trainAcc: 95.3125% (671/704)\n",
            "11 13 Epoch: 96 | ANN: trainLoss: 0.1483 | trainAcc: 94.9219% (729/768)\n",
            "12 13 Epoch: 96 | ANN: trainLoss: 0.1389 | trainAcc: 94.9482% (733/772)\n",
            "0 4 Epoch: 96 | ANN: testLoss: 0.5253 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 96 | ANN: testLoss: 0.6079 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 96 | ANN: testLoss: 0.6541 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 96 | ANN: testLoss: 0.8477 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 97 | ANN: trainLoss: 0.0703 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 97 | ANN: trainLoss: 0.0872 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 97 | ANN: trainLoss: 0.1128 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 97 | ANN: trainLoss: 0.1170 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 97 | ANN: trainLoss: 0.1251 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 97 | ANN: trainLoss: 0.1180 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 97 | ANN: trainLoss: 0.1146 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 97 | ANN: trainLoss: 0.1118 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 97 | ANN: trainLoss: 0.1198 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 97 | ANN: trainLoss: 0.1342 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 97 | ANN: trainLoss: 0.1298 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 97 | ANN: trainLoss: 0.1282 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 97 | ANN: trainLoss: 0.1698 | trainAcc: 96.1140% (742/772)\n",
            "0 4 Epoch: 97 | ANN: testLoss: 0.6352 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 97 | ANN: testLoss: 0.7408 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 97 | ANN: testLoss: 0.6885 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 97 | ANN: testLoss: 0.8453 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 98 | ANN: trainLoss: 0.1069 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 98 | ANN: trainLoss: 0.1404 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 98 | ANN: trainLoss: 0.1461 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 98 | ANN: trainLoss: 0.1319 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 98 | ANN: trainLoss: 0.1256 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 98 | ANN: trainLoss: 0.1249 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 98 | ANN: trainLoss: 0.1202 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 98 | ANN: trainLoss: 0.1200 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 98 | ANN: trainLoss: 0.1296 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 98 | ANN: trainLoss: 0.1347 | trainAcc: 95.6250% (612/640)\n",
            "10 13 Epoch: 98 | ANN: trainLoss: 0.1354 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 98 | ANN: trainLoss: 0.1320 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 98 | ANN: trainLoss: 0.1492 | trainAcc: 95.9845% (741/772)\n",
            "0 4 Epoch: 98 | ANN: testLoss: 0.6037 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 98 | ANN: testLoss: 0.6580 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 98 | ANN: testLoss: 0.6672 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 98 | ANN: testLoss: 0.5005 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 99 | ANN: trainLoss: 0.1119 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 99 | ANN: trainLoss: 0.1076 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 99 | ANN: trainLoss: 0.1366 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 99 | ANN: trainLoss: 0.1588 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 99 | ANN: trainLoss: 0.1532 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 99 | ANN: trainLoss: 0.1408 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 99 | ANN: trainLoss: 0.1418 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 99 | ANN: trainLoss: 0.1312 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 99 | ANN: trainLoss: 0.1352 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 99 | ANN: trainLoss: 0.1394 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 99 | ANN: trainLoss: 0.1368 | trainAcc: 96.0227% (676/704)\n",
            "11 13 Epoch: 99 | ANN: trainLoss: 0.1383 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 99 | ANN: trainLoss: 0.2343 | trainAcc: 95.7254% (739/772)\n",
            "0 4 Epoch: 99 | ANN: testLoss: 0.8137 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 99 | ANN: testLoss: 0.7842 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 99 | ANN: testLoss: 0.6621 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 99 | ANN: testLoss: 0.9144 | testAcc: 72.5389% (140/193)\n",
            "---------------------------------------------\n",
            "Converting using MaxNorm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 419.18it/s]\n",
            "100%|██████████| 13/13 [00:00<00:00, 358.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ANN accuracy: Test: 72.5400%\n",
            "SNN accuracy: max_norm: 73.0570%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NN Visualization\n",
        "\n",
        "# This block should be placed after the training and SNN conversion\n",
        "# of the model you wish to visualize. For example, after the\n",
        "# \"LENet to SNN Conversion Framework execution\" cell or\n",
        "# after the \"LENet_FCL to SNN Conversion Framework execution\" cell.\n",
        "# The `cnn_model_lenet` and `Snn_model_lenet` variables from that preceding cell\n",
        "# will be used for visualization.\n",
        "\n",
        "# Install nnviz if you haven't already (uncomment the line below if needed)\n",
        "# !pip install nnviz\n",
        "\n",
        "# Import necessary nnviz modules\n",
        "from nnviz import drawing, inspection\n",
        "import torch # Ensure torch is imported\n",
        "\n",
        "print(\"Starting Neural Network Visualization...\")\n",
        "\n",
        "# --- CNN Model Visualization ---\n",
        "if 'cnn_model_lenet' in locals() and isinstance(cnn_model_lenet, torch.nn.Module):\n",
        "    model_to_visualize_cnn = cnn_model_lenet\n",
        "    cnn_model_lenet_name = type(model_to_visualize_cnn).__name__\n",
        "    print(f\"\\nVisualizing LENet CNN model: {cnn_model_lenet_name}\")\n",
        "\n",
        "    try:\n",
        "        # Move model to CPU for inspection (safer for fx tracing)\n",
        "        cnn_model_lenet_cpu = model_to_visualize_cnn.to('cpu')\n",
        "\n",
        "        # Create an inspector\n",
        "        # TorchFxInspector uses torch.fx to trace the model graph\n",
        "        cnn_inspector = inspection.TorchFxInspector()\n",
        "\n",
        "        # Inspect the CNN model\n",
        "        # For models like LENet_FCL, training (as done in your script) ensures that\n",
        "        # dynamically created layers (e.g., self.fc) are initialized before inspection.\n",
        "        print(f\"Inspecting {cnn_model_lenet_name} on CPU...\")\n",
        "        cnn_graph = cnn_inspector.inspect(cnn_model_lenet_cpu)\n",
        "\n",
        "        # Create a drawer for saving the visualization (e.g., to a PDF file)\n",
        "        # You can change the output format by changing the extension (e.g., .png, .svg)\n",
        "        cnn_viz_filename = f\"{cnn_model_lenet_name}_LENet_cnn_architecture.png\"\n",
        "        cnn_drawer = drawing.GraphvizDrawer(cnn_viz_filename)\n",
        "\n",
        "        # Draw the graph and save it to the file\n",
        "        cnn_drawer.draw(cnn_graph)\n",
        "        print(f\"LENet CNN model visualization saved to: {cnn_viz_filename}\")\n",
        "        print(\n",
        "            f\"Note: If the output file is empty or shows an error, ensure Graphviz \"\n",
        "            f\"(specifically the 'dot' command) is installed and accessible in your system's PATH.\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\n",
        "            f\"An error occurred during CNN model ({cnn_model_lenet_name}) visualization: {e}\"\n",
        "        )\n",
        "        print(\"Troubleshooting tips:\")\n",
        "        print(\"- Ensure 'nnviz' is installed.\")\n",
        "        print(\n",
        "            \"- Ensure 'graphviz' (dot executable) is installed and in your system's PATH.\"\n",
        "        )\n",
        "        print(\n",
        "            \"- The model structure might contain operations not traceable by torch.fx. \"\n",
        "            \"Check nnviz documentation for advanced usage or alternative inspectors if needed.\"\n",
        "        )\n",
        "else:\n",
        "    print(\n",
        "        \"\\nCNN model ('cnn_model_lenet') not found in the current scope, or it's not a torch.nn.Module. \"\n",
        "        \"Skipping CNN visualization.\"\n",
        "    )\n",
        "print(\"\\nNeural Network Visualization process complete.\")"
      ],
      "metadata": {
        "id": "V8p4PokzI0HX",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LENet_FCL to SNN Conversion Framework execution\n",
        "\n",
        "# Hyperparameters\n",
        "#EPOCHS = 300\n",
        "#BATCH_SIZE = 128\n",
        "#TIME_STEPS = 100  # T for SNN\n",
        "#TEST_SIZE = 0.2\n",
        "#DROP_OUT = 0.5\n",
        "\n",
        "\n",
        "# Split the data\n",
        "print(f\"{100 - (TEST_SIZE * 100)}% of the dataset is used for training and {TEST_SIZE * 100}% is used for testing.\")\n",
        "train_data, test_data, train_label, test_label = train_test_split(datasetX, datasetY, test_size=TEST_SIZE, shuffle=True,\n",
        "                                                                  random_state=0)\n",
        "\n",
        "# Initialize model\n",
        "cnn_model_lenet_fcl = LENet_FCL(classes_num=3, channel_count=channel_count, drop_out = DROP_OUT).to(device)\n",
        "cnn_model_lenet_fcl.apply(initialize_weights)\n",
        "\n",
        "# Train CNN model\n",
        "train_acc, test_acc,  cnn_model_lenet_fcl = train_ann(cnn_model_lenet_fcl, train_data, train_label, test_data, test_label,\n",
        "                                              ep=EPOCHS, batch=BATCH_SIZE)\n",
        "max_norm_acc = anntosnn( cnn_model_lenet_fcl, train_data, train_label, test_data, test_label,\n",
        "                        batch=BATCH_SIZE, T=TIME_STEPS)\n",
        "snn_model_lenet_fcl = ann2snn.Converter(mode='max', dataloader=data_loader(train_data, train_label, batch=BATCH_SIZE))(cnn_model_lenet_fcl)\n",
        "\n",
        "print('\\n')\n",
        "print('ANN accuracy: Test: %.4f%%' % (test_acc * 100))\n",
        "print('SNN accuracy: max_norm: %.4f%%' % (max_norm_acc[-1] * 100))"
      ],
      "metadata": {
        "id": "AeiiLk4TQp-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "outputId": "db05b42d-043a-402c-ee93-b03fa71ac6b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80.0% of the dataset is used for training and 20.0% is used for testing.\n",
            "\n",
            "\n",
            "0 13 Epoch: 0 | ANN: trainLoss: 3.1822 | trainAcc: 31.2500% (20/64)\n",
            "1 13 Epoch: 0 | ANN: trainLoss: 2.8351 | trainAcc: 35.9375% (46/128)\n",
            "2 13 Epoch: 0 | ANN: trainLoss: 2.8573 | trainAcc: 35.4167% (68/192)\n",
            "3 13 Epoch: 0 | ANN: trainLoss: 2.5634 | trainAcc: 38.6719% (99/256)\n",
            "4 13 Epoch: 0 | ANN: trainLoss: 2.6083 | trainAcc: 35.6250% (114/320)\n",
            "5 13 Epoch: 0 | ANN: trainLoss: 2.5967 | trainAcc: 35.6771% (137/384)\n",
            "6 13 Epoch: 0 | ANN: trainLoss: 2.5266 | trainAcc: 36.6071% (164/448)\n",
            "7 13 Epoch: 0 | ANN: trainLoss: 2.4522 | trainAcc: 37.3047% (191/512)\n",
            "8 13 Epoch: 0 | ANN: trainLoss: 2.4524 | trainAcc: 37.3264% (215/576)\n",
            "9 13 Epoch: 0 | ANN: trainLoss: 2.4030 | trainAcc: 37.8125% (242/640)\n",
            "10 13 Epoch: 0 | ANN: trainLoss: 2.4027 | trainAcc: 37.7841% (266/704)\n",
            "11 13 Epoch: 0 | ANN: trainLoss: 2.3559 | trainAcc: 38.1510% (293/768)\n",
            "12 13 Epoch: 0 | ANN: trainLoss: 2.3596 | trainAcc: 38.0829% (294/772)\n",
            "0 4 Epoch: 0 | ANN: testLoss: 1.2917 | testAcc: 42.1875% (27/64)\n",
            "1 4 Epoch: 0 | ANN: testLoss: 1.4653 | testAcc: 42.1875% (54/128)\n",
            "2 4 Epoch: 0 | ANN: testLoss: 1.4709 | testAcc: 42.7083% (82/192)\n",
            "3 4 Epoch: 0 | ANN: testLoss: 1.8340 | testAcc: 42.4870% (82/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 1 | ANN: trainLoss: 1.4301 | trainAcc: 56.2500% (36/64)\n",
            "1 13 Epoch: 1 | ANN: trainLoss: 1.5386 | trainAcc: 50.7812% (65/128)\n",
            "2 13 Epoch: 1 | ANN: trainLoss: 1.4700 | trainAcc: 52.6042% (101/192)\n",
            "3 13 Epoch: 1 | ANN: trainLoss: 1.7594 | trainAcc: 45.7031% (117/256)\n",
            "4 13 Epoch: 1 | ANN: trainLoss: 1.7154 | trainAcc: 45.9375% (147/320)\n",
            "5 13 Epoch: 1 | ANN: trainLoss: 1.6897 | trainAcc: 45.0521% (173/384)\n",
            "6 13 Epoch: 1 | ANN: trainLoss: 1.7459 | trainAcc: 44.6429% (200/448)\n",
            "7 13 Epoch: 1 | ANN: trainLoss: 1.8085 | trainAcc: 44.5312% (228/512)\n",
            "8 13 Epoch: 1 | ANN: trainLoss: 1.7767 | trainAcc: 45.1389% (260/576)\n",
            "9 13 Epoch: 1 | ANN: trainLoss: 1.7521 | trainAcc: 46.0938% (295/640)\n",
            "10 13 Epoch: 1 | ANN: trainLoss: 1.7472 | trainAcc: 45.8807% (323/704)\n",
            "11 13 Epoch: 1 | ANN: trainLoss: 1.7054 | trainAcc: 46.7448% (359/768)\n",
            "12 13 Epoch: 1 | ANN: trainLoss: 1.6646 | trainAcc: 46.7617% (361/772)\n",
            "0 4 Epoch: 1 | ANN: testLoss: 2.0739 | testAcc: 50.0000% (32/64)\n",
            "1 4 Epoch: 1 | ANN: testLoss: 1.9846 | testAcc: 42.9688% (55/128)\n",
            "2 4 Epoch: 1 | ANN: testLoss: 1.8842 | testAcc: 43.2292% (83/192)\n",
            "3 4 Epoch: 1 | ANN: testLoss: 1.4440 | testAcc: 43.5233% (84/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 2 | ANN: trainLoss: 1.5072 | trainAcc: 54.6875% (35/64)\n",
            "1 13 Epoch: 2 | ANN: trainLoss: 1.7215 | trainAcc: 53.1250% (68/128)\n",
            "2 13 Epoch: 2 | ANN: trainLoss: 1.5545 | trainAcc: 53.6458% (103/192)\n",
            "3 13 Epoch: 2 | ANN: trainLoss: 1.5854 | trainAcc: 52.3438% (134/256)\n",
            "4 13 Epoch: 2 | ANN: trainLoss: 1.5240 | trainAcc: 52.5000% (168/320)\n",
            "5 13 Epoch: 2 | ANN: trainLoss: 1.4661 | trainAcc: 52.8646% (203/384)\n",
            "6 13 Epoch: 2 | ANN: trainLoss: 1.4841 | trainAcc: 52.6786% (236/448)\n",
            "7 13 Epoch: 2 | ANN: trainLoss: 1.4979 | trainAcc: 51.9531% (266/512)\n",
            "8 13 Epoch: 2 | ANN: trainLoss: 1.4924 | trainAcc: 52.4306% (302/576)\n",
            "9 13 Epoch: 2 | ANN: trainLoss: 1.4727 | trainAcc: 51.4062% (329/640)\n",
            "10 13 Epoch: 2 | ANN: trainLoss: 1.4560 | trainAcc: 51.7045% (364/704)\n",
            "11 13 Epoch: 2 | ANN: trainLoss: 1.4799 | trainAcc: 51.5625% (396/768)\n",
            "12 13 Epoch: 2 | ANN: trainLoss: 1.5665 | trainAcc: 51.4249% (397/772)\n",
            "0 4 Epoch: 2 | ANN: testLoss: 1.5071 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 2 | ANN: testLoss: 1.4302 | testAcc: 53.1250% (68/128)\n",
            "2 4 Epoch: 2 | ANN: testLoss: 1.3262 | testAcc: 52.6042% (101/192)\n",
            "3 4 Epoch: 2 | ANN: testLoss: 1.1080 | testAcc: 52.8497% (102/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 3 | ANN: trainLoss: 1.4282 | trainAcc: 51.5625% (33/64)\n",
            "1 13 Epoch: 3 | ANN: trainLoss: 1.2912 | trainAcc: 53.9062% (69/128)\n",
            "2 13 Epoch: 3 | ANN: trainLoss: 1.3596 | trainAcc: 53.6458% (103/192)\n",
            "3 13 Epoch: 3 | ANN: trainLoss: 1.3832 | trainAcc: 52.3438% (134/256)\n",
            "4 13 Epoch: 3 | ANN: trainLoss: 1.3389 | trainAcc: 53.4375% (171/320)\n",
            "5 13 Epoch: 3 | ANN: trainLoss: 1.3008 | trainAcc: 52.6042% (202/384)\n",
            "6 13 Epoch: 3 | ANN: trainLoss: 1.2965 | trainAcc: 53.3482% (239/448)\n",
            "7 13 Epoch: 3 | ANN: trainLoss: 1.2909 | trainAcc: 52.5391% (269/512)\n",
            "8 13 Epoch: 3 | ANN: trainLoss: 1.2867 | trainAcc: 51.2153% (295/576)\n",
            "9 13 Epoch: 3 | ANN: trainLoss: 1.2577 | trainAcc: 51.2500% (328/640)\n",
            "10 13 Epoch: 3 | ANN: trainLoss: 1.2506 | trainAcc: 51.1364% (360/704)\n",
            "11 13 Epoch: 3 | ANN: trainLoss: 1.2669 | trainAcc: 50.1302% (385/768)\n",
            "12 13 Epoch: 3 | ANN: trainLoss: 1.3938 | trainAcc: 50.0000% (386/772)\n",
            "0 4 Epoch: 3 | ANN: testLoss: 0.9687 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 3 | ANN: testLoss: 1.0530 | testAcc: 58.5938% (75/128)\n",
            "2 4 Epoch: 3 | ANN: testLoss: 1.0231 | testAcc: 55.7292% (107/192)\n",
            "3 4 Epoch: 3 | ANN: testLoss: 0.7829 | testAcc: 55.9585% (108/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 4 | ANN: trainLoss: 1.0569 | trainAcc: 50.0000% (32/64)\n",
            "1 13 Epoch: 4 | ANN: trainLoss: 1.0854 | trainAcc: 53.1250% (68/128)\n",
            "2 13 Epoch: 4 | ANN: trainLoss: 1.1457 | trainAcc: 49.4792% (95/192)\n",
            "3 13 Epoch: 4 | ANN: trainLoss: 1.1540 | trainAcc: 50.7812% (130/256)\n",
            "4 13 Epoch: 4 | ANN: trainLoss: 1.1818 | trainAcc: 50.3125% (161/320)\n",
            "5 13 Epoch: 4 | ANN: trainLoss: 1.2062 | trainAcc: 49.4792% (190/384)\n",
            "6 13 Epoch: 4 | ANN: trainLoss: 1.1812 | trainAcc: 49.3304% (221/448)\n",
            "7 13 Epoch: 4 | ANN: trainLoss: 1.1743 | trainAcc: 49.4141% (253/512)\n",
            "8 13 Epoch: 4 | ANN: trainLoss: 1.1548 | trainAcc: 49.8264% (287/576)\n",
            "9 13 Epoch: 4 | ANN: trainLoss: 1.1485 | trainAcc: 50.9375% (326/640)\n",
            "10 13 Epoch: 4 | ANN: trainLoss: 1.1365 | trainAcc: 52.2727% (368/704)\n",
            "11 13 Epoch: 4 | ANN: trainLoss: 1.1310 | trainAcc: 52.4740% (403/768)\n",
            "12 13 Epoch: 4 | ANN: trainLoss: 1.1634 | trainAcc: 52.3316% (404/772)\n",
            "0 4 Epoch: 4 | ANN: testLoss: 1.0304 | testAcc: 46.8750% (30/64)\n",
            "1 4 Epoch: 4 | ANN: testLoss: 0.9017 | testAcc: 54.6875% (70/128)\n",
            "2 4 Epoch: 4 | ANN: testLoss: 0.8867 | testAcc: 53.6458% (103/192)\n",
            "3 4 Epoch: 4 | ANN: testLoss: 0.7494 | testAcc: 53.8860% (104/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 5 | ANN: trainLoss: 1.0684 | trainAcc: 53.1250% (34/64)\n",
            "1 13 Epoch: 5 | ANN: trainLoss: 1.0461 | trainAcc: 53.9062% (69/128)\n",
            "2 13 Epoch: 5 | ANN: trainLoss: 1.0352 | trainAcc: 53.1250% (102/192)\n",
            "3 13 Epoch: 5 | ANN: trainLoss: 0.9815 | trainAcc: 54.6875% (140/256)\n",
            "4 13 Epoch: 5 | ANN: trainLoss: 1.0092 | trainAcc: 55.9375% (179/320)\n",
            "5 13 Epoch: 5 | ANN: trainLoss: 0.9922 | trainAcc: 57.0312% (219/384)\n",
            "6 13 Epoch: 5 | ANN: trainLoss: 0.9778 | trainAcc: 56.9196% (255/448)\n",
            "7 13 Epoch: 5 | ANN: trainLoss: 0.9676 | trainAcc: 57.4219% (294/512)\n",
            "8 13 Epoch: 5 | ANN: trainLoss: 0.9758 | trainAcc: 57.8125% (333/576)\n",
            "9 13 Epoch: 5 | ANN: trainLoss: 0.9659 | trainAcc: 58.1250% (372/640)\n",
            "10 13 Epoch: 5 | ANN: trainLoss: 0.9746 | trainAcc: 58.2386% (410/704)\n",
            "11 13 Epoch: 5 | ANN: trainLoss: 0.9802 | trainAcc: 58.5938% (450/768)\n",
            "12 13 Epoch: 5 | ANN: trainLoss: 1.0210 | trainAcc: 58.5492% (452/772)\n",
            "0 4 Epoch: 5 | ANN: testLoss: 0.8569 | testAcc: 54.6875% (35/64)\n",
            "1 4 Epoch: 5 | ANN: testLoss: 0.9105 | testAcc: 52.3438% (67/128)\n",
            "2 4 Epoch: 5 | ANN: testLoss: 0.8700 | testAcc: 56.2500% (108/192)\n",
            "3 4 Epoch: 5 | ANN: testLoss: 0.8227 | testAcc: 56.4767% (109/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 6 | ANN: trainLoss: 1.0342 | trainAcc: 54.6875% (35/64)\n",
            "1 13 Epoch: 6 | ANN: trainLoss: 1.0926 | trainAcc: 54.6875% (70/128)\n",
            "2 13 Epoch: 6 | ANN: trainLoss: 1.0400 | trainAcc: 55.2083% (106/192)\n",
            "3 13 Epoch: 6 | ANN: trainLoss: 1.0457 | trainAcc: 54.6875% (140/256)\n",
            "4 13 Epoch: 6 | ANN: trainLoss: 1.0575 | trainAcc: 52.1875% (167/320)\n",
            "5 13 Epoch: 6 | ANN: trainLoss: 0.9927 | trainAcc: 54.9479% (211/384)\n",
            "6 13 Epoch: 6 | ANN: trainLoss: 0.9859 | trainAcc: 54.6875% (245/448)\n",
            "7 13 Epoch: 6 | ANN: trainLoss: 1.0071 | trainAcc: 54.4922% (279/512)\n",
            "8 13 Epoch: 6 | ANN: trainLoss: 1.0002 | trainAcc: 55.2083% (318/576)\n",
            "9 13 Epoch: 6 | ANN: trainLoss: 0.9852 | trainAcc: 56.8750% (364/640)\n",
            "10 13 Epoch: 6 | ANN: trainLoss: 0.9856 | trainAcc: 56.2500% (396/704)\n",
            "11 13 Epoch: 6 | ANN: trainLoss: 0.9829 | trainAcc: 56.3802% (433/768)\n",
            "12 13 Epoch: 6 | ANN: trainLoss: 0.9305 | trainAcc: 56.6062% (437/772)\n",
            "0 4 Epoch: 6 | ANN: testLoss: 0.7851 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 6 | ANN: testLoss: 0.8131 | testAcc: 59.3750% (76/128)\n",
            "2 4 Epoch: 6 | ANN: testLoss: 0.8110 | testAcc: 60.4167% (116/192)\n",
            "3 4 Epoch: 6 | ANN: testLoss: 0.8004 | testAcc: 60.6218% (117/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 7 | ANN: trainLoss: 0.8759 | trainAcc: 56.2500% (36/64)\n",
            "1 13 Epoch: 7 | ANN: trainLoss: 0.8337 | trainAcc: 60.9375% (78/128)\n",
            "2 13 Epoch: 7 | ANN: trainLoss: 0.8821 | trainAcc: 57.8125% (111/192)\n",
            "3 13 Epoch: 7 | ANN: trainLoss: 0.8865 | trainAcc: 56.2500% (144/256)\n",
            "4 13 Epoch: 7 | ANN: trainLoss: 0.8784 | trainAcc: 57.1875% (183/320)\n",
            "5 13 Epoch: 7 | ANN: trainLoss: 0.8714 | trainAcc: 58.3333% (224/384)\n",
            "6 13 Epoch: 7 | ANN: trainLoss: 0.8555 | trainAcc: 58.7054% (263/448)\n",
            "7 13 Epoch: 7 | ANN: trainLoss: 0.8690 | trainAcc: 58.0078% (297/512)\n",
            "8 13 Epoch: 7 | ANN: trainLoss: 0.8641 | trainAcc: 57.6389% (332/576)\n",
            "9 13 Epoch: 7 | ANN: trainLoss: 0.8578 | trainAcc: 58.1250% (372/640)\n",
            "10 13 Epoch: 7 | ANN: trainLoss: 0.8474 | trainAcc: 58.6648% (413/704)\n",
            "11 13 Epoch: 7 | ANN: trainLoss: 0.8613 | trainAcc: 58.0729% (446/768)\n",
            "12 13 Epoch: 7 | ANN: trainLoss: 0.8651 | trainAcc: 58.1606% (449/772)\n",
            "0 4 Epoch: 7 | ANN: testLoss: 0.8466 | testAcc: 50.0000% (32/64)\n",
            "1 4 Epoch: 7 | ANN: testLoss: 0.8224 | testAcc: 53.1250% (68/128)\n",
            "2 4 Epoch: 7 | ANN: testLoss: 0.8132 | testAcc: 54.1667% (104/192)\n",
            "3 4 Epoch: 7 | ANN: testLoss: 0.6407 | testAcc: 54.4041% (105/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 8 | ANN: trainLoss: 0.7353 | trainAcc: 60.9375% (39/64)\n",
            "1 13 Epoch: 8 | ANN: trainLoss: 0.8267 | trainAcc: 57.0312% (73/128)\n",
            "2 13 Epoch: 8 | ANN: trainLoss: 0.8236 | trainAcc: 57.2917% (110/192)\n",
            "3 13 Epoch: 8 | ANN: trainLoss: 0.8273 | trainAcc: 59.7656% (153/256)\n",
            "4 13 Epoch: 8 | ANN: trainLoss: 0.8361 | trainAcc: 59.6875% (191/320)\n",
            "5 13 Epoch: 8 | ANN: trainLoss: 0.8365 | trainAcc: 59.6354% (229/384)\n",
            "6 13 Epoch: 8 | ANN: trainLoss: 0.8088 | trainAcc: 60.9375% (273/448)\n",
            "7 13 Epoch: 8 | ANN: trainLoss: 0.8063 | trainAcc: 61.1328% (313/512)\n",
            "8 13 Epoch: 8 | ANN: trainLoss: 0.7955 | trainAcc: 60.9375% (351/576)\n",
            "9 13 Epoch: 8 | ANN: trainLoss: 0.8029 | trainAcc: 60.6250% (388/640)\n",
            "10 13 Epoch: 8 | ANN: trainLoss: 0.8012 | trainAcc: 60.7955% (428/704)\n",
            "11 13 Epoch: 8 | ANN: trainLoss: 0.7997 | trainAcc: 61.3281% (471/768)\n",
            "12 13 Epoch: 8 | ANN: trainLoss: 0.7762 | trainAcc: 61.5285% (475/772)\n",
            "0 4 Epoch: 8 | ANN: testLoss: 0.8185 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 8 | ANN: testLoss: 0.8689 | testAcc: 55.4688% (71/128)\n",
            "2 4 Epoch: 8 | ANN: testLoss: 0.8384 | testAcc: 55.2083% (106/192)\n",
            "3 4 Epoch: 8 | ANN: testLoss: 0.6289 | testAcc: 55.4404% (107/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 9 | ANN: trainLoss: 0.8549 | trainAcc: 60.9375% (39/64)\n",
            "1 13 Epoch: 9 | ANN: trainLoss: 0.8591 | trainAcc: 60.1562% (77/128)\n",
            "2 13 Epoch: 9 | ANN: trainLoss: 0.8321 | trainAcc: 59.8958% (115/192)\n",
            "3 13 Epoch: 9 | ANN: trainLoss: 0.8616 | trainAcc: 60.1562% (154/256)\n",
            "4 13 Epoch: 9 | ANN: trainLoss: 0.9127 | trainAcc: 59.0625% (189/320)\n",
            "5 13 Epoch: 9 | ANN: trainLoss: 0.9181 | trainAcc: 58.5938% (225/384)\n",
            "6 13 Epoch: 9 | ANN: trainLoss: 0.8914 | trainAcc: 59.8214% (268/448)\n",
            "7 13 Epoch: 9 | ANN: trainLoss: 0.8647 | trainAcc: 60.7422% (311/512)\n",
            "8 13 Epoch: 9 | ANN: trainLoss: 0.8661 | trainAcc: 60.4167% (348/576)\n",
            "9 13 Epoch: 9 | ANN: trainLoss: 0.8622 | trainAcc: 60.3125% (386/640)\n",
            "10 13 Epoch: 9 | ANN: trainLoss: 0.8700 | trainAcc: 59.9432% (422/704)\n",
            "11 13 Epoch: 9 | ANN: trainLoss: 0.8585 | trainAcc: 60.1562% (462/768)\n",
            "12 13 Epoch: 9 | ANN: trainLoss: 0.8481 | trainAcc: 60.2332% (465/772)\n",
            "0 4 Epoch: 9 | ANN: testLoss: 0.8839 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 9 | ANN: testLoss: 0.8213 | testAcc: 61.7188% (79/128)\n",
            "2 4 Epoch: 9 | ANN: testLoss: 0.8177 | testAcc: 58.3333% (112/192)\n",
            "3 4 Epoch: 9 | ANN: testLoss: 0.6691 | testAcc: 58.5492% (113/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 10 | ANN: trainLoss: 0.6380 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 10 | ANN: trainLoss: 0.6832 | trainAcc: 74.2188% (95/128)\n",
            "2 13 Epoch: 10 | ANN: trainLoss: 0.7479 | trainAcc: 68.7500% (132/192)\n",
            "3 13 Epoch: 10 | ANN: trainLoss: 0.7367 | trainAcc: 68.7500% (176/256)\n",
            "4 13 Epoch: 10 | ANN: trainLoss: 0.7403 | trainAcc: 67.5000% (216/320)\n",
            "5 13 Epoch: 10 | ANN: trainLoss: 0.7469 | trainAcc: 66.1458% (254/384)\n",
            "6 13 Epoch: 10 | ANN: trainLoss: 0.7530 | trainAcc: 66.5179% (298/448)\n",
            "7 13 Epoch: 10 | ANN: trainLoss: 0.7519 | trainAcc: 66.7969% (342/512)\n",
            "8 13 Epoch: 10 | ANN: trainLoss: 0.7695 | trainAcc: 65.2778% (376/576)\n",
            "9 13 Epoch: 10 | ANN: trainLoss: 0.7833 | trainAcc: 64.6875% (414/640)\n",
            "10 13 Epoch: 10 | ANN: trainLoss: 0.7971 | trainAcc: 63.7784% (449/704)\n",
            "11 13 Epoch: 10 | ANN: trainLoss: 0.7833 | trainAcc: 64.7135% (497/768)\n",
            "12 13 Epoch: 10 | ANN: trainLoss: 0.7749 | trainAcc: 64.7668% (500/772)\n",
            "0 4 Epoch: 10 | ANN: testLoss: 0.8670 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 10 | ANN: testLoss: 0.7866 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 10 | ANN: testLoss: 0.8091 | testAcc: 59.8958% (115/192)\n",
            "3 4 Epoch: 10 | ANN: testLoss: 0.7210 | testAcc: 60.1036% (116/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 11 | ANN: trainLoss: 0.8544 | trainAcc: 57.8125% (37/64)\n",
            "1 13 Epoch: 11 | ANN: trainLoss: 0.8313 | trainAcc: 60.1562% (77/128)\n",
            "2 13 Epoch: 11 | ANN: trainLoss: 0.8102 | trainAcc: 59.3750% (114/192)\n",
            "3 13 Epoch: 11 | ANN: trainLoss: 0.8100 | trainAcc: 59.7656% (153/256)\n",
            "4 13 Epoch: 11 | ANN: trainLoss: 0.8020 | trainAcc: 60.6250% (194/320)\n",
            "5 13 Epoch: 11 | ANN: trainLoss: 0.7800 | trainAcc: 61.9792% (238/384)\n",
            "6 13 Epoch: 11 | ANN: trainLoss: 0.7935 | trainAcc: 60.2679% (270/448)\n",
            "7 13 Epoch: 11 | ANN: trainLoss: 0.7960 | trainAcc: 60.3516% (309/512)\n",
            "8 13 Epoch: 11 | ANN: trainLoss: 0.7834 | trainAcc: 61.1111% (352/576)\n",
            "9 13 Epoch: 11 | ANN: trainLoss: 0.7795 | trainAcc: 62.0312% (397/640)\n",
            "10 13 Epoch: 11 | ANN: trainLoss: 0.7684 | trainAcc: 63.0682% (444/704)\n",
            "11 13 Epoch: 11 | ANN: trainLoss: 0.7665 | trainAcc: 63.5417% (488/768)\n",
            "12 13 Epoch: 11 | ANN: trainLoss: 0.7641 | trainAcc: 63.6010% (491/772)\n",
            "0 4 Epoch: 11 | ANN: testLoss: 0.8331 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 11 | ANN: testLoss: 0.7831 | testAcc: 57.8125% (74/128)\n",
            "2 4 Epoch: 11 | ANN: testLoss: 0.7631 | testAcc: 61.4583% (118/192)\n",
            "3 4 Epoch: 11 | ANN: testLoss: 0.7890 | testAcc: 61.6580% (119/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 12 | ANN: trainLoss: 0.8660 | trainAcc: 57.8125% (37/64)\n",
            "1 13 Epoch: 12 | ANN: trainLoss: 0.7782 | trainAcc: 69.5312% (89/128)\n",
            "2 13 Epoch: 12 | ANN: trainLoss: 0.7428 | trainAcc: 69.2708% (133/192)\n",
            "3 13 Epoch: 12 | ANN: trainLoss: 0.7352 | trainAcc: 67.5781% (173/256)\n",
            "4 13 Epoch: 12 | ANN: trainLoss: 0.7217 | trainAcc: 68.7500% (220/320)\n",
            "5 13 Epoch: 12 | ANN: trainLoss: 0.7050 | trainAcc: 70.0521% (269/384)\n",
            "6 13 Epoch: 12 | ANN: trainLoss: 0.7176 | trainAcc: 68.0804% (305/448)\n",
            "7 13 Epoch: 12 | ANN: trainLoss: 0.7161 | trainAcc: 68.1641% (349/512)\n",
            "8 13 Epoch: 12 | ANN: trainLoss: 0.7094 | trainAcc: 67.7083% (390/576)\n",
            "9 13 Epoch: 12 | ANN: trainLoss: 0.7004 | trainAcc: 68.4375% (438/640)\n",
            "10 13 Epoch: 12 | ANN: trainLoss: 0.7122 | trainAcc: 68.0398% (479/704)\n",
            "11 13 Epoch: 12 | ANN: trainLoss: 0.7126 | trainAcc: 67.7083% (520/768)\n",
            "12 13 Epoch: 12 | ANN: trainLoss: 0.6979 | trainAcc: 67.7461% (523/772)\n",
            "0 4 Epoch: 12 | ANN: testLoss: 0.7764 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 12 | ANN: testLoss: 0.7719 | testAcc: 60.1562% (77/128)\n",
            "2 4 Epoch: 12 | ANN: testLoss: 0.7382 | testAcc: 61.4583% (118/192)\n",
            "3 4 Epoch: 12 | ANN: testLoss: 0.7606 | testAcc: 61.1399% (118/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 13 | ANN: trainLoss: 0.6916 | trainAcc: 68.7500% (44/64)\n",
            "1 13 Epoch: 13 | ANN: trainLoss: 0.6952 | trainAcc: 65.6250% (84/128)\n",
            "2 13 Epoch: 13 | ANN: trainLoss: 0.7251 | trainAcc: 62.5000% (120/192)\n",
            "3 13 Epoch: 13 | ANN: trainLoss: 0.7359 | trainAcc: 63.2812% (162/256)\n",
            "4 13 Epoch: 13 | ANN: trainLoss: 0.7313 | trainAcc: 63.7500% (204/320)\n",
            "5 13 Epoch: 13 | ANN: trainLoss: 0.7237 | trainAcc: 64.5833% (248/384)\n",
            "6 13 Epoch: 13 | ANN: trainLoss: 0.7224 | trainAcc: 64.5089% (289/448)\n",
            "7 13 Epoch: 13 | ANN: trainLoss: 0.7170 | trainAcc: 65.4297% (335/512)\n",
            "8 13 Epoch: 13 | ANN: trainLoss: 0.7116 | trainAcc: 65.9722% (380/576)\n",
            "9 13 Epoch: 13 | ANN: trainLoss: 0.7219 | trainAcc: 65.3125% (418/640)\n",
            "10 13 Epoch: 13 | ANN: trainLoss: 0.7348 | trainAcc: 64.9148% (457/704)\n",
            "11 13 Epoch: 13 | ANN: trainLoss: 0.7314 | trainAcc: 64.9740% (499/768)\n",
            "12 13 Epoch: 13 | ANN: trainLoss: 0.7087 | trainAcc: 65.0259% (502/772)\n",
            "0 4 Epoch: 13 | ANN: testLoss: 0.7213 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 13 | ANN: testLoss: 0.7235 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 13 | ANN: testLoss: 0.7269 | testAcc: 63.0208% (121/192)\n",
            "3 4 Epoch: 13 | ANN: testLoss: 0.5924 | testAcc: 63.2124% (122/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 14 | ANN: trainLoss: 0.8255 | trainAcc: 67.1875% (43/64)\n",
            "1 13 Epoch: 14 | ANN: trainLoss: 0.7533 | trainAcc: 66.4062% (85/128)\n",
            "2 13 Epoch: 14 | ANN: trainLoss: 0.7716 | trainAcc: 63.0208% (121/192)\n",
            "3 13 Epoch: 14 | ANN: trainLoss: 0.7285 | trainAcc: 64.8438% (166/256)\n",
            "4 13 Epoch: 14 | ANN: trainLoss: 0.7271 | trainAcc: 64.3750% (206/320)\n",
            "5 13 Epoch: 14 | ANN: trainLoss: 0.7520 | trainAcc: 61.7188% (237/384)\n",
            "6 13 Epoch: 14 | ANN: trainLoss: 0.7512 | trainAcc: 62.0536% (278/448)\n",
            "7 13 Epoch: 14 | ANN: trainLoss: 0.7314 | trainAcc: 63.6719% (326/512)\n",
            "8 13 Epoch: 14 | ANN: trainLoss: 0.7204 | trainAcc: 64.2361% (370/576)\n",
            "9 13 Epoch: 14 | ANN: trainLoss: 0.7079 | trainAcc: 64.8438% (415/640)\n",
            "10 13 Epoch: 14 | ANN: trainLoss: 0.7073 | trainAcc: 64.3466% (453/704)\n",
            "11 13 Epoch: 14 | ANN: trainLoss: 0.7033 | trainAcc: 65.6250% (504/768)\n",
            "12 13 Epoch: 14 | ANN: trainLoss: 0.6882 | trainAcc: 65.6736% (507/772)\n",
            "0 4 Epoch: 14 | ANN: testLoss: 0.7940 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 14 | ANN: testLoss: 0.7335 | testAcc: 60.1562% (77/128)\n",
            "2 4 Epoch: 14 | ANN: testLoss: 0.6988 | testAcc: 61.9792% (119/192)\n",
            "3 4 Epoch: 14 | ANN: testLoss: 0.5944 | testAcc: 62.1762% (120/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 15 | ANN: trainLoss: 0.7718 | trainAcc: 68.7500% (44/64)\n",
            "1 13 Epoch: 15 | ANN: trainLoss: 0.8079 | trainAcc: 64.0625% (82/128)\n",
            "2 13 Epoch: 15 | ANN: trainLoss: 0.7888 | trainAcc: 64.5833% (124/192)\n",
            "3 13 Epoch: 15 | ANN: trainLoss: 0.7212 | trainAcc: 67.5781% (173/256)\n",
            "4 13 Epoch: 15 | ANN: trainLoss: 0.7387 | trainAcc: 65.9375% (211/320)\n",
            "5 13 Epoch: 15 | ANN: trainLoss: 0.7221 | trainAcc: 66.9271% (257/384)\n",
            "6 13 Epoch: 15 | ANN: trainLoss: 0.7170 | trainAcc: 67.4107% (302/448)\n",
            "7 13 Epoch: 15 | ANN: trainLoss: 0.7379 | trainAcc: 67.1875% (344/512)\n",
            "8 13 Epoch: 15 | ANN: trainLoss: 0.7172 | trainAcc: 68.4028% (394/576)\n",
            "9 13 Epoch: 15 | ANN: trainLoss: 0.7157 | trainAcc: 68.2812% (437/640)\n",
            "10 13 Epoch: 15 | ANN: trainLoss: 0.7261 | trainAcc: 67.6136% (476/704)\n",
            "11 13 Epoch: 15 | ANN: trainLoss: 0.7397 | trainAcc: 66.6667% (512/768)\n",
            "12 13 Epoch: 15 | ANN: trainLoss: 0.8330 | trainAcc: 66.5803% (514/772)\n",
            "0 4 Epoch: 15 | ANN: testLoss: 0.7392 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 15 | ANN: testLoss: 0.7501 | testAcc: 61.7188% (79/128)\n",
            "2 4 Epoch: 15 | ANN: testLoss: 0.7038 | testAcc: 63.0208% (121/192)\n",
            "3 4 Epoch: 15 | ANN: testLoss: 0.5726 | testAcc: 63.2124% (122/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 16 | ANN: trainLoss: 0.5846 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 16 | ANN: trainLoss: 0.5746 | trainAcc: 76.5625% (98/128)\n",
            "2 13 Epoch: 16 | ANN: trainLoss: 0.6299 | trainAcc: 71.3542% (137/192)\n",
            "3 13 Epoch: 16 | ANN: trainLoss: 0.6493 | trainAcc: 70.3125% (180/256)\n",
            "4 13 Epoch: 16 | ANN: trainLoss: 0.6499 | trainAcc: 69.3750% (222/320)\n",
            "5 13 Epoch: 16 | ANN: trainLoss: 0.6511 | trainAcc: 69.7917% (268/384)\n",
            "6 13 Epoch: 16 | ANN: trainLoss: 0.6453 | trainAcc: 69.1964% (310/448)\n",
            "7 13 Epoch: 16 | ANN: trainLoss: 0.6612 | trainAcc: 68.3594% (350/512)\n",
            "8 13 Epoch: 16 | ANN: trainLoss: 0.6759 | trainAcc: 67.1875% (387/576)\n",
            "9 13 Epoch: 16 | ANN: trainLoss: 0.6782 | trainAcc: 66.8750% (428/640)\n",
            "10 13 Epoch: 16 | ANN: trainLoss: 0.6837 | trainAcc: 66.6193% (469/704)\n",
            "11 13 Epoch: 16 | ANN: trainLoss: 0.6993 | trainAcc: 66.2760% (509/768)\n",
            "12 13 Epoch: 16 | ANN: trainLoss: 0.7259 | trainAcc: 66.1917% (511/772)\n",
            "0 4 Epoch: 16 | ANN: testLoss: 0.7521 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 16 | ANN: testLoss: 0.7518 | testAcc: 58.5938% (75/128)\n",
            "2 4 Epoch: 16 | ANN: testLoss: 0.7650 | testAcc: 58.8542% (113/192)\n",
            "3 4 Epoch: 16 | ANN: testLoss: 0.7755 | testAcc: 58.5492% (113/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 17 | ANN: trainLoss: 0.5615 | trainAcc: 73.4375% (47/64)\n",
            "1 13 Epoch: 17 | ANN: trainLoss: 0.6136 | trainAcc: 69.5312% (89/128)\n",
            "2 13 Epoch: 17 | ANN: trainLoss: 0.7071 | trainAcc: 66.6667% (128/192)\n",
            "3 13 Epoch: 17 | ANN: trainLoss: 0.7142 | trainAcc: 66.4062% (170/256)\n",
            "4 13 Epoch: 17 | ANN: trainLoss: 0.7505 | trainAcc: 65.3125% (209/320)\n",
            "5 13 Epoch: 17 | ANN: trainLoss: 0.7498 | trainAcc: 63.5417% (244/384)\n",
            "6 13 Epoch: 17 | ANN: trainLoss: 0.7360 | trainAcc: 64.5089% (289/448)\n",
            "7 13 Epoch: 17 | ANN: trainLoss: 0.7274 | trainAcc: 65.2344% (334/512)\n",
            "8 13 Epoch: 17 | ANN: trainLoss: 0.7258 | trainAcc: 65.6250% (378/576)\n",
            "9 13 Epoch: 17 | ANN: trainLoss: 0.7193 | trainAcc: 66.0938% (423/640)\n",
            "10 13 Epoch: 17 | ANN: trainLoss: 0.7076 | trainAcc: 66.7614% (470/704)\n",
            "11 13 Epoch: 17 | ANN: trainLoss: 0.6990 | trainAcc: 67.0573% (515/768)\n",
            "12 13 Epoch: 17 | ANN: trainLoss: 0.6812 | trainAcc: 67.0984% (518/772)\n",
            "0 4 Epoch: 17 | ANN: testLoss: 0.7145 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 17 | ANN: testLoss: 0.8027 | testAcc: 55.4688% (71/128)\n",
            "2 4 Epoch: 17 | ANN: testLoss: 0.8332 | testAcc: 54.6875% (105/192)\n",
            "3 4 Epoch: 17 | ANN: testLoss: 0.6327 | testAcc: 54.9223% (106/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 18 | ANN: trainLoss: 0.6398 | trainAcc: 71.8750% (46/64)\n",
            "1 13 Epoch: 18 | ANN: trainLoss: 0.6481 | trainAcc: 70.3125% (90/128)\n",
            "2 13 Epoch: 18 | ANN: trainLoss: 0.6393 | trainAcc: 70.3125% (135/192)\n",
            "3 13 Epoch: 18 | ANN: trainLoss: 0.6190 | trainAcc: 70.3125% (180/256)\n",
            "4 13 Epoch: 18 | ANN: trainLoss: 0.6286 | trainAcc: 70.6250% (226/320)\n",
            "5 13 Epoch: 18 | ANN: trainLoss: 0.6439 | trainAcc: 70.0521% (269/384)\n",
            "6 13 Epoch: 18 | ANN: trainLoss: 0.6732 | trainAcc: 69.6429% (312/448)\n",
            "7 13 Epoch: 18 | ANN: trainLoss: 0.6851 | trainAcc: 68.3594% (350/512)\n",
            "8 13 Epoch: 18 | ANN: trainLoss: 0.6809 | trainAcc: 68.9236% (397/576)\n",
            "9 13 Epoch: 18 | ANN: trainLoss: 0.6752 | trainAcc: 68.7500% (440/640)\n",
            "10 13 Epoch: 18 | ANN: trainLoss: 0.6681 | trainAcc: 69.0341% (486/704)\n",
            "11 13 Epoch: 18 | ANN: trainLoss: 0.6711 | trainAcc: 69.4010% (533/768)\n",
            "12 13 Epoch: 18 | ANN: trainLoss: 0.6981 | trainAcc: 69.3005% (535/772)\n",
            "0 4 Epoch: 18 | ANN: testLoss: 0.7539 | testAcc: 53.1250% (34/64)\n",
            "1 4 Epoch: 18 | ANN: testLoss: 0.7576 | testAcc: 58.5938% (75/128)\n",
            "2 4 Epoch: 18 | ANN: testLoss: 0.7373 | testAcc: 59.8958% (115/192)\n",
            "3 4 Epoch: 18 | ANN: testLoss: 0.8023 | testAcc: 59.5855% (115/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 19 | ANN: trainLoss: 0.6884 | trainAcc: 64.0625% (41/64)\n",
            "1 13 Epoch: 19 | ANN: trainLoss: 0.6537 | trainAcc: 68.7500% (88/128)\n",
            "2 13 Epoch: 19 | ANN: trainLoss: 0.6541 | trainAcc: 70.3125% (135/192)\n",
            "3 13 Epoch: 19 | ANN: trainLoss: 0.6670 | trainAcc: 69.9219% (179/256)\n",
            "4 13 Epoch: 19 | ANN: trainLoss: 0.6722 | trainAcc: 69.0625% (221/320)\n",
            "5 13 Epoch: 19 | ANN: trainLoss: 0.6784 | trainAcc: 68.4896% (263/384)\n",
            "6 13 Epoch: 19 | ANN: trainLoss: 0.6767 | trainAcc: 68.7500% (308/448)\n",
            "7 13 Epoch: 19 | ANN: trainLoss: 0.6617 | trainAcc: 69.1406% (354/512)\n",
            "8 13 Epoch: 19 | ANN: trainLoss: 0.6622 | trainAcc: 68.9236% (397/576)\n",
            "9 13 Epoch: 19 | ANN: trainLoss: 0.6669 | trainAcc: 67.9688% (435/640)\n",
            "10 13 Epoch: 19 | ANN: trainLoss: 0.6698 | trainAcc: 68.1818% (480/704)\n",
            "11 13 Epoch: 19 | ANN: trainLoss: 0.6688 | trainAcc: 68.2292% (524/768)\n",
            "12 13 Epoch: 19 | ANN: trainLoss: 0.6670 | trainAcc: 68.2642% (527/772)\n",
            "0 4 Epoch: 19 | ANN: testLoss: 0.7763 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 19 | ANN: testLoss: 0.7614 | testAcc: 62.5000% (80/128)\n",
            "2 4 Epoch: 19 | ANN: testLoss: 0.7627 | testAcc: 58.8542% (113/192)\n",
            "3 4 Epoch: 19 | ANN: testLoss: 0.8271 | testAcc: 58.5492% (113/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 20 | ANN: trainLoss: 0.6835 | trainAcc: 64.0625% (41/64)\n",
            "1 13 Epoch: 20 | ANN: trainLoss: 0.6177 | trainAcc: 71.8750% (92/128)\n",
            "2 13 Epoch: 20 | ANN: trainLoss: 0.6659 | trainAcc: 69.7917% (134/192)\n",
            "3 13 Epoch: 20 | ANN: trainLoss: 0.6673 | trainAcc: 70.3125% (180/256)\n",
            "4 13 Epoch: 20 | ANN: trainLoss: 0.6702 | trainAcc: 70.0000% (224/320)\n",
            "5 13 Epoch: 20 | ANN: trainLoss: 0.6617 | trainAcc: 69.7917% (268/384)\n",
            "6 13 Epoch: 20 | ANN: trainLoss: 0.6658 | trainAcc: 69.4196% (311/448)\n",
            "7 13 Epoch: 20 | ANN: trainLoss: 0.6583 | trainAcc: 69.5312% (356/512)\n",
            "8 13 Epoch: 20 | ANN: trainLoss: 0.6586 | trainAcc: 69.0972% (398/576)\n",
            "9 13 Epoch: 20 | ANN: trainLoss: 0.6577 | trainAcc: 68.5938% (439/640)\n",
            "10 13 Epoch: 20 | ANN: trainLoss: 0.6504 | trainAcc: 68.8920% (485/704)\n",
            "11 13 Epoch: 20 | ANN: trainLoss: 0.6453 | trainAcc: 69.1406% (531/768)\n",
            "12 13 Epoch: 20 | ANN: trainLoss: 0.6399 | trainAcc: 69.0415% (533/772)\n",
            "0 4 Epoch: 20 | ANN: testLoss: 0.6578 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 20 | ANN: testLoss: 0.6985 | testAcc: 61.7188% (79/128)\n",
            "2 4 Epoch: 20 | ANN: testLoss: 0.7074 | testAcc: 64.0625% (123/192)\n",
            "3 4 Epoch: 20 | ANN: testLoss: 0.7535 | testAcc: 64.2487% (124/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 21 | ANN: trainLoss: 0.6398 | trainAcc: 64.0625% (41/64)\n",
            "1 13 Epoch: 21 | ANN: trainLoss: 0.6363 | trainAcc: 64.0625% (82/128)\n",
            "2 13 Epoch: 21 | ANN: trainLoss: 0.6462 | trainAcc: 66.6667% (128/192)\n",
            "3 13 Epoch: 21 | ANN: trainLoss: 0.6461 | trainAcc: 67.5781% (173/256)\n",
            "4 13 Epoch: 21 | ANN: trainLoss: 0.6458 | trainAcc: 66.8750% (214/320)\n",
            "5 13 Epoch: 21 | ANN: trainLoss: 0.6349 | trainAcc: 67.7083% (260/384)\n",
            "6 13 Epoch: 21 | ANN: trainLoss: 0.6384 | trainAcc: 67.4107% (302/448)\n",
            "7 13 Epoch: 21 | ANN: trainLoss: 0.6433 | trainAcc: 66.7969% (342/512)\n",
            "8 13 Epoch: 21 | ANN: trainLoss: 0.6373 | trainAcc: 66.8403% (385/576)\n",
            "9 13 Epoch: 21 | ANN: trainLoss: 0.6361 | trainAcc: 67.0312% (429/640)\n",
            "10 13 Epoch: 21 | ANN: trainLoss: 0.6374 | trainAcc: 67.7557% (477/704)\n",
            "11 13 Epoch: 21 | ANN: trainLoss: 0.6458 | trainAcc: 67.5781% (519/768)\n",
            "12 13 Epoch: 21 | ANN: trainLoss: 0.6176 | trainAcc: 67.7461% (523/772)\n",
            "0 4 Epoch: 21 | ANN: testLoss: 0.6929 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 21 | ANN: testLoss: 0.6889 | testAcc: 59.3750% (76/128)\n",
            "2 4 Epoch: 21 | ANN: testLoss: 0.7254 | testAcc: 57.8125% (111/192)\n",
            "3 4 Epoch: 21 | ANN: testLoss: 0.8622 | testAcc: 57.5130% (111/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 22 | ANN: trainLoss: 0.5020 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 22 | ANN: trainLoss: 0.5409 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 22 | ANN: trainLoss: 0.5591 | trainAcc: 71.3542% (137/192)\n",
            "3 13 Epoch: 22 | ANN: trainLoss: 0.5404 | trainAcc: 74.2188% (190/256)\n",
            "4 13 Epoch: 22 | ANN: trainLoss: 0.5827 | trainAcc: 71.8750% (230/320)\n",
            "5 13 Epoch: 22 | ANN: trainLoss: 0.6046 | trainAcc: 71.3542% (274/384)\n",
            "6 13 Epoch: 22 | ANN: trainLoss: 0.6211 | trainAcc: 70.5357% (316/448)\n",
            "7 13 Epoch: 22 | ANN: trainLoss: 0.6048 | trainAcc: 72.0703% (369/512)\n",
            "8 13 Epoch: 22 | ANN: trainLoss: 0.6129 | trainAcc: 71.7014% (413/576)\n",
            "9 13 Epoch: 22 | ANN: trainLoss: 0.6096 | trainAcc: 71.5625% (458/640)\n",
            "10 13 Epoch: 22 | ANN: trainLoss: 0.6129 | trainAcc: 71.5909% (504/704)\n",
            "11 13 Epoch: 22 | ANN: trainLoss: 0.6249 | trainAcc: 71.2240% (547/768)\n",
            "12 13 Epoch: 22 | ANN: trainLoss: 0.6476 | trainAcc: 71.1140% (549/772)\n",
            "0 4 Epoch: 22 | ANN: testLoss: 0.6649 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 22 | ANN: testLoss: 0.6752 | testAcc: 62.5000% (80/128)\n",
            "2 4 Epoch: 22 | ANN: testLoss: 0.6931 | testAcc: 60.9375% (117/192)\n",
            "3 4 Epoch: 22 | ANN: testLoss: 0.7862 | testAcc: 60.6218% (117/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 23 | ANN: trainLoss: 0.5128 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 23 | ANN: trainLoss: 0.5745 | trainAcc: 76.5625% (98/128)\n",
            "2 13 Epoch: 23 | ANN: trainLoss: 0.6146 | trainAcc: 73.4375% (141/192)\n",
            "3 13 Epoch: 23 | ANN: trainLoss: 0.6356 | trainAcc: 71.0938% (182/256)\n",
            "4 13 Epoch: 23 | ANN: trainLoss: 0.6424 | trainAcc: 70.0000% (224/320)\n",
            "5 13 Epoch: 23 | ANN: trainLoss: 0.6342 | trainAcc: 70.3125% (270/384)\n",
            "6 13 Epoch: 23 | ANN: trainLoss: 0.6448 | trainAcc: 69.6429% (312/448)\n",
            "7 13 Epoch: 23 | ANN: trainLoss: 0.6397 | trainAcc: 69.7266% (357/512)\n",
            "8 13 Epoch: 23 | ANN: trainLoss: 0.6304 | trainAcc: 70.3125% (405/576)\n",
            "9 13 Epoch: 23 | ANN: trainLoss: 0.6224 | trainAcc: 71.2500% (456/640)\n",
            "10 13 Epoch: 23 | ANN: trainLoss: 0.6498 | trainAcc: 70.4545% (496/704)\n",
            "11 13 Epoch: 23 | ANN: trainLoss: 0.6504 | trainAcc: 70.3125% (540/768)\n",
            "12 13 Epoch: 23 | ANN: trainLoss: 0.6789 | trainAcc: 70.2073% (542/772)\n",
            "0 4 Epoch: 23 | ANN: testLoss: 0.6790 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 23 | ANN: testLoss: 0.7167 | testAcc: 60.1562% (77/128)\n",
            "2 4 Epoch: 23 | ANN: testLoss: 0.7071 | testAcc: 59.8958% (115/192)\n",
            "3 4 Epoch: 23 | ANN: testLoss: 0.6814 | testAcc: 60.1036% (116/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 24 | ANN: trainLoss: 0.6407 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 24 | ANN: trainLoss: 0.6536 | trainAcc: 73.4375% (94/128)\n",
            "2 13 Epoch: 24 | ANN: trainLoss: 0.6757 | trainAcc: 69.2708% (133/192)\n",
            "3 13 Epoch: 24 | ANN: trainLoss: 0.6959 | trainAcc: 69.1406% (177/256)\n",
            "4 13 Epoch: 24 | ANN: trainLoss: 0.7106 | trainAcc: 68.7500% (220/320)\n",
            "5 13 Epoch: 24 | ANN: trainLoss: 0.7198 | trainAcc: 67.4479% (259/384)\n",
            "6 13 Epoch: 24 | ANN: trainLoss: 0.7135 | trainAcc: 67.1875% (301/448)\n",
            "7 13 Epoch: 24 | ANN: trainLoss: 0.7089 | trainAcc: 66.7969% (342/512)\n",
            "8 13 Epoch: 24 | ANN: trainLoss: 0.7042 | trainAcc: 67.8819% (391/576)\n",
            "9 13 Epoch: 24 | ANN: trainLoss: 0.6884 | trainAcc: 68.5938% (439/640)\n",
            "10 13 Epoch: 24 | ANN: trainLoss: 0.6742 | trainAcc: 69.3182% (488/704)\n",
            "11 13 Epoch: 24 | ANN: trainLoss: 0.6648 | trainAcc: 69.2708% (532/768)\n",
            "12 13 Epoch: 24 | ANN: trainLoss: 0.6592 | trainAcc: 69.1710% (534/772)\n",
            "0 4 Epoch: 24 | ANN: testLoss: 0.4454 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 24 | ANN: testLoss: 0.6175 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 24 | ANN: testLoss: 0.6874 | testAcc: 63.0208% (121/192)\n",
            "3 4 Epoch: 24 | ANN: testLoss: 0.7231 | testAcc: 62.6943% (121/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 25 | ANN: trainLoss: 0.5608 | trainAcc: 70.3125% (45/64)\n",
            "1 13 Epoch: 25 | ANN: trainLoss: 0.6336 | trainAcc: 69.5312% (89/128)\n",
            "2 13 Epoch: 25 | ANN: trainLoss: 0.7121 | trainAcc: 65.6250% (126/192)\n",
            "3 13 Epoch: 25 | ANN: trainLoss: 0.7498 | trainAcc: 64.4531% (165/256)\n",
            "4 13 Epoch: 25 | ANN: trainLoss: 0.7150 | trainAcc: 66.8750% (214/320)\n",
            "5 13 Epoch: 25 | ANN: trainLoss: 0.7209 | trainAcc: 67.9688% (261/384)\n",
            "6 13 Epoch: 25 | ANN: trainLoss: 0.7127 | trainAcc: 68.3036% (306/448)\n",
            "7 13 Epoch: 25 | ANN: trainLoss: 0.7013 | trainAcc: 68.3594% (350/512)\n",
            "8 13 Epoch: 25 | ANN: trainLoss: 0.7007 | trainAcc: 68.2292% (393/576)\n",
            "9 13 Epoch: 25 | ANN: trainLoss: 0.7062 | trainAcc: 68.1250% (436/640)\n",
            "10 13 Epoch: 25 | ANN: trainLoss: 0.6909 | trainAcc: 68.3239% (481/704)\n",
            "11 13 Epoch: 25 | ANN: trainLoss: 0.6934 | trainAcc: 68.6198% (527/768)\n",
            "12 13 Epoch: 25 | ANN: trainLoss: 0.6738 | trainAcc: 68.7824% (531/772)\n",
            "0 4 Epoch: 25 | ANN: testLoss: 0.7025 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 25 | ANN: testLoss: 0.6889 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 25 | ANN: testLoss: 0.6750 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 25 | ANN: testLoss: 0.8843 | testAcc: 64.2487% (124/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 26 | ANN: trainLoss: 0.6039 | trainAcc: 70.3125% (45/64)\n",
            "1 13 Epoch: 26 | ANN: trainLoss: 0.6310 | trainAcc: 71.8750% (92/128)\n",
            "2 13 Epoch: 26 | ANN: trainLoss: 0.6198 | trainAcc: 73.4375% (141/192)\n",
            "3 13 Epoch: 26 | ANN: trainLoss: 0.5955 | trainAcc: 75.3906% (193/256)\n",
            "4 13 Epoch: 26 | ANN: trainLoss: 0.6063 | trainAcc: 75.6250% (242/320)\n",
            "5 13 Epoch: 26 | ANN: trainLoss: 0.6383 | trainAcc: 74.2188% (285/384)\n",
            "6 13 Epoch: 26 | ANN: trainLoss: 0.6306 | trainAcc: 74.3304% (333/448)\n",
            "7 13 Epoch: 26 | ANN: trainLoss: 0.6270 | trainAcc: 74.4141% (381/512)\n",
            "8 13 Epoch: 26 | ANN: trainLoss: 0.6179 | trainAcc: 74.1319% (427/576)\n",
            "9 13 Epoch: 26 | ANN: trainLoss: 0.6186 | trainAcc: 73.2812% (469/640)\n",
            "10 13 Epoch: 26 | ANN: trainLoss: 0.6187 | trainAcc: 73.4375% (517/704)\n",
            "11 13 Epoch: 26 | ANN: trainLoss: 0.6280 | trainAcc: 72.3958% (556/768)\n",
            "12 13 Epoch: 26 | ANN: trainLoss: 0.6406 | trainAcc: 72.4093% (559/772)\n",
            "0 4 Epoch: 26 | ANN: testLoss: 0.6493 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 26 | ANN: testLoss: 0.6901 | testAcc: 57.8125% (74/128)\n",
            "2 4 Epoch: 26 | ANN: testLoss: 0.6651 | testAcc: 60.9375% (117/192)\n",
            "3 4 Epoch: 26 | ANN: testLoss: 0.7041 | testAcc: 60.6218% (117/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 27 | ANN: trainLoss: 0.4945 | trainAcc: 73.4375% (47/64)\n",
            "1 13 Epoch: 27 | ANN: trainLoss: 0.4900 | trainAcc: 73.4375% (94/128)\n",
            "2 13 Epoch: 27 | ANN: trainLoss: 0.5384 | trainAcc: 71.3542% (137/192)\n",
            "3 13 Epoch: 27 | ANN: trainLoss: 0.5563 | trainAcc: 71.8750% (184/256)\n",
            "4 13 Epoch: 27 | ANN: trainLoss: 0.5501 | trainAcc: 72.1875% (231/320)\n",
            "5 13 Epoch: 27 | ANN: trainLoss: 0.5777 | trainAcc: 70.3125% (270/384)\n",
            "6 13 Epoch: 27 | ANN: trainLoss: 0.5767 | trainAcc: 71.6518% (321/448)\n",
            "7 13 Epoch: 27 | ANN: trainLoss: 0.5775 | trainAcc: 72.0703% (369/512)\n",
            "8 13 Epoch: 27 | ANN: trainLoss: 0.5973 | trainAcc: 71.5278% (412/576)\n",
            "9 13 Epoch: 27 | ANN: trainLoss: 0.6063 | trainAcc: 71.4062% (457/640)\n",
            "10 13 Epoch: 27 | ANN: trainLoss: 0.6144 | trainAcc: 70.1705% (494/704)\n",
            "11 13 Epoch: 27 | ANN: trainLoss: 0.6108 | trainAcc: 70.7031% (543/768)\n",
            "12 13 Epoch: 27 | ANN: trainLoss: 0.6078 | trainAcc: 70.7254% (546/772)\n",
            "0 4 Epoch: 27 | ANN: testLoss: 0.6028 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 27 | ANN: testLoss: 0.6551 | testAcc: 57.0312% (73/128)\n",
            "2 4 Epoch: 27 | ANN: testLoss: 0.6970 | testAcc: 58.8542% (113/192)\n",
            "3 4 Epoch: 27 | ANN: testLoss: 0.7449 | testAcc: 58.5492% (113/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 28 | ANN: trainLoss: 0.6162 | trainAcc: 75.0000% (48/64)\n",
            "1 13 Epoch: 28 | ANN: trainLoss: 0.5966 | trainAcc: 76.5625% (98/128)\n",
            "2 13 Epoch: 28 | ANN: trainLoss: 0.5847 | trainAcc: 76.0417% (146/192)\n",
            "3 13 Epoch: 28 | ANN: trainLoss: 0.6062 | trainAcc: 73.8281% (189/256)\n",
            "4 13 Epoch: 28 | ANN: trainLoss: 0.5870 | trainAcc: 74.3750% (238/320)\n",
            "5 13 Epoch: 28 | ANN: trainLoss: 0.5849 | trainAcc: 73.9583% (284/384)\n",
            "6 13 Epoch: 28 | ANN: trainLoss: 0.5742 | trainAcc: 74.7768% (335/448)\n",
            "7 13 Epoch: 28 | ANN: trainLoss: 0.5547 | trainAcc: 75.7812% (388/512)\n",
            "8 13 Epoch: 28 | ANN: trainLoss: 0.5682 | trainAcc: 75.0000% (432/576)\n",
            "9 13 Epoch: 28 | ANN: trainLoss: 0.5661 | trainAcc: 75.0000% (480/640)\n",
            "10 13 Epoch: 28 | ANN: trainLoss: 0.5840 | trainAcc: 73.7216% (519/704)\n",
            "11 13 Epoch: 28 | ANN: trainLoss: 0.5864 | trainAcc: 73.0469% (561/768)\n",
            "12 13 Epoch: 28 | ANN: trainLoss: 0.5645 | trainAcc: 73.0570% (564/772)\n",
            "0 4 Epoch: 28 | ANN: testLoss: 0.6883 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 28 | ANN: testLoss: 0.7167 | testAcc: 57.0312% (73/128)\n",
            "2 4 Epoch: 28 | ANN: testLoss: 0.6772 | testAcc: 58.8542% (113/192)\n",
            "3 4 Epoch: 28 | ANN: testLoss: 0.5119 | testAcc: 59.0674% (114/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 29 | ANN: trainLoss: 0.4995 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 29 | ANN: trainLoss: 0.6030 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 29 | ANN: trainLoss: 0.5699 | trainAcc: 75.0000% (144/192)\n",
            "3 13 Epoch: 29 | ANN: trainLoss: 0.5637 | trainAcc: 75.3906% (193/256)\n",
            "4 13 Epoch: 29 | ANN: trainLoss: 0.5485 | trainAcc: 77.5000% (248/320)\n",
            "5 13 Epoch: 29 | ANN: trainLoss: 0.5608 | trainAcc: 76.5625% (294/384)\n",
            "6 13 Epoch: 29 | ANN: trainLoss: 0.5630 | trainAcc: 76.1161% (341/448)\n",
            "7 13 Epoch: 29 | ANN: trainLoss: 0.5660 | trainAcc: 76.1719% (390/512)\n",
            "8 13 Epoch: 29 | ANN: trainLoss: 0.5655 | trainAcc: 76.2153% (439/576)\n",
            "9 13 Epoch: 29 | ANN: trainLoss: 0.5685 | trainAcc: 75.1562% (481/640)\n",
            "10 13 Epoch: 29 | ANN: trainLoss: 0.5769 | trainAcc: 74.4318% (524/704)\n",
            "11 13 Epoch: 29 | ANN: trainLoss: 0.5748 | trainAcc: 74.7396% (574/768)\n",
            "12 13 Epoch: 29 | ANN: trainLoss: 0.5528 | trainAcc: 74.8705% (578/772)\n",
            "0 4 Epoch: 29 | ANN: testLoss: 0.6351 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 29 | ANN: testLoss: 0.6678 | testAcc: 60.9375% (78/128)\n",
            "2 4 Epoch: 29 | ANN: testLoss: 0.6688 | testAcc: 58.3333% (112/192)\n",
            "3 4 Epoch: 29 | ANN: testLoss: 0.5111 | testAcc: 58.5492% (113/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 30 | ANN: trainLoss: 0.6665 | trainAcc: 67.1875% (43/64)\n",
            "1 13 Epoch: 30 | ANN: trainLoss: 0.6198 | trainAcc: 68.7500% (88/128)\n",
            "2 13 Epoch: 30 | ANN: trainLoss: 0.6078 | trainAcc: 70.3125% (135/192)\n",
            "3 13 Epoch: 30 | ANN: trainLoss: 0.6366 | trainAcc: 69.1406% (177/256)\n",
            "4 13 Epoch: 30 | ANN: trainLoss: 0.6089 | trainAcc: 71.2500% (228/320)\n",
            "5 13 Epoch: 30 | ANN: trainLoss: 0.6126 | trainAcc: 71.0938% (273/384)\n",
            "6 13 Epoch: 30 | ANN: trainLoss: 0.5871 | trainAcc: 72.7679% (326/448)\n",
            "7 13 Epoch: 30 | ANN: trainLoss: 0.5724 | trainAcc: 73.8281% (378/512)\n",
            "8 13 Epoch: 30 | ANN: trainLoss: 0.5732 | trainAcc: 74.1319% (427/576)\n",
            "9 13 Epoch: 30 | ANN: trainLoss: 0.5872 | trainAcc: 73.9062% (473/640)\n",
            "10 13 Epoch: 30 | ANN: trainLoss: 0.5865 | trainAcc: 74.0057% (521/704)\n",
            "11 13 Epoch: 30 | ANN: trainLoss: 0.5890 | trainAcc: 73.5677% (565/768)\n",
            "12 13 Epoch: 30 | ANN: trainLoss: 0.5798 | trainAcc: 73.5751% (568/772)\n",
            "0 4 Epoch: 30 | ANN: testLoss: 0.6452 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 30 | ANN: testLoss: 0.6899 | testAcc: 59.3750% (76/128)\n",
            "2 4 Epoch: 30 | ANN: testLoss: 0.6803 | testAcc: 57.8125% (111/192)\n",
            "3 4 Epoch: 30 | ANN: testLoss: 0.6832 | testAcc: 58.0311% (112/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 31 | ANN: trainLoss: 0.5688 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 31 | ANN: trainLoss: 0.5909 | trainAcc: 73.4375% (94/128)\n",
            "2 13 Epoch: 31 | ANN: trainLoss: 0.5732 | trainAcc: 72.3958% (139/192)\n",
            "3 13 Epoch: 31 | ANN: trainLoss: 0.5779 | trainAcc: 72.2656% (185/256)\n",
            "4 13 Epoch: 31 | ANN: trainLoss: 0.5596 | trainAcc: 73.4375% (235/320)\n",
            "5 13 Epoch: 31 | ANN: trainLoss: 0.5576 | trainAcc: 72.9167% (280/384)\n",
            "6 13 Epoch: 31 | ANN: trainLoss: 0.5576 | trainAcc: 72.5446% (325/448)\n",
            "7 13 Epoch: 31 | ANN: trainLoss: 0.5558 | trainAcc: 72.8516% (373/512)\n",
            "8 13 Epoch: 31 | ANN: trainLoss: 0.5633 | trainAcc: 72.7431% (419/576)\n",
            "9 13 Epoch: 31 | ANN: trainLoss: 0.5588 | trainAcc: 72.5000% (464/640)\n",
            "10 13 Epoch: 31 | ANN: trainLoss: 0.5582 | trainAcc: 72.3011% (509/704)\n",
            "11 13 Epoch: 31 | ANN: trainLoss: 0.5557 | trainAcc: 73.1771% (562/768)\n",
            "12 13 Epoch: 31 | ANN: trainLoss: 0.6040 | trainAcc: 73.0570% (564/772)\n",
            "0 4 Epoch: 31 | ANN: testLoss: 0.7774 | testAcc: 50.0000% (32/64)\n",
            "1 4 Epoch: 31 | ANN: testLoss: 0.7160 | testAcc: 58.5938% (75/128)\n",
            "2 4 Epoch: 31 | ANN: testLoss: 0.6693 | testAcc: 59.8958% (115/192)\n",
            "3 4 Epoch: 31 | ANN: testLoss: 0.5032 | testAcc: 60.1036% (116/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 32 | ANN: trainLoss: 0.7322 | trainAcc: 57.8125% (37/64)\n",
            "1 13 Epoch: 32 | ANN: trainLoss: 0.6229 | trainAcc: 65.6250% (84/128)\n",
            "2 13 Epoch: 32 | ANN: trainLoss: 0.5875 | trainAcc: 68.2292% (131/192)\n",
            "3 13 Epoch: 32 | ANN: trainLoss: 0.5885 | trainAcc: 68.3594% (175/256)\n",
            "4 13 Epoch: 32 | ANN: trainLoss: 0.5643 | trainAcc: 70.6250% (226/320)\n",
            "5 13 Epoch: 32 | ANN: trainLoss: 0.5652 | trainAcc: 71.3542% (274/384)\n",
            "6 13 Epoch: 32 | ANN: trainLoss: 0.5671 | trainAcc: 71.2054% (319/448)\n",
            "7 13 Epoch: 32 | ANN: trainLoss: 0.5797 | trainAcc: 71.6797% (367/512)\n",
            "8 13 Epoch: 32 | ANN: trainLoss: 0.5765 | trainAcc: 72.2222% (416/576)\n",
            "9 13 Epoch: 32 | ANN: trainLoss: 0.5752 | trainAcc: 72.5000% (464/640)\n",
            "10 13 Epoch: 32 | ANN: trainLoss: 0.5956 | trainAcc: 71.5909% (504/704)\n",
            "11 13 Epoch: 32 | ANN: trainLoss: 0.5942 | trainAcc: 72.0052% (553/768)\n",
            "12 13 Epoch: 32 | ANN: trainLoss: 0.5718 | trainAcc: 72.1503% (557/772)\n",
            "0 4 Epoch: 32 | ANN: testLoss: 0.7186 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 32 | ANN: testLoss: 0.7150 | testAcc: 59.3750% (76/128)\n",
            "2 4 Epoch: 32 | ANN: testLoss: 0.7126 | testAcc: 59.8958% (115/192)\n",
            "3 4 Epoch: 32 | ANN: testLoss: 0.6710 | testAcc: 60.1036% (116/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 33 | ANN: trainLoss: 0.5884 | trainAcc: 75.0000% (48/64)\n",
            "1 13 Epoch: 33 | ANN: trainLoss: 0.5425 | trainAcc: 77.3438% (99/128)\n",
            "2 13 Epoch: 33 | ANN: trainLoss: 0.5280 | trainAcc: 77.6042% (149/192)\n",
            "3 13 Epoch: 33 | ANN: trainLoss: 0.5355 | trainAcc: 75.3906% (193/256)\n",
            "4 13 Epoch: 33 | ANN: trainLoss: 0.5710 | trainAcc: 74.3750% (238/320)\n",
            "5 13 Epoch: 33 | ANN: trainLoss: 0.5957 | trainAcc: 73.4375% (282/384)\n",
            "6 13 Epoch: 33 | ANN: trainLoss: 0.5876 | trainAcc: 72.3214% (324/448)\n",
            "7 13 Epoch: 33 | ANN: trainLoss: 0.5806 | trainAcc: 73.2422% (375/512)\n",
            "8 13 Epoch: 33 | ANN: trainLoss: 0.5884 | trainAcc: 72.5694% (418/576)\n",
            "9 13 Epoch: 33 | ANN: trainLoss: 0.5951 | trainAcc: 72.3438% (463/640)\n",
            "10 13 Epoch: 33 | ANN: trainLoss: 0.5938 | trainAcc: 72.3011% (509/704)\n",
            "11 13 Epoch: 33 | ANN: trainLoss: 0.5881 | trainAcc: 72.9167% (560/768)\n",
            "12 13 Epoch: 33 | ANN: trainLoss: 0.6060 | trainAcc: 72.9275% (563/772)\n",
            "0 4 Epoch: 33 | ANN: testLoss: 0.5878 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 33 | ANN: testLoss: 0.6413 | testAcc: 62.5000% (80/128)\n",
            "2 4 Epoch: 33 | ANN: testLoss: 0.6344 | testAcc: 63.0208% (121/192)\n",
            "3 4 Epoch: 33 | ANN: testLoss: 0.6820 | testAcc: 63.2124% (122/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 34 | ANN: trainLoss: 0.4510 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 34 | ANN: trainLoss: 0.4781 | trainAcc: 80.4688% (103/128)\n",
            "2 13 Epoch: 34 | ANN: trainLoss: 0.4915 | trainAcc: 79.1667% (152/192)\n",
            "3 13 Epoch: 34 | ANN: trainLoss: 0.5306 | trainAcc: 76.5625% (196/256)\n",
            "4 13 Epoch: 34 | ANN: trainLoss: 0.5807 | trainAcc: 72.5000% (232/320)\n",
            "5 13 Epoch: 34 | ANN: trainLoss: 0.5698 | trainAcc: 73.6979% (283/384)\n",
            "6 13 Epoch: 34 | ANN: trainLoss: 0.5677 | trainAcc: 74.1071% (332/448)\n",
            "7 13 Epoch: 34 | ANN: trainLoss: 0.5698 | trainAcc: 74.0234% (379/512)\n",
            "8 13 Epoch: 34 | ANN: trainLoss: 0.5698 | trainAcc: 73.7847% (425/576)\n",
            "9 13 Epoch: 34 | ANN: trainLoss: 0.5672 | trainAcc: 74.3750% (476/640)\n",
            "10 13 Epoch: 34 | ANN: trainLoss: 0.5728 | trainAcc: 73.7216% (519/704)\n",
            "11 13 Epoch: 34 | ANN: trainLoss: 0.5756 | trainAcc: 73.1771% (562/768)\n",
            "12 13 Epoch: 34 | ANN: trainLoss: 0.5653 | trainAcc: 73.3161% (566/772)\n",
            "0 4 Epoch: 34 | ANN: testLoss: 0.6911 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 34 | ANN: testLoss: 0.7187 | testAcc: 57.8125% (74/128)\n",
            "2 4 Epoch: 34 | ANN: testLoss: 0.6821 | testAcc: 58.8542% (113/192)\n",
            "3 4 Epoch: 34 | ANN: testLoss: 0.6611 | testAcc: 59.0674% (114/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 35 | ANN: trainLoss: 0.5316 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 35 | ANN: trainLoss: 0.5998 | trainAcc: 71.0938% (91/128)\n",
            "2 13 Epoch: 35 | ANN: trainLoss: 0.6059 | trainAcc: 71.3542% (137/192)\n",
            "3 13 Epoch: 35 | ANN: trainLoss: 0.5844 | trainAcc: 73.4375% (188/256)\n",
            "4 13 Epoch: 35 | ANN: trainLoss: 0.5880 | trainAcc: 73.1250% (234/320)\n",
            "5 13 Epoch: 35 | ANN: trainLoss: 0.5970 | trainAcc: 72.1354% (277/384)\n",
            "6 13 Epoch: 35 | ANN: trainLoss: 0.5919 | trainAcc: 71.8750% (322/448)\n",
            "7 13 Epoch: 35 | ANN: trainLoss: 0.5935 | trainAcc: 71.4844% (366/512)\n",
            "8 13 Epoch: 35 | ANN: trainLoss: 0.5993 | trainAcc: 72.2222% (416/576)\n",
            "9 13 Epoch: 35 | ANN: trainLoss: 0.6045 | trainAcc: 72.0312% (461/640)\n",
            "10 13 Epoch: 35 | ANN: trainLoss: 0.5897 | trainAcc: 73.4375% (517/704)\n",
            "11 13 Epoch: 35 | ANN: trainLoss: 0.5843 | trainAcc: 73.9583% (568/768)\n",
            "12 13 Epoch: 35 | ANN: trainLoss: 0.6144 | trainAcc: 73.8342% (570/772)\n",
            "0 4 Epoch: 35 | ANN: testLoss: 0.6042 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 35 | ANN: testLoss: 0.6116 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 35 | ANN: testLoss: 0.6290 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 35 | ANN: testLoss: 0.4954 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 36 | ANN: trainLoss: 0.4612 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 36 | ANN: trainLoss: 0.5704 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 36 | ANN: trainLoss: 0.5639 | trainAcc: 75.0000% (144/192)\n",
            "3 13 Epoch: 36 | ANN: trainLoss: 0.5399 | trainAcc: 74.6094% (191/256)\n",
            "4 13 Epoch: 36 | ANN: trainLoss: 0.5449 | trainAcc: 74.6875% (239/320)\n",
            "5 13 Epoch: 36 | ANN: trainLoss: 0.5682 | trainAcc: 74.4792% (286/384)\n",
            "6 13 Epoch: 36 | ANN: trainLoss: 0.5370 | trainAcc: 77.0089% (345/448)\n",
            "7 13 Epoch: 36 | ANN: trainLoss: 0.5478 | trainAcc: 75.7812% (388/512)\n",
            "8 13 Epoch: 36 | ANN: trainLoss: 0.5486 | trainAcc: 75.3472% (434/576)\n",
            "9 13 Epoch: 36 | ANN: trainLoss: 0.5444 | trainAcc: 75.4688% (483/640)\n",
            "10 13 Epoch: 36 | ANN: trainLoss: 0.5437 | trainAcc: 75.5682% (532/704)\n",
            "11 13 Epoch: 36 | ANN: trainLoss: 0.5403 | trainAcc: 75.6510% (581/768)\n",
            "12 13 Epoch: 36 | ANN: trainLoss: 0.5218 | trainAcc: 75.7772% (585/772)\n",
            "0 4 Epoch: 36 | ANN: testLoss: 0.5330 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 36 | ANN: testLoss: 0.6094 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 36 | ANN: testLoss: 0.6263 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 36 | ANN: testLoss: 0.6088 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 37 | ANN: trainLoss: 0.5158 | trainAcc: 70.3125% (45/64)\n",
            "1 13 Epoch: 37 | ANN: trainLoss: 0.5584 | trainAcc: 74.2188% (95/128)\n",
            "2 13 Epoch: 37 | ANN: trainLoss: 0.5591 | trainAcc: 72.9167% (140/192)\n",
            "3 13 Epoch: 37 | ANN: trainLoss: 0.5853 | trainAcc: 72.6562% (186/256)\n",
            "4 13 Epoch: 37 | ANN: trainLoss: 0.5897 | trainAcc: 72.1875% (231/320)\n",
            "5 13 Epoch: 37 | ANN: trainLoss: 0.5848 | trainAcc: 73.1771% (281/384)\n",
            "6 13 Epoch: 37 | ANN: trainLoss: 0.5983 | trainAcc: 71.6518% (321/448)\n",
            "7 13 Epoch: 37 | ANN: trainLoss: 0.5989 | trainAcc: 71.6797% (367/512)\n",
            "8 13 Epoch: 37 | ANN: trainLoss: 0.5894 | trainAcc: 72.3958% (417/576)\n",
            "9 13 Epoch: 37 | ANN: trainLoss: 0.5800 | trainAcc: 73.2812% (469/640)\n",
            "10 13 Epoch: 37 | ANN: trainLoss: 0.5892 | trainAcc: 72.4432% (510/704)\n",
            "11 13 Epoch: 37 | ANN: trainLoss: 0.5788 | trainAcc: 72.9167% (560/768)\n",
            "12 13 Epoch: 37 | ANN: trainLoss: 0.6298 | trainAcc: 72.6684% (561/772)\n",
            "0 4 Epoch: 37 | ANN: testLoss: 0.6270 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 37 | ANN: testLoss: 0.6458 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 37 | ANN: testLoss: 0.6207 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 37 | ANN: testLoss: 0.6669 | testAcc: 65.2850% (126/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 38 | ANN: trainLoss: 0.4759 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 38 | ANN: trainLoss: 0.5021 | trainAcc: 77.3438% (99/128)\n",
            "2 13 Epoch: 38 | ANN: trainLoss: 0.4914 | trainAcc: 78.1250% (150/192)\n",
            "3 13 Epoch: 38 | ANN: trainLoss: 0.5019 | trainAcc: 77.7344% (199/256)\n",
            "4 13 Epoch: 38 | ANN: trainLoss: 0.5446 | trainAcc: 75.3125% (241/320)\n",
            "5 13 Epoch: 38 | ANN: trainLoss: 0.5329 | trainAcc: 75.5208% (290/384)\n",
            "6 13 Epoch: 38 | ANN: trainLoss: 0.5420 | trainAcc: 74.7768% (335/448)\n",
            "7 13 Epoch: 38 | ANN: trainLoss: 0.5528 | trainAcc: 73.8281% (378/512)\n",
            "8 13 Epoch: 38 | ANN: trainLoss: 0.5447 | trainAcc: 74.3056% (428/576)\n",
            "9 13 Epoch: 38 | ANN: trainLoss: 0.5346 | trainAcc: 75.1562% (481/640)\n",
            "10 13 Epoch: 38 | ANN: trainLoss: 0.5289 | trainAcc: 75.9943% (535/704)\n",
            "11 13 Epoch: 38 | ANN: trainLoss: 0.5444 | trainAcc: 75.0000% (576/768)\n",
            "12 13 Epoch: 38 | ANN: trainLoss: 0.5466 | trainAcc: 75.0000% (579/772)\n",
            "0 4 Epoch: 38 | ANN: testLoss: 0.5922 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 38 | ANN: testLoss: 0.6304 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 38 | ANN: testLoss: 0.6337 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 38 | ANN: testLoss: 0.7704 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 39 | ANN: trainLoss: 0.5935 | trainAcc: 68.7500% (44/64)\n",
            "1 13 Epoch: 39 | ANN: trainLoss: 0.6238 | trainAcc: 68.7500% (88/128)\n",
            "2 13 Epoch: 39 | ANN: trainLoss: 0.5976 | trainAcc: 70.3125% (135/192)\n",
            "3 13 Epoch: 39 | ANN: trainLoss: 0.6138 | trainAcc: 70.7031% (181/256)\n",
            "4 13 Epoch: 39 | ANN: trainLoss: 0.5837 | trainAcc: 72.8125% (233/320)\n",
            "5 13 Epoch: 39 | ANN: trainLoss: 0.5935 | trainAcc: 73.6979% (283/384)\n",
            "6 13 Epoch: 39 | ANN: trainLoss: 0.5811 | trainAcc: 74.1071% (332/448)\n",
            "7 13 Epoch: 39 | ANN: trainLoss: 0.5598 | trainAcc: 75.0000% (384/512)\n",
            "8 13 Epoch: 39 | ANN: trainLoss: 0.5457 | trainAcc: 75.8681% (437/576)\n",
            "9 13 Epoch: 39 | ANN: trainLoss: 0.5441 | trainAcc: 76.0938% (487/640)\n",
            "10 13 Epoch: 39 | ANN: trainLoss: 0.5432 | trainAcc: 76.2784% (537/704)\n",
            "11 13 Epoch: 39 | ANN: trainLoss: 0.5400 | trainAcc: 76.0417% (584/768)\n",
            "12 13 Epoch: 39 | ANN: trainLoss: 0.5136 | trainAcc: 76.1658% (588/772)\n",
            "0 4 Epoch: 39 | ANN: testLoss: 0.7162 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 39 | ANN: testLoss: 0.6672 | testAcc: 62.5000% (80/128)\n",
            "2 4 Epoch: 39 | ANN: testLoss: 0.6872 | testAcc: 63.0208% (121/192)\n",
            "3 4 Epoch: 39 | ANN: testLoss: 0.8154 | testAcc: 62.6943% (121/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 40 | ANN: trainLoss: 0.3791 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 40 | ANN: trainLoss: 0.4439 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 40 | ANN: trainLoss: 0.4619 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 40 | ANN: trainLoss: 0.4773 | trainAcc: 80.4688% (206/256)\n",
            "4 13 Epoch: 40 | ANN: trainLoss: 0.4738 | trainAcc: 80.6250% (258/320)\n",
            "5 13 Epoch: 40 | ANN: trainLoss: 0.4887 | trainAcc: 80.7292% (310/384)\n",
            "6 13 Epoch: 40 | ANN: trainLoss: 0.5271 | trainAcc: 79.6875% (357/448)\n",
            "7 13 Epoch: 40 | ANN: trainLoss: 0.5403 | trainAcc: 78.9062% (404/512)\n",
            "8 13 Epoch: 40 | ANN: trainLoss: 0.5312 | trainAcc: 78.2986% (451/576)\n",
            "9 13 Epoch: 40 | ANN: trainLoss: 0.5309 | trainAcc: 77.9688% (499/640)\n",
            "10 13 Epoch: 40 | ANN: trainLoss: 0.5366 | trainAcc: 77.8409% (548/704)\n",
            "11 13 Epoch: 40 | ANN: trainLoss: 0.5421 | trainAcc: 77.3438% (594/768)\n",
            "12 13 Epoch: 40 | ANN: trainLoss: 0.5432 | trainAcc: 77.2021% (596/772)\n",
            "0 4 Epoch: 40 | ANN: testLoss: 0.6370 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 40 | ANN: testLoss: 0.5843 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 40 | ANN: testLoss: 0.6347 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 40 | ANN: testLoss: 0.6807 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 41 | ANN: trainLoss: 0.5844 | trainAcc: 75.0000% (48/64)\n",
            "1 13 Epoch: 41 | ANN: trainLoss: 0.5338 | trainAcc: 76.5625% (98/128)\n",
            "2 13 Epoch: 41 | ANN: trainLoss: 0.5306 | trainAcc: 76.5625% (147/192)\n",
            "3 13 Epoch: 41 | ANN: trainLoss: 0.5480 | trainAcc: 75.0000% (192/256)\n",
            "4 13 Epoch: 41 | ANN: trainLoss: 0.5266 | trainAcc: 76.2500% (244/320)\n",
            "5 13 Epoch: 41 | ANN: trainLoss: 0.5276 | trainAcc: 76.5625% (294/384)\n",
            "6 13 Epoch: 41 | ANN: trainLoss: 0.5430 | trainAcc: 76.5625% (343/448)\n",
            "7 13 Epoch: 41 | ANN: trainLoss: 0.5672 | trainAcc: 75.3906% (386/512)\n",
            "8 13 Epoch: 41 | ANN: trainLoss: 0.5515 | trainAcc: 75.6944% (436/576)\n",
            "9 13 Epoch: 41 | ANN: trainLoss: 0.5450 | trainAcc: 75.9375% (486/640)\n",
            "10 13 Epoch: 41 | ANN: trainLoss: 0.5368 | trainAcc: 75.8523% (534/704)\n",
            "11 13 Epoch: 41 | ANN: trainLoss: 0.5420 | trainAcc: 75.5208% (580/768)\n",
            "12 13 Epoch: 41 | ANN: trainLoss: 0.5368 | trainAcc: 75.5181% (583/772)\n",
            "0 4 Epoch: 41 | ANN: testLoss: 0.5462 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 41 | ANN: testLoss: 0.6906 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 41 | ANN: testLoss: 0.7167 | testAcc: 61.4583% (118/192)\n",
            "3 4 Epoch: 41 | ANN: testLoss: 0.7950 | testAcc: 61.1399% (118/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 42 | ANN: trainLoss: 0.6065 | trainAcc: 75.0000% (48/64)\n",
            "1 13 Epoch: 42 | ANN: trainLoss: 0.6441 | trainAcc: 66.4062% (85/128)\n",
            "2 13 Epoch: 42 | ANN: trainLoss: 0.6080 | trainAcc: 68.7500% (132/192)\n",
            "3 13 Epoch: 42 | ANN: trainLoss: 0.5925 | trainAcc: 69.9219% (179/256)\n",
            "4 13 Epoch: 42 | ANN: trainLoss: 0.5674 | trainAcc: 71.2500% (228/320)\n",
            "5 13 Epoch: 42 | ANN: trainLoss: 0.5646 | trainAcc: 72.6562% (279/384)\n",
            "6 13 Epoch: 42 | ANN: trainLoss: 0.5544 | trainAcc: 74.3304% (333/448)\n",
            "7 13 Epoch: 42 | ANN: trainLoss: 0.5427 | trainAcc: 74.8047% (383/512)\n",
            "8 13 Epoch: 42 | ANN: trainLoss: 0.5466 | trainAcc: 74.8264% (431/576)\n",
            "9 13 Epoch: 42 | ANN: trainLoss: 0.5499 | trainAcc: 75.6250% (484/640)\n",
            "10 13 Epoch: 42 | ANN: trainLoss: 0.5458 | trainAcc: 75.7102% (533/704)\n",
            "11 13 Epoch: 42 | ANN: trainLoss: 0.5420 | trainAcc: 76.0417% (584/768)\n",
            "12 13 Epoch: 42 | ANN: trainLoss: 0.6014 | trainAcc: 76.0363% (587/772)\n",
            "0 4 Epoch: 42 | ANN: testLoss: 0.5659 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 42 | ANN: testLoss: 0.6111 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 42 | ANN: testLoss: 0.6995 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 42 | ANN: testLoss: 0.6853 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 43 | ANN: trainLoss: 0.5416 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 43 | ANN: trainLoss: 0.5023 | trainAcc: 76.5625% (98/128)\n",
            "2 13 Epoch: 43 | ANN: trainLoss: 0.4748 | trainAcc: 77.6042% (149/192)\n",
            "3 13 Epoch: 43 | ANN: trainLoss: 0.4612 | trainAcc: 78.1250% (200/256)\n",
            "4 13 Epoch: 43 | ANN: trainLoss: 0.4644 | trainAcc: 77.8125% (249/320)\n",
            "5 13 Epoch: 43 | ANN: trainLoss: 0.4698 | trainAcc: 77.3438% (297/384)\n",
            "6 13 Epoch: 43 | ANN: trainLoss: 0.5182 | trainAcc: 75.4464% (338/448)\n",
            "7 13 Epoch: 43 | ANN: trainLoss: 0.5121 | trainAcc: 75.9766% (389/512)\n",
            "8 13 Epoch: 43 | ANN: trainLoss: 0.5066 | trainAcc: 76.7361% (442/576)\n",
            "9 13 Epoch: 43 | ANN: trainLoss: 0.5040 | trainAcc: 76.8750% (492/640)\n",
            "10 13 Epoch: 43 | ANN: trainLoss: 0.5107 | trainAcc: 76.5625% (539/704)\n",
            "11 13 Epoch: 43 | ANN: trainLoss: 0.5237 | trainAcc: 76.3021% (586/768)\n",
            "12 13 Epoch: 43 | ANN: trainLoss: 0.5727 | trainAcc: 76.1658% (588/772)\n",
            "0 4 Epoch: 43 | ANN: testLoss: 0.6192 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 43 | ANN: testLoss: 0.7061 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 43 | ANN: testLoss: 0.6983 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 43 | ANN: testLoss: 0.6334 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 44 | ANN: trainLoss: 0.4872 | trainAcc: 70.3125% (45/64)\n",
            "1 13 Epoch: 44 | ANN: trainLoss: 0.5039 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 44 | ANN: trainLoss: 0.5236 | trainAcc: 74.4792% (143/192)\n",
            "3 13 Epoch: 44 | ANN: trainLoss: 0.5091 | trainAcc: 75.7812% (194/256)\n",
            "4 13 Epoch: 44 | ANN: trainLoss: 0.5231 | trainAcc: 74.0625% (237/320)\n",
            "5 13 Epoch: 44 | ANN: trainLoss: 0.5091 | trainAcc: 75.0000% (288/384)\n",
            "6 13 Epoch: 44 | ANN: trainLoss: 0.5055 | trainAcc: 75.0000% (336/448)\n",
            "7 13 Epoch: 44 | ANN: trainLoss: 0.5095 | trainAcc: 75.1953% (385/512)\n",
            "8 13 Epoch: 44 | ANN: trainLoss: 0.5156 | trainAcc: 74.6528% (430/576)\n",
            "9 13 Epoch: 44 | ANN: trainLoss: 0.5139 | trainAcc: 75.0000% (480/640)\n",
            "10 13 Epoch: 44 | ANN: trainLoss: 0.5176 | trainAcc: 75.0000% (528/704)\n",
            "11 13 Epoch: 44 | ANN: trainLoss: 0.5155 | trainAcc: 74.6094% (573/768)\n",
            "12 13 Epoch: 44 | ANN: trainLoss: 0.5869 | trainAcc: 74.6114% (576/772)\n",
            "0 4 Epoch: 44 | ANN: testLoss: 0.7060 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 44 | ANN: testLoss: 0.7330 | testAcc: 60.9375% (78/128)\n",
            "2 4 Epoch: 44 | ANN: testLoss: 0.7010 | testAcc: 63.5417% (122/192)\n",
            "3 4 Epoch: 44 | ANN: testLoss: 0.7250 | testAcc: 63.2124% (122/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 45 | ANN: trainLoss: 0.4571 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 45 | ANN: trainLoss: 0.5020 | trainAcc: 79.6875% (102/128)\n",
            "2 13 Epoch: 45 | ANN: trainLoss: 0.5402 | trainAcc: 73.9583% (142/192)\n",
            "3 13 Epoch: 45 | ANN: trainLoss: 0.5098 | trainAcc: 76.5625% (196/256)\n",
            "4 13 Epoch: 45 | ANN: trainLoss: 0.5258 | trainAcc: 75.6250% (242/320)\n",
            "5 13 Epoch: 45 | ANN: trainLoss: 0.5229 | trainAcc: 75.5208% (290/384)\n",
            "6 13 Epoch: 45 | ANN: trainLoss: 0.5345 | trainAcc: 75.0000% (336/448)\n",
            "7 13 Epoch: 45 | ANN: trainLoss: 0.5198 | trainAcc: 75.9766% (389/512)\n",
            "8 13 Epoch: 45 | ANN: trainLoss: 0.5283 | trainAcc: 75.8681% (437/576)\n",
            "9 13 Epoch: 45 | ANN: trainLoss: 0.5297 | trainAcc: 75.9375% (486/640)\n",
            "10 13 Epoch: 45 | ANN: trainLoss: 0.5398 | trainAcc: 75.1420% (529/704)\n",
            "11 13 Epoch: 45 | ANN: trainLoss: 0.5340 | trainAcc: 75.7812% (582/768)\n",
            "12 13 Epoch: 45 | ANN: trainLoss: 0.7014 | trainAcc: 75.5181% (583/772)\n",
            "0 4 Epoch: 45 | ANN: testLoss: 0.7098 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 45 | ANN: testLoss: 0.6649 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 45 | ANN: testLoss: 0.6839 | testAcc: 60.9375% (117/192)\n",
            "3 4 Epoch: 45 | ANN: testLoss: 0.8185 | testAcc: 60.6218% (117/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 46 | ANN: trainLoss: 0.5266 | trainAcc: 71.8750% (46/64)\n",
            "1 13 Epoch: 46 | ANN: trainLoss: 0.5282 | trainAcc: 73.4375% (94/128)\n",
            "2 13 Epoch: 46 | ANN: trainLoss: 0.4893 | trainAcc: 78.1250% (150/192)\n",
            "3 13 Epoch: 46 | ANN: trainLoss: 0.4688 | trainAcc: 80.0781% (205/256)\n",
            "4 13 Epoch: 46 | ANN: trainLoss: 0.4926 | trainAcc: 78.1250% (250/320)\n",
            "5 13 Epoch: 46 | ANN: trainLoss: 0.5025 | trainAcc: 77.3438% (297/384)\n",
            "6 13 Epoch: 46 | ANN: trainLoss: 0.5129 | trainAcc: 77.0089% (345/448)\n",
            "7 13 Epoch: 46 | ANN: trainLoss: 0.5184 | trainAcc: 77.7344% (398/512)\n",
            "8 13 Epoch: 46 | ANN: trainLoss: 0.5258 | trainAcc: 76.7361% (442/576)\n",
            "9 13 Epoch: 46 | ANN: trainLoss: 0.5241 | trainAcc: 76.7188% (491/640)\n",
            "10 13 Epoch: 46 | ANN: trainLoss: 0.5295 | trainAcc: 76.9886% (542/704)\n",
            "11 13 Epoch: 46 | ANN: trainLoss: 0.5342 | trainAcc: 76.3021% (586/768)\n",
            "12 13 Epoch: 46 | ANN: trainLoss: 0.6121 | trainAcc: 76.0363% (587/772)\n",
            "0 4 Epoch: 46 | ANN: testLoss: 0.5923 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 46 | ANN: testLoss: 0.6989 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 46 | ANN: testLoss: 0.7220 | testAcc: 65.1042% (125/192)\n",
            "3 4 Epoch: 46 | ANN: testLoss: 0.7680 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 47 | ANN: trainLoss: 0.6562 | trainAcc: 71.8750% (46/64)\n",
            "1 13 Epoch: 47 | ANN: trainLoss: 0.5811 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 47 | ANN: trainLoss: 0.6468 | trainAcc: 71.8750% (138/192)\n",
            "3 13 Epoch: 47 | ANN: trainLoss: 0.6231 | trainAcc: 73.8281% (189/256)\n",
            "4 13 Epoch: 47 | ANN: trainLoss: 0.6486 | trainAcc: 72.1875% (231/320)\n",
            "5 13 Epoch: 47 | ANN: trainLoss: 0.6406 | trainAcc: 72.3958% (278/384)\n",
            "6 13 Epoch: 47 | ANN: trainLoss: 0.6335 | trainAcc: 73.2143% (328/448)\n",
            "7 13 Epoch: 47 | ANN: trainLoss: 0.6341 | trainAcc: 72.6562% (372/512)\n",
            "8 13 Epoch: 47 | ANN: trainLoss: 0.6394 | trainAcc: 71.5278% (412/576)\n",
            "9 13 Epoch: 47 | ANN: trainLoss: 0.6367 | trainAcc: 71.7188% (459/640)\n",
            "10 13 Epoch: 47 | ANN: trainLoss: 0.6306 | trainAcc: 71.7330% (505/704)\n",
            "11 13 Epoch: 47 | ANN: trainLoss: 0.6264 | trainAcc: 71.6146% (550/768)\n",
            "12 13 Epoch: 47 | ANN: trainLoss: 0.6437 | trainAcc: 71.5026% (552/772)\n",
            "0 4 Epoch: 47 | ANN: testLoss: 0.7486 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 47 | ANN: testLoss: 0.6809 | testAcc: 62.5000% (80/128)\n",
            "2 4 Epoch: 47 | ANN: testLoss: 0.6743 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 47 | ANN: testLoss: 0.7533 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 48 | ANN: trainLoss: 0.6064 | trainAcc: 67.1875% (43/64)\n",
            "1 13 Epoch: 48 | ANN: trainLoss: 0.5544 | trainAcc: 71.8750% (92/128)\n",
            "2 13 Epoch: 48 | ANN: trainLoss: 0.5904 | trainAcc: 70.3125% (135/192)\n",
            "3 13 Epoch: 48 | ANN: trainLoss: 0.5793 | trainAcc: 69.1406% (177/256)\n",
            "4 13 Epoch: 48 | ANN: trainLoss: 0.5639 | trainAcc: 70.0000% (224/320)\n",
            "5 13 Epoch: 48 | ANN: trainLoss: 0.5731 | trainAcc: 69.7917% (268/384)\n",
            "6 13 Epoch: 48 | ANN: trainLoss: 0.5669 | trainAcc: 70.5357% (316/448)\n",
            "7 13 Epoch: 48 | ANN: trainLoss: 0.5513 | trainAcc: 72.2656% (370/512)\n",
            "8 13 Epoch: 48 | ANN: trainLoss: 0.5452 | trainAcc: 73.7847% (425/576)\n",
            "9 13 Epoch: 48 | ANN: trainLoss: 0.5517 | trainAcc: 73.7500% (472/640)\n",
            "10 13 Epoch: 48 | ANN: trainLoss: 0.5550 | trainAcc: 74.1477% (522/704)\n",
            "11 13 Epoch: 48 | ANN: trainLoss: 0.5454 | trainAcc: 74.7396% (574/768)\n",
            "12 13 Epoch: 48 | ANN: trainLoss: 0.5767 | trainAcc: 74.6114% (576/772)\n",
            "0 4 Epoch: 48 | ANN: testLoss: 0.6715 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 48 | ANN: testLoss: 0.6748 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 48 | ANN: testLoss: 0.6652 | testAcc: 63.5417% (122/192)\n",
            "3 4 Epoch: 48 | ANN: testLoss: 0.6228 | testAcc: 63.7306% (123/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 49 | ANN: trainLoss: 0.3547 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 49 | ANN: trainLoss: 0.4169 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 49 | ANN: trainLoss: 0.4377 | trainAcc: 83.3333% (160/192)\n",
            "3 13 Epoch: 49 | ANN: trainLoss: 0.4465 | trainAcc: 82.0312% (210/256)\n",
            "4 13 Epoch: 49 | ANN: trainLoss: 0.4538 | trainAcc: 80.6250% (258/320)\n",
            "5 13 Epoch: 49 | ANN: trainLoss: 0.4617 | trainAcc: 80.7292% (310/384)\n",
            "6 13 Epoch: 49 | ANN: trainLoss: 0.4805 | trainAcc: 79.4643% (356/448)\n",
            "7 13 Epoch: 49 | ANN: trainLoss: 0.4739 | trainAcc: 79.2969% (406/512)\n",
            "8 13 Epoch: 49 | ANN: trainLoss: 0.4834 | trainAcc: 78.2986% (451/576)\n",
            "9 13 Epoch: 49 | ANN: trainLoss: 0.4924 | trainAcc: 77.9688% (499/640)\n",
            "10 13 Epoch: 49 | ANN: trainLoss: 0.4936 | trainAcc: 77.6989% (547/704)\n",
            "11 13 Epoch: 49 | ANN: trainLoss: 0.5068 | trainAcc: 77.0833% (592/768)\n",
            "12 13 Epoch: 49 | ANN: trainLoss: 0.5805 | trainAcc: 76.9430% (594/772)\n",
            "0 4 Epoch: 49 | ANN: testLoss: 0.6508 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 49 | ANN: testLoss: 0.7107 | testAcc: 62.5000% (80/128)\n",
            "2 4 Epoch: 49 | ANN: testLoss: 0.6539 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 49 | ANN: testLoss: 0.6669 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 50 | ANN: trainLoss: 0.6092 | trainAcc: 73.4375% (47/64)\n",
            "1 13 Epoch: 50 | ANN: trainLoss: 0.5569 | trainAcc: 72.6562% (93/128)\n",
            "2 13 Epoch: 50 | ANN: trainLoss: 0.5273 | trainAcc: 76.0417% (146/192)\n",
            "3 13 Epoch: 50 | ANN: trainLoss: 0.5252 | trainAcc: 75.7812% (194/256)\n",
            "4 13 Epoch: 50 | ANN: trainLoss: 0.5228 | trainAcc: 75.3125% (241/320)\n",
            "5 13 Epoch: 50 | ANN: trainLoss: 0.5406 | trainAcc: 74.4792% (286/384)\n",
            "6 13 Epoch: 50 | ANN: trainLoss: 0.5414 | trainAcc: 74.7768% (335/448)\n",
            "7 13 Epoch: 50 | ANN: trainLoss: 0.5491 | trainAcc: 74.0234% (379/512)\n",
            "8 13 Epoch: 50 | ANN: trainLoss: 0.5497 | trainAcc: 74.3056% (428/576)\n",
            "9 13 Epoch: 50 | ANN: trainLoss: 0.5510 | trainAcc: 74.2188% (475/640)\n",
            "10 13 Epoch: 50 | ANN: trainLoss: 0.5533 | trainAcc: 74.2898% (523/704)\n",
            "11 13 Epoch: 50 | ANN: trainLoss: 0.5500 | trainAcc: 74.4792% (572/768)\n",
            "12 13 Epoch: 50 | ANN: trainLoss: 0.5635 | trainAcc: 74.3523% (574/772)\n",
            "0 4 Epoch: 50 | ANN: testLoss: 0.6496 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 50 | ANN: testLoss: 0.6282 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 50 | ANN: testLoss: 0.6528 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 50 | ANN: testLoss: 0.5892 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 51 | ANN: trainLoss: 0.4350 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 51 | ANN: trainLoss: 0.4550 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 51 | ANN: trainLoss: 0.4864 | trainAcc: 78.1250% (150/192)\n",
            "3 13 Epoch: 51 | ANN: trainLoss: 0.4506 | trainAcc: 80.8594% (207/256)\n",
            "4 13 Epoch: 51 | ANN: trainLoss: 0.4448 | trainAcc: 81.5625% (261/320)\n",
            "5 13 Epoch: 51 | ANN: trainLoss: 0.4593 | trainAcc: 79.9479% (307/384)\n",
            "6 13 Epoch: 51 | ANN: trainLoss: 0.4648 | trainAcc: 79.6875% (357/448)\n",
            "7 13 Epoch: 51 | ANN: trainLoss: 0.4713 | trainAcc: 79.2969% (406/512)\n",
            "8 13 Epoch: 51 | ANN: trainLoss: 0.4802 | trainAcc: 79.5139% (458/576)\n",
            "9 13 Epoch: 51 | ANN: trainLoss: 0.4801 | trainAcc: 79.3750% (508/640)\n",
            "10 13 Epoch: 51 | ANN: trainLoss: 0.4914 | trainAcc: 78.2670% (551/704)\n",
            "11 13 Epoch: 51 | ANN: trainLoss: 0.5030 | trainAcc: 77.6042% (596/768)\n",
            "12 13 Epoch: 51 | ANN: trainLoss: 0.5063 | trainAcc: 77.5907% (599/772)\n",
            "0 4 Epoch: 51 | ANN: testLoss: 0.7789 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 51 | ANN: testLoss: 0.6914 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 51 | ANN: testLoss: 0.6708 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 51 | ANN: testLoss: 0.7150 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 52 | ANN: trainLoss: 0.4763 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 52 | ANN: trainLoss: 0.4743 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 52 | ANN: trainLoss: 0.4692 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 52 | ANN: trainLoss: 0.4422 | trainAcc: 82.0312% (210/256)\n",
            "4 13 Epoch: 52 | ANN: trainLoss: 0.4441 | trainAcc: 82.1875% (263/320)\n",
            "5 13 Epoch: 52 | ANN: trainLoss: 0.4748 | trainAcc: 80.9896% (311/384)\n",
            "6 13 Epoch: 52 | ANN: trainLoss: 0.4937 | trainAcc: 79.0179% (354/448)\n",
            "7 13 Epoch: 52 | ANN: trainLoss: 0.4973 | trainAcc: 78.1250% (400/512)\n",
            "8 13 Epoch: 52 | ANN: trainLoss: 0.4924 | trainAcc: 77.9514% (449/576)\n",
            "9 13 Epoch: 52 | ANN: trainLoss: 0.4841 | trainAcc: 78.2812% (501/640)\n",
            "10 13 Epoch: 52 | ANN: trainLoss: 0.4853 | trainAcc: 78.4091% (552/704)\n",
            "11 13 Epoch: 52 | ANN: trainLoss: 0.4924 | trainAcc: 77.7344% (597/768)\n",
            "12 13 Epoch: 52 | ANN: trainLoss: 0.5199 | trainAcc: 77.5907% (599/772)\n",
            "0 4 Epoch: 52 | ANN: testLoss: 0.7355 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 52 | ANN: testLoss: 0.6937 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 52 | ANN: testLoss: 0.6653 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 52 | ANN: testLoss: 0.7122 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 53 | ANN: trainLoss: 0.3413 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 53 | ANN: trainLoss: 0.4203 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 53 | ANN: trainLoss: 0.4932 | trainAcc: 79.1667% (152/192)\n",
            "3 13 Epoch: 53 | ANN: trainLoss: 0.4786 | trainAcc: 79.2969% (203/256)\n",
            "4 13 Epoch: 53 | ANN: trainLoss: 0.4941 | trainAcc: 77.1875% (247/320)\n",
            "5 13 Epoch: 53 | ANN: trainLoss: 0.5047 | trainAcc: 76.0417% (292/384)\n",
            "6 13 Epoch: 53 | ANN: trainLoss: 0.5154 | trainAcc: 76.1161% (341/448)\n",
            "7 13 Epoch: 53 | ANN: trainLoss: 0.5197 | trainAcc: 75.9766% (389/512)\n",
            "8 13 Epoch: 53 | ANN: trainLoss: 0.5162 | trainAcc: 76.0417% (438/576)\n",
            "9 13 Epoch: 53 | ANN: trainLoss: 0.5091 | trainAcc: 75.9375% (486/640)\n",
            "10 13 Epoch: 53 | ANN: trainLoss: 0.5016 | trainAcc: 76.4205% (538/704)\n",
            "11 13 Epoch: 53 | ANN: trainLoss: 0.4973 | trainAcc: 76.5625% (588/768)\n",
            "12 13 Epoch: 53 | ANN: trainLoss: 0.4776 | trainAcc: 76.6839% (592/772)\n",
            "0 4 Epoch: 53 | ANN: testLoss: 0.5947 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 53 | ANN: testLoss: 0.5965 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 53 | ANN: testLoss: 0.6357 | testAcc: 65.1042% (125/192)\n",
            "3 4 Epoch: 53 | ANN: testLoss: 0.5700 | testAcc: 65.2850% (126/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 54 | ANN: trainLoss: 0.3909 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 54 | ANN: trainLoss: 0.3867 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 54 | ANN: trainLoss: 0.4025 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 54 | ANN: trainLoss: 0.4445 | trainAcc: 80.0781% (205/256)\n",
            "4 13 Epoch: 54 | ANN: trainLoss: 0.4287 | trainAcc: 80.9375% (259/320)\n",
            "5 13 Epoch: 54 | ANN: trainLoss: 0.4308 | trainAcc: 80.7292% (310/384)\n",
            "6 13 Epoch: 54 | ANN: trainLoss: 0.4395 | trainAcc: 80.3571% (360/448)\n",
            "7 13 Epoch: 54 | ANN: trainLoss: 0.4462 | trainAcc: 80.4688% (412/512)\n",
            "8 13 Epoch: 54 | ANN: trainLoss: 0.4477 | trainAcc: 80.0347% (461/576)\n",
            "9 13 Epoch: 54 | ANN: trainLoss: 0.4586 | trainAcc: 79.2188% (507/640)\n",
            "10 13 Epoch: 54 | ANN: trainLoss: 0.4691 | trainAcc: 79.4034% (559/704)\n",
            "11 13 Epoch: 54 | ANN: trainLoss: 0.4697 | trainAcc: 79.4271% (610/768)\n",
            "12 13 Epoch: 54 | ANN: trainLoss: 0.4691 | trainAcc: 79.4041% (613/772)\n",
            "0 4 Epoch: 54 | ANN: testLoss: 0.6392 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 54 | ANN: testLoss: 0.6729 | testAcc: 61.7188% (79/128)\n",
            "2 4 Epoch: 54 | ANN: testLoss: 0.6303 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 54 | ANN: testLoss: 0.4801 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 55 | ANN: trainLoss: 0.5879 | trainAcc: 73.4375% (47/64)\n",
            "1 13 Epoch: 55 | ANN: trainLoss: 0.5378 | trainAcc: 75.7812% (97/128)\n",
            "2 13 Epoch: 55 | ANN: trainLoss: 0.5158 | trainAcc: 76.0417% (146/192)\n",
            "3 13 Epoch: 55 | ANN: trainLoss: 0.4970 | trainAcc: 76.1719% (195/256)\n",
            "4 13 Epoch: 55 | ANN: trainLoss: 0.5250 | trainAcc: 75.9375% (243/320)\n",
            "5 13 Epoch: 55 | ANN: trainLoss: 0.5090 | trainAcc: 76.5625% (294/384)\n",
            "6 13 Epoch: 55 | ANN: trainLoss: 0.5161 | trainAcc: 76.3393% (342/448)\n",
            "7 13 Epoch: 55 | ANN: trainLoss: 0.5044 | trainAcc: 77.3438% (396/512)\n",
            "8 13 Epoch: 55 | ANN: trainLoss: 0.4760 | trainAcc: 78.6458% (453/576)\n",
            "9 13 Epoch: 55 | ANN: trainLoss: 0.4726 | trainAcc: 78.5938% (503/640)\n",
            "10 13 Epoch: 55 | ANN: trainLoss: 0.4714 | trainAcc: 78.9773% (556/704)\n",
            "11 13 Epoch: 55 | ANN: trainLoss: 0.4678 | trainAcc: 79.1667% (608/768)\n",
            "12 13 Epoch: 55 | ANN: trainLoss: 0.4649 | trainAcc: 79.1451% (611/772)\n",
            "0 4 Epoch: 55 | ANN: testLoss: 0.6732 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 55 | ANN: testLoss: 0.6905 | testAcc: 61.7188% (79/128)\n",
            "2 4 Epoch: 55 | ANN: testLoss: 0.6159 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 55 | ANN: testLoss: 0.7064 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 56 | ANN: trainLoss: 0.4130 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 56 | ANN: trainLoss: 0.3769 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 56 | ANN: trainLoss: 0.3932 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 56 | ANN: trainLoss: 0.4004 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 56 | ANN: trainLoss: 0.4195 | trainAcc: 83.1250% (266/320)\n",
            "5 13 Epoch: 56 | ANN: trainLoss: 0.4142 | trainAcc: 83.3333% (320/384)\n",
            "6 13 Epoch: 56 | ANN: trainLoss: 0.4068 | trainAcc: 83.4821% (374/448)\n",
            "7 13 Epoch: 56 | ANN: trainLoss: 0.4098 | trainAcc: 82.6172% (423/512)\n",
            "8 13 Epoch: 56 | ANN: trainLoss: 0.4103 | trainAcc: 82.2917% (474/576)\n",
            "9 13 Epoch: 56 | ANN: trainLoss: 0.4114 | trainAcc: 82.5000% (528/640)\n",
            "10 13 Epoch: 56 | ANN: trainLoss: 0.4114 | trainAcc: 82.2443% (579/704)\n",
            "11 13 Epoch: 56 | ANN: trainLoss: 0.4143 | trainAcc: 81.9010% (629/768)\n",
            "12 13 Epoch: 56 | ANN: trainLoss: 0.4938 | trainAcc: 81.7358% (631/772)\n",
            "0 4 Epoch: 56 | ANN: testLoss: 0.5221 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 56 | ANN: testLoss: 0.6163 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 56 | ANN: testLoss: 0.6153 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 56 | ANN: testLoss: 0.7759 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 57 | ANN: trainLoss: 0.4996 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 57 | ANN: trainLoss: 0.4765 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 57 | ANN: trainLoss: 0.4733 | trainAcc: 80.2083% (154/192)\n",
            "3 13 Epoch: 57 | ANN: trainLoss: 0.4388 | trainAcc: 82.0312% (210/256)\n",
            "4 13 Epoch: 57 | ANN: trainLoss: 0.4599 | trainAcc: 82.8125% (265/320)\n",
            "5 13 Epoch: 57 | ANN: trainLoss: 0.4514 | trainAcc: 83.0729% (319/384)\n",
            "6 13 Epoch: 57 | ANN: trainLoss: 0.4515 | trainAcc: 82.1429% (368/448)\n",
            "7 13 Epoch: 57 | ANN: trainLoss: 0.4469 | trainAcc: 83.0078% (425/512)\n",
            "8 13 Epoch: 57 | ANN: trainLoss: 0.4479 | trainAcc: 82.6389% (476/576)\n",
            "9 13 Epoch: 57 | ANN: trainLoss: 0.4572 | trainAcc: 82.6562% (529/640)\n",
            "10 13 Epoch: 57 | ANN: trainLoss: 0.4670 | trainAcc: 81.8182% (576/704)\n",
            "11 13 Epoch: 57 | ANN: trainLoss: 0.4873 | trainAcc: 80.9896% (622/768)\n",
            "12 13 Epoch: 57 | ANN: trainLoss: 0.4672 | trainAcc: 81.0881% (626/772)\n",
            "0 4 Epoch: 57 | ANN: testLoss: 0.6156 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 57 | ANN: testLoss: 0.6534 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 57 | ANN: testLoss: 0.6920 | testAcc: 61.4583% (118/192)\n",
            "3 4 Epoch: 57 | ANN: testLoss: 0.5537 | testAcc: 61.6580% (119/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 58 | ANN: trainLoss: 0.6084 | trainAcc: 70.3125% (45/64)\n",
            "1 13 Epoch: 58 | ANN: trainLoss: 0.5505 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 58 | ANN: trainLoss: 0.5090 | trainAcc: 77.6042% (149/192)\n",
            "3 13 Epoch: 58 | ANN: trainLoss: 0.4850 | trainAcc: 78.5156% (201/256)\n",
            "4 13 Epoch: 58 | ANN: trainLoss: 0.4741 | trainAcc: 78.4375% (251/320)\n",
            "5 13 Epoch: 58 | ANN: trainLoss: 0.4778 | trainAcc: 78.3854% (301/384)\n",
            "6 13 Epoch: 58 | ANN: trainLoss: 0.4949 | trainAcc: 77.6786% (348/448)\n",
            "7 13 Epoch: 58 | ANN: trainLoss: 0.4791 | trainAcc: 79.1016% (405/512)\n",
            "8 13 Epoch: 58 | ANN: trainLoss: 0.4822 | trainAcc: 79.1667% (456/576)\n",
            "9 13 Epoch: 58 | ANN: trainLoss: 0.4931 | trainAcc: 78.7500% (504/640)\n",
            "10 13 Epoch: 58 | ANN: trainLoss: 0.4949 | trainAcc: 78.2670% (551/704)\n",
            "11 13 Epoch: 58 | ANN: trainLoss: 0.4904 | trainAcc: 78.5156% (603/768)\n",
            "12 13 Epoch: 58 | ANN: trainLoss: 0.5043 | trainAcc: 78.3679% (605/772)\n",
            "0 4 Epoch: 58 | ANN: testLoss: 0.5985 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 58 | ANN: testLoss: 0.6703 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 58 | ANN: testLoss: 0.6638 | testAcc: 62.5000% (120/192)\n",
            "3 4 Epoch: 58 | ANN: testLoss: 0.6620 | testAcc: 62.6943% (121/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 59 | ANN: trainLoss: 0.4456 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 59 | ANN: trainLoss: 0.4570 | trainAcc: 76.5625% (98/128)\n",
            "2 13 Epoch: 59 | ANN: trainLoss: 0.4394 | trainAcc: 80.7292% (155/192)\n",
            "3 13 Epoch: 59 | ANN: trainLoss: 0.4439 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 59 | ANN: trainLoss: 0.4587 | trainAcc: 79.6875% (255/320)\n",
            "5 13 Epoch: 59 | ANN: trainLoss: 0.4592 | trainAcc: 79.9479% (307/384)\n",
            "6 13 Epoch: 59 | ANN: trainLoss: 0.4578 | trainAcc: 79.4643% (356/448)\n",
            "7 13 Epoch: 59 | ANN: trainLoss: 0.4612 | trainAcc: 78.9062% (404/512)\n",
            "8 13 Epoch: 59 | ANN: trainLoss: 0.4519 | trainAcc: 80.0347% (461/576)\n",
            "9 13 Epoch: 59 | ANN: trainLoss: 0.4540 | trainAcc: 79.6875% (510/640)\n",
            "10 13 Epoch: 59 | ANN: trainLoss: 0.4508 | trainAcc: 79.8295% (562/704)\n",
            "11 13 Epoch: 59 | ANN: trainLoss: 0.4607 | trainAcc: 79.8177% (613/768)\n",
            "12 13 Epoch: 59 | ANN: trainLoss: 0.4821 | trainAcc: 79.5337% (614/772)\n",
            "0 4 Epoch: 59 | ANN: testLoss: 0.7198 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 59 | ANN: testLoss: 0.6442 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 59 | ANN: testLoss: 0.6547 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 59 | ANN: testLoss: 0.6727 | testAcc: 64.2487% (124/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 60 | ANN: trainLoss: 0.6285 | trainAcc: 67.1875% (43/64)\n",
            "1 13 Epoch: 60 | ANN: trainLoss: 0.5411 | trainAcc: 72.6562% (93/128)\n",
            "2 13 Epoch: 60 | ANN: trainLoss: 0.4665 | trainAcc: 78.6458% (151/192)\n",
            "3 13 Epoch: 60 | ANN: trainLoss: 0.4535 | trainAcc: 80.4688% (206/256)\n",
            "4 13 Epoch: 60 | ANN: trainLoss: 0.4590 | trainAcc: 79.6875% (255/320)\n",
            "5 13 Epoch: 60 | ANN: trainLoss: 0.4572 | trainAcc: 79.9479% (307/384)\n",
            "6 13 Epoch: 60 | ANN: trainLoss: 0.4618 | trainAcc: 79.9107% (358/448)\n",
            "7 13 Epoch: 60 | ANN: trainLoss: 0.4588 | trainAcc: 80.0781% (410/512)\n",
            "8 13 Epoch: 60 | ANN: trainLoss: 0.4578 | trainAcc: 79.8611% (460/576)\n",
            "9 13 Epoch: 60 | ANN: trainLoss: 0.4708 | trainAcc: 79.0625% (506/640)\n",
            "10 13 Epoch: 60 | ANN: trainLoss: 0.4700 | trainAcc: 79.1193% (557/704)\n",
            "11 13 Epoch: 60 | ANN: trainLoss: 0.4599 | trainAcc: 79.6875% (612/768)\n",
            "12 13 Epoch: 60 | ANN: trainLoss: 0.4412 | trainAcc: 79.7927% (616/772)\n",
            "0 4 Epoch: 60 | ANN: testLoss: 0.5986 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 60 | ANN: testLoss: 0.6445 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 60 | ANN: testLoss: 0.6395 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 60 | ANN: testLoss: 0.4798 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 61 | ANN: trainLoss: 0.4723 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 61 | ANN: trainLoss: 0.4706 | trainAcc: 78.1250% (100/128)\n",
            "2 13 Epoch: 61 | ANN: trainLoss: 0.4860 | trainAcc: 79.6875% (153/192)\n",
            "3 13 Epoch: 61 | ANN: trainLoss: 0.4808 | trainAcc: 80.0781% (205/256)\n",
            "4 13 Epoch: 61 | ANN: trainLoss: 0.4662 | trainAcc: 80.6250% (258/320)\n",
            "5 13 Epoch: 61 | ANN: trainLoss: 0.4588 | trainAcc: 81.2500% (312/384)\n",
            "6 13 Epoch: 61 | ANN: trainLoss: 0.4596 | trainAcc: 81.6964% (366/448)\n",
            "7 13 Epoch: 61 | ANN: trainLoss: 0.4525 | trainAcc: 81.4453% (417/512)\n",
            "8 13 Epoch: 61 | ANN: trainLoss: 0.4596 | trainAcc: 81.2500% (468/576)\n",
            "9 13 Epoch: 61 | ANN: trainLoss: 0.4655 | trainAcc: 81.0938% (519/640)\n",
            "10 13 Epoch: 61 | ANN: trainLoss: 0.4565 | trainAcc: 81.6761% (575/704)\n",
            "11 13 Epoch: 61 | ANN: trainLoss: 0.4617 | trainAcc: 81.3802% (625/768)\n",
            "12 13 Epoch: 61 | ANN: trainLoss: 0.4922 | trainAcc: 81.2176% (627/772)\n",
            "0 4 Epoch: 61 | ANN: testLoss: 0.6639 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 61 | ANN: testLoss: 0.7026 | testAcc: 62.5000% (80/128)\n",
            "2 4 Epoch: 61 | ANN: testLoss: 0.6468 | testAcc: 65.1042% (125/192)\n",
            "3 4 Epoch: 61 | ANN: testLoss: 0.6969 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 62 | ANN: trainLoss: 0.4636 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 62 | ANN: trainLoss: 0.4101 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 62 | ANN: trainLoss: 0.3959 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 62 | ANN: trainLoss: 0.4171 | trainAcc: 83.2031% (213/256)\n",
            "4 13 Epoch: 62 | ANN: trainLoss: 0.4029 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 62 | ANN: trainLoss: 0.4207 | trainAcc: 82.8125% (318/384)\n",
            "6 13 Epoch: 62 | ANN: trainLoss: 0.4252 | trainAcc: 82.1429% (368/448)\n",
            "7 13 Epoch: 62 | ANN: trainLoss: 0.4222 | trainAcc: 82.8125% (424/512)\n",
            "8 13 Epoch: 62 | ANN: trainLoss: 0.4189 | trainAcc: 82.6389% (476/576)\n",
            "9 13 Epoch: 62 | ANN: trainLoss: 0.4214 | trainAcc: 83.1250% (532/640)\n",
            "10 13 Epoch: 62 | ANN: trainLoss: 0.4184 | trainAcc: 83.2386% (586/704)\n",
            "11 13 Epoch: 62 | ANN: trainLoss: 0.4249 | trainAcc: 82.8125% (636/768)\n",
            "12 13 Epoch: 62 | ANN: trainLoss: 0.4245 | trainAcc: 82.7720% (639/772)\n",
            "0 4 Epoch: 62 | ANN: testLoss: 0.6467 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 62 | ANN: testLoss: 0.6652 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 62 | ANN: testLoss: 0.6757 | testAcc: 65.1042% (125/192)\n",
            "3 4 Epoch: 62 | ANN: testLoss: 0.6305 | testAcc: 65.2850% (126/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 63 | ANN: trainLoss: 0.4974 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 63 | ANN: trainLoss: 0.4632 | trainAcc: 79.6875% (102/128)\n",
            "2 13 Epoch: 63 | ANN: trainLoss: 0.4844 | trainAcc: 77.0833% (148/192)\n",
            "3 13 Epoch: 63 | ANN: trainLoss: 0.4653 | trainAcc: 78.9062% (202/256)\n",
            "4 13 Epoch: 63 | ANN: trainLoss: 0.4762 | trainAcc: 78.4375% (251/320)\n",
            "5 13 Epoch: 63 | ANN: trainLoss: 0.4749 | trainAcc: 78.6458% (302/384)\n",
            "6 13 Epoch: 63 | ANN: trainLoss: 0.4767 | trainAcc: 77.9018% (349/448)\n",
            "7 13 Epoch: 63 | ANN: trainLoss: 0.4594 | trainAcc: 78.7109% (403/512)\n",
            "8 13 Epoch: 63 | ANN: trainLoss: 0.4476 | trainAcc: 79.5139% (458/576)\n",
            "9 13 Epoch: 63 | ANN: trainLoss: 0.4432 | trainAcc: 80.3125% (514/640)\n",
            "10 13 Epoch: 63 | ANN: trainLoss: 0.4365 | trainAcc: 80.5398% (567/704)\n",
            "11 13 Epoch: 63 | ANN: trainLoss: 0.4274 | trainAcc: 81.3802% (625/768)\n",
            "12 13 Epoch: 63 | ANN: trainLoss: 0.4466 | trainAcc: 81.3472% (628/772)\n",
            "0 4 Epoch: 63 | ANN: testLoss: 0.6834 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 63 | ANN: testLoss: 0.6595 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 63 | ANN: testLoss: 0.6470 | testAcc: 65.1042% (125/192)\n",
            "3 4 Epoch: 63 | ANN: testLoss: 0.7239 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 64 | ANN: trainLoss: 0.3577 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 64 | ANN: trainLoss: 0.3587 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 64 | ANN: trainLoss: 0.3435 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 64 | ANN: trainLoss: 0.3814 | trainAcc: 83.2031% (213/256)\n",
            "4 13 Epoch: 64 | ANN: trainLoss: 0.3869 | trainAcc: 83.1250% (266/320)\n",
            "5 13 Epoch: 64 | ANN: trainLoss: 0.4059 | trainAcc: 81.7708% (314/384)\n",
            "6 13 Epoch: 64 | ANN: trainLoss: 0.4055 | trainAcc: 81.9196% (367/448)\n",
            "7 13 Epoch: 64 | ANN: trainLoss: 0.4151 | trainAcc: 81.6406% (418/512)\n",
            "8 13 Epoch: 64 | ANN: trainLoss: 0.4124 | trainAcc: 81.9444% (472/576)\n",
            "9 13 Epoch: 64 | ANN: trainLoss: 0.4210 | trainAcc: 81.2500% (520/640)\n",
            "10 13 Epoch: 64 | ANN: trainLoss: 0.4264 | trainAcc: 80.9659% (570/704)\n",
            "11 13 Epoch: 64 | ANN: trainLoss: 0.4198 | trainAcc: 80.9896% (622/768)\n",
            "12 13 Epoch: 64 | ANN: trainLoss: 0.4331 | trainAcc: 80.9585% (625/772)\n",
            "0 4 Epoch: 64 | ANN: testLoss: 0.6502 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 64 | ANN: testLoss: 0.6382 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 64 | ANN: testLoss: 0.6571 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 64 | ANN: testLoss: 0.6343 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 65 | ANN: trainLoss: 0.4726 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 65 | ANN: trainLoss: 0.4308 | trainAcc: 78.1250% (100/128)\n",
            "2 13 Epoch: 65 | ANN: trainLoss: 0.4051 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 65 | ANN: trainLoss: 0.3889 | trainAcc: 82.4219% (211/256)\n",
            "4 13 Epoch: 65 | ANN: trainLoss: 0.3703 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 65 | ANN: trainLoss: 0.3793 | trainAcc: 83.5938% (321/384)\n",
            "6 13 Epoch: 65 | ANN: trainLoss: 0.3908 | trainAcc: 82.5893% (370/448)\n",
            "7 13 Epoch: 65 | ANN: trainLoss: 0.4051 | trainAcc: 81.8359% (419/512)\n",
            "8 13 Epoch: 65 | ANN: trainLoss: 0.4094 | trainAcc: 81.7708% (471/576)\n",
            "9 13 Epoch: 65 | ANN: trainLoss: 0.4025 | trainAcc: 82.1875% (526/640)\n",
            "10 13 Epoch: 65 | ANN: trainLoss: 0.4067 | trainAcc: 82.2443% (579/704)\n",
            "11 13 Epoch: 65 | ANN: trainLoss: 0.4019 | trainAcc: 82.4219% (633/768)\n",
            "12 13 Epoch: 65 | ANN: trainLoss: 0.6323 | trainAcc: 82.1244% (634/772)\n",
            "0 4 Epoch: 65 | ANN: testLoss: 0.6116 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 65 | ANN: testLoss: 0.6582 | testAcc: 60.1562% (77/128)\n",
            "2 4 Epoch: 65 | ANN: testLoss: 0.6472 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 65 | ANN: testLoss: 0.7470 | testAcc: 64.2487% (124/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 66 | ANN: trainLoss: 0.3711 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 66 | ANN: trainLoss: 0.4162 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 66 | ANN: trainLoss: 0.3768 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 66 | ANN: trainLoss: 0.3706 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 66 | ANN: trainLoss: 0.3651 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 66 | ANN: trainLoss: 0.3743 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 66 | ANN: trainLoss: 0.3946 | trainAcc: 84.5982% (379/448)\n",
            "7 13 Epoch: 66 | ANN: trainLoss: 0.3913 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 66 | ANN: trainLoss: 0.3983 | trainAcc: 84.5486% (487/576)\n",
            "9 13 Epoch: 66 | ANN: trainLoss: 0.4150 | trainAcc: 82.9688% (531/640)\n",
            "10 13 Epoch: 66 | ANN: trainLoss: 0.4231 | trainAcc: 82.2443% (579/704)\n",
            "11 13 Epoch: 66 | ANN: trainLoss: 0.4254 | trainAcc: 82.2917% (632/768)\n",
            "12 13 Epoch: 66 | ANN: trainLoss: 0.4349 | trainAcc: 82.2539% (635/772)\n",
            "0 4 Epoch: 66 | ANN: testLoss: 0.7086 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 66 | ANN: testLoss: 0.6494 | testAcc: 60.1562% (77/128)\n",
            "2 4 Epoch: 66 | ANN: testLoss: 0.6771 | testAcc: 61.4583% (118/192)\n",
            "3 4 Epoch: 66 | ANN: testLoss: 0.5121 | testAcc: 61.6580% (119/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 67 | ANN: trainLoss: 0.3909 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 67 | ANN: trainLoss: 0.4092 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 67 | ANN: trainLoss: 0.3976 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 67 | ANN: trainLoss: 0.4004 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 67 | ANN: trainLoss: 0.4038 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 67 | ANN: trainLoss: 0.4103 | trainAcc: 82.2917% (316/384)\n",
            "6 13 Epoch: 67 | ANN: trainLoss: 0.4103 | trainAcc: 82.8125% (371/448)\n",
            "7 13 Epoch: 67 | ANN: trainLoss: 0.4172 | trainAcc: 82.4219% (422/512)\n",
            "8 13 Epoch: 67 | ANN: trainLoss: 0.4093 | trainAcc: 83.1597% (479/576)\n",
            "9 13 Epoch: 67 | ANN: trainLoss: 0.4172 | trainAcc: 82.3438% (527/640)\n",
            "10 13 Epoch: 67 | ANN: trainLoss: 0.4307 | trainAcc: 81.9602% (577/704)\n",
            "11 13 Epoch: 67 | ANN: trainLoss: 0.4252 | trainAcc: 82.1615% (631/768)\n",
            "12 13 Epoch: 67 | ANN: trainLoss: 0.4175 | trainAcc: 82.1244% (634/772)\n",
            "0 4 Epoch: 67 | ANN: testLoss: 0.7094 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 67 | ANN: testLoss: 0.6819 | testAcc: 61.7188% (79/128)\n",
            "2 4 Epoch: 67 | ANN: testLoss: 0.6714 | testAcc: 63.0208% (121/192)\n",
            "3 4 Epoch: 67 | ANN: testLoss: 0.5039 | testAcc: 63.2124% (122/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 68 | ANN: trainLoss: 0.3503 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 68 | ANN: trainLoss: 0.4646 | trainAcc: 78.9062% (101/128)\n",
            "2 13 Epoch: 68 | ANN: trainLoss: 0.4109 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 68 | ANN: trainLoss: 0.4280 | trainAcc: 80.0781% (205/256)\n",
            "4 13 Epoch: 68 | ANN: trainLoss: 0.4161 | trainAcc: 81.8750% (262/320)\n",
            "5 13 Epoch: 68 | ANN: trainLoss: 0.4227 | trainAcc: 82.2917% (316/384)\n",
            "6 13 Epoch: 68 | ANN: trainLoss: 0.4154 | trainAcc: 82.3661% (369/448)\n",
            "7 13 Epoch: 68 | ANN: trainLoss: 0.4186 | trainAcc: 82.0312% (420/512)\n",
            "8 13 Epoch: 68 | ANN: trainLoss: 0.4148 | trainAcc: 82.2917% (474/576)\n",
            "9 13 Epoch: 68 | ANN: trainLoss: 0.4144 | trainAcc: 82.3438% (527/640)\n",
            "10 13 Epoch: 68 | ANN: trainLoss: 0.4056 | trainAcc: 82.9545% (584/704)\n",
            "11 13 Epoch: 68 | ANN: trainLoss: 0.4092 | trainAcc: 82.6823% (635/768)\n",
            "12 13 Epoch: 68 | ANN: trainLoss: 0.4158 | trainAcc: 82.6425% (638/772)\n",
            "0 4 Epoch: 68 | ANN: testLoss: 0.7068 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 68 | ANN: testLoss: 0.6924 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 68 | ANN: testLoss: 0.6376 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 68 | ANN: testLoss: 0.4796 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 69 | ANN: trainLoss: 0.3926 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 69 | ANN: trainLoss: 0.3492 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 69 | ANN: trainLoss: 0.3579 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 69 | ANN: trainLoss: 0.3609 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 69 | ANN: trainLoss: 0.4075 | trainAcc: 82.1875% (263/320)\n",
            "5 13 Epoch: 69 | ANN: trainLoss: 0.3954 | trainAcc: 82.2917% (316/384)\n",
            "6 13 Epoch: 69 | ANN: trainLoss: 0.3939 | trainAcc: 82.3661% (369/448)\n",
            "7 13 Epoch: 69 | ANN: trainLoss: 0.4018 | trainAcc: 81.8359% (419/512)\n",
            "8 13 Epoch: 69 | ANN: trainLoss: 0.3957 | trainAcc: 82.4653% (475/576)\n",
            "9 13 Epoch: 69 | ANN: trainLoss: 0.3895 | trainAcc: 82.8125% (530/640)\n",
            "10 13 Epoch: 69 | ANN: trainLoss: 0.3970 | trainAcc: 82.2443% (579/704)\n",
            "11 13 Epoch: 69 | ANN: trainLoss: 0.4052 | trainAcc: 82.0312% (630/768)\n",
            "12 13 Epoch: 69 | ANN: trainLoss: 0.4988 | trainAcc: 81.7358% (631/772)\n",
            "0 4 Epoch: 69 | ANN: testLoss: 0.6029 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 69 | ANN: testLoss: 0.6965 | testAcc: 60.1562% (77/128)\n",
            "2 4 Epoch: 69 | ANN: testLoss: 0.6406 | testAcc: 63.5417% (122/192)\n",
            "3 4 Epoch: 69 | ANN: testLoss: 0.5435 | testAcc: 63.7306% (123/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 70 | ANN: trainLoss: 0.3295 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 70 | ANN: trainLoss: 0.3299 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 70 | ANN: trainLoss: 0.3637 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 70 | ANN: trainLoss: 0.3748 | trainAcc: 84.7656% (217/256)\n",
            "4 13 Epoch: 70 | ANN: trainLoss: 0.3686 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 70 | ANN: trainLoss: 0.3791 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 70 | ANN: trainLoss: 0.3804 | trainAcc: 84.8214% (380/448)\n",
            "7 13 Epoch: 70 | ANN: trainLoss: 0.3791 | trainAcc: 85.5469% (438/512)\n",
            "8 13 Epoch: 70 | ANN: trainLoss: 0.3804 | trainAcc: 85.4167% (492/576)\n",
            "9 13 Epoch: 70 | ANN: trainLoss: 0.3753 | trainAcc: 85.7812% (549/640)\n",
            "10 13 Epoch: 70 | ANN: trainLoss: 0.3966 | trainAcc: 85.0852% (599/704)\n",
            "11 13 Epoch: 70 | ANN: trainLoss: 0.4002 | trainAcc: 84.8958% (652/768)\n",
            "12 13 Epoch: 70 | ANN: trainLoss: 0.4571 | trainAcc: 84.7150% (654/772)\n",
            "0 4 Epoch: 70 | ANN: testLoss: 0.6256 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 70 | ANN: testLoss: 0.6420 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 70 | ANN: testLoss: 0.6474 | testAcc: 63.5417% (122/192)\n",
            "3 4 Epoch: 70 | ANN: testLoss: 0.5026 | testAcc: 63.7306% (123/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 71 | ANN: trainLoss: 0.4773 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 71 | ANN: trainLoss: 0.4653 | trainAcc: 79.6875% (102/128)\n",
            "2 13 Epoch: 71 | ANN: trainLoss: 0.4267 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 71 | ANN: trainLoss: 0.4073 | trainAcc: 82.4219% (211/256)\n",
            "4 13 Epoch: 71 | ANN: trainLoss: 0.4050 | trainAcc: 82.8125% (265/320)\n",
            "5 13 Epoch: 71 | ANN: trainLoss: 0.4312 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 71 | ANN: trainLoss: 0.4135 | trainAcc: 83.7054% (375/448)\n",
            "7 13 Epoch: 71 | ANN: trainLoss: 0.4161 | trainAcc: 83.5938% (428/512)\n",
            "8 13 Epoch: 71 | ANN: trainLoss: 0.4133 | trainAcc: 83.6806% (482/576)\n",
            "9 13 Epoch: 71 | ANN: trainLoss: 0.4122 | trainAcc: 83.2812% (533/640)\n",
            "10 13 Epoch: 71 | ANN: trainLoss: 0.4107 | trainAcc: 83.0966% (585/704)\n",
            "11 13 Epoch: 71 | ANN: trainLoss: 0.4094 | trainAcc: 83.0729% (638/768)\n",
            "12 13 Epoch: 71 | ANN: trainLoss: 0.3918 | trainAcc: 83.1606% (642/772)\n",
            "0 4 Epoch: 71 | ANN: testLoss: 0.8367 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 71 | ANN: testLoss: 0.7069 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 71 | ANN: testLoss: 0.6810 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 71 | ANN: testLoss: 0.7071 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 72 | ANN: trainLoss: 0.4127 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 72 | ANN: trainLoss: 0.3982 | trainAcc: 76.5625% (98/128)\n",
            "2 13 Epoch: 72 | ANN: trainLoss: 0.3765 | trainAcc: 79.1667% (152/192)\n",
            "3 13 Epoch: 72 | ANN: trainLoss: 0.3665 | trainAcc: 80.8594% (207/256)\n",
            "4 13 Epoch: 72 | ANN: trainLoss: 0.3632 | trainAcc: 82.8125% (265/320)\n",
            "5 13 Epoch: 72 | ANN: trainLoss: 0.3568 | trainAcc: 84.1146% (323/384)\n",
            "6 13 Epoch: 72 | ANN: trainLoss: 0.3575 | trainAcc: 83.9286% (376/448)\n",
            "7 13 Epoch: 72 | ANN: trainLoss: 0.3536 | trainAcc: 84.1797% (431/512)\n",
            "8 13 Epoch: 72 | ANN: trainLoss: 0.3576 | trainAcc: 84.2014% (485/576)\n",
            "9 13 Epoch: 72 | ANN: trainLoss: 0.3498 | trainAcc: 84.6875% (542/640)\n",
            "10 13 Epoch: 72 | ANN: trainLoss: 0.3631 | trainAcc: 84.0909% (592/704)\n",
            "11 13 Epoch: 72 | ANN: trainLoss: 0.3648 | trainAcc: 84.3750% (648/768)\n",
            "12 13 Epoch: 72 | ANN: trainLoss: 0.3499 | trainAcc: 84.4560% (652/772)\n",
            "0 4 Epoch: 72 | ANN: testLoss: 0.7444 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 72 | ANN: testLoss: 0.7050 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 72 | ANN: testLoss: 0.6893 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 72 | ANN: testLoss: 0.5214 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 73 | ANN: trainLoss: 0.4538 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 73 | ANN: trainLoss: 0.4815 | trainAcc: 76.5625% (98/128)\n",
            "2 13 Epoch: 73 | ANN: trainLoss: 0.4567 | trainAcc: 79.1667% (152/192)\n",
            "3 13 Epoch: 73 | ANN: trainLoss: 0.4372 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 73 | ANN: trainLoss: 0.4146 | trainAcc: 82.1875% (263/320)\n",
            "5 13 Epoch: 73 | ANN: trainLoss: 0.4270 | trainAcc: 81.7708% (314/384)\n",
            "6 13 Epoch: 73 | ANN: trainLoss: 0.4197 | trainAcc: 81.4732% (365/448)\n",
            "7 13 Epoch: 73 | ANN: trainLoss: 0.4343 | trainAcc: 80.6641% (413/512)\n",
            "8 13 Epoch: 73 | ANN: trainLoss: 0.4230 | trainAcc: 81.2500% (468/576)\n",
            "9 13 Epoch: 73 | ANN: trainLoss: 0.4197 | trainAcc: 81.5625% (522/640)\n",
            "10 13 Epoch: 73 | ANN: trainLoss: 0.4097 | trainAcc: 81.9602% (577/704)\n",
            "11 13 Epoch: 73 | ANN: trainLoss: 0.4074 | trainAcc: 82.5521% (634/768)\n",
            "12 13 Epoch: 73 | ANN: trainLoss: 0.3961 | trainAcc: 82.6425% (638/772)\n",
            "0 4 Epoch: 73 | ANN: testLoss: 0.7682 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 73 | ANN: testLoss: 0.6804 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 73 | ANN: testLoss: 0.6722 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 73 | ANN: testLoss: 1.0676 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 74 | ANN: trainLoss: 0.3062 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 74 | ANN: trainLoss: 0.3129 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 74 | ANN: trainLoss: 0.3385 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 74 | ANN: trainLoss: 0.3534 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 74 | ANN: trainLoss: 0.3494 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 74 | ANN: trainLoss: 0.3562 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 74 | ANN: trainLoss: 0.3547 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 74 | ANN: trainLoss: 0.3542 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 74 | ANN: trainLoss: 0.3577 | trainAcc: 85.9375% (495/576)\n",
            "9 13 Epoch: 74 | ANN: trainLoss: 0.3687 | trainAcc: 85.0000% (544/640)\n",
            "10 13 Epoch: 74 | ANN: trainLoss: 0.3698 | trainAcc: 84.6591% (596/704)\n",
            "11 13 Epoch: 74 | ANN: trainLoss: 0.3807 | trainAcc: 84.1146% (646/768)\n",
            "12 13 Epoch: 74 | ANN: trainLoss: 0.4591 | trainAcc: 83.9378% (648/772)\n",
            "0 4 Epoch: 74 | ANN: testLoss: 0.6463 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 74 | ANN: testLoss: 0.7168 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 74 | ANN: testLoss: 0.6882 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 74 | ANN: testLoss: 0.5254 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 75 | ANN: trainLoss: 0.3716 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 75 | ANN: trainLoss: 0.3561 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 75 | ANN: trainLoss: 0.3817 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 75 | ANN: trainLoss: 0.4051 | trainAcc: 85.1562% (218/256)\n",
            "4 13 Epoch: 75 | ANN: trainLoss: 0.4130 | trainAcc: 83.4375% (267/320)\n",
            "5 13 Epoch: 75 | ANN: trainLoss: 0.3915 | trainAcc: 84.1146% (323/384)\n",
            "6 13 Epoch: 75 | ANN: trainLoss: 0.3874 | trainAcc: 84.5982% (379/448)\n",
            "7 13 Epoch: 75 | ANN: trainLoss: 0.3814 | trainAcc: 84.3750% (432/512)\n",
            "8 13 Epoch: 75 | ANN: trainLoss: 0.3804 | trainAcc: 84.5486% (487/576)\n",
            "9 13 Epoch: 75 | ANN: trainLoss: 0.3838 | trainAcc: 84.3750% (540/640)\n",
            "10 13 Epoch: 75 | ANN: trainLoss: 0.3807 | trainAcc: 84.2330% (593/704)\n",
            "11 13 Epoch: 75 | ANN: trainLoss: 0.3773 | trainAcc: 84.2448% (647/768)\n",
            "12 13 Epoch: 75 | ANN: trainLoss: 0.3902 | trainAcc: 84.1969% (650/772)\n",
            "0 4 Epoch: 75 | ANN: testLoss: 0.6863 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 75 | ANN: testLoss: 0.6422 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 75 | ANN: testLoss: 0.6720 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 75 | ANN: testLoss: 0.6396 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 76 | ANN: trainLoss: 0.3463 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 76 | ANN: trainLoss: 0.3528 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 76 | ANN: trainLoss: 0.3752 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 76 | ANN: trainLoss: 0.3821 | trainAcc: 83.2031% (213/256)\n",
            "4 13 Epoch: 76 | ANN: trainLoss: 0.3754 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 76 | ANN: trainLoss: 0.3650 | trainAcc: 84.3750% (324/384)\n",
            "6 13 Epoch: 76 | ANN: trainLoss: 0.3716 | trainAcc: 83.9286% (376/448)\n",
            "7 13 Epoch: 76 | ANN: trainLoss: 0.3847 | trainAcc: 83.3984% (427/512)\n",
            "8 13 Epoch: 76 | ANN: trainLoss: 0.3914 | trainAcc: 83.1597% (479/576)\n",
            "9 13 Epoch: 76 | ANN: trainLoss: 0.3929 | trainAcc: 83.2812% (533/640)\n",
            "10 13 Epoch: 76 | ANN: trainLoss: 0.3807 | trainAcc: 84.2330% (593/704)\n",
            "11 13 Epoch: 76 | ANN: trainLoss: 0.3742 | trainAcc: 84.6354% (650/768)\n",
            "12 13 Epoch: 76 | ANN: trainLoss: 0.3915 | trainAcc: 84.5855% (653/772)\n",
            "0 4 Epoch: 76 | ANN: testLoss: 0.7118 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 76 | ANN: testLoss: 0.6405 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 76 | ANN: testLoss: 0.6592 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 76 | ANN: testLoss: 0.8939 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 77 | ANN: trainLoss: 0.3839 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 77 | ANN: trainLoss: 0.3614 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 77 | ANN: trainLoss: 0.3390 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 77 | ANN: trainLoss: 0.3531 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 77 | ANN: trainLoss: 0.3508 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 77 | ANN: trainLoss: 0.3382 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 77 | ANN: trainLoss: 0.3443 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 77 | ANN: trainLoss: 0.3424 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 77 | ANN: trainLoss: 0.3536 | trainAcc: 87.1528% (502/576)\n",
            "9 13 Epoch: 77 | ANN: trainLoss: 0.3453 | trainAcc: 87.3438% (559/640)\n",
            "10 13 Epoch: 77 | ANN: trainLoss: 0.3407 | trainAcc: 87.9261% (619/704)\n",
            "11 13 Epoch: 77 | ANN: trainLoss: 0.3460 | trainAcc: 88.0208% (676/768)\n",
            "12 13 Epoch: 77 | ANN: trainLoss: 0.3759 | trainAcc: 87.8238% (678/772)\n",
            "0 4 Epoch: 77 | ANN: testLoss: 0.8525 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 77 | ANN: testLoss: 0.7306 | testAcc: 61.7188% (79/128)\n",
            "2 4 Epoch: 77 | ANN: testLoss: 0.6573 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 77 | ANN: testLoss: 0.8247 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 78 | ANN: trainLoss: 0.3303 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 78 | ANN: trainLoss: 0.3652 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 78 | ANN: trainLoss: 0.3593 | trainAcc: 83.3333% (160/192)\n",
            "3 13 Epoch: 78 | ANN: trainLoss: 0.3493 | trainAcc: 84.7656% (217/256)\n",
            "4 13 Epoch: 78 | ANN: trainLoss: 0.3447 | trainAcc: 85.3125% (273/320)\n",
            "5 13 Epoch: 78 | ANN: trainLoss: 0.3516 | trainAcc: 85.1562% (327/384)\n",
            "6 13 Epoch: 78 | ANN: trainLoss: 0.3550 | trainAcc: 85.2679% (382/448)\n",
            "7 13 Epoch: 78 | ANN: trainLoss: 0.3646 | trainAcc: 84.5703% (433/512)\n",
            "8 13 Epoch: 78 | ANN: trainLoss: 0.3842 | trainAcc: 83.5069% (481/576)\n",
            "9 13 Epoch: 78 | ANN: trainLoss: 0.3791 | trainAcc: 83.5938% (535/640)\n",
            "10 13 Epoch: 78 | ANN: trainLoss: 0.3869 | trainAcc: 83.6648% (589/704)\n",
            "11 13 Epoch: 78 | ANN: trainLoss: 0.3893 | trainAcc: 83.5938% (642/768)\n",
            "12 13 Epoch: 78 | ANN: trainLoss: 0.3650 | trainAcc: 83.6788% (646/772)\n",
            "0 4 Epoch: 78 | ANN: testLoss: 0.7476 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 78 | ANN: testLoss: 0.6938 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 78 | ANN: testLoss: 0.6654 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 78 | ANN: testLoss: 0.6786 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 79 | ANN: trainLoss: 0.2878 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 79 | ANN: trainLoss: 0.3017 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 79 | ANN: trainLoss: 0.3249 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 79 | ANN: trainLoss: 0.3423 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 79 | ANN: trainLoss: 0.3413 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 79 | ANN: trainLoss: 0.3566 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 79 | ANN: trainLoss: 0.3427 | trainAcc: 85.4911% (383/448)\n",
            "7 13 Epoch: 79 | ANN: trainLoss: 0.3465 | trainAcc: 85.9375% (440/512)\n",
            "8 13 Epoch: 79 | ANN: trainLoss: 0.3426 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 79 | ANN: trainLoss: 0.3389 | trainAcc: 86.8750% (556/640)\n",
            "10 13 Epoch: 79 | ANN: trainLoss: 0.3450 | trainAcc: 86.0795% (606/704)\n",
            "11 13 Epoch: 79 | ANN: trainLoss: 0.3439 | trainAcc: 86.3281% (663/768)\n",
            "12 13 Epoch: 79 | ANN: trainLoss: 0.3356 | trainAcc: 86.3990% (667/772)\n",
            "0 4 Epoch: 79 | ANN: testLoss: 0.6424 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 79 | ANN: testLoss: 0.6915 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 79 | ANN: testLoss: 0.6634 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 79 | ANN: testLoss: 0.7468 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 80 | ANN: trainLoss: 0.3603 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 80 | ANN: trainLoss: 0.3665 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 80 | ANN: trainLoss: 0.3789 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 80 | ANN: trainLoss: 0.3747 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 80 | ANN: trainLoss: 0.3773 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 80 | ANN: trainLoss: 0.3734 | trainAcc: 84.3750% (324/384)\n",
            "6 13 Epoch: 80 | ANN: trainLoss: 0.3642 | trainAcc: 84.5982% (379/448)\n",
            "7 13 Epoch: 80 | ANN: trainLoss: 0.3733 | trainAcc: 84.5703% (433/512)\n",
            "8 13 Epoch: 80 | ANN: trainLoss: 0.3754 | trainAcc: 84.3750% (486/576)\n",
            "9 13 Epoch: 80 | ANN: trainLoss: 0.3786 | trainAcc: 84.0625% (538/640)\n",
            "10 13 Epoch: 80 | ANN: trainLoss: 0.3743 | trainAcc: 84.5170% (595/704)\n",
            "11 13 Epoch: 80 | ANN: trainLoss: 0.3668 | trainAcc: 84.7656% (651/768)\n",
            "12 13 Epoch: 80 | ANN: trainLoss: 0.4055 | trainAcc: 84.7150% (654/772)\n",
            "0 4 Epoch: 80 | ANN: testLoss: 0.5926 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 80 | ANN: testLoss: 0.6395 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 80 | ANN: testLoss: 0.6583 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 80 | ANN: testLoss: 0.6094 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 81 | ANN: trainLoss: 0.3234 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 81 | ANN: trainLoss: 0.3510 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 81 | ANN: trainLoss: 0.3203 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 81 | ANN: trainLoss: 0.3889 | trainAcc: 82.8125% (212/256)\n",
            "4 13 Epoch: 81 | ANN: trainLoss: 0.3953 | trainAcc: 82.1875% (263/320)\n",
            "5 13 Epoch: 81 | ANN: trainLoss: 0.3902 | trainAcc: 81.7708% (314/384)\n",
            "6 13 Epoch: 81 | ANN: trainLoss: 0.3781 | trainAcc: 82.3661% (369/448)\n",
            "7 13 Epoch: 81 | ANN: trainLoss: 0.3788 | trainAcc: 82.6172% (423/512)\n",
            "8 13 Epoch: 81 | ANN: trainLoss: 0.3757 | trainAcc: 83.1597% (479/576)\n",
            "9 13 Epoch: 81 | ANN: trainLoss: 0.3902 | trainAcc: 82.6562% (529/640)\n",
            "10 13 Epoch: 81 | ANN: trainLoss: 0.3891 | trainAcc: 82.6705% (582/704)\n",
            "11 13 Epoch: 81 | ANN: trainLoss: 0.3850 | trainAcc: 83.0729% (638/768)\n",
            "12 13 Epoch: 81 | ANN: trainLoss: 0.4378 | trainAcc: 82.9016% (640/772)\n",
            "0 4 Epoch: 81 | ANN: testLoss: 0.6201 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 81 | ANN: testLoss: 0.6492 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 81 | ANN: testLoss: 0.6548 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 81 | ANN: testLoss: 0.5665 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 82 | ANN: trainLoss: 0.4367 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 82 | ANN: trainLoss: 0.4474 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 82 | ANN: trainLoss: 0.4357 | trainAcc: 82.8125% (159/192)\n",
            "3 13 Epoch: 82 | ANN: trainLoss: 0.4068 | trainAcc: 83.5938% (214/256)\n",
            "4 13 Epoch: 82 | ANN: trainLoss: 0.4105 | trainAcc: 82.8125% (265/320)\n",
            "5 13 Epoch: 82 | ANN: trainLoss: 0.3860 | trainAcc: 83.8542% (322/384)\n",
            "6 13 Epoch: 82 | ANN: trainLoss: 0.3989 | trainAcc: 82.5893% (370/448)\n",
            "7 13 Epoch: 82 | ANN: trainLoss: 0.3931 | trainAcc: 83.2031% (426/512)\n",
            "8 13 Epoch: 82 | ANN: trainLoss: 0.3880 | trainAcc: 83.3333% (480/576)\n",
            "9 13 Epoch: 82 | ANN: trainLoss: 0.3870 | trainAcc: 83.2812% (533/640)\n",
            "10 13 Epoch: 82 | ANN: trainLoss: 0.3859 | trainAcc: 83.0966% (585/704)\n",
            "11 13 Epoch: 82 | ANN: trainLoss: 0.3827 | trainAcc: 83.3333% (640/768)\n",
            "12 13 Epoch: 82 | ANN: trainLoss: 0.3742 | trainAcc: 83.4197% (644/772)\n",
            "0 4 Epoch: 82 | ANN: testLoss: 0.6453 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 82 | ANN: testLoss: 0.7036 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 82 | ANN: testLoss: 0.6564 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 82 | ANN: testLoss: 0.6015 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 83 | ANN: trainLoss: 0.3876 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 83 | ANN: trainLoss: 0.4008 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 83 | ANN: trainLoss: 0.3965 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 83 | ANN: trainLoss: 0.4018 | trainAcc: 83.2031% (213/256)\n",
            "4 13 Epoch: 83 | ANN: trainLoss: 0.3900 | trainAcc: 84.3750% (270/320)\n",
            "5 13 Epoch: 83 | ANN: trainLoss: 0.3868 | trainAcc: 83.8542% (322/384)\n",
            "6 13 Epoch: 83 | ANN: trainLoss: 0.3944 | trainAcc: 83.4821% (374/448)\n",
            "7 13 Epoch: 83 | ANN: trainLoss: 0.3965 | trainAcc: 83.0078% (425/512)\n",
            "8 13 Epoch: 83 | ANN: trainLoss: 0.3948 | trainAcc: 82.8125% (477/576)\n",
            "9 13 Epoch: 83 | ANN: trainLoss: 0.4025 | trainAcc: 83.5938% (535/640)\n",
            "10 13 Epoch: 83 | ANN: trainLoss: 0.4031 | trainAcc: 83.5227% (588/704)\n",
            "11 13 Epoch: 83 | ANN: trainLoss: 0.4090 | trainAcc: 83.5938% (642/768)\n",
            "12 13 Epoch: 83 | ANN: trainLoss: 0.4042 | trainAcc: 83.5492% (645/772)\n",
            "0 4 Epoch: 83 | ANN: testLoss: 0.6252 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 83 | ANN: testLoss: 0.6453 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 83 | ANN: testLoss: 0.6620 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 83 | ANN: testLoss: 0.5422 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 84 | ANN: trainLoss: 0.3548 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 84 | ANN: trainLoss: 0.3356 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 84 | ANN: trainLoss: 0.3601 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 84 | ANN: trainLoss: 0.3455 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 84 | ANN: trainLoss: 0.3412 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 84 | ANN: trainLoss: 0.3615 | trainAcc: 85.6771% (329/384)\n",
            "6 13 Epoch: 84 | ANN: trainLoss: 0.3539 | trainAcc: 85.7143% (384/448)\n",
            "7 13 Epoch: 84 | ANN: trainLoss: 0.3456 | trainAcc: 85.7422% (439/512)\n",
            "8 13 Epoch: 84 | ANN: trainLoss: 0.3580 | trainAcc: 85.2431% (491/576)\n",
            "9 13 Epoch: 84 | ANN: trainLoss: 0.3672 | trainAcc: 84.8438% (543/640)\n",
            "10 13 Epoch: 84 | ANN: trainLoss: 0.3702 | trainAcc: 84.6591% (596/704)\n",
            "11 13 Epoch: 84 | ANN: trainLoss: 0.3723 | trainAcc: 84.1146% (646/768)\n",
            "12 13 Epoch: 84 | ANN: trainLoss: 0.3635 | trainAcc: 84.1969% (650/772)\n",
            "0 4 Epoch: 84 | ANN: testLoss: 0.6953 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 84 | ANN: testLoss: 0.6565 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 84 | ANN: testLoss: 0.6570 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 84 | ANN: testLoss: 1.0594 | testAcc: 65.2850% (126/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 85 | ANN: trainLoss: 0.4972 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 85 | ANN: trainLoss: 0.4133 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 85 | ANN: trainLoss: 0.4097 | trainAcc: 81.7708% (157/192)\n",
            "3 13 Epoch: 85 | ANN: trainLoss: 0.4131 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 85 | ANN: trainLoss: 0.3828 | trainAcc: 83.1250% (266/320)\n",
            "5 13 Epoch: 85 | ANN: trainLoss: 0.3629 | trainAcc: 84.6354% (325/384)\n",
            "6 13 Epoch: 85 | ANN: trainLoss: 0.3742 | trainAcc: 84.8214% (380/448)\n",
            "7 13 Epoch: 85 | ANN: trainLoss: 0.3718 | trainAcc: 84.3750% (432/512)\n",
            "8 13 Epoch: 85 | ANN: trainLoss: 0.3634 | trainAcc: 85.0694% (490/576)\n",
            "9 13 Epoch: 85 | ANN: trainLoss: 0.3668 | trainAcc: 85.1562% (545/640)\n",
            "10 13 Epoch: 85 | ANN: trainLoss: 0.3584 | trainAcc: 85.5114% (602/704)\n",
            "11 13 Epoch: 85 | ANN: trainLoss: 0.3618 | trainAcc: 85.1562% (654/768)\n",
            "12 13 Epoch: 85 | ANN: trainLoss: 0.3522 | trainAcc: 85.2332% (658/772)\n",
            "0 4 Epoch: 85 | ANN: testLoss: 0.7810 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 85 | ANN: testLoss: 0.6974 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 85 | ANN: testLoss: 0.6909 | testAcc: 65.1042% (125/192)\n",
            "3 4 Epoch: 85 | ANN: testLoss: 0.8923 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 86 | ANN: trainLoss: 0.3233 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 86 | ANN: trainLoss: 0.3018 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 86 | ANN: trainLoss: 0.2946 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 86 | ANN: trainLoss: 0.3095 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 86 | ANN: trainLoss: 0.3229 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 86 | ANN: trainLoss: 0.3305 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 86 | ANN: trainLoss: 0.3247 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 86 | ANN: trainLoss: 0.3295 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 86 | ANN: trainLoss: 0.3207 | trainAcc: 86.8056% (500/576)\n",
            "9 13 Epoch: 86 | ANN: trainLoss: 0.3324 | trainAcc: 86.2500% (552/640)\n",
            "10 13 Epoch: 86 | ANN: trainLoss: 0.3298 | trainAcc: 86.0795% (606/704)\n",
            "11 13 Epoch: 86 | ANN: trainLoss: 0.3307 | trainAcc: 86.1979% (662/768)\n",
            "12 13 Epoch: 86 | ANN: trainLoss: 0.4321 | trainAcc: 86.1399% (665/772)\n",
            "0 4 Epoch: 86 | ANN: testLoss: 0.7682 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 86 | ANN: testLoss: 0.7426 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 86 | ANN: testLoss: 0.6623 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 86 | ANN: testLoss: 0.4994 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 87 | ANN: trainLoss: 0.2835 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 87 | ANN: trainLoss: 0.3138 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 87 | ANN: trainLoss: 0.3415 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 87 | ANN: trainLoss: 0.3572 | trainAcc: 84.7656% (217/256)\n",
            "4 13 Epoch: 87 | ANN: trainLoss: 0.3614 | trainAcc: 85.3125% (273/320)\n",
            "5 13 Epoch: 87 | ANN: trainLoss: 0.3558 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 87 | ANN: trainLoss: 0.3577 | trainAcc: 85.7143% (384/448)\n",
            "7 13 Epoch: 87 | ANN: trainLoss: 0.3468 | trainAcc: 86.3281% (442/512)\n",
            "8 13 Epoch: 87 | ANN: trainLoss: 0.3552 | trainAcc: 85.2431% (491/576)\n",
            "9 13 Epoch: 87 | ANN: trainLoss: 0.3581 | trainAcc: 85.3125% (546/640)\n",
            "10 13 Epoch: 87 | ANN: trainLoss: 0.3613 | trainAcc: 85.3693% (601/704)\n",
            "11 13 Epoch: 87 | ANN: trainLoss: 0.3515 | trainAcc: 86.0677% (661/768)\n",
            "12 13 Epoch: 87 | ANN: trainLoss: 0.3324 | trainAcc: 86.1399% (665/772)\n",
            "0 4 Epoch: 87 | ANN: testLoss: 0.6487 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 87 | ANN: testLoss: 0.6467 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 87 | ANN: testLoss: 0.6516 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 87 | ANN: testLoss: 1.0807 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 88 | ANN: trainLoss: 0.2422 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 88 | ANN: trainLoss: 0.3132 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 88 | ANN: trainLoss: 0.3141 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 88 | ANN: trainLoss: 0.3193 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 88 | ANN: trainLoss: 0.3224 | trainAcc: 85.3125% (273/320)\n",
            "5 13 Epoch: 88 | ANN: trainLoss: 0.3471 | trainAcc: 83.3333% (320/384)\n",
            "6 13 Epoch: 88 | ANN: trainLoss: 0.3585 | trainAcc: 83.0357% (372/448)\n",
            "7 13 Epoch: 88 | ANN: trainLoss: 0.3589 | trainAcc: 83.5938% (428/512)\n",
            "8 13 Epoch: 88 | ANN: trainLoss: 0.3521 | trainAcc: 84.3750% (486/576)\n",
            "9 13 Epoch: 88 | ANN: trainLoss: 0.3575 | trainAcc: 83.9062% (537/640)\n",
            "10 13 Epoch: 88 | ANN: trainLoss: 0.3562 | trainAcc: 84.2330% (593/704)\n",
            "11 13 Epoch: 88 | ANN: trainLoss: 0.3608 | trainAcc: 84.2448% (647/768)\n",
            "12 13 Epoch: 88 | ANN: trainLoss: 0.4423 | trainAcc: 84.0674% (649/772)\n",
            "0 4 Epoch: 88 | ANN: testLoss: 0.5778 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 88 | ANN: testLoss: 0.5780 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 88 | ANN: testLoss: 0.6552 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 88 | ANN: testLoss: 0.4972 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 89 | ANN: trainLoss: 0.3156 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 89 | ANN: trainLoss: 0.3072 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 89 | ANN: trainLoss: 0.3394 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 89 | ANN: trainLoss: 0.3384 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 89 | ANN: trainLoss: 0.3344 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 89 | ANN: trainLoss: 0.3431 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 89 | ANN: trainLoss: 0.3402 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 89 | ANN: trainLoss: 0.3479 | trainAcc: 85.9375% (440/512)\n",
            "8 13 Epoch: 89 | ANN: trainLoss: 0.3433 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 89 | ANN: trainLoss: 0.3493 | trainAcc: 85.6250% (548/640)\n",
            "10 13 Epoch: 89 | ANN: trainLoss: 0.3446 | trainAcc: 85.7955% (604/704)\n",
            "11 13 Epoch: 89 | ANN: trainLoss: 0.3451 | trainAcc: 86.0677% (661/768)\n",
            "12 13 Epoch: 89 | ANN: trainLoss: 0.3518 | trainAcc: 86.0104% (664/772)\n",
            "0 4 Epoch: 89 | ANN: testLoss: 0.6412 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 89 | ANN: testLoss: 0.6566 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 89 | ANN: testLoss: 0.6578 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 89 | ANN: testLoss: 0.5157 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 90 | ANN: trainLoss: 0.3257 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 90 | ANN: trainLoss: 0.3135 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 90 | ANN: trainLoss: 0.3450 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 90 | ANN: trainLoss: 0.3529 | trainAcc: 82.8125% (212/256)\n",
            "4 13 Epoch: 90 | ANN: trainLoss: 0.3409 | trainAcc: 83.7500% (268/320)\n",
            "5 13 Epoch: 90 | ANN: trainLoss: 0.3507 | trainAcc: 84.1146% (323/384)\n",
            "6 13 Epoch: 90 | ANN: trainLoss: 0.3576 | trainAcc: 84.1518% (377/448)\n",
            "7 13 Epoch: 90 | ANN: trainLoss: 0.3514 | trainAcc: 84.9609% (435/512)\n",
            "8 13 Epoch: 90 | ANN: trainLoss: 0.3554 | trainAcc: 84.7222% (488/576)\n",
            "9 13 Epoch: 90 | ANN: trainLoss: 0.3553 | trainAcc: 85.0000% (544/640)\n",
            "10 13 Epoch: 90 | ANN: trainLoss: 0.3482 | trainAcc: 85.7955% (604/704)\n",
            "11 13 Epoch: 90 | ANN: trainLoss: 0.3417 | trainAcc: 86.0677% (661/768)\n",
            "12 13 Epoch: 90 | ANN: trainLoss: 0.3254 | trainAcc: 86.1399% (665/772)\n",
            "0 4 Epoch: 90 | ANN: testLoss: 0.6206 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 90 | ANN: testLoss: 0.6733 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 90 | ANN: testLoss: 0.6578 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 90 | ANN: testLoss: 0.4936 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 91 | ANN: trainLoss: 0.3122 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 91 | ANN: trainLoss: 0.3707 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 91 | ANN: trainLoss: 0.3579 | trainAcc: 82.8125% (159/192)\n",
            "3 13 Epoch: 91 | ANN: trainLoss: 0.3524 | trainAcc: 83.2031% (213/256)\n",
            "4 13 Epoch: 91 | ANN: trainLoss: 0.3543 | trainAcc: 83.4375% (267/320)\n",
            "5 13 Epoch: 91 | ANN: trainLoss: 0.3506 | trainAcc: 84.1146% (323/384)\n",
            "6 13 Epoch: 91 | ANN: trainLoss: 0.3589 | trainAcc: 83.7054% (375/448)\n",
            "7 13 Epoch: 91 | ANN: trainLoss: 0.3596 | trainAcc: 84.1797% (431/512)\n",
            "8 13 Epoch: 91 | ANN: trainLoss: 0.3591 | trainAcc: 84.0278% (484/576)\n",
            "9 13 Epoch: 91 | ANN: trainLoss: 0.3675 | trainAcc: 83.2812% (533/640)\n",
            "10 13 Epoch: 91 | ANN: trainLoss: 0.3746 | trainAcc: 83.3807% (587/704)\n",
            "11 13 Epoch: 91 | ANN: trainLoss: 0.3749 | trainAcc: 83.4635% (641/768)\n",
            "12 13 Epoch: 91 | ANN: trainLoss: 0.4142 | trainAcc: 83.2902% (643/772)\n",
            "0 4 Epoch: 91 | ANN: testLoss: 0.6456 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 91 | ANN: testLoss: 0.6612 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 91 | ANN: testLoss: 0.6549 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 91 | ANN: testLoss: 0.5594 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 92 | ANN: trainLoss: 0.3063 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 92 | ANN: trainLoss: 0.3020 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 92 | ANN: trainLoss: 0.3299 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 92 | ANN: trainLoss: 0.3606 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 92 | ANN: trainLoss: 0.3622 | trainAcc: 85.0000% (272/320)\n",
            "5 13 Epoch: 92 | ANN: trainLoss: 0.3521 | trainAcc: 84.6354% (325/384)\n",
            "6 13 Epoch: 92 | ANN: trainLoss: 0.3611 | trainAcc: 84.5982% (379/448)\n",
            "7 13 Epoch: 92 | ANN: trainLoss: 0.3504 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 92 | ANN: trainLoss: 0.3530 | trainAcc: 85.4167% (492/576)\n",
            "9 13 Epoch: 92 | ANN: trainLoss: 0.3442 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 92 | ANN: trainLoss: 0.3493 | trainAcc: 85.7955% (604/704)\n",
            "11 13 Epoch: 92 | ANN: trainLoss: 0.3456 | trainAcc: 85.6771% (658/768)\n",
            "12 13 Epoch: 92 | ANN: trainLoss: 0.4236 | trainAcc: 85.6218% (661/772)\n",
            "0 4 Epoch: 92 | ANN: testLoss: 0.5519 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 92 | ANN: testLoss: 0.6301 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 92 | ANN: testLoss: 0.6691 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 92 | ANN: testLoss: 0.5147 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 93 | ANN: trainLoss: 0.3013 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 93 | ANN: trainLoss: 0.2924 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 93 | ANN: trainLoss: 0.3124 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 93 | ANN: trainLoss: 0.3231 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 93 | ANN: trainLoss: 0.3722 | trainAcc: 81.8750% (262/320)\n",
            "5 13 Epoch: 93 | ANN: trainLoss: 0.3749 | trainAcc: 81.7708% (314/384)\n",
            "6 13 Epoch: 93 | ANN: trainLoss: 0.3752 | trainAcc: 81.6964% (366/448)\n",
            "7 13 Epoch: 93 | ANN: trainLoss: 0.3837 | trainAcc: 82.0312% (420/512)\n",
            "8 13 Epoch: 93 | ANN: trainLoss: 0.3975 | trainAcc: 81.5972% (470/576)\n",
            "9 13 Epoch: 93 | ANN: trainLoss: 0.3894 | trainAcc: 82.1875% (526/640)\n",
            "10 13 Epoch: 93 | ANN: trainLoss: 0.3849 | trainAcc: 82.6705% (582/704)\n",
            "11 13 Epoch: 93 | ANN: trainLoss: 0.3868 | trainAcc: 82.4219% (633/768)\n",
            "12 13 Epoch: 93 | ANN: trainLoss: 0.3904 | trainAcc: 82.2539% (635/772)\n",
            "0 4 Epoch: 93 | ANN: testLoss: 0.5831 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 93 | ANN: testLoss: 0.6782 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 93 | ANN: testLoss: 0.6734 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 93 | ANN: testLoss: 0.6028 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 94 | ANN: trainLoss: 0.3673 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 94 | ANN: trainLoss: 0.3470 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 94 | ANN: trainLoss: 0.3351 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 94 | ANN: trainLoss: 0.3567 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 94 | ANN: trainLoss: 0.3527 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 94 | ANN: trainLoss: 0.3625 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 94 | ANN: trainLoss: 0.3665 | trainAcc: 85.0446% (381/448)\n",
            "7 13 Epoch: 94 | ANN: trainLoss: 0.3501 | trainAcc: 86.1328% (441/512)\n",
            "8 13 Epoch: 94 | ANN: trainLoss: 0.3358 | trainAcc: 86.6319% (499/576)\n",
            "9 13 Epoch: 94 | ANN: trainLoss: 0.3499 | trainAcc: 85.1562% (545/640)\n",
            "10 13 Epoch: 94 | ANN: trainLoss: 0.3456 | trainAcc: 85.5114% (602/704)\n",
            "11 13 Epoch: 94 | ANN: trainLoss: 0.3451 | trainAcc: 85.4167% (656/768)\n",
            "12 13 Epoch: 94 | ANN: trainLoss: 0.3803 | trainAcc: 85.3627% (659/772)\n",
            "0 4 Epoch: 94 | ANN: testLoss: 0.6031 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 94 | ANN: testLoss: 0.6364 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 94 | ANN: testLoss: 0.6552 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 94 | ANN: testLoss: 0.4954 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 95 | ANN: trainLoss: 0.3083 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 95 | ANN: trainLoss: 0.3455 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 95 | ANN: trainLoss: 0.3116 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 95 | ANN: trainLoss: 0.3047 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 95 | ANN: trainLoss: 0.3198 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 95 | ANN: trainLoss: 0.3133 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 95 | ANN: trainLoss: 0.3156 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 95 | ANN: trainLoss: 0.3258 | trainAcc: 87.6953% (449/512)\n",
            "8 13 Epoch: 95 | ANN: trainLoss: 0.3182 | trainAcc: 88.0208% (507/576)\n",
            "9 13 Epoch: 95 | ANN: trainLoss: 0.3230 | trainAcc: 87.3438% (559/640)\n",
            "10 13 Epoch: 95 | ANN: trainLoss: 0.3355 | trainAcc: 86.3636% (608/704)\n",
            "11 13 Epoch: 95 | ANN: trainLoss: 0.3310 | trainAcc: 86.4583% (664/768)\n",
            "12 13 Epoch: 95 | ANN: trainLoss: 0.3333 | trainAcc: 86.3990% (667/772)\n",
            "0 4 Epoch: 95 | ANN: testLoss: 0.7376 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 95 | ANN: testLoss: 0.6702 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 95 | ANN: testLoss: 0.6586 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 95 | ANN: testLoss: 0.5910 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 96 | ANN: trainLoss: 0.3751 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 96 | ANN: trainLoss: 0.2986 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 96 | ANN: trainLoss: 0.3155 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 96 | ANN: trainLoss: 0.3283 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 96 | ANN: trainLoss: 0.3176 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 96 | ANN: trainLoss: 0.3181 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 96 | ANN: trainLoss: 0.3189 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 96 | ANN: trainLoss: 0.3159 | trainAcc: 87.6953% (449/512)\n",
            "8 13 Epoch: 96 | ANN: trainLoss: 0.3191 | trainAcc: 87.5000% (504/576)\n",
            "9 13 Epoch: 96 | ANN: trainLoss: 0.3333 | trainAcc: 86.7188% (555/640)\n",
            "10 13 Epoch: 96 | ANN: trainLoss: 0.3372 | trainAcc: 86.7898% (611/704)\n",
            "11 13 Epoch: 96 | ANN: trainLoss: 0.3322 | trainAcc: 86.8490% (667/768)\n",
            "12 13 Epoch: 96 | ANN: trainLoss: 0.3423 | trainAcc: 86.7876% (670/772)\n",
            "0 4 Epoch: 96 | ANN: testLoss: 0.7356 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 96 | ANN: testLoss: 0.7040 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 96 | ANN: testLoss: 0.6697 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 96 | ANN: testLoss: 0.8067 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 97 | ANN: trainLoss: 0.2701 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 97 | ANN: trainLoss: 0.3169 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 97 | ANN: trainLoss: 0.3600 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 97 | ANN: trainLoss: 0.3580 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 97 | ANN: trainLoss: 0.3862 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 97 | ANN: trainLoss: 0.3673 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 97 | ANN: trainLoss: 0.3671 | trainAcc: 85.4911% (383/448)\n",
            "7 13 Epoch: 97 | ANN: trainLoss: 0.3628 | trainAcc: 85.5469% (438/512)\n",
            "8 13 Epoch: 97 | ANN: trainLoss: 0.3549 | trainAcc: 85.9375% (495/576)\n",
            "9 13 Epoch: 97 | ANN: trainLoss: 0.3521 | trainAcc: 86.4062% (553/640)\n",
            "10 13 Epoch: 97 | ANN: trainLoss: 0.3541 | trainAcc: 86.2216% (607/704)\n",
            "11 13 Epoch: 97 | ANN: trainLoss: 0.3547 | trainAcc: 86.3281% (663/768)\n",
            "12 13 Epoch: 97 | ANN: trainLoss: 0.3644 | trainAcc: 86.2694% (666/772)\n",
            "0 4 Epoch: 97 | ANN: testLoss: 0.7255 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 97 | ANN: testLoss: 0.6629 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 97 | ANN: testLoss: 0.6702 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 97 | ANN: testLoss: 0.6593 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 98 | ANN: trainLoss: 0.3110 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 98 | ANN: trainLoss: 0.2893 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 98 | ANN: trainLoss: 0.3166 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 98 | ANN: trainLoss: 0.3376 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 98 | ANN: trainLoss: 0.3461 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 98 | ANN: trainLoss: 0.3344 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 98 | ANN: trainLoss: 0.3298 | trainAcc: 85.2679% (382/448)\n",
            "7 13 Epoch: 98 | ANN: trainLoss: 0.3319 | trainAcc: 85.5469% (438/512)\n",
            "8 13 Epoch: 98 | ANN: trainLoss: 0.3280 | trainAcc: 85.9375% (495/576)\n",
            "9 13 Epoch: 98 | ANN: trainLoss: 0.3335 | trainAcc: 85.9375% (550/640)\n",
            "10 13 Epoch: 98 | ANN: trainLoss: 0.3420 | trainAcc: 85.6534% (603/704)\n",
            "11 13 Epoch: 98 | ANN: trainLoss: 0.3391 | trainAcc: 86.0677% (661/768)\n",
            "12 13 Epoch: 98 | ANN: trainLoss: 0.3715 | trainAcc: 85.8808% (663/772)\n",
            "0 4 Epoch: 98 | ANN: testLoss: 0.7194 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 98 | ANN: testLoss: 0.7356 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 98 | ANN: testLoss: 0.6617 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 98 | ANN: testLoss: 0.5690 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 99 | ANN: trainLoss: 0.5293 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 99 | ANN: trainLoss: 0.3901 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 99 | ANN: trainLoss: 0.3525 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 99 | ANN: trainLoss: 0.3349 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 99 | ANN: trainLoss: 0.3300 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 99 | ANN: trainLoss: 0.3302 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 99 | ANN: trainLoss: 0.3342 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 99 | ANN: trainLoss: 0.3397 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 99 | ANN: trainLoss: 0.3430 | trainAcc: 86.6319% (499/576)\n",
            "9 13 Epoch: 99 | ANN: trainLoss: 0.3389 | trainAcc: 86.7188% (555/640)\n",
            "10 13 Epoch: 99 | ANN: trainLoss: 0.3446 | trainAcc: 86.3636% (608/704)\n",
            "11 13 Epoch: 99 | ANN: trainLoss: 0.3475 | trainAcc: 86.1979% (662/768)\n",
            "12 13 Epoch: 99 | ANN: trainLoss: 0.3563 | trainAcc: 86.1399% (665/772)\n",
            "0 4 Epoch: 99 | ANN: testLoss: 0.5829 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 99 | ANN: testLoss: 0.7017 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 99 | ANN: testLoss: 0.6743 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 99 | ANN: testLoss: 0.5089 | testAcc: 66.8394% (129/193)\n",
            "---------------------------------------------\n",
            "Converting using MaxNorm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 395.18it/s]\n",
            "100%|██████████| 13/13 [00:00<00:00, 450.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ANN accuracy: Test: 66.8400%\n",
            "SNN accuracy: max_norm: 67.3575%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7o4VT4r0r-xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "outputId": "31f474ca-32c1-4197-d374-a427f7541a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions for all models...\n",
            "Evaluating LENet CNN...\n",
            "Evaluating LENet SNN...\n",
            "Evaluating LENet_FCL CNN...\n",
            "Evaluating LENet_FCL SNN...\n",
            "All predictions generated.\n",
            "LENet CNN|LENet SNN|LENet_FCL CNN|LENet_FCL SNN|"
          ]
        }
      ],
      "source": [
        "# @title Evaluate models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib # For colormaps access\n",
        "\n",
        "# Ensure all necessary variables from previous cells are available:\n",
        "# cnn_model_lenet, snn_model_lenet, cnn_model_lenet_fcl, snn_model_lenet_fcl\n",
        "# test_data, test_label (from the last train_test_split)\n",
        "# BATCH_SIZE, TIME_STEPS, device\n",
        "# data_loader function\n",
        "\n",
        "# 0. Prepare data loader for evaluation (using test_data from the last split)\n",
        "eval_test_loader = data_loader(test_data, test_label, batch=BATCH_SIZE, shuffle=False, drop=False)\n",
        "\n",
        "true_labels_list = []\n",
        "for _, targets_batch in eval_test_loader:\n",
        "    true_labels_list.extend(targets_batch.cpu().numpy())\n",
        "true_labels_np = np.array(true_labels_list)\n",
        "\n",
        "# --- 1. Get predictions for all models ---\n",
        "print(\"Generating predictions for all models...\")\n",
        "all_model_predictions = {}\n",
        "model_objects = {\n",
        "    \"LENet CNN\": cnn_model_lenet,\n",
        "    \"LENet SNN\": snn_model_lenet,\n",
        "    \"LENet_FCL CNN\": cnn_model_lenet_fcl,\n",
        "    \"LENet_FCL SNN\": snn_model_lenet_fcl\n",
        "}\n",
        "\n",
        "for model_name, model_obj in model_objects.items():\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    model_obj.eval().to(device)\n",
        "    current_preds = []\n",
        "    is_snn = \"SNN\" in model_name\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in eval_test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            if is_snn:\n",
        "                for m_module in model_obj.modules():\n",
        "                    if hasattr(m_module, 'reset'):\n",
        "                        m_module.reset()\n",
        "                accumulated_outputs = None\n",
        "                for t in range(TIME_STEPS):\n",
        "                    outputs_t = model_obj(inputs)\n",
        "                    if accumulated_outputs is None:\n",
        "                        accumulated_outputs = outputs_t.clone()\n",
        "                    else:\n",
        "                        accumulated_outputs += outputs_t\n",
        "                _, predicted = accumulated_outputs.max(1)\n",
        "            else: # ANN\n",
        "                outputs = model_obj(inputs)\n",
        "                _, predicted = outputs.max(1)\n",
        "            current_preds.extend(predicted.cpu().numpy())\n",
        "    all_model_predictions[model_name] = current_preds\n",
        "print(\"All predictions generated.\")\n",
        "\n",
        "# --- 2. Calculate Accuracies for all models ---\n",
        "accuracy_results = {}\n",
        "class_names = ['Rest', 'Elbow', 'Hand'] # Corresponds to labels 0, 1, 2\n",
        "\n",
        "for model_name, predictions in all_model_predictions.items():\n",
        "    overall_acc = accuracy_score(true_labels_np, predictions)\n",
        "    accuracy_results[model_name] = {\"Overall\": overall_acc}\n",
        "    for class_idx, class_name_key in enumerate(class_names):\n",
        "        class_indices = np.where(true_labels_np == class_idx)[0]\n",
        "        if len(class_indices) > 0:\n",
        "            class_true = true_labels_np[class_indices]\n",
        "            class_pred = np.array(predictions)[class_indices]\n",
        "            class_acc = accuracy_score(class_true, class_pred)\n",
        "            accuracy_results[model_name][class_name_key] = class_acc\n",
        "        else:\n",
        "            accuracy_results[model_name][class_name_key] = np.nan\n",
        "for model_name in all_model_predictions:\n",
        "  print(model_name, end=\"|\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title visualize results (Custom confusion matrix, green good, red bad)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib # For colormaps access\n",
        "\n",
        "# Ensure all necessary variables from previous cells are available:\n",
        "# cnn_model_lenet, snn_model_lenet, cnn_model_lenet_fcl, snn_model_lenet_fcl\n",
        "# test_data, test_label (from the last train_test_split)\n",
        "# EPOCHS, DROP_OUT, TEST_SIZE, BATCH_SIZE, TIME_STEPS, device (from training blocks)\n",
        "# channel_count, num_classes, data_length (from data loading cell)\n",
        "# all_data, all_label (from data loading cell, for full dataset stats)\n",
        "# all_model_predictions, accuracy_results, true_labels_np (from previous evaluation steps)\n",
        "# data_loader function (defined in a previous cell)\n",
        "\n",
        "# --- 0. Report Header: Hyperparameters and Data Specifications (Table Format) ---\n",
        "print(\"--- Experiment Configuration & Data Specifications ---\")\n",
        "\n",
        "# Collect all parameters\n",
        "config_params = {\n",
        "    \"Dataset File\": \"sub-011_eeg.mat\",\n",
        "    \"EEG Channels\": channel_count if 'channel_count' in locals() else \"N/A\",\n",
        "    \"Data Shape (Full Dataset)\": (\n",
        "        f\"Samples: {all_data.shape[0]}, Channels: {all_data.shape[1]}, Length: {all_data.shape[2]}\"\n",
        "        if 'all_data' in locals() and hasattr(all_data, 'shape') and len(all_data.shape) == 3\n",
        "        else \"N/A\"),\n",
        "    \"Class Distribution (Full Dataset)\": (\n",
        "        f\"Rest: {np.sum(all_label == 0)}, \"\n",
        "        f\"Elbow: {np.sum(all_label == 1)}, \"\n",
        "        f\"Hand: {np.sum(all_label == 2)}\"\n",
        "    ) if 'all_label' in locals() and isinstance(all_label, np.ndarray) and all_label.ndim >=1 else \"N/A\",\n",
        "    \"Bandpass Filter (Reported)\": \"7-35Hz\", #Change in case other filtering technique was used\n",
        "    \"Sampling Ratio (Reported)\": \"90Hz\", #Change in case other sampling was used\n",
        "    \"TRAINING EPOCHS\": EPOCHS if 'EPOCHS' in locals() else \"N/A\",\n",
        "    \"Dropout\": DROP_OUT if 'DROP_OUT' in locals() else \"N/A\",\n",
        "    \"Test Split\": TEST_SIZE if 'TEST_SIZE' in locals() else \"N/A\",\n",
        "    \"Batch Size (Evaluation)\": BATCH_SIZE if 'BATCH_SIZE' in locals() else \"N/A\",\n",
        "    \"SNN Time Steps\": TIME_STEPS if 'TIME_STEPS' in locals() else \"N/A\"\n",
        "}\n",
        "\n",
        "# Determine column widths\n",
        "max_key_len = max(len(key) for key in config_params.keys())\n",
        "# Ensure values are strings for len() calculation\n",
        "str_config_values = [str(val) for val in config_params.values()]\n",
        "max_val_len = max(len(val) for val in str_config_values)\n",
        "\n",
        "key_col_width = max_key_len + 2 # Add some padding\n",
        "val_col_width = max_val_len + 2\n",
        "\n",
        "# Print table header\n",
        "print(\"+\" + \"-\" * (key_col_width) + \"+\" + \"-\" * (val_col_width) + \"+\")\n",
        "print(f\"| {'Parameter':<{key_col_width-1}} | {'Value':<{val_col_width-1}} |\")\n",
        "print(\"+\" + \"=\" * (key_col_width) + \"+\" + \"=\" * (val_col_width) + \"+\")\n",
        "\n",
        "# Print table rows\n",
        "for idx, (key, value) in enumerate(config_params.items()):\n",
        "    print(f\"| {key:<{key_col_width-1}} | {str(value):<{val_col_width-1}} |\")\n",
        "\n",
        "print(\"+\" + \"-\" * (key_col_width) + \"+\" + \"-\" * (val_col_width) + \"+\")\n",
        "print(\"\\n\") # Add a bit of space before the main output\n",
        "\n",
        "# --- 1. Plot Confusion Matrices and Print Descriptions ---\n",
        "# This section assumes all_model_predictions, true_labels_np, accuracy_results are pre-computed\n",
        "# from previous cells if this cell is run in isolation.\n",
        "print(\"\\n--- Confusion Matrices and Descriptions ---\")\n",
        "class_names_display = ['Rest', 'Elbow', 'Hand'] # For display purposes in CM\n",
        "figure_counter = 1\n",
        "\n",
        "try:\n",
        "    cmap_greens = matplotlib.colormaps['Greens']\n",
        "    cmap_reds = matplotlib.colormaps['Reds']\n",
        "except AttributeError: # Older matplotlib\n",
        "    cmap_greens = plt.cm.get_cmap('Greens')\n",
        "    cmap_reds = plt.cm.get_cmap('Reds')\n",
        "\n",
        "# Thresholds for color logic\n",
        "threshold_diagonal_good = 0.5  # Above this is green on diagonal\n",
        "threshold_off_diagonal_bad = 0.2 # Above this is red on off-diagonal (significant misclassification)\n",
        "\n",
        "# Check if necessary variables for plotting exist\n",
        "if 'all_model_predictions' in locals() and 'true_labels_np' in locals() and 'accuracy_results' in locals():\n",
        "    for model_name, predictions in all_model_predictions.items():\n",
        "        cm = confusion_matrix(true_labels_np, predictions, labels=range(len(class_names_display)))\n",
        "\n",
        "        # --- Custom Color Logic V2 ---\n",
        "        row_sums = cm.sum(axis=1, keepdims=True)\n",
        "        cm_normalized_row = np.zeros_like(cm, dtype=float)\n",
        "        for r_idx in range(cm.shape[0]):\n",
        "            if row_sums[r_idx, 0] > 0:\n",
        "                cm_normalized_row[r_idx, :] = cm[r_idx, :] / row_sums[r_idx, 0]\n",
        "\n",
        "        num_classes_cm = cm.shape[0]\n",
        "        color_matrix_rgb = np.zeros((num_classes_cm, num_classes_cm, 3))\n",
        "        colormap_input_values = np.zeros((num_classes_cm, num_classes_cm))\n",
        "\n",
        "\n",
        "        for i in range(num_classes_cm):\n",
        "            for j in range(num_classes_cm):\n",
        "                norm_value = cm_normalized_row[i, j]\n",
        "                color_val_for_cmap = 0.0\n",
        "\n",
        "                if row_sums[i, 0] == 0:\n",
        "                    color_matrix_rgb[i, j, :] = [0.95, 0.95, 0.95]\n",
        "                elif i == j:\n",
        "                    if norm_value > threshold_diagonal_good:\n",
        "                        color_val_for_cmap = norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_greens(color_val_for_cmap)[:3]\n",
        "                    else:\n",
        "                        color_val_for_cmap = 1.0 - norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_reds(color_val_for_cmap)[:3]\n",
        "                else:\n",
        "                    if norm_value > threshold_off_diagonal_bad:\n",
        "                        color_val_for_cmap = norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_reds(color_val_for_cmap)[:3]\n",
        "                    else:\n",
        "                        color_val_for_cmap = 1.0 - norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_greens(color_val_for_cmap)[:3]\n",
        "                colormap_input_values[i,j] = color_val_for_cmap\n",
        "        # --- End Custom Color Logic V2 ---\n",
        "\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names_display)\n",
        "        fig, ax = plt.subplots(figsize=(8, 7))\n",
        "        disp.plot(ax=ax, include_values=True, cmap='Greys', colorbar=False, values_format='d')\n",
        "\n",
        "        if ax.images:\n",
        "            ax.images[0].remove()\n",
        "        ax.imshow(color_matrix_rgb)\n",
        "\n",
        "        if disp.text_ is not None:\n",
        "            for i in range(num_classes_cm):\n",
        "                for j in range(num_classes_cm):\n",
        "                    if disp.text_[i, j] is not None:\n",
        "                        text_color = \"white\" if colormap_input_values[i,j] > 0.5 else \"black\"\n",
        "                        if row_sums[i,0] == 0:\n",
        "                            text_color = \"black\"\n",
        "                        disp.text_[i, j].set_color(text_color)\n",
        "\n",
        "        ax.set_title(f'Confusion Matrix - {model_name}\\n(Custom Good/Bad Row-Normalized Colors)')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"\\nFigure {figure_counter}: Confusion Matrix for {model_name}.\")\n",
        "        print(f\"This matrix visualizes classification performance with custom colors based on row-normalized values\")\n",
        "\n",
        "\n",
        "        # Ensure class_names is defined for accessing accuracy_results\n",
        "        class_names_for_acc = ['Rest', 'Elbow', 'Hand'] # Define or ensure it's passed from global scope\n",
        "\n",
        "        overall_acc_val = accuracy_results[model_name]['Overall'] * 100\n",
        "        print(f\"Overall Accuracy: {overall_acc_val:.2f}%.\")\n",
        "        for class_idx, class_name_label in enumerate(class_names_display):\n",
        "            # Use a consistent key for accuracy_results, assuming class_names_for_acc matches keys\n",
        "            current_class_key = class_names_for_acc[class_idx]\n",
        "            if current_class_key in accuracy_results[model_name]:\n",
        "                class_acc_val = accuracy_results[model_name][current_class_key]\n",
        "                if not np.isnan(class_acc_val):\n",
        "                    print(f\"Accuracy for {class_name_label}: {class_acc_val*100:.2f}%.\")\n",
        "                else:\n",
        "                    print(f\"Accuracy for {class_name_label}: N/A (value is NaN).\")\n",
        "            else:\n",
        "                print(f\"Accuracy for {class_name_label}: N/A (key not found).\")\n",
        "        print(\"-\" * 70)\n",
        "        figure_counter += 1\n",
        "else:\n",
        "    print(\"Required variables (all_model_predictions, true_labels_np, or accuracy_results) not found for plotting confusion matrices.\")\n",
        "\n",
        "\n",
        "# --- 2. Accuracy Table ---\n",
        "print(\"\\n\\n--- Model Performance Summary Table ---\")\n",
        "# Ensure class_names is defined for table header\n",
        "class_names_for_table = ['Rest', 'Elbow', 'Hand'] # Define or ensure it's passed\n",
        "\n",
        "if 'accuracy_results' in locals():\n",
        "    header = f\"| {'Model':<17} | {'Overall Acc.':<15} | {class_names_for_table[0]+' Acc.':<12} | {class_names_for_table[1]+' Acc.':<12} | {class_names_for_table[2]+' Acc.':<12} |\"\n",
        "    separator = \"|-------------------|-----------------|--------------|--------------|--------------|\"\n",
        "    print(header)\n",
        "    print(separator)\n",
        "    for model_name_key in accuracy_results:\n",
        "        overall_str = f\"{accuracy_results[model_name_key]['Overall']*100:.2f}%\"\n",
        "\n",
        "        rest_acc_val = accuracy_results[model_name_key].get(class_names_for_table[0], np.nan)\n",
        "        rest_str = f\"{rest_acc_val*100:.2f}%\" if not np.isnan(rest_acc_val) else \"N/A\"\n",
        "\n",
        "        elbow_acc_val = accuracy_results[model_name_key].get(class_names_for_table[1], np.nan)\n",
        "        elbow_str = f\"{elbow_acc_val*100:.2f}%\" if not np.isnan(elbow_acc_val) else \"N/A\"\n",
        "\n",
        "        hand_acc_val = accuracy_results[model_name_key].get(class_names_for_table[2], np.nan)\n",
        "        hand_str = f\"{hand_acc_val*100:.2f}%\" if not np.isnan(hand_acc_val) else \"N/A\"\n",
        "\n",
        "        row = f\"| {model_name_key:<17} | {overall_str:<15} | {rest_str:<12} | {elbow_str:<12} | {hand_str:<12} |\"\n",
        "        print(row)\n",
        "else:\n",
        "    print(\"Accuracy results not available for summary table.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "collapsed": true,
        "id": "sBup_2nVClAB",
        "outputId": "839fb0e8-0030-4a2e-a820-039f53c85387"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Experiment Configuration & Data Specifications ---\n",
            "+-----------------------------------+-----------------------------------------+\n",
            "| Parameter                          | Value                                    |\n",
            "+===================================+=========================================+\n",
            "| Dataset File                       | sub-011_eeg.mat                          |\n",
            "| EEG Channels                       | 62                                       |\n",
            "| Data Shape (Full Dataset)          | Samples: 965, Channels: 62, Length: 360  |\n",
            "| Class Distribution (Full Dataset)  | Rest: 365, Elbow: 300, Hand: 300         |\n",
            "| Bandpass Filter (Reported)         | 2-45Hz                                   |\n",
            "| Sampling Ratio (Reported)          | 90Hz                                     |\n",
            "| TRAINING EPOCHS                    | 100                                      |\n",
            "| Dropout                            | 0.25                                     |\n",
            "| Test Split                         | 0.2                                      |\n",
            "| Batch Size (Evaluation)            | 64                                       |\n",
            "| SNN Time Steps                     | 100                                      |\n",
            "+-----------------------------------+-----------------------------------------+\n",
            "\n",
            "\n",
            "\n",
            "--- Confusion Matrices and Descriptions ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 1: Confusion Matrix for LENet CNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 72.54%.\n",
            "Accuracy for Rest: 97.06%.\n",
            "Accuracy for Elbow: 60.34%.\n",
            "Accuracy for Hand: 58.21%.\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 2: Confusion Matrix for LENet SNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 73.58%.\n",
            "Accuracy for Rest: 97.06%.\n",
            "Accuracy for Elbow: 65.52%.\n",
            "Accuracy for Hand: 56.72%.\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 3: Confusion Matrix for LENet_FCL CNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 66.84%.\n",
            "Accuracy for Rest: 98.53%.\n",
            "Accuracy for Elbow: 53.45%.\n",
            "Accuracy for Hand: 46.27%.\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 4: Confusion Matrix for LENet_FCL SNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 66.84%.\n",
            "Accuracy for Rest: 97.06%.\n",
            "Accuracy for Elbow: 55.17%.\n",
            "Accuracy for Hand: 46.27%.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "--- Model Performance Summary Table ---\n",
            "| Model             | Overall Acc.    | Rest Acc.    | Elbow Acc.   | Hand Acc.    |\n",
            "|-------------------|-----------------|--------------|--------------|--------------|\n",
            "| LENet CNN         | 72.54%          | 97.06%       | 60.34%       | 58.21%       |\n",
            "| LENet SNN         | 73.58%          | 97.06%       | 65.52%       | 56.72%       |\n",
            "| LENet_FCL CNN     | 66.84%          | 98.53%       | 53.45%       | 46.27%       |\n",
            "| LENet_FCL SNN     | 66.84%          | 97.06%       | 55.17%       | 46.27%       |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Energy consumption estimation and comparison\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# Note: Ensure that the LENet, LENet_FCL classes, and initialize_weights function\n",
        "# are defined in a previous cell. Also, 'device', 'channel_count',\n",
        "# and 'data_length' should be available from your data loading and setup cells.\n",
        "# If cnn_model_lenet and cnn_model_lenet_fcl (trained models) exist,\n",
        "# they will be used; otherwise, new instances are created for architecture analysis.\n",
        "\n",
        "# --- Energy Calculation Constants (original in pJ) ---\n",
        "E_MAC_PJ = 4.6  # pJ per MAC operation\n",
        "E_AC_PJ = 0.9   # pJ per AC operation (for SNN spike processing)\n",
        "\n",
        "# Conversion factor from pJ to µJ\n",
        "PJ_TO_UJ = 1e-6\n",
        "\n",
        "def get_macs_and_energy_final(\n",
        "    model, sample_input, model_name=\"Model\", T_snn_param=100, avg_firing_rate=0.02, conversion_factor=1.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates MACs and estimates energy consumption for ANN and SNN versions.\n",
        "    Returns energy in units determined by conversion_factor.\n",
        "    Formats energy values as floats with two decimal places.\n",
        "    Formats Total MACs as integers with commas.\n",
        "    \"\"\"\n",
        "    layer_flops_mac_list = []\n",
        "    hooks = []\n",
        "\n",
        "    def hook_fn_flops(module, input_tensor_tuple, output_tensor):\n",
        "        macs = 0\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            c_in = module.in_channels\n",
        "            c_out = module.out_channels\n",
        "            k_h, k_w = module.kernel_size\n",
        "            h_out, w_out = output_tensor.shape[2], output_tensor.shape[3]\n",
        "            groups = module.groups\n",
        "            macs = (c_in // groups) * k_h * k_w * c_out * h_out * w_out\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            n_in = module.in_features\n",
        "            n_out = module.out_features\n",
        "            macs = n_in * n_out\n",
        "        if macs > 0:\n",
        "            layer_flops_mac_list.append(macs)\n",
        "\n",
        "    for _, module_item in model.named_modules():\n",
        "        if isinstance(module_item, (nn.Conv2d, nn.Linear)):\n",
        "            hooks.append(module_item.register_forward_hook(hook_fn_flops))\n",
        "\n",
        "    model.eval()\n",
        "    model_device = next(model.parameters()).device\n",
        "    with torch.no_grad():\n",
        "        model(sample_input.to(model_device))\n",
        "\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    total_macs_all_layers = sum(layer_flops_mac_list) # Integer\n",
        "    energy_ann_pj = E_MAC_PJ * float(total_macs_all_layers) # Use float for energy calc\n",
        "\n",
        "    if not layer_flops_mac_list:\n",
        "        energy_snn_pj = 0.0\n",
        "    else:\n",
        "        macs_layer1_for_snn = float(layer_flops_mac_list[0])\n",
        "        energy_snn_term1_pj = E_MAC_PJ * macs_layer1_for_snn\n",
        "        sops_sum_layers_2_L = 0.0\n",
        "        for i in range(1, len(layer_flops_mac_list)):\n",
        "            macs_layer_i = float(layer_flops_mac_list[i])\n",
        "            sops_layer_i = avg_firing_rate * T_snn_param * macs_layer_i\n",
        "            sops_sum_layers_2_L += sops_layer_i\n",
        "        energy_snn_term2_pj = E_AC_PJ * sops_sum_layers_2_L\n",
        "        energy_snn_pj = energy_snn_term1_pj + energy_snn_term2_pj\n",
        "\n",
        "    energy_ann_converted = energy_ann_pj * conversion_factor\n",
        "    energy_snn_converted = energy_snn_pj * conversion_factor\n",
        "\n",
        "    output_unit_str = \"µJ\" if conversion_factor == PJ_TO_UJ else \"pJ\"\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Total MACs\": f\"{total_macs_all_layers:,}\", # Integer with commas\n",
        "        f\"ANN Energy ({output_unit_str})\": f\"{energy_ann_converted:,.2f}\",\n",
        "        f\"SNN Energy ({output_unit_str})\": f\"{energy_snn_converted:,.2f}\",\n",
        "    }\n",
        "\n",
        "# --- Prepare models and input for energy calculation ---\n",
        "if 'device' not in locals():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # print(f\"Warning: 'device' not found, defaulting to {device}\") # Silenced for cleaner output\n",
        "if 'channel_count' not in locals():\n",
        "    channel_count = 60\n",
        "    # print(f\"Warning: 'channel_count' not found, defaulting to {channel_count}\")\n",
        "if 'data_length' not in locals():\n",
        "    data_length = 1000\n",
        "    # print(f\"Warning: 'data_length' not found, defaulting to {data_length}\")\n",
        "if 'TIME_STEPS' not in locals():\n",
        "    TIME_STEPS = 100\n",
        "    # print(f\"Warning: 'TIME_STEPS' not found, defaulting to {TIME_STEPS}\")\n",
        "if 'DROP_OUT' not in locals():\n",
        "    DROP_OUT = 0.25\n",
        "    # print(f\"Warning: 'DROP_OUT' not found, defaulting to {DROP_OUT}\")\n",
        "\n",
        "sample_input_tensor = torch.randn(1, 1, channel_count, data_length)\n",
        "\n",
        "# Model 1: LENet (CCB)\n",
        "if 'cnn_model_lenet' in locals() and isinstance(cnn_model_lenet, nn.Module):\n",
        "    # print(\"Using existing 'cnn_model_lenet' for LENet (CCB) energy calculation.\") # Silenced\n",
        "    model_for_energy_lenet_ccb = cnn_model_lenet\n",
        "else:\n",
        "    print(\"Warning: 'cnn_model_lenet' not found. Initializing new LENet for energy calculation.\")\n",
        "    try:\n",
        "        model_for_energy_lenet_ccb = LENet(classes_num=3, channel_count=channel_count, drop_out=DROP_OUT).to(device)\n",
        "        if 'initialize_weights' in locals(): model_for_energy_lenet_ccb.apply(initialize_weights)\n",
        "    except NameError:\n",
        "        print(\"Error: LENet class definition not found.\")\n",
        "        model_for_energy_lenet_ccb = None\n",
        "\n",
        "# Model 2: LENet_FCL\n",
        "if 'cnn_model_lenet_fcl' in locals() and isinstance(cnn_model_lenet_fcl, nn.Module):\n",
        "    # print(\"Using existing 'cnn_model_lenet_fcl' for LENet_FCL energy calculation.\") # Silenced\n",
        "    model_for_energy_lenet_fcl = cnn_model_lenet_fcl\n",
        "    model_for_energy_lenet_fcl.eval()\n",
        "    with torch.no_grad():\n",
        "        model_for_energy_lenet_fcl(sample_input_tensor.to(device))\n",
        "else:\n",
        "    print(\"Warning: 'cnn_model_lenet_fcl' not found. Initializing new LENet_FCL for energy calculation.\")\n",
        "    try:\n",
        "        model_for_energy_lenet_fcl = LENet_FCL(classes_num=3, channel_count=channel_count, drop_out=DROP_OUT).to(device)\n",
        "        if 'initialize_weights' in locals(): model_for_energy_lenet_fcl.apply(initialize_weights)\n",
        "        model_for_energy_lenet_fcl.eval()\n",
        "        with torch.no_grad():\n",
        "            model_for_energy_lenet_fcl(sample_input_tensor.to(device))\n",
        "    except NameError:\n",
        "        print(\"Error: LENet_FCL class definition not found.\")\n",
        "        model_for_energy_lenet_fcl = None\n",
        "\n",
        "# --- Perform Energy Calculations ---\n",
        "AVG_FIRING_RATE = 0.02 # 2%\n",
        "energy_results_list_final = []\n",
        "\n",
        "CURRENT_CONVERSION_FACTOR = PJ_TO_UJ\n",
        "OUTPUT_UNIT_NAME = \"µJ\"\n",
        "\n",
        "if model_for_energy_lenet_ccb:\n",
        "    results_lenet_final = get_macs_and_energy_final(\n",
        "        model_for_energy_lenet_ccb,\n",
        "        sample_input_tensor,\n",
        "        model_name=\"LENet (CCB)\",\n",
        "        T_snn_param=TIME_STEPS,\n",
        "        avg_firing_rate=AVG_FIRING_RATE,\n",
        "        conversion_factor=CURRENT_CONVERSION_FACTOR\n",
        "    )\n",
        "    energy_results_list_final.append(results_lenet_final)\n",
        "# else: # Silenced for cleaner output\n",
        "    # print(\"Skipping LENet (CCB) energy calculation as model was not available.\")\n",
        "\n",
        "if model_for_energy_lenet_fcl:\n",
        "    results_lenet_fcl_final = get_macs_and_energy_final(\n",
        "        model_for_energy_lenet_fcl,\n",
        "        sample_input_tensor,\n",
        "        model_name=\"LENet FCL\",\n",
        "        T_snn_param=TIME_STEPS,\n",
        "        avg_firing_rate=AVG_FIRING_RATE,\n",
        "        conversion_factor=CURRENT_CONVERSION_FACTOR\n",
        "    )\n",
        "    energy_results_list_final.append(results_lenet_fcl_final)\n",
        "# else: # Silenced for cleaner output\n",
        "    # print(\"Skipping LENet FCL energy calculation as model was not available.\")\n",
        "\n",
        "# --- Generate Markdown Table ---\n",
        "if energy_results_list_final:\n",
        "    # Table title is now part of the initial print statements from your notebook\n",
        "    # markdown_table_final = f\"Table 2. Estimated Theoretical Energy Consumption per Inference.\\n\"\n",
        "    markdown_table_final = \"\" # Start with an empty string for the table itself\n",
        "\n",
        "    # Adjusted column widths and alignment to match the image\n",
        "    # | Model         | Total MACs   | ANN Energy (µJ) | SNN Energy (µJ) |\n",
        "    # |:--------------|:-------------|:----------------|:----------------|\n",
        "    # Widths: Model: 13, Total MACs: 12, ANN Energy: 15, SNN Energy: 15 (approx from image)\n",
        "\n",
        "    markdown_table_final += f\"| Model       | Total MACs   | ANN Energy ({OUTPUT_UNIT_NAME}) | SNN Energy ({OUTPUT_UNIT_NAME}) |\\n\"\n",
        "    markdown_table_final += f\"|:------------|:-------------|:----------------|:----------------|\\n\"\n",
        "\n",
        "    for result in energy_results_list_final:\n",
        "        markdown_table_final += (\n",
        "            f\"| {result['Model']:<11} | \"  # Adjusted width\n",
        "            f\"{result['Total MACs']:>12} | \"\n",
        "            f\"{result[f'ANN Energy ({OUTPUT_UNIT_NAME})']:>15} | \"\n",
        "            f\"{result[f'SNN Energy ({OUTPUT_UNIT_NAME})']:>15} |\\n\"\n",
        "        )\n",
        "\n",
        "    # The newline after the table is handled by the print statement for constants\n",
        "    # markdown_table_final += \"\\n\"\n",
        "\n",
        "    constants_line = (\n",
        "        f\"Constants used: E_MAC = ({E_MAC_PJ:.1f} pJ), \"\n",
        "        f\"E_AC = ({E_AC_PJ:.1f} pJ). \"\n",
        "        f\"SNN parameters: T = {TIME_STEPS}, \"\n",
        "        f\"Assumed Average Firing Rate (fr) = {AVG_FIRING_RATE*100:.0f}%.\"\n",
        "    )\n",
        "    # Print the table first, then the constants on a new line\n",
        "    print(markdown_table_final)\n",
        "    print(constants_line)\n",
        "else:\n",
        "    print(\"No energy consumption results to display. Please check model availability and definitions.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXy2ibddkwob",
        "outputId": "bd592c0f-d8eb-49b8-debd-9e31c7b10a85"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Model       | Total MACs   | ANN Energy (µJ) | SNN Energy (µJ) |\n",
            "|:------------|:-------------|:----------------|:----------------|\n",
            "| LENet (CCB) |   33,973,008 |          156.28 |           93.15 |\n",
            "| LENet FCL   |   33,973,008 |          156.28 |           93.15 |\n",
            "\n",
            "Constants used: E_MAC = (4.6 pJ), E_AC = (0.9 pJ). SNN parameters: T = 100, Assumed Average Firing Rate (fr) = 2%.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}