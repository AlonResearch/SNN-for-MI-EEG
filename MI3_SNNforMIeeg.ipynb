{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlonResearch/SNN-for-MI-EEG/blob/main/MI3_SNNforMIeeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xy9K5bJ9aB7D",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as da\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import scipy.io as scio\n",
        "#!pip install spikingjelly -q\n",
        "from spikingjelly.activation_based import ann2snn\n",
        "#!apt install -y graphviz graphviz-dev -q\n",
        "#!pip install nnviz -q\n",
        "from nnviz import drawing\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lAEV1BHaBp9",
        "outputId": "76e80670-94b1-4309-cfb2-b56c4358eb09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wed May  7 07:48:54 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   78C    P0             70W /   70W |    1192MiB /  15360MiB |     85%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CYNshZR5ZhDy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Data loader and other functions\n",
        "#Defining functions\n",
        "\n",
        "def data_loader(data, label, batch=64, shuffle=True, drop=False):\n",
        "    \"\"\"\n",
        "    Preprocess the data to fit model.\n",
        "    Feed data into data_loader.\n",
        "    input:\n",
        "        data (float): samples*length*ch (samples*ch*length).\n",
        "        label (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        batch (int): batch size\n",
        "        shuffle (bool): shuffle data before input into decoder\n",
        "        drop (bool): drop the last samples if True\n",
        "    output:\n",
        "        data loader\n",
        "    \"\"\"\n",
        "    label = torch.LongTensor(label.flatten()).to(device)\n",
        "    if data.shape[1] >= data.shape[2]:\n",
        "        data = torch.tensor(data.swapaxes(1, 2))\n",
        "    data = torch.unsqueeze(data, dim=1).type('torch.FloatTensor').to(device)\n",
        "    data = da.TensorDataset(data, label)\n",
        "    loader = da.DataLoader(dataset=data, batch_size=batch, shuffle=shuffle, drop_last=drop)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def val_snn(Dec, test_loader, T=None):\n",
        "    Dec.eval().to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if T is not None:\n",
        "        corrects = np.zeros(T)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            if T is None:\n",
        "                outputs = Dec(inputs)\n",
        "                correct += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            else:\n",
        "                for m in Dec.modules():\n",
        "                    if hasattr(m, 'reset'):\n",
        "                        m.reset()\n",
        "                for t in range(T):\n",
        "                    if t == 0:\n",
        "                        outputs = Dec(inputs)\n",
        "                    else:\n",
        "                        outputs += Dec(inputs)\n",
        "                    corrects[t] += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            total += targets.shape[0]\n",
        "    return correct / total if T is None else corrects / total\n",
        "\n",
        "\n",
        "def anntosnn(cnn_model, train_x, train_y, test_x, test_y, batch=64, T=None):\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    print('---------------------------------------------')\n",
        "    print('Converting using MaxNorm')\n",
        "    model_converter = ann2snn.Converter(mode='max', dataloader=train_loader)\n",
        "    snn_model = model_converter(cnn_model)\n",
        "    mode_max_accs = val_snn(snn_model, test_loader, T=T)\n",
        "\n",
        "    return mode_max_accs\n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def train_ann(cnn_model, train_x, train_y, test_x, test_y, ep=500, batch=64):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        train_x, test_x (float): samples*length*ch (samples*ch*length).\n",
        "        train_y, test_y (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        ep (int): total train and test epoch\n",
        "        batch (int): batch size\n",
        "    output:\n",
        "        train acc, test acc, weight_file\n",
        "    \"\"\"\n",
        "    # Define training configuration\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.01)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ep)\n",
        "\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "    for epoch in range(ep):\n",
        "        # Train ANN\n",
        "        cnn_model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        print('\\n')\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = cnn_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            print(batch_idx, len(train_loader), 'Epoch: %d | ANN: trainLoss: %.4f | trainAcc: %.4f%% (%d/%d)'\n",
        "                  % (epoch, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        train_acc.append(round(correct / total, 4))\n",
        "\n",
        "        # Test ANN\n",
        "        cnn_model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "                outputs = cnn_model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "                print(batch_idx, len(test_loader), 'Epoch: %d | ANN: testLoss: %.4f | testAcc: %.4f%% (%d/%d)'\n",
        "                      % (epoch, val_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        test_acc.append(round(correct / total, 4))\n",
        "\n",
        "    train_acc = np.asarray(train_acc[-1])\n",
        "    test_acc = np.asarray(test_acc[-1])\n",
        "    return train_acc, test_acc,cnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EC8xjyPDaM6G",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Model definitions\n",
        "# Model 2a\n",
        "\n",
        "class LENet(nn.Module):\n",
        "    \"\"\"\n",
        "        LENet Model\n",
        "    input:\n",
        "         data shape as: batch_size*1*channel*length (64*1*22*1000) BCI IV-2a\n",
        "         batch_size：64\n",
        "         channel：22\n",
        "         length：1000\n",
        "    output:\n",
        "        classes_num\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classes_num=3, channel_count=22, drop_out = 0.5):\n",
        "        super(LENet, self).__init__()\n",
        "        self.drop_out = drop_out\n",
        "\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,64) #\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,32) #\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,16) #\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            # Temporal Convolution block fusion kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            # Spatial Convolution block kernel_size (channel,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(channel_count, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            # Feature Fusion Convolution block kernel_size (1,16) and (1,1) #\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),  #\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.CCB = nn.Sequential(\n",
        "            # Classification Convolution block kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=classes_num,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "        x = self.CCB(x)\n",
        "        return x\n",
        "\n",
        "class LENet_FCL(nn.Module):\n",
        "    def __init__(self, classes_num=3, channel_count=60, drop_out=0.5):\n",
        "        super(LENet_FCL, self).__init__()\n",
        "        self.drop_out = drop_out\n",
        "\n",
        "        # Keep all the convolutional layers the same\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(channel_count, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        # We'll determine the size of the FC layer in the forward pass\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = None\n",
        "        self.classes_num = classes_num\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Create the FC layer on first forward pass if it doesn't exist\n",
        "        if self.fc is None:\n",
        "            in_features = x.shape[1]\n",
        "            self.fc = nn.Linear(in_features, self.classes_num).to(x.device)\n",
        "            # Initialize weights for the new layer\n",
        "            nn.init.kaiming_normal_(self.fc.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            if self.fc.bias is not None:\n",
        "                nn.init.constant_(self.fc.bias, 0)\n",
        "\n",
        "        # Apply the FC layer\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T0hmMue2aMqA",
        "outputId": "3c82a0b8-3f20-4601-ff2e-42613ad6d03b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channel count: 62\n",
            "Data shape: (965, 62, 4000)\n",
            "Label shape: (965, 1)\n",
            "Class distribution: Rest: 365, Elbow: 300, Hand: 300\n"
          ]
        }
      ],
      "source": [
        "# @title Loading the data\n",
        "\"\"\"\n",
        "Loading the data\n",
        "\"\"\"\n",
        "\n",
        "# Getting real samples\n",
        "#Locally load the dataset\n",
        "#file = scio.loadmat('Datasets\\BCICIV_2a_gdf\\Derivatives\\A01T.mat')\n",
        "\n",
        "# Google Colab load the dataset\n",
        "file = scio.loadmat('/content/sub-011_eeg.mat')\n",
        "\n",
        "\n",
        "all_data = file['all_data']\n",
        "all_label = file['all_label']\n",
        "\n",
        "# Print data information\n",
        "channel_count = all_data.shape[1]\n",
        "num_classes = len(np.unique(all_label.flatten()))\n",
        "data_length = all_data.shape[2] # Assuming data is samples*ch*length\n",
        "print(f\"Channel count: {channel_count}\")\n",
        "print(f\"Data shape: {all_data.shape}\")\n",
        "print(f\"Label shape: {all_label.shape}\")\n",
        "print(f\"Class distribution: Rest: {np.sum(all_label == 0)}, Elbow: {np.sum(all_label == 1)}, Hand: {np.sum(all_label == 2)}\")\n",
        "\n",
        "datasetX = torch.tensor(all_data, dtype=torch.float32)\n",
        "datasetY = torch.tensor(all_label, dtype=torch.int64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LENet to SNN Conversion Framework execution\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 60\n",
        "BATCH_SIZE = 64\n",
        "TIME_STEPS = 100  # T for SNN\n",
        "TEST_SIZE = 0.1\n",
        "DROP_OUT = 0.25\n",
        "\n",
        "# Split the data\n",
        "print(f\"{100 - (TEST_SIZE * 100)}% of the dataset is used for training and {TEST_SIZE * 100}% is used for testing.\")\n",
        "train_data, test_data, train_label, test_label = train_test_split(datasetX, datasetY, test_size=TEST_SIZE, shuffle=True,\n",
        "                                                                  random_state=0)\n",
        "\n",
        "# Initialize model\n",
        "cnn_model_lenet = LENet(classes_num=3, channel_count=channel_count, drop_out = DROP_OUT).to(device)\n",
        "cnn_model_lenet.apply(initialize_weights)\n",
        "\n",
        "# Train CNN model\n",
        "train_acc, test_acc,  cnn_model_lenet = train_ann(cnn_model_lenet, train_data, train_label, test_data, test_label,\n",
        "                                              ep=EPOCHS, batch=BATCH_SIZE)\n",
        "max_norm_acc = anntosnn( cnn_model_lenet, train_data, train_label, test_data, test_label,\n",
        "                        batch=BATCH_SIZE, T=TIME_STEPS)\n",
        "snn_model_lenet = ann2snn.Converter(mode='max', dataloader=data_loader(train_data, train_label, batch=BATCH_SIZE))( cnn_model_lenet)\n",
        "\n",
        "print('\\n')\n",
        "print('ANN accuracy: Test: %.4f%%' % (test_acc * 100))\n",
        "print('SNN accuracy: max_norm: %.4f%%' % (max_norm_acc[-1] * 100))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JiMa-ylsC2bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ed2c1d-ca88-4be7-eb17-7fb55d08b7be"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90.0% of the dataset is used for training and 10.0% is used for testing.\n",
            "\n",
            "\n",
            "0 14 Epoch: 0 | ANN: trainLoss: 1.3130 | trainAcc: 29.6875% (19/64)\n",
            "1 14 Epoch: 0 | ANN: trainLoss: 1.3082 | trainAcc: 27.3438% (35/128)\n",
            "2 14 Epoch: 0 | ANN: trainLoss: 1.2498 | trainAcc: 30.7292% (59/192)\n",
            "3 14 Epoch: 0 | ANN: trainLoss: 1.2140 | trainAcc: 33.9844% (87/256)\n",
            "4 14 Epoch: 0 | ANN: trainLoss: 1.1902 | trainAcc: 34.6875% (111/320)\n",
            "5 14 Epoch: 0 | ANN: trainLoss: 1.1692 | trainAcc: 35.4167% (136/384)\n",
            "6 14 Epoch: 0 | ANN: trainLoss: 1.1471 | trainAcc: 37.9464% (170/448)\n",
            "7 14 Epoch: 0 | ANN: trainLoss: 1.1390 | trainAcc: 38.4766% (197/512)\n",
            "8 14 Epoch: 0 | ANN: trainLoss: 1.1343 | trainAcc: 38.5417% (222/576)\n",
            "9 14 Epoch: 0 | ANN: trainLoss: 1.1232 | trainAcc: 39.8438% (255/640)\n",
            "10 14 Epoch: 0 | ANN: trainLoss: 1.1176 | trainAcc: 40.4830% (285/704)\n",
            "11 14 Epoch: 0 | ANN: trainLoss: 1.1157 | trainAcc: 40.8854% (314/768)\n",
            "12 14 Epoch: 0 | ANN: trainLoss: 1.1090 | trainAcc: 41.7067% (347/832)\n",
            "13 14 Epoch: 0 | ANN: trainLoss: 1.1010 | trainAcc: 41.5899% (361/868)\n",
            "0 2 Epoch: 0 | ANN: testLoss: 1.8919 | testAcc: 34.3750% (22/64)\n",
            "1 2 Epoch: 0 | ANN: testLoss: 1.6608 | testAcc: 30.9278% (30/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 1 | ANN: trainLoss: 0.9149 | trainAcc: 57.8125% (37/64)\n",
            "1 14 Epoch: 1 | ANN: trainLoss: 0.9583 | trainAcc: 55.4688% (71/128)\n",
            "2 14 Epoch: 1 | ANN: trainLoss: 0.9338 | trainAcc: 56.2500% (108/192)\n",
            "3 14 Epoch: 1 | ANN: trainLoss: 0.9559 | trainAcc: 53.9062% (138/256)\n",
            "4 14 Epoch: 1 | ANN: trainLoss: 0.9484 | trainAcc: 54.3750% (174/320)\n",
            "5 14 Epoch: 1 | ANN: trainLoss: 0.9393 | trainAcc: 55.2083% (212/384)\n",
            "6 14 Epoch: 1 | ANN: trainLoss: 0.9345 | trainAcc: 54.9107% (246/448)\n",
            "7 14 Epoch: 1 | ANN: trainLoss: 0.9411 | trainAcc: 54.1016% (277/512)\n",
            "8 14 Epoch: 1 | ANN: trainLoss: 0.9332 | trainAcc: 53.2986% (307/576)\n",
            "9 14 Epoch: 1 | ANN: trainLoss: 0.9257 | trainAcc: 54.3750% (348/640)\n",
            "10 14 Epoch: 1 | ANN: trainLoss: 0.9164 | trainAcc: 54.5455% (384/704)\n",
            "11 14 Epoch: 1 | ANN: trainLoss: 0.9045 | trainAcc: 55.2083% (424/768)\n",
            "12 14 Epoch: 1 | ANN: trainLoss: 0.8947 | trainAcc: 56.0096% (466/832)\n",
            "13 14 Epoch: 1 | ANN: trainLoss: 0.8853 | trainAcc: 56.6820% (492/868)\n",
            "0 2 Epoch: 1 | ANN: testLoss: 1.2551 | testAcc: 45.3125% (29/64)\n",
            "1 2 Epoch: 1 | ANN: testLoss: 1.2548 | testAcc: 41.2371% (40/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 2 | ANN: trainLoss: 0.7628 | trainAcc: 65.6250% (42/64)\n",
            "1 14 Epoch: 2 | ANN: trainLoss: 0.7735 | trainAcc: 64.8438% (83/128)\n",
            "2 14 Epoch: 2 | ANN: trainLoss: 0.7917 | trainAcc: 61.4583% (118/192)\n",
            "3 14 Epoch: 2 | ANN: trainLoss: 0.7888 | trainAcc: 61.3281% (157/256)\n",
            "4 14 Epoch: 2 | ANN: trainLoss: 0.7801 | trainAcc: 61.8750% (198/320)\n",
            "5 14 Epoch: 2 | ANN: trainLoss: 0.7652 | trainAcc: 62.5000% (240/384)\n",
            "6 14 Epoch: 2 | ANN: trainLoss: 0.7586 | trainAcc: 61.6071% (276/448)\n",
            "7 14 Epoch: 2 | ANN: trainLoss: 0.7486 | trainAcc: 62.3047% (319/512)\n",
            "8 14 Epoch: 2 | ANN: trainLoss: 0.7393 | trainAcc: 62.3264% (359/576)\n",
            "9 14 Epoch: 2 | ANN: trainLoss: 0.7324 | trainAcc: 63.4375% (406/640)\n",
            "10 14 Epoch: 2 | ANN: trainLoss: 0.7294 | trainAcc: 64.2045% (452/704)\n",
            "11 14 Epoch: 2 | ANN: trainLoss: 0.7255 | trainAcc: 63.9323% (491/768)\n",
            "12 14 Epoch: 2 | ANN: trainLoss: 0.7354 | trainAcc: 63.2212% (526/832)\n",
            "13 14 Epoch: 2 | ANN: trainLoss: 0.7297 | trainAcc: 63.2488% (549/868)\n",
            "0 2 Epoch: 2 | ANN: testLoss: 1.2279 | testAcc: 40.6250% (26/64)\n",
            "1 2 Epoch: 2 | ANN: testLoss: 1.1639 | testAcc: 43.2990% (42/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 3 | ANN: trainLoss: 0.6096 | trainAcc: 67.1875% (43/64)\n",
            "1 14 Epoch: 3 | ANN: trainLoss: 0.6311 | trainAcc: 66.4062% (85/128)\n",
            "2 14 Epoch: 3 | ANN: trainLoss: 0.6626 | trainAcc: 66.1458% (127/192)\n",
            "3 14 Epoch: 3 | ANN: trainLoss: 0.6347 | trainAcc: 68.7500% (176/256)\n",
            "4 14 Epoch: 3 | ANN: trainLoss: 0.6486 | trainAcc: 68.4375% (219/320)\n",
            "5 14 Epoch: 3 | ANN: trainLoss: 0.6534 | trainAcc: 67.7083% (260/384)\n",
            "6 14 Epoch: 3 | ANN: trainLoss: 0.6552 | trainAcc: 66.9643% (300/448)\n",
            "7 14 Epoch: 3 | ANN: trainLoss: 0.6592 | trainAcc: 65.6250% (336/512)\n",
            "8 14 Epoch: 3 | ANN: trainLoss: 0.6601 | trainAcc: 65.2778% (376/576)\n",
            "9 14 Epoch: 3 | ANN: trainLoss: 0.6541 | trainAcc: 66.0938% (423/640)\n",
            "10 14 Epoch: 3 | ANN: trainLoss: 0.6535 | trainAcc: 66.0511% (465/704)\n",
            "11 14 Epoch: 3 | ANN: trainLoss: 0.6474 | trainAcc: 66.6667% (512/768)\n",
            "12 14 Epoch: 3 | ANN: trainLoss: 0.6449 | trainAcc: 66.2260% (551/832)\n",
            "13 14 Epoch: 3 | ANN: trainLoss: 0.6461 | trainAcc: 66.3594% (576/868)\n",
            "0 2 Epoch: 3 | ANN: testLoss: 0.7256 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 3 | ANN: testLoss: 0.7449 | testAcc: 61.8557% (60/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 4 | ANN: trainLoss: 0.7028 | trainAcc: 59.3750% (38/64)\n",
            "1 14 Epoch: 4 | ANN: trainLoss: 0.6675 | trainAcc: 60.1562% (77/128)\n",
            "2 14 Epoch: 4 | ANN: trainLoss: 0.6671 | trainAcc: 61.9792% (119/192)\n",
            "3 14 Epoch: 4 | ANN: trainLoss: 0.6788 | trainAcc: 62.1094% (159/256)\n",
            "4 14 Epoch: 4 | ANN: trainLoss: 0.6567 | trainAcc: 64.3750% (206/320)\n",
            "5 14 Epoch: 4 | ANN: trainLoss: 0.6490 | trainAcc: 63.8021% (245/384)\n",
            "6 14 Epoch: 4 | ANN: trainLoss: 0.6389 | trainAcc: 64.5089% (289/448)\n",
            "7 14 Epoch: 4 | ANN: trainLoss: 0.6286 | trainAcc: 66.0156% (338/512)\n",
            "8 14 Epoch: 4 | ANN: trainLoss: 0.6281 | trainAcc: 66.1458% (381/576)\n",
            "9 14 Epoch: 4 | ANN: trainLoss: 0.6254 | trainAcc: 65.7812% (421/640)\n",
            "10 14 Epoch: 4 | ANN: trainLoss: 0.6156 | trainAcc: 66.0511% (465/704)\n",
            "11 14 Epoch: 4 | ANN: trainLoss: 0.6129 | trainAcc: 66.1458% (508/768)\n",
            "12 14 Epoch: 4 | ANN: trainLoss: 0.6107 | trainAcc: 66.8269% (556/832)\n",
            "13 14 Epoch: 4 | ANN: trainLoss: 0.6117 | trainAcc: 67.0507% (582/868)\n",
            "0 2 Epoch: 4 | ANN: testLoss: 0.7382 | testAcc: 65.6250% (42/64)\n",
            "1 2 Epoch: 4 | ANN: testLoss: 0.6651 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 5 | ANN: trainLoss: 0.5822 | trainAcc: 71.8750% (46/64)\n",
            "1 14 Epoch: 5 | ANN: trainLoss: 0.5701 | trainAcc: 71.0938% (91/128)\n",
            "2 14 Epoch: 5 | ANN: trainLoss: 0.5444 | trainAcc: 72.3958% (139/192)\n",
            "3 14 Epoch: 5 | ANN: trainLoss: 0.5442 | trainAcc: 71.8750% (184/256)\n",
            "4 14 Epoch: 5 | ANN: trainLoss: 0.5585 | trainAcc: 70.9375% (227/320)\n",
            "5 14 Epoch: 5 | ANN: trainLoss: 0.5676 | trainAcc: 70.3125% (270/384)\n",
            "6 14 Epoch: 5 | ANN: trainLoss: 0.5591 | trainAcc: 71.2054% (319/448)\n",
            "7 14 Epoch: 5 | ANN: trainLoss: 0.5542 | trainAcc: 70.8984% (363/512)\n",
            "8 14 Epoch: 5 | ANN: trainLoss: 0.5621 | trainAcc: 70.3125% (405/576)\n",
            "9 14 Epoch: 5 | ANN: trainLoss: 0.5559 | trainAcc: 70.9375% (454/640)\n",
            "10 14 Epoch: 5 | ANN: trainLoss: 0.5607 | trainAcc: 70.3125% (495/704)\n",
            "11 14 Epoch: 5 | ANN: trainLoss: 0.5634 | trainAcc: 69.9219% (537/768)\n",
            "12 14 Epoch: 5 | ANN: trainLoss: 0.5675 | trainAcc: 69.1106% (575/832)\n",
            "13 14 Epoch: 5 | ANN: trainLoss: 0.5701 | trainAcc: 69.0092% (599/868)\n",
            "0 2 Epoch: 5 | ANN: testLoss: 0.5877 | testAcc: 70.3125% (45/64)\n",
            "1 2 Epoch: 5 | ANN: testLoss: 0.5899 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 6 | ANN: trainLoss: 0.5551 | trainAcc: 67.1875% (43/64)\n",
            "1 14 Epoch: 6 | ANN: trainLoss: 0.5050 | trainAcc: 69.5312% (89/128)\n",
            "2 14 Epoch: 6 | ANN: trainLoss: 0.5316 | trainAcc: 70.3125% (135/192)\n",
            "3 14 Epoch: 6 | ANN: trainLoss: 0.5357 | trainAcc: 70.3125% (180/256)\n",
            "4 14 Epoch: 6 | ANN: trainLoss: 0.5239 | trainAcc: 71.2500% (228/320)\n",
            "5 14 Epoch: 6 | ANN: trainLoss: 0.5158 | trainAcc: 71.6146% (275/384)\n",
            "6 14 Epoch: 6 | ANN: trainLoss: 0.5159 | trainAcc: 72.3214% (324/448)\n",
            "7 14 Epoch: 6 | ANN: trainLoss: 0.5152 | trainAcc: 71.8750% (368/512)\n",
            "8 14 Epoch: 6 | ANN: trainLoss: 0.5132 | trainAcc: 72.2222% (416/576)\n",
            "9 14 Epoch: 6 | ANN: trainLoss: 0.5106 | trainAcc: 72.1875% (462/640)\n",
            "10 14 Epoch: 6 | ANN: trainLoss: 0.5118 | trainAcc: 72.3011% (509/704)\n",
            "11 14 Epoch: 6 | ANN: trainLoss: 0.5123 | trainAcc: 71.7448% (551/768)\n",
            "12 14 Epoch: 6 | ANN: trainLoss: 0.5150 | trainAcc: 71.2740% (593/832)\n",
            "13 14 Epoch: 6 | ANN: trainLoss: 0.5105 | trainAcc: 71.1982% (618/868)\n",
            "0 2 Epoch: 6 | ANN: testLoss: 0.5651 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 6 | ANN: testLoss: 0.5773 | testAcc: 60.8247% (59/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 7 | ANN: trainLoss: 0.4589 | trainAcc: 89.0625% (57/64)\n",
            "1 14 Epoch: 7 | ANN: trainLoss: 0.4838 | trainAcc: 81.2500% (104/128)\n",
            "2 14 Epoch: 7 | ANN: trainLoss: 0.5069 | trainAcc: 76.5625% (147/192)\n",
            "3 14 Epoch: 7 | ANN: trainLoss: 0.5185 | trainAcc: 75.0000% (192/256)\n",
            "4 14 Epoch: 7 | ANN: trainLoss: 0.5033 | trainAcc: 76.2500% (244/320)\n",
            "5 14 Epoch: 7 | ANN: trainLoss: 0.4984 | trainAcc: 75.7812% (291/384)\n",
            "6 14 Epoch: 7 | ANN: trainLoss: 0.4918 | trainAcc: 76.1161% (341/448)\n",
            "7 14 Epoch: 7 | ANN: trainLoss: 0.5048 | trainAcc: 74.2188% (380/512)\n",
            "8 14 Epoch: 7 | ANN: trainLoss: 0.5126 | trainAcc: 73.4375% (423/576)\n",
            "9 14 Epoch: 7 | ANN: trainLoss: 0.5210 | trainAcc: 72.8125% (466/640)\n",
            "10 14 Epoch: 7 | ANN: trainLoss: 0.5207 | trainAcc: 72.7273% (512/704)\n",
            "11 14 Epoch: 7 | ANN: trainLoss: 0.5187 | trainAcc: 72.9167% (560/768)\n",
            "12 14 Epoch: 7 | ANN: trainLoss: 0.5135 | trainAcc: 72.5962% (604/832)\n",
            "13 14 Epoch: 7 | ANN: trainLoss: 0.5134 | trainAcc: 72.6959% (631/868)\n",
            "0 2 Epoch: 7 | ANN: testLoss: 0.6915 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 7 | ANN: testLoss: 0.6781 | testAcc: 62.8866% (61/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 8 | ANN: trainLoss: 0.4623 | trainAcc: 78.1250% (50/64)\n",
            "1 14 Epoch: 8 | ANN: trainLoss: 0.4865 | trainAcc: 78.1250% (100/128)\n",
            "2 14 Epoch: 8 | ANN: trainLoss: 0.4916 | trainAcc: 75.0000% (144/192)\n",
            "3 14 Epoch: 8 | ANN: trainLoss: 0.4925 | trainAcc: 74.2188% (190/256)\n",
            "4 14 Epoch: 8 | ANN: trainLoss: 0.4794 | trainAcc: 75.9375% (243/320)\n",
            "5 14 Epoch: 8 | ANN: trainLoss: 0.4720 | trainAcc: 76.8229% (295/384)\n",
            "6 14 Epoch: 8 | ANN: trainLoss: 0.4849 | trainAcc: 77.0089% (345/448)\n",
            "7 14 Epoch: 8 | ANN: trainLoss: 0.4980 | trainAcc: 75.7812% (388/512)\n",
            "8 14 Epoch: 8 | ANN: trainLoss: 0.4924 | trainAcc: 75.3472% (434/576)\n",
            "9 14 Epoch: 8 | ANN: trainLoss: 0.4933 | trainAcc: 74.8438% (479/640)\n",
            "10 14 Epoch: 8 | ANN: trainLoss: 0.4915 | trainAcc: 75.2841% (530/704)\n",
            "11 14 Epoch: 8 | ANN: trainLoss: 0.4892 | trainAcc: 75.2604% (578/768)\n",
            "12 14 Epoch: 8 | ANN: trainLoss: 0.4891 | trainAcc: 75.6010% (629/832)\n",
            "13 14 Epoch: 8 | ANN: trainLoss: 0.4826 | trainAcc: 75.8065% (658/868)\n",
            "0 2 Epoch: 8 | ANN: testLoss: 0.6589 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 8 | ANN: testLoss: 0.6638 | testAcc: 62.8866% (61/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 9 | ANN: trainLoss: 0.4845 | trainAcc: 68.7500% (44/64)\n",
            "1 14 Epoch: 9 | ANN: trainLoss: 0.4667 | trainAcc: 76.5625% (98/128)\n",
            "2 14 Epoch: 9 | ANN: trainLoss: 0.4477 | trainAcc: 78.6458% (151/192)\n",
            "3 14 Epoch: 9 | ANN: trainLoss: 0.4252 | trainAcc: 80.8594% (207/256)\n",
            "4 14 Epoch: 9 | ANN: trainLoss: 0.4311 | trainAcc: 80.9375% (259/320)\n",
            "5 14 Epoch: 9 | ANN: trainLoss: 0.4378 | trainAcc: 79.1667% (304/384)\n",
            "6 14 Epoch: 9 | ANN: trainLoss: 0.4595 | trainAcc: 77.2321% (346/448)\n",
            "7 14 Epoch: 9 | ANN: trainLoss: 0.4575 | trainAcc: 76.5625% (392/512)\n",
            "8 14 Epoch: 9 | ANN: trainLoss: 0.4650 | trainAcc: 75.8681% (437/576)\n",
            "9 14 Epoch: 9 | ANN: trainLoss: 0.4675 | trainAcc: 75.7812% (485/640)\n",
            "10 14 Epoch: 9 | ANN: trainLoss: 0.4679 | trainAcc: 75.8523% (534/704)\n",
            "11 14 Epoch: 9 | ANN: trainLoss: 0.4647 | trainAcc: 76.4323% (587/768)\n",
            "12 14 Epoch: 9 | ANN: trainLoss: 0.4656 | trainAcc: 76.0817% (633/832)\n",
            "13 14 Epoch: 9 | ANN: trainLoss: 0.4605 | trainAcc: 76.2673% (662/868)\n",
            "0 2 Epoch: 9 | ANN: testLoss: 0.6049 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 9 | ANN: testLoss: 0.6123 | testAcc: 58.7629% (57/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 10 | ANN: trainLoss: 0.3798 | trainAcc: 84.3750% (54/64)\n",
            "1 14 Epoch: 10 | ANN: trainLoss: 0.4076 | trainAcc: 78.9062% (101/128)\n",
            "2 14 Epoch: 10 | ANN: trainLoss: 0.4533 | trainAcc: 76.0417% (146/192)\n",
            "3 14 Epoch: 10 | ANN: trainLoss: 0.4621 | trainAcc: 76.5625% (196/256)\n",
            "4 14 Epoch: 10 | ANN: trainLoss: 0.4603 | trainAcc: 75.6250% (242/320)\n",
            "5 14 Epoch: 10 | ANN: trainLoss: 0.4583 | trainAcc: 76.3021% (293/384)\n",
            "6 14 Epoch: 10 | ANN: trainLoss: 0.4626 | trainAcc: 75.6696% (339/448)\n",
            "7 14 Epoch: 10 | ANN: trainLoss: 0.4599 | trainAcc: 76.3672% (391/512)\n",
            "8 14 Epoch: 10 | ANN: trainLoss: 0.4785 | trainAcc: 75.8681% (437/576)\n",
            "9 14 Epoch: 10 | ANN: trainLoss: 0.4825 | trainAcc: 75.0000% (480/640)\n",
            "10 14 Epoch: 10 | ANN: trainLoss: 0.4766 | trainAcc: 75.2841% (530/704)\n",
            "11 14 Epoch: 10 | ANN: trainLoss: 0.4802 | trainAcc: 75.2604% (578/768)\n",
            "12 14 Epoch: 10 | ANN: trainLoss: 0.4787 | trainAcc: 75.2404% (626/832)\n",
            "13 14 Epoch: 10 | ANN: trainLoss: 0.4777 | trainAcc: 75.1152% (652/868)\n",
            "0 2 Epoch: 10 | ANN: testLoss: 0.5612 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 10 | ANN: testLoss: 0.5252 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 11 | ANN: trainLoss: 0.3854 | trainAcc: 78.1250% (50/64)\n",
            "1 14 Epoch: 11 | ANN: trainLoss: 0.4718 | trainAcc: 75.7812% (97/128)\n",
            "2 14 Epoch: 11 | ANN: trainLoss: 0.4392 | trainAcc: 78.6458% (151/192)\n",
            "3 14 Epoch: 11 | ANN: trainLoss: 0.4403 | trainAcc: 77.7344% (199/256)\n",
            "4 14 Epoch: 11 | ANN: trainLoss: 0.4421 | trainAcc: 78.7500% (252/320)\n",
            "5 14 Epoch: 11 | ANN: trainLoss: 0.4382 | trainAcc: 78.6458% (302/384)\n",
            "6 14 Epoch: 11 | ANN: trainLoss: 0.4260 | trainAcc: 79.6875% (357/448)\n",
            "7 14 Epoch: 11 | ANN: trainLoss: 0.4324 | trainAcc: 79.8828% (409/512)\n",
            "8 14 Epoch: 11 | ANN: trainLoss: 0.4424 | trainAcc: 78.9931% (455/576)\n",
            "9 14 Epoch: 11 | ANN: trainLoss: 0.4469 | trainAcc: 78.9062% (505/640)\n",
            "10 14 Epoch: 11 | ANN: trainLoss: 0.4412 | trainAcc: 79.4034% (559/704)\n",
            "11 14 Epoch: 11 | ANN: trainLoss: 0.4383 | trainAcc: 79.5573% (611/768)\n",
            "12 14 Epoch: 11 | ANN: trainLoss: 0.4373 | trainAcc: 79.4471% (661/832)\n",
            "13 14 Epoch: 11 | ANN: trainLoss: 0.4390 | trainAcc: 79.3779% (689/868)\n",
            "0 2 Epoch: 11 | ANN: testLoss: 0.5398 | testAcc: 70.3125% (45/64)\n",
            "1 2 Epoch: 11 | ANN: testLoss: 0.6460 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 12 | ANN: trainLoss: 0.3952 | trainAcc: 81.2500% (52/64)\n",
            "1 14 Epoch: 12 | ANN: trainLoss: 0.4257 | trainAcc: 79.6875% (102/128)\n",
            "2 14 Epoch: 12 | ANN: trainLoss: 0.4358 | trainAcc: 79.1667% (152/192)\n",
            "3 14 Epoch: 12 | ANN: trainLoss: 0.4392 | trainAcc: 78.1250% (200/256)\n",
            "4 14 Epoch: 12 | ANN: trainLoss: 0.4394 | trainAcc: 77.8125% (249/320)\n",
            "5 14 Epoch: 12 | ANN: trainLoss: 0.4320 | trainAcc: 79.4271% (305/384)\n",
            "6 14 Epoch: 12 | ANN: trainLoss: 0.4293 | trainAcc: 79.6875% (357/448)\n",
            "7 14 Epoch: 12 | ANN: trainLoss: 0.4314 | trainAcc: 79.2969% (406/512)\n",
            "8 14 Epoch: 12 | ANN: trainLoss: 0.4222 | trainAcc: 80.0347% (461/576)\n",
            "9 14 Epoch: 12 | ANN: trainLoss: 0.4213 | trainAcc: 80.3125% (514/640)\n",
            "10 14 Epoch: 12 | ANN: trainLoss: 0.4205 | trainAcc: 80.2557% (565/704)\n",
            "11 14 Epoch: 12 | ANN: trainLoss: 0.4200 | trainAcc: 80.2083% (616/768)\n",
            "12 14 Epoch: 12 | ANN: trainLoss: 0.4192 | trainAcc: 80.1683% (667/832)\n",
            "13 14 Epoch: 12 | ANN: trainLoss: 0.4132 | trainAcc: 80.5300% (699/868)\n",
            "0 2 Epoch: 12 | ANN: testLoss: 0.5415 | testAcc: 65.6250% (42/64)\n",
            "1 2 Epoch: 12 | ANN: testLoss: 0.5676 | testAcc: 67.0103% (65/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 13 | ANN: trainLoss: 0.3631 | trainAcc: 87.5000% (56/64)\n",
            "1 14 Epoch: 13 | ANN: trainLoss: 0.3763 | trainAcc: 85.1562% (109/128)\n",
            "2 14 Epoch: 13 | ANN: trainLoss: 0.4262 | trainAcc: 78.1250% (150/192)\n",
            "3 14 Epoch: 13 | ANN: trainLoss: 0.4328 | trainAcc: 76.9531% (197/256)\n",
            "4 14 Epoch: 13 | ANN: trainLoss: 0.4303 | trainAcc: 76.8750% (246/320)\n",
            "5 14 Epoch: 13 | ANN: trainLoss: 0.4174 | trainAcc: 77.8646% (299/384)\n",
            "6 14 Epoch: 13 | ANN: trainLoss: 0.4140 | trainAcc: 78.5714% (352/448)\n",
            "7 14 Epoch: 13 | ANN: trainLoss: 0.4313 | trainAcc: 78.1250% (400/512)\n",
            "8 14 Epoch: 13 | ANN: trainLoss: 0.4212 | trainAcc: 79.3403% (457/576)\n",
            "9 14 Epoch: 13 | ANN: trainLoss: 0.4205 | trainAcc: 79.3750% (508/640)\n",
            "10 14 Epoch: 13 | ANN: trainLoss: 0.4165 | trainAcc: 79.4034% (559/704)\n",
            "11 14 Epoch: 13 | ANN: trainLoss: 0.4182 | trainAcc: 79.5573% (611/768)\n",
            "12 14 Epoch: 13 | ANN: trainLoss: 0.4138 | trainAcc: 80.1683% (667/832)\n",
            "13 14 Epoch: 13 | ANN: trainLoss: 0.4109 | trainAcc: 80.0691% (695/868)\n",
            "0 2 Epoch: 13 | ANN: testLoss: 0.5530 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 13 | ANN: testLoss: 0.5451 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 14 | ANN: trainLoss: 0.3712 | trainAcc: 79.6875% (51/64)\n",
            "1 14 Epoch: 14 | ANN: trainLoss: 0.3466 | trainAcc: 79.6875% (102/128)\n",
            "2 14 Epoch: 14 | ANN: trainLoss: 0.3663 | trainAcc: 80.2083% (154/192)\n",
            "3 14 Epoch: 14 | ANN: trainLoss: 0.3733 | trainAcc: 80.0781% (205/256)\n",
            "4 14 Epoch: 14 | ANN: trainLoss: 0.3737 | trainAcc: 80.9375% (259/320)\n",
            "5 14 Epoch: 14 | ANN: trainLoss: 0.3956 | trainAcc: 80.7292% (310/384)\n",
            "6 14 Epoch: 14 | ANN: trainLoss: 0.3906 | trainAcc: 81.0268% (363/448)\n",
            "7 14 Epoch: 14 | ANN: trainLoss: 0.3897 | trainAcc: 81.0547% (415/512)\n",
            "8 14 Epoch: 14 | ANN: trainLoss: 0.4079 | trainAcc: 79.1667% (456/576)\n",
            "9 14 Epoch: 14 | ANN: trainLoss: 0.4019 | trainAcc: 79.5312% (509/640)\n",
            "10 14 Epoch: 14 | ANN: trainLoss: 0.4095 | trainAcc: 78.9773% (556/704)\n",
            "11 14 Epoch: 14 | ANN: trainLoss: 0.4115 | trainAcc: 78.7760% (605/768)\n",
            "12 14 Epoch: 14 | ANN: trainLoss: 0.4100 | trainAcc: 79.2067% (659/832)\n",
            "13 14 Epoch: 14 | ANN: trainLoss: 0.4084 | trainAcc: 79.1475% (687/868)\n",
            "0 2 Epoch: 14 | ANN: testLoss: 0.5382 | testAcc: 65.6250% (42/64)\n",
            "1 2 Epoch: 14 | ANN: testLoss: 0.5414 | testAcc: 67.0103% (65/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 15 | ANN: trainLoss: 0.3767 | trainAcc: 79.6875% (51/64)\n",
            "1 14 Epoch: 15 | ANN: trainLoss: 0.4049 | trainAcc: 81.2500% (104/128)\n",
            "2 14 Epoch: 15 | ANN: trainLoss: 0.3984 | trainAcc: 81.7708% (157/192)\n",
            "3 14 Epoch: 15 | ANN: trainLoss: 0.3953 | trainAcc: 83.2031% (213/256)\n",
            "4 14 Epoch: 15 | ANN: trainLoss: 0.4180 | trainAcc: 81.5625% (261/320)\n",
            "5 14 Epoch: 15 | ANN: trainLoss: 0.4071 | trainAcc: 81.7708% (314/384)\n",
            "6 14 Epoch: 15 | ANN: trainLoss: 0.4003 | trainAcc: 81.6964% (366/448)\n",
            "7 14 Epoch: 15 | ANN: trainLoss: 0.4067 | trainAcc: 81.2500% (416/512)\n",
            "8 14 Epoch: 15 | ANN: trainLoss: 0.4055 | trainAcc: 81.2500% (468/576)\n",
            "9 14 Epoch: 15 | ANN: trainLoss: 0.4034 | trainAcc: 81.4062% (521/640)\n",
            "10 14 Epoch: 15 | ANN: trainLoss: 0.4051 | trainAcc: 80.6818% (568/704)\n",
            "11 14 Epoch: 15 | ANN: trainLoss: 0.4000 | trainAcc: 81.1198% (623/768)\n",
            "12 14 Epoch: 15 | ANN: trainLoss: 0.3980 | trainAcc: 81.0096% (674/832)\n",
            "13 14 Epoch: 15 | ANN: trainLoss: 0.3968 | trainAcc: 81.1060% (704/868)\n",
            "0 2 Epoch: 15 | ANN: testLoss: 0.5093 | testAcc: 70.3125% (45/64)\n",
            "1 2 Epoch: 15 | ANN: testLoss: 0.5892 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 16 | ANN: trainLoss: 0.3151 | trainAcc: 84.3750% (54/64)\n",
            "1 14 Epoch: 16 | ANN: trainLoss: 0.3211 | trainAcc: 85.9375% (110/128)\n",
            "2 14 Epoch: 16 | ANN: trainLoss: 0.3383 | trainAcc: 85.9375% (165/192)\n",
            "3 14 Epoch: 16 | ANN: trainLoss: 0.3403 | trainAcc: 86.3281% (221/256)\n",
            "4 14 Epoch: 16 | ANN: trainLoss: 0.3468 | trainAcc: 85.3125% (273/320)\n",
            "5 14 Epoch: 16 | ANN: trainLoss: 0.3591 | trainAcc: 84.3750% (324/384)\n",
            "6 14 Epoch: 16 | ANN: trainLoss: 0.3580 | trainAcc: 84.5982% (379/448)\n",
            "7 14 Epoch: 16 | ANN: trainLoss: 0.3581 | trainAcc: 84.5703% (433/512)\n",
            "8 14 Epoch: 16 | ANN: trainLoss: 0.3537 | trainAcc: 84.8958% (489/576)\n",
            "9 14 Epoch: 16 | ANN: trainLoss: 0.3515 | trainAcc: 85.0000% (544/640)\n",
            "10 14 Epoch: 16 | ANN: trainLoss: 0.3548 | trainAcc: 84.6591% (596/704)\n",
            "11 14 Epoch: 16 | ANN: trainLoss: 0.3554 | trainAcc: 84.5052% (649/768)\n",
            "12 14 Epoch: 16 | ANN: trainLoss: 0.3579 | trainAcc: 84.4952% (703/832)\n",
            "13 14 Epoch: 16 | ANN: trainLoss: 0.3632 | trainAcc: 84.2166% (731/868)\n",
            "0 2 Epoch: 16 | ANN: testLoss: 0.7703 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 16 | ANN: testLoss: 0.6660 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 17 | ANN: trainLoss: 0.3399 | trainAcc: 81.2500% (52/64)\n",
            "1 14 Epoch: 17 | ANN: trainLoss: 0.3457 | trainAcc: 82.8125% (106/128)\n",
            "2 14 Epoch: 17 | ANN: trainLoss: 0.3349 | trainAcc: 86.4583% (166/192)\n",
            "3 14 Epoch: 17 | ANN: trainLoss: 0.3553 | trainAcc: 83.9844% (215/256)\n",
            "4 14 Epoch: 17 | ANN: trainLoss: 0.3494 | trainAcc: 84.6875% (271/320)\n",
            "5 14 Epoch: 17 | ANN: trainLoss: 0.3587 | trainAcc: 84.6354% (325/384)\n",
            "6 14 Epoch: 17 | ANN: trainLoss: 0.3569 | trainAcc: 84.8214% (380/448)\n",
            "7 14 Epoch: 17 | ANN: trainLoss: 0.3563 | trainAcc: 85.1562% (436/512)\n",
            "8 14 Epoch: 17 | ANN: trainLoss: 0.3608 | trainAcc: 84.3750% (486/576)\n",
            "9 14 Epoch: 17 | ANN: trainLoss: 0.3586 | trainAcc: 84.5312% (541/640)\n",
            "10 14 Epoch: 17 | ANN: trainLoss: 0.3546 | trainAcc: 85.0852% (599/704)\n",
            "11 14 Epoch: 17 | ANN: trainLoss: 0.3599 | trainAcc: 84.5052% (649/768)\n",
            "12 14 Epoch: 17 | ANN: trainLoss: 0.3588 | trainAcc: 84.7356% (705/832)\n",
            "13 14 Epoch: 17 | ANN: trainLoss: 0.3649 | trainAcc: 84.3318% (732/868)\n",
            "0 2 Epoch: 17 | ANN: testLoss: 0.5183 | testAcc: 64.0625% (41/64)\n",
            "1 2 Epoch: 17 | ANN: testLoss: 0.5454 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 18 | ANN: trainLoss: 0.3240 | trainAcc: 84.3750% (54/64)\n",
            "1 14 Epoch: 18 | ANN: trainLoss: 0.3388 | trainAcc: 82.8125% (106/128)\n",
            "2 14 Epoch: 18 | ANN: trainLoss: 0.3594 | trainAcc: 81.2500% (156/192)\n",
            "3 14 Epoch: 18 | ANN: trainLoss: 0.3647 | trainAcc: 81.2500% (208/256)\n",
            "4 14 Epoch: 18 | ANN: trainLoss: 0.3673 | trainAcc: 80.9375% (259/320)\n",
            "5 14 Epoch: 18 | ANN: trainLoss: 0.3625 | trainAcc: 82.5521% (317/384)\n",
            "6 14 Epoch: 18 | ANN: trainLoss: 0.3574 | trainAcc: 83.4821% (374/448)\n",
            "7 14 Epoch: 18 | ANN: trainLoss: 0.3621 | trainAcc: 83.3984% (427/512)\n",
            "8 14 Epoch: 18 | ANN: trainLoss: 0.3714 | trainAcc: 82.9861% (478/576)\n",
            "9 14 Epoch: 18 | ANN: trainLoss: 0.3665 | trainAcc: 83.4375% (534/640)\n",
            "10 14 Epoch: 18 | ANN: trainLoss: 0.3622 | trainAcc: 83.9489% (591/704)\n",
            "11 14 Epoch: 18 | ANN: trainLoss: 0.3608 | trainAcc: 84.3750% (648/768)\n",
            "12 14 Epoch: 18 | ANN: trainLoss: 0.3632 | trainAcc: 84.4952% (703/832)\n",
            "13 14 Epoch: 18 | ANN: trainLoss: 0.3679 | trainAcc: 84.2166% (731/868)\n",
            "0 2 Epoch: 18 | ANN: testLoss: 0.7050 | testAcc: 57.8125% (37/64)\n",
            "1 2 Epoch: 18 | ANN: testLoss: 0.6305 | testAcc: 62.8866% (61/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 19 | ANN: trainLoss: 0.3425 | trainAcc: 81.2500% (52/64)\n",
            "1 14 Epoch: 19 | ANN: trainLoss: 0.3697 | trainAcc: 83.5938% (107/128)\n",
            "2 14 Epoch: 19 | ANN: trainLoss: 0.3510 | trainAcc: 84.3750% (162/192)\n",
            "3 14 Epoch: 19 | ANN: trainLoss: 0.3638 | trainAcc: 84.7656% (217/256)\n",
            "4 14 Epoch: 19 | ANN: trainLoss: 0.3528 | trainAcc: 85.6250% (274/320)\n",
            "5 14 Epoch: 19 | ANN: trainLoss: 0.3375 | trainAcc: 86.4583% (332/384)\n",
            "6 14 Epoch: 19 | ANN: trainLoss: 0.3387 | trainAcc: 85.9375% (385/448)\n",
            "7 14 Epoch: 19 | ANN: trainLoss: 0.3371 | trainAcc: 86.1328% (441/512)\n",
            "8 14 Epoch: 19 | ANN: trainLoss: 0.3402 | trainAcc: 85.4167% (492/576)\n",
            "9 14 Epoch: 19 | ANN: trainLoss: 0.3364 | trainAcc: 85.9375% (550/640)\n",
            "10 14 Epoch: 19 | ANN: trainLoss: 0.3356 | trainAcc: 85.6534% (603/704)\n",
            "11 14 Epoch: 19 | ANN: trainLoss: 0.3380 | trainAcc: 85.1562% (654/768)\n",
            "12 14 Epoch: 19 | ANN: trainLoss: 0.3427 | trainAcc: 85.0962% (708/832)\n",
            "13 14 Epoch: 19 | ANN: trainLoss: 0.3478 | trainAcc: 84.9078% (737/868)\n",
            "0 2 Epoch: 19 | ANN: testLoss: 0.6628 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 19 | ANN: testLoss: 0.5195 | testAcc: 67.0103% (65/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 20 | ANN: trainLoss: 0.5006 | trainAcc: 75.0000% (48/64)\n",
            "1 14 Epoch: 20 | ANN: trainLoss: 0.4356 | trainAcc: 78.9062% (101/128)\n",
            "2 14 Epoch: 20 | ANN: trainLoss: 0.3978 | trainAcc: 81.7708% (157/192)\n",
            "3 14 Epoch: 20 | ANN: trainLoss: 0.3719 | trainAcc: 83.5938% (214/256)\n",
            "4 14 Epoch: 20 | ANN: trainLoss: 0.3813 | trainAcc: 82.1875% (263/320)\n",
            "5 14 Epoch: 20 | ANN: trainLoss: 0.3752 | trainAcc: 82.2917% (316/384)\n",
            "6 14 Epoch: 20 | ANN: trainLoss: 0.3690 | trainAcc: 82.5893% (370/448)\n",
            "7 14 Epoch: 20 | ANN: trainLoss: 0.3663 | trainAcc: 83.0078% (425/512)\n",
            "8 14 Epoch: 20 | ANN: trainLoss: 0.3624 | trainAcc: 83.5069% (481/576)\n",
            "9 14 Epoch: 20 | ANN: trainLoss: 0.3602 | trainAcc: 83.9062% (537/640)\n",
            "10 14 Epoch: 20 | ANN: trainLoss: 0.3555 | trainAcc: 84.5170% (595/704)\n",
            "11 14 Epoch: 20 | ANN: trainLoss: 0.3446 | trainAcc: 85.4167% (656/768)\n",
            "12 14 Epoch: 20 | ANN: trainLoss: 0.3454 | trainAcc: 85.3365% (710/832)\n",
            "13 14 Epoch: 20 | ANN: trainLoss: 0.3535 | trainAcc: 84.5622% (734/868)\n",
            "0 2 Epoch: 20 | ANN: testLoss: 0.6493 | testAcc: 65.6250% (42/64)\n",
            "1 2 Epoch: 20 | ANN: testLoss: 0.5896 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 21 | ANN: trainLoss: 0.3924 | trainAcc: 85.9375% (55/64)\n",
            "1 14 Epoch: 21 | ANN: trainLoss: 0.3387 | trainAcc: 89.0625% (114/128)\n",
            "2 14 Epoch: 21 | ANN: trainLoss: 0.3315 | trainAcc: 87.5000% (168/192)\n",
            "3 14 Epoch: 21 | ANN: trainLoss: 0.3332 | trainAcc: 86.7188% (222/256)\n",
            "4 14 Epoch: 21 | ANN: trainLoss: 0.3383 | trainAcc: 86.5625% (277/320)\n",
            "5 14 Epoch: 21 | ANN: trainLoss: 0.3403 | trainAcc: 85.9375% (330/384)\n",
            "6 14 Epoch: 21 | ANN: trainLoss: 0.3387 | trainAcc: 86.6071% (388/448)\n",
            "7 14 Epoch: 21 | ANN: trainLoss: 0.3345 | trainAcc: 86.1328% (441/512)\n",
            "8 14 Epoch: 21 | ANN: trainLoss: 0.3323 | trainAcc: 86.4583% (498/576)\n",
            "9 14 Epoch: 21 | ANN: trainLoss: 0.3378 | trainAcc: 85.9375% (550/640)\n",
            "10 14 Epoch: 21 | ANN: trainLoss: 0.3340 | trainAcc: 85.9375% (605/704)\n",
            "11 14 Epoch: 21 | ANN: trainLoss: 0.3358 | trainAcc: 85.9375% (660/768)\n",
            "12 14 Epoch: 21 | ANN: trainLoss: 0.3364 | trainAcc: 85.5769% (712/832)\n",
            "13 14 Epoch: 21 | ANN: trainLoss: 0.3391 | trainAcc: 85.3687% (741/868)\n",
            "0 2 Epoch: 21 | ANN: testLoss: 0.6305 | testAcc: 68.7500% (44/64)\n",
            "1 2 Epoch: 21 | ANN: testLoss: 0.5747 | testAcc: 69.0722% (67/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 22 | ANN: trainLoss: 0.3206 | trainAcc: 87.5000% (56/64)\n",
            "1 14 Epoch: 22 | ANN: trainLoss: 0.2795 | trainAcc: 89.0625% (114/128)\n",
            "2 14 Epoch: 22 | ANN: trainLoss: 0.3057 | trainAcc: 87.5000% (168/192)\n",
            "3 14 Epoch: 22 | ANN: trainLoss: 0.3146 | trainAcc: 87.8906% (225/256)\n",
            "4 14 Epoch: 22 | ANN: trainLoss: 0.3003 | trainAcc: 88.7500% (284/320)\n",
            "5 14 Epoch: 22 | ANN: trainLoss: 0.2990 | trainAcc: 88.5417% (340/384)\n",
            "6 14 Epoch: 22 | ANN: trainLoss: 0.3046 | trainAcc: 88.1696% (395/448)\n",
            "7 14 Epoch: 22 | ANN: trainLoss: 0.3122 | trainAcc: 87.5000% (448/512)\n",
            "8 14 Epoch: 22 | ANN: trainLoss: 0.3080 | trainAcc: 87.8472% (506/576)\n",
            "9 14 Epoch: 22 | ANN: trainLoss: 0.3115 | trainAcc: 87.6562% (561/640)\n",
            "10 14 Epoch: 22 | ANN: trainLoss: 0.3095 | trainAcc: 87.9261% (619/704)\n",
            "11 14 Epoch: 22 | ANN: trainLoss: 0.3092 | trainAcc: 87.3698% (671/768)\n",
            "12 14 Epoch: 22 | ANN: trainLoss: 0.3080 | trainAcc: 87.8606% (731/832)\n",
            "13 14 Epoch: 22 | ANN: trainLoss: 0.3107 | trainAcc: 87.7880% (762/868)\n",
            "0 2 Epoch: 22 | ANN: testLoss: 0.4863 | testAcc: 68.7500% (44/64)\n",
            "1 2 Epoch: 22 | ANN: testLoss: 0.6581 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 23 | ANN: trainLoss: 0.2871 | trainAcc: 93.7500% (60/64)\n",
            "1 14 Epoch: 23 | ANN: trainLoss: 0.2529 | trainAcc: 94.5312% (121/128)\n",
            "2 14 Epoch: 23 | ANN: trainLoss: 0.2794 | trainAcc: 90.1042% (173/192)\n",
            "3 14 Epoch: 23 | ANN: trainLoss: 0.2743 | trainAcc: 89.8438% (230/256)\n",
            "4 14 Epoch: 23 | ANN: trainLoss: 0.2759 | trainAcc: 90.3125% (289/320)\n",
            "5 14 Epoch: 23 | ANN: trainLoss: 0.2744 | trainAcc: 90.6250% (348/384)\n",
            "6 14 Epoch: 23 | ANN: trainLoss: 0.2766 | trainAcc: 91.0714% (408/448)\n",
            "7 14 Epoch: 23 | ANN: trainLoss: 0.2790 | trainAcc: 90.6250% (464/512)\n",
            "8 14 Epoch: 23 | ANN: trainLoss: 0.2787 | trainAcc: 90.7986% (523/576)\n",
            "9 14 Epoch: 23 | ANN: trainLoss: 0.2958 | trainAcc: 89.0625% (570/640)\n",
            "10 14 Epoch: 23 | ANN: trainLoss: 0.2947 | trainAcc: 89.0625% (627/704)\n",
            "11 14 Epoch: 23 | ANN: trainLoss: 0.3010 | trainAcc: 88.1510% (677/768)\n",
            "12 14 Epoch: 23 | ANN: trainLoss: 0.3080 | trainAcc: 87.7404% (730/832)\n",
            "13 14 Epoch: 23 | ANN: trainLoss: 0.3214 | trainAcc: 87.2120% (757/868)\n",
            "0 2 Epoch: 23 | ANN: testLoss: 0.5999 | testAcc: 65.6250% (42/64)\n",
            "1 2 Epoch: 23 | ANN: testLoss: 0.6271 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 24 | ANN: trainLoss: 0.2837 | trainAcc: 87.5000% (56/64)\n",
            "1 14 Epoch: 24 | ANN: trainLoss: 0.3117 | trainAcc: 86.7188% (111/128)\n",
            "2 14 Epoch: 24 | ANN: trainLoss: 0.2883 | trainAcc: 87.5000% (168/192)\n",
            "3 14 Epoch: 24 | ANN: trainLoss: 0.2733 | trainAcc: 88.2812% (226/256)\n",
            "4 14 Epoch: 24 | ANN: trainLoss: 0.2803 | trainAcc: 88.7500% (284/320)\n",
            "5 14 Epoch: 24 | ANN: trainLoss: 0.2821 | trainAcc: 88.0208% (338/384)\n",
            "6 14 Epoch: 24 | ANN: trainLoss: 0.2819 | trainAcc: 88.1696% (395/448)\n",
            "7 14 Epoch: 24 | ANN: trainLoss: 0.2914 | trainAcc: 87.3047% (447/512)\n",
            "8 14 Epoch: 24 | ANN: trainLoss: 0.2888 | trainAcc: 86.9792% (501/576)\n",
            "9 14 Epoch: 24 | ANN: trainLoss: 0.2881 | trainAcc: 87.5000% (560/640)\n",
            "10 14 Epoch: 24 | ANN: trainLoss: 0.2852 | trainAcc: 87.6420% (617/704)\n",
            "11 14 Epoch: 24 | ANN: trainLoss: 0.2941 | trainAcc: 87.3698% (671/768)\n",
            "12 14 Epoch: 24 | ANN: trainLoss: 0.2931 | trainAcc: 87.5000% (728/832)\n",
            "13 14 Epoch: 24 | ANN: trainLoss: 0.3098 | trainAcc: 87.3272% (758/868)\n",
            "0 2 Epoch: 24 | ANN: testLoss: 0.5644 | testAcc: 71.8750% (46/64)\n",
            "1 2 Epoch: 24 | ANN: testLoss: 0.8459 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 25 | ANN: trainLoss: 0.2781 | trainAcc: 90.6250% (58/64)\n",
            "1 14 Epoch: 25 | ANN: trainLoss: 0.2948 | trainAcc: 87.5000% (112/128)\n",
            "2 14 Epoch: 25 | ANN: trainLoss: 0.3024 | trainAcc: 85.9375% (165/192)\n",
            "3 14 Epoch: 25 | ANN: trainLoss: 0.2939 | trainAcc: 87.1094% (223/256)\n",
            "4 14 Epoch: 25 | ANN: trainLoss: 0.3095 | trainAcc: 86.5625% (277/320)\n",
            "5 14 Epoch: 25 | ANN: trainLoss: 0.3065 | trainAcc: 87.2396% (335/384)\n",
            "6 14 Epoch: 25 | ANN: trainLoss: 0.3001 | trainAcc: 87.7232% (393/448)\n",
            "7 14 Epoch: 25 | ANN: trainLoss: 0.2975 | trainAcc: 87.8906% (450/512)\n",
            "8 14 Epoch: 25 | ANN: trainLoss: 0.2911 | trainAcc: 88.1944% (508/576)\n",
            "9 14 Epoch: 25 | ANN: trainLoss: 0.2829 | trainAcc: 89.0625% (570/640)\n",
            "10 14 Epoch: 25 | ANN: trainLoss: 0.2864 | trainAcc: 89.0625% (627/704)\n",
            "11 14 Epoch: 25 | ANN: trainLoss: 0.2854 | trainAcc: 88.8021% (682/768)\n",
            "12 14 Epoch: 25 | ANN: trainLoss: 0.2855 | trainAcc: 88.7019% (738/832)\n",
            "13 14 Epoch: 25 | ANN: trainLoss: 0.2879 | trainAcc: 88.4793% (768/868)\n",
            "0 2 Epoch: 25 | ANN: testLoss: 0.7842 | testAcc: 64.0625% (41/64)\n",
            "1 2 Epoch: 25 | ANN: testLoss: 0.6439 | testAcc: 68.0412% (66/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 26 | ANN: trainLoss: 0.2437 | trainAcc: 90.6250% (58/64)\n",
            "1 14 Epoch: 26 | ANN: trainLoss: 0.2483 | trainAcc: 92.1875% (118/128)\n",
            "2 14 Epoch: 26 | ANN: trainLoss: 0.2499 | trainAcc: 90.6250% (174/192)\n",
            "3 14 Epoch: 26 | ANN: trainLoss: 0.2595 | trainAcc: 89.8438% (230/256)\n",
            "4 14 Epoch: 26 | ANN: trainLoss: 0.2390 | trainAcc: 91.2500% (292/320)\n",
            "5 14 Epoch: 26 | ANN: trainLoss: 0.2495 | trainAcc: 90.6250% (348/384)\n",
            "6 14 Epoch: 26 | ANN: trainLoss: 0.2533 | trainAcc: 90.4018% (405/448)\n",
            "7 14 Epoch: 26 | ANN: trainLoss: 0.2589 | trainAcc: 90.2344% (462/512)\n",
            "8 14 Epoch: 26 | ANN: trainLoss: 0.2573 | trainAcc: 90.7986% (523/576)\n",
            "9 14 Epoch: 26 | ANN: trainLoss: 0.2556 | trainAcc: 91.2500% (584/640)\n",
            "10 14 Epoch: 26 | ANN: trainLoss: 0.2596 | trainAcc: 91.0511% (641/704)\n",
            "11 14 Epoch: 26 | ANN: trainLoss: 0.2680 | trainAcc: 90.6250% (696/768)\n",
            "12 14 Epoch: 26 | ANN: trainLoss: 0.2749 | trainAcc: 89.9038% (748/832)\n",
            "13 14 Epoch: 26 | ANN: trainLoss: 0.2740 | trainAcc: 89.8618% (780/868)\n",
            "0 2 Epoch: 26 | ANN: testLoss: 0.6372 | testAcc: 68.7500% (44/64)\n",
            "1 2 Epoch: 26 | ANN: testLoss: 0.6826 | testAcc: 67.0103% (65/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 27 | ANN: trainLoss: 0.3832 | trainAcc: 81.2500% (52/64)\n",
            "1 14 Epoch: 27 | ANN: trainLoss: 0.3218 | trainAcc: 85.1562% (109/128)\n",
            "2 14 Epoch: 27 | ANN: trainLoss: 0.2965 | trainAcc: 86.9792% (167/192)\n",
            "3 14 Epoch: 27 | ANN: trainLoss: 0.2872 | trainAcc: 87.5000% (224/256)\n",
            "4 14 Epoch: 27 | ANN: trainLoss: 0.2677 | trainAcc: 88.4375% (283/320)\n",
            "5 14 Epoch: 27 | ANN: trainLoss: 0.2708 | trainAcc: 88.8021% (341/384)\n",
            "6 14 Epoch: 27 | ANN: trainLoss: 0.2654 | trainAcc: 89.2857% (400/448)\n",
            "7 14 Epoch: 27 | ANN: trainLoss: 0.2665 | trainAcc: 89.6484% (459/512)\n",
            "8 14 Epoch: 27 | ANN: trainLoss: 0.2690 | trainAcc: 89.4097% (515/576)\n",
            "9 14 Epoch: 27 | ANN: trainLoss: 0.2707 | trainAcc: 89.3750% (572/640)\n",
            "10 14 Epoch: 27 | ANN: trainLoss: 0.2773 | trainAcc: 89.3466% (629/704)\n",
            "11 14 Epoch: 27 | ANN: trainLoss: 0.2745 | trainAcc: 89.4531% (687/768)\n",
            "12 14 Epoch: 27 | ANN: trainLoss: 0.2749 | trainAcc: 89.6635% (746/832)\n",
            "13 14 Epoch: 27 | ANN: trainLoss: 0.2735 | trainAcc: 89.6313% (778/868)\n",
            "0 2 Epoch: 27 | ANN: testLoss: 0.8685 | testAcc: 67.1875% (43/64)\n",
            "1 2 Epoch: 27 | ANN: testLoss: 0.7277 | testAcc: 68.0412% (66/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 28 | ANN: trainLoss: 0.2225 | trainAcc: 92.1875% (59/64)\n",
            "1 14 Epoch: 28 | ANN: trainLoss: 0.2615 | trainAcc: 88.2812% (113/128)\n",
            "2 14 Epoch: 28 | ANN: trainLoss: 0.2518 | trainAcc: 89.0625% (171/192)\n",
            "3 14 Epoch: 28 | ANN: trainLoss: 0.2576 | trainAcc: 89.0625% (228/256)\n",
            "4 14 Epoch: 28 | ANN: trainLoss: 0.2469 | trainAcc: 90.0000% (288/320)\n",
            "5 14 Epoch: 28 | ANN: trainLoss: 0.2522 | trainAcc: 89.5833% (344/384)\n",
            "6 14 Epoch: 28 | ANN: trainLoss: 0.2488 | trainAcc: 89.7321% (402/448)\n",
            "7 14 Epoch: 28 | ANN: trainLoss: 0.2527 | trainAcc: 89.6484% (459/512)\n",
            "8 14 Epoch: 28 | ANN: trainLoss: 0.2609 | trainAcc: 88.7153% (511/576)\n",
            "9 14 Epoch: 28 | ANN: trainLoss: 0.2588 | trainAcc: 88.9062% (569/640)\n",
            "10 14 Epoch: 28 | ANN: trainLoss: 0.2576 | trainAcc: 89.3466% (629/704)\n",
            "11 14 Epoch: 28 | ANN: trainLoss: 0.2560 | trainAcc: 89.5833% (688/768)\n",
            "12 14 Epoch: 28 | ANN: trainLoss: 0.2598 | trainAcc: 89.5433% (745/832)\n",
            "13 14 Epoch: 28 | ANN: trainLoss: 0.2680 | trainAcc: 89.5161% (777/868)\n",
            "0 2 Epoch: 28 | ANN: testLoss: 0.6691 | testAcc: 65.6250% (42/64)\n",
            "1 2 Epoch: 28 | ANN: testLoss: 0.8000 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 29 | ANN: trainLoss: 0.1908 | trainAcc: 96.8750% (62/64)\n",
            "1 14 Epoch: 29 | ANN: trainLoss: 0.2514 | trainAcc: 92.1875% (118/128)\n",
            "2 14 Epoch: 29 | ANN: trainLoss: 0.2318 | trainAcc: 93.2292% (179/192)\n",
            "3 14 Epoch: 29 | ANN: trainLoss: 0.2492 | trainAcc: 89.8438% (230/256)\n",
            "4 14 Epoch: 29 | ANN: trainLoss: 0.2487 | trainAcc: 90.6250% (290/320)\n",
            "5 14 Epoch: 29 | ANN: trainLoss: 0.2563 | trainAcc: 90.6250% (348/384)\n",
            "6 14 Epoch: 29 | ANN: trainLoss: 0.2561 | trainAcc: 90.6250% (406/448)\n",
            "7 14 Epoch: 29 | ANN: trainLoss: 0.2545 | trainAcc: 90.6250% (464/512)\n",
            "8 14 Epoch: 29 | ANN: trainLoss: 0.2530 | trainAcc: 90.6250% (522/576)\n",
            "9 14 Epoch: 29 | ANN: trainLoss: 0.2525 | trainAcc: 90.9375% (582/640)\n",
            "10 14 Epoch: 29 | ANN: trainLoss: 0.2460 | trainAcc: 91.0511% (641/704)\n",
            "11 14 Epoch: 29 | ANN: trainLoss: 0.2465 | trainAcc: 91.4062% (702/768)\n",
            "12 14 Epoch: 29 | ANN: trainLoss: 0.2529 | trainAcc: 90.9856% (757/832)\n",
            "13 14 Epoch: 29 | ANN: trainLoss: 0.2586 | trainAcc: 91.0138% (790/868)\n",
            "0 2 Epoch: 29 | ANN: testLoss: 0.6215 | testAcc: 70.3125% (45/64)\n",
            "1 2 Epoch: 29 | ANN: testLoss: 0.6628 | testAcc: 68.0412% (66/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 30 | ANN: trainLoss: 0.2517 | trainAcc: 87.5000% (56/64)\n",
            "1 14 Epoch: 30 | ANN: trainLoss: 0.2269 | trainAcc: 91.4062% (117/128)\n",
            "2 14 Epoch: 30 | ANN: trainLoss: 0.2539 | trainAcc: 89.0625% (171/192)\n",
            "3 14 Epoch: 30 | ANN: trainLoss: 0.2549 | trainAcc: 89.8438% (230/256)\n",
            "4 14 Epoch: 30 | ANN: trainLoss: 0.2593 | trainAcc: 89.6875% (287/320)\n",
            "5 14 Epoch: 30 | ANN: trainLoss: 0.2488 | trainAcc: 90.1042% (346/384)\n",
            "6 14 Epoch: 30 | ANN: trainLoss: 0.2399 | trainAcc: 90.4018% (405/448)\n",
            "7 14 Epoch: 30 | ANN: trainLoss: 0.2398 | trainAcc: 90.6250% (464/512)\n",
            "8 14 Epoch: 30 | ANN: trainLoss: 0.2406 | trainAcc: 90.6250% (522/576)\n",
            "9 14 Epoch: 30 | ANN: trainLoss: 0.2419 | trainAcc: 90.4688% (579/640)\n",
            "10 14 Epoch: 30 | ANN: trainLoss: 0.2477 | trainAcc: 90.4830% (637/704)\n",
            "11 14 Epoch: 30 | ANN: trainLoss: 0.2428 | trainAcc: 91.0156% (699/768)\n",
            "12 14 Epoch: 30 | ANN: trainLoss: 0.2428 | trainAcc: 90.9856% (757/832)\n",
            "13 14 Epoch: 30 | ANN: trainLoss: 0.2432 | trainAcc: 90.8986% (789/868)\n",
            "0 2 Epoch: 30 | ANN: testLoss: 0.8407 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 30 | ANN: testLoss: 0.7805 | testAcc: 62.8866% (61/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 31 | ANN: trainLoss: 0.1971 | trainAcc: 93.7500% (60/64)\n",
            "1 14 Epoch: 31 | ANN: trainLoss: 0.2576 | trainAcc: 89.8438% (115/128)\n",
            "2 14 Epoch: 31 | ANN: trainLoss: 0.2469 | trainAcc: 91.6667% (176/192)\n",
            "3 14 Epoch: 31 | ANN: trainLoss: 0.2325 | trainAcc: 92.5781% (237/256)\n",
            "4 14 Epoch: 31 | ANN: trainLoss: 0.2228 | trainAcc: 92.8125% (297/320)\n",
            "5 14 Epoch: 31 | ANN: trainLoss: 0.2265 | trainAcc: 92.4479% (355/384)\n",
            "6 14 Epoch: 31 | ANN: trainLoss: 0.2207 | trainAcc: 93.0804% (417/448)\n",
            "7 14 Epoch: 31 | ANN: trainLoss: 0.2150 | trainAcc: 93.3594% (478/512)\n",
            "8 14 Epoch: 31 | ANN: trainLoss: 0.2139 | trainAcc: 93.4028% (538/576)\n",
            "9 14 Epoch: 31 | ANN: trainLoss: 0.2196 | trainAcc: 93.4375% (598/640)\n",
            "10 14 Epoch: 31 | ANN: trainLoss: 0.2263 | trainAcc: 93.0398% (655/704)\n",
            "11 14 Epoch: 31 | ANN: trainLoss: 0.2254 | trainAcc: 92.8385% (713/768)\n",
            "12 14 Epoch: 31 | ANN: trainLoss: 0.2218 | trainAcc: 93.2692% (776/832)\n",
            "13 14 Epoch: 31 | ANN: trainLoss: 0.2238 | trainAcc: 93.0876% (808/868)\n",
            "0 2 Epoch: 31 | ANN: testLoss: 0.6211 | testAcc: 68.7500% (44/64)\n",
            "1 2 Epoch: 31 | ANN: testLoss: 0.6893 | testAcc: 69.0722% (67/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 32 | ANN: trainLoss: 0.1900 | trainAcc: 95.3125% (61/64)\n",
            "1 14 Epoch: 32 | ANN: trainLoss: 0.2020 | trainAcc: 94.5312% (121/128)\n",
            "2 14 Epoch: 32 | ANN: trainLoss: 0.1919 | trainAcc: 94.2708% (181/192)\n",
            "3 14 Epoch: 32 | ANN: trainLoss: 0.2096 | trainAcc: 92.9688% (238/256)\n",
            "4 14 Epoch: 32 | ANN: trainLoss: 0.2111 | trainAcc: 92.8125% (297/320)\n",
            "5 14 Epoch: 32 | ANN: trainLoss: 0.2256 | trainAcc: 91.9271% (353/384)\n",
            "6 14 Epoch: 32 | ANN: trainLoss: 0.2139 | trainAcc: 92.6339% (415/448)\n",
            "7 14 Epoch: 32 | ANN: trainLoss: 0.2106 | trainAcc: 92.9688% (476/512)\n",
            "8 14 Epoch: 32 | ANN: trainLoss: 0.2077 | trainAcc: 93.4028% (538/576)\n",
            "9 14 Epoch: 32 | ANN: trainLoss: 0.2076 | trainAcc: 93.5938% (599/640)\n",
            "10 14 Epoch: 32 | ANN: trainLoss: 0.2080 | trainAcc: 93.7500% (660/704)\n",
            "11 14 Epoch: 32 | ANN: trainLoss: 0.2114 | trainAcc: 93.6198% (719/768)\n",
            "12 14 Epoch: 32 | ANN: trainLoss: 0.2111 | trainAcc: 93.3894% (777/832)\n",
            "13 14 Epoch: 32 | ANN: trainLoss: 0.2103 | trainAcc: 93.4332% (811/868)\n",
            "0 2 Epoch: 32 | ANN: testLoss: 0.6683 | testAcc: 67.1875% (43/64)\n",
            "1 2 Epoch: 32 | ANN: testLoss: 0.7032 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 33 | ANN: trainLoss: 0.1519 | trainAcc: 96.8750% (62/64)\n",
            "1 14 Epoch: 33 | ANN: trainLoss: 0.1591 | trainAcc: 95.3125% (122/128)\n",
            "2 14 Epoch: 33 | ANN: trainLoss: 0.1559 | trainAcc: 96.3542% (185/192)\n",
            "3 14 Epoch: 33 | ANN: trainLoss: 0.1551 | trainAcc: 96.4844% (247/256)\n",
            "4 14 Epoch: 33 | ANN: trainLoss: 0.1732 | trainAcc: 95.0000% (304/320)\n",
            "5 14 Epoch: 33 | ANN: trainLoss: 0.1718 | trainAcc: 95.3125% (366/384)\n",
            "6 14 Epoch: 33 | ANN: trainLoss: 0.1805 | trainAcc: 95.0893% (426/448)\n",
            "7 14 Epoch: 33 | ANN: trainLoss: 0.1875 | trainAcc: 94.7266% (485/512)\n",
            "8 14 Epoch: 33 | ANN: trainLoss: 0.1894 | trainAcc: 94.7917% (546/576)\n",
            "9 14 Epoch: 33 | ANN: trainLoss: 0.1950 | trainAcc: 94.6875% (606/640)\n",
            "10 14 Epoch: 33 | ANN: trainLoss: 0.1927 | trainAcc: 95.0284% (669/704)\n",
            "11 14 Epoch: 33 | ANN: trainLoss: 0.1943 | trainAcc: 94.7917% (728/768)\n",
            "12 14 Epoch: 33 | ANN: trainLoss: 0.1946 | trainAcc: 94.7115% (788/832)\n",
            "13 14 Epoch: 33 | ANN: trainLoss: 0.2097 | trainAcc: 94.1244% (817/868)\n",
            "0 2 Epoch: 33 | ANN: testLoss: 0.8424 | testAcc: 57.8125% (37/64)\n",
            "1 2 Epoch: 33 | ANN: testLoss: 0.6330 | testAcc: 63.9175% (62/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 34 | ANN: trainLoss: 0.2012 | trainAcc: 93.7500% (60/64)\n",
            "1 14 Epoch: 34 | ANN: trainLoss: 0.1748 | trainAcc: 96.0938% (123/128)\n",
            "2 14 Epoch: 34 | ANN: trainLoss: 0.1880 | trainAcc: 95.3125% (183/192)\n",
            "3 14 Epoch: 34 | ANN: trainLoss: 0.1898 | trainAcc: 95.3125% (244/256)\n",
            "4 14 Epoch: 34 | ANN: trainLoss: 0.1933 | trainAcc: 94.3750% (302/320)\n",
            "5 14 Epoch: 34 | ANN: trainLoss: 0.1954 | trainAcc: 94.7917% (364/384)\n",
            "6 14 Epoch: 34 | ANN: trainLoss: 0.2024 | trainAcc: 94.6429% (424/448)\n",
            "7 14 Epoch: 34 | ANN: trainLoss: 0.2033 | trainAcc: 94.5312% (484/512)\n",
            "8 14 Epoch: 34 | ANN: trainLoss: 0.2046 | trainAcc: 94.2708% (543/576)\n",
            "9 14 Epoch: 34 | ANN: trainLoss: 0.1969 | trainAcc: 94.3750% (604/640)\n",
            "10 14 Epoch: 34 | ANN: trainLoss: 0.1951 | trainAcc: 94.4602% (665/704)\n",
            "11 14 Epoch: 34 | ANN: trainLoss: 0.1982 | trainAcc: 94.2708% (724/768)\n",
            "12 14 Epoch: 34 | ANN: trainLoss: 0.1992 | trainAcc: 94.2308% (784/832)\n",
            "13 14 Epoch: 34 | ANN: trainLoss: 0.1986 | trainAcc: 94.1244% (817/868)\n",
            "0 2 Epoch: 34 | ANN: testLoss: 0.7355 | testAcc: 65.6250% (42/64)\n",
            "1 2 Epoch: 34 | ANN: testLoss: 0.8163 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 35 | ANN: trainLoss: 0.1627 | trainAcc: 96.8750% (62/64)\n",
            "1 14 Epoch: 35 | ANN: trainLoss: 0.1625 | trainAcc: 96.0938% (123/128)\n",
            "2 14 Epoch: 35 | ANN: trainLoss: 0.1843 | trainAcc: 94.2708% (181/192)\n",
            "3 14 Epoch: 35 | ANN: trainLoss: 0.1786 | trainAcc: 94.9219% (243/256)\n",
            "4 14 Epoch: 35 | ANN: trainLoss: 0.1762 | trainAcc: 95.6250% (306/320)\n",
            "5 14 Epoch: 35 | ANN: trainLoss: 0.1729 | trainAcc: 95.8333% (368/384)\n",
            "6 14 Epoch: 35 | ANN: trainLoss: 0.1778 | trainAcc: 95.7589% (429/448)\n",
            "7 14 Epoch: 35 | ANN: trainLoss: 0.1779 | trainAcc: 95.7031% (490/512)\n",
            "8 14 Epoch: 35 | ANN: trainLoss: 0.1810 | trainAcc: 95.4861% (550/576)\n",
            "9 14 Epoch: 35 | ANN: trainLoss: 0.1807 | trainAcc: 95.4688% (611/640)\n",
            "10 14 Epoch: 35 | ANN: trainLoss: 0.1761 | trainAcc: 95.5966% (673/704)\n",
            "11 14 Epoch: 35 | ANN: trainLoss: 0.1816 | trainAcc: 95.0521% (730/768)\n",
            "12 14 Epoch: 35 | ANN: trainLoss: 0.1843 | trainAcc: 94.8317% (789/832)\n",
            "13 14 Epoch: 35 | ANN: trainLoss: 0.1876 | trainAcc: 94.5853% (821/868)\n",
            "0 2 Epoch: 35 | ANN: testLoss: 0.5649 | testAcc: 71.8750% (46/64)\n",
            "1 2 Epoch: 35 | ANN: testLoss: 0.7754 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 36 | ANN: trainLoss: 0.1515 | trainAcc: 98.4375% (63/64)\n",
            "1 14 Epoch: 36 | ANN: trainLoss: 0.1315 | trainAcc: 99.2188% (127/128)\n",
            "2 14 Epoch: 36 | ANN: trainLoss: 0.1384 | trainAcc: 97.9167% (188/192)\n",
            "3 14 Epoch: 36 | ANN: trainLoss: 0.1350 | trainAcc: 98.4375% (252/256)\n",
            "4 14 Epoch: 36 | ANN: trainLoss: 0.1418 | trainAcc: 97.8125% (313/320)\n",
            "5 14 Epoch: 36 | ANN: trainLoss: 0.1755 | trainAcc: 95.5729% (367/384)\n",
            "6 14 Epoch: 36 | ANN: trainLoss: 0.1748 | trainAcc: 95.5357% (428/448)\n",
            "7 14 Epoch: 36 | ANN: trainLoss: 0.1815 | trainAcc: 94.9219% (486/512)\n",
            "8 14 Epoch: 36 | ANN: trainLoss: 0.1887 | trainAcc: 94.7917% (546/576)\n",
            "9 14 Epoch: 36 | ANN: trainLoss: 0.1843 | trainAcc: 95.0000% (608/640)\n",
            "10 14 Epoch: 36 | ANN: trainLoss: 0.1848 | trainAcc: 94.4602% (665/704)\n",
            "11 14 Epoch: 36 | ANN: trainLoss: 0.1833 | trainAcc: 94.6615% (727/768)\n",
            "12 14 Epoch: 36 | ANN: trainLoss: 0.1835 | trainAcc: 94.5913% (787/832)\n",
            "13 14 Epoch: 36 | ANN: trainLoss: 0.1856 | trainAcc: 94.5853% (821/868)\n",
            "0 2 Epoch: 36 | ANN: testLoss: 0.6548 | testAcc: 73.4375% (47/64)\n",
            "1 2 Epoch: 36 | ANN: testLoss: 0.8055 | testAcc: 69.0722% (67/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 37 | ANN: trainLoss: 0.2203 | trainAcc: 87.5000% (56/64)\n",
            "1 14 Epoch: 37 | ANN: trainLoss: 0.1725 | trainAcc: 92.9688% (119/128)\n",
            "2 14 Epoch: 37 | ANN: trainLoss: 0.1623 | trainAcc: 94.2708% (181/192)\n",
            "3 14 Epoch: 37 | ANN: trainLoss: 0.1611 | trainAcc: 94.5312% (242/256)\n",
            "4 14 Epoch: 37 | ANN: trainLoss: 0.1607 | trainAcc: 94.6875% (303/320)\n",
            "5 14 Epoch: 37 | ANN: trainLoss: 0.1719 | trainAcc: 94.0104% (361/384)\n",
            "6 14 Epoch: 37 | ANN: trainLoss: 0.1691 | trainAcc: 94.4196% (423/448)\n",
            "7 14 Epoch: 37 | ANN: trainLoss: 0.1632 | trainAcc: 94.9219% (486/512)\n",
            "8 14 Epoch: 37 | ANN: trainLoss: 0.1626 | trainAcc: 95.1389% (548/576)\n",
            "9 14 Epoch: 37 | ANN: trainLoss: 0.1629 | trainAcc: 95.0000% (608/640)\n",
            "10 14 Epoch: 37 | ANN: trainLoss: 0.1590 | trainAcc: 95.1705% (670/704)\n",
            "11 14 Epoch: 37 | ANN: trainLoss: 0.1603 | trainAcc: 95.4427% (733/768)\n",
            "12 14 Epoch: 37 | ANN: trainLoss: 0.1580 | trainAcc: 95.6731% (796/832)\n",
            "13 14 Epoch: 37 | ANN: trainLoss: 0.1614 | trainAcc: 95.5069% (829/868)\n",
            "0 2 Epoch: 37 | ANN: testLoss: 0.7894 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 37 | ANN: testLoss: 0.7310 | testAcc: 68.0412% (66/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 38 | ANN: trainLoss: 0.1078 | trainAcc: 98.4375% (63/64)\n",
            "1 14 Epoch: 38 | ANN: trainLoss: 0.1185 | trainAcc: 97.6562% (125/128)\n",
            "2 14 Epoch: 38 | ANN: trainLoss: 0.1441 | trainAcc: 96.3542% (185/192)\n",
            "3 14 Epoch: 38 | ANN: trainLoss: 0.1390 | trainAcc: 96.8750% (248/256)\n",
            "4 14 Epoch: 38 | ANN: trainLoss: 0.1476 | trainAcc: 95.9375% (307/320)\n",
            "5 14 Epoch: 38 | ANN: trainLoss: 0.1631 | trainAcc: 95.3125% (366/384)\n",
            "6 14 Epoch: 38 | ANN: trainLoss: 0.1598 | trainAcc: 95.7589% (429/448)\n",
            "7 14 Epoch: 38 | ANN: trainLoss: 0.1575 | trainAcc: 96.2891% (493/512)\n",
            "8 14 Epoch: 38 | ANN: trainLoss: 0.1599 | trainAcc: 96.0069% (553/576)\n",
            "9 14 Epoch: 38 | ANN: trainLoss: 0.1585 | trainAcc: 96.0938% (615/640)\n",
            "10 14 Epoch: 38 | ANN: trainLoss: 0.1600 | trainAcc: 96.3068% (678/704)\n",
            "11 14 Epoch: 38 | ANN: trainLoss: 0.1598 | trainAcc: 96.3542% (740/768)\n",
            "12 14 Epoch: 38 | ANN: trainLoss: 0.1585 | trainAcc: 96.5144% (803/832)\n",
            "13 14 Epoch: 38 | ANN: trainLoss: 0.1562 | trainAcc: 96.6590% (839/868)\n",
            "0 2 Epoch: 38 | ANN: testLoss: 0.7367 | testAcc: 67.1875% (43/64)\n",
            "1 2 Epoch: 38 | ANN: testLoss: 0.7475 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 39 | ANN: trainLoss: 0.2481 | trainAcc: 87.5000% (56/64)\n",
            "1 14 Epoch: 39 | ANN: trainLoss: 0.2200 | trainAcc: 92.1875% (118/128)\n",
            "2 14 Epoch: 39 | ANN: trainLoss: 0.1922 | trainAcc: 93.7500% (180/192)\n",
            "3 14 Epoch: 39 | ANN: trainLoss: 0.1668 | trainAcc: 95.3125% (244/256)\n",
            "4 14 Epoch: 39 | ANN: trainLoss: 0.1605 | trainAcc: 95.9375% (307/320)\n",
            "5 14 Epoch: 39 | ANN: trainLoss: 0.1661 | trainAcc: 95.0521% (365/384)\n",
            "6 14 Epoch: 39 | ANN: trainLoss: 0.1599 | trainAcc: 95.3125% (427/448)\n",
            "7 14 Epoch: 39 | ANN: trainLoss: 0.1603 | trainAcc: 95.5078% (489/512)\n",
            "8 14 Epoch: 39 | ANN: trainLoss: 0.1572 | trainAcc: 95.8333% (552/576)\n",
            "9 14 Epoch: 39 | ANN: trainLoss: 0.1609 | trainAcc: 95.7812% (613/640)\n",
            "10 14 Epoch: 39 | ANN: trainLoss: 0.1563 | trainAcc: 96.1648% (677/704)\n",
            "11 14 Epoch: 39 | ANN: trainLoss: 0.1533 | trainAcc: 96.2240% (739/768)\n",
            "12 14 Epoch: 39 | ANN: trainLoss: 0.1542 | trainAcc: 96.2740% (801/832)\n",
            "13 14 Epoch: 39 | ANN: trainLoss: 0.1558 | trainAcc: 96.0829% (834/868)\n",
            "0 2 Epoch: 39 | ANN: testLoss: 0.7947 | testAcc: 65.6250% (42/64)\n",
            "1 2 Epoch: 39 | ANN: testLoss: 0.7330 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 40 | ANN: trainLoss: 0.1148 | trainAcc: 98.4375% (63/64)\n",
            "1 14 Epoch: 40 | ANN: trainLoss: 0.1183 | trainAcc: 98.4375% (126/128)\n",
            "2 14 Epoch: 40 | ANN: trainLoss: 0.1244 | trainAcc: 98.4375% (189/192)\n",
            "3 14 Epoch: 40 | ANN: trainLoss: 0.1309 | trainAcc: 98.0469% (251/256)\n",
            "4 14 Epoch: 40 | ANN: trainLoss: 0.1260 | trainAcc: 98.4375% (315/320)\n",
            "5 14 Epoch: 40 | ANN: trainLoss: 0.1259 | trainAcc: 98.1771% (377/384)\n",
            "6 14 Epoch: 40 | ANN: trainLoss: 0.1259 | trainAcc: 97.7679% (438/448)\n",
            "7 14 Epoch: 40 | ANN: trainLoss: 0.1297 | trainAcc: 97.4609% (499/512)\n",
            "8 14 Epoch: 40 | ANN: trainLoss: 0.1249 | trainAcc: 97.7431% (563/576)\n",
            "9 14 Epoch: 40 | ANN: trainLoss: 0.1250 | trainAcc: 97.8125% (626/640)\n",
            "10 14 Epoch: 40 | ANN: trainLoss: 0.1245 | trainAcc: 97.7273% (688/704)\n",
            "11 14 Epoch: 40 | ANN: trainLoss: 0.1260 | trainAcc: 97.6562% (750/768)\n",
            "12 14 Epoch: 40 | ANN: trainLoss: 0.1312 | trainAcc: 97.5962% (812/832)\n",
            "13 14 Epoch: 40 | ANN: trainLoss: 0.1324 | trainAcc: 97.5806% (847/868)\n",
            "0 2 Epoch: 40 | ANN: testLoss: 0.7183 | testAcc: 67.1875% (43/64)\n",
            "1 2 Epoch: 40 | ANN: testLoss: 0.7527 | testAcc: 68.0412% (66/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 41 | ANN: trainLoss: 0.1244 | trainAcc: 98.4375% (63/64)\n",
            "1 14 Epoch: 41 | ANN: trainLoss: 0.1254 | trainAcc: 98.4375% (126/128)\n",
            "2 14 Epoch: 41 | ANN: trainLoss: 0.1311 | trainAcc: 97.9167% (188/192)\n",
            "3 14 Epoch: 41 | ANN: trainLoss: 0.1271 | trainAcc: 97.6562% (250/256)\n",
            "4 14 Epoch: 41 | ANN: trainLoss: 0.1370 | trainAcc: 97.5000% (312/320)\n",
            "5 14 Epoch: 41 | ANN: trainLoss: 0.1350 | trainAcc: 97.6562% (375/384)\n",
            "6 14 Epoch: 41 | ANN: trainLoss: 0.1306 | trainAcc: 97.7679% (438/448)\n",
            "7 14 Epoch: 41 | ANN: trainLoss: 0.1329 | trainAcc: 97.8516% (501/512)\n",
            "8 14 Epoch: 41 | ANN: trainLoss: 0.1402 | trainAcc: 97.5694% (562/576)\n",
            "9 14 Epoch: 41 | ANN: trainLoss: 0.1423 | trainAcc: 97.3438% (623/640)\n",
            "10 14 Epoch: 41 | ANN: trainLoss: 0.1418 | trainAcc: 97.4432% (686/704)\n",
            "11 14 Epoch: 41 | ANN: trainLoss: 0.1375 | trainAcc: 97.6562% (750/768)\n",
            "12 14 Epoch: 41 | ANN: trainLoss: 0.1379 | trainAcc: 97.5962% (812/832)\n",
            "13 14 Epoch: 41 | ANN: trainLoss: 0.1419 | trainAcc: 97.3502% (845/868)\n",
            "0 2 Epoch: 41 | ANN: testLoss: 0.7194 | testAcc: 68.7500% (44/64)\n",
            "1 2 Epoch: 41 | ANN: testLoss: 0.7551 | testAcc: 67.0103% (65/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 42 | ANN: trainLoss: 0.1318 | trainAcc: 96.8750% (62/64)\n",
            "1 14 Epoch: 42 | ANN: trainLoss: 0.1338 | trainAcc: 95.3125% (122/128)\n",
            "2 14 Epoch: 42 | ANN: trainLoss: 0.1329 | trainAcc: 95.8333% (184/192)\n",
            "3 14 Epoch: 42 | ANN: trainLoss: 0.1452 | trainAcc: 96.0938% (246/256)\n",
            "4 14 Epoch: 42 | ANN: trainLoss: 0.1444 | trainAcc: 96.2500% (308/320)\n",
            "5 14 Epoch: 42 | ANN: trainLoss: 0.1440 | trainAcc: 96.3542% (370/384)\n",
            "6 14 Epoch: 42 | ANN: trainLoss: 0.1353 | trainAcc: 96.8750% (434/448)\n",
            "7 14 Epoch: 42 | ANN: trainLoss: 0.1309 | trainAcc: 97.0703% (497/512)\n",
            "8 14 Epoch: 42 | ANN: trainLoss: 0.1337 | trainAcc: 96.8750% (558/576)\n",
            "9 14 Epoch: 42 | ANN: trainLoss: 0.1353 | trainAcc: 96.8750% (620/640)\n",
            "10 14 Epoch: 42 | ANN: trainLoss: 0.1333 | trainAcc: 97.0170% (683/704)\n",
            "11 14 Epoch: 42 | ANN: trainLoss: 0.1343 | trainAcc: 96.8750% (744/768)\n",
            "12 14 Epoch: 42 | ANN: trainLoss: 0.1353 | trainAcc: 96.8750% (806/832)\n",
            "13 14 Epoch: 42 | ANN: trainLoss: 0.1425 | trainAcc: 96.4286% (837/868)\n",
            "0 2 Epoch: 42 | ANN: testLoss: 0.7034 | testAcc: 68.7500% (44/64)\n",
            "1 2 Epoch: 42 | ANN: testLoss: 0.7335 | testAcc: 67.0103% (65/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 43 | ANN: trainLoss: 0.0875 | trainAcc: 100.0000% (64/64)\n",
            "1 14 Epoch: 43 | ANN: trainLoss: 0.1225 | trainAcc: 97.6562% (125/128)\n",
            "2 14 Epoch: 43 | ANN: trainLoss: 0.1151 | trainAcc: 97.3958% (187/192)\n",
            "3 14 Epoch: 43 | ANN: trainLoss: 0.1190 | trainAcc: 96.4844% (247/256)\n",
            "4 14 Epoch: 43 | ANN: trainLoss: 0.1123 | trainAcc: 97.1875% (311/320)\n",
            "5 14 Epoch: 43 | ANN: trainLoss: 0.1156 | trainAcc: 97.1354% (373/384)\n",
            "6 14 Epoch: 43 | ANN: trainLoss: 0.1146 | trainAcc: 97.5446% (437/448)\n",
            "7 14 Epoch: 43 | ANN: trainLoss: 0.1132 | trainAcc: 97.8516% (501/512)\n",
            "8 14 Epoch: 43 | ANN: trainLoss: 0.1134 | trainAcc: 97.7431% (563/576)\n",
            "9 14 Epoch: 43 | ANN: trainLoss: 0.1154 | trainAcc: 97.8125% (626/640)\n",
            "10 14 Epoch: 43 | ANN: trainLoss: 0.1149 | trainAcc: 98.0114% (690/704)\n",
            "11 14 Epoch: 43 | ANN: trainLoss: 0.1171 | trainAcc: 98.0469% (753/768)\n",
            "12 14 Epoch: 43 | ANN: trainLoss: 0.1175 | trainAcc: 98.0769% (816/832)\n",
            "13 14 Epoch: 43 | ANN: trainLoss: 0.1189 | trainAcc: 98.0415% (851/868)\n",
            "0 2 Epoch: 43 | ANN: testLoss: 0.7101 | testAcc: 68.7500% (44/64)\n",
            "1 2 Epoch: 43 | ANN: testLoss: 0.8374 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 44 | ANN: trainLoss: 0.0861 | trainAcc: 100.0000% (64/64)\n",
            "1 14 Epoch: 44 | ANN: trainLoss: 0.0880 | trainAcc: 100.0000% (128/128)\n",
            "2 14 Epoch: 44 | ANN: trainLoss: 0.0949 | trainAcc: 98.9583% (190/192)\n",
            "3 14 Epoch: 44 | ANN: trainLoss: 0.0986 | trainAcc: 98.8281% (253/256)\n",
            "4 14 Epoch: 44 | ANN: trainLoss: 0.1002 | trainAcc: 99.0625% (317/320)\n",
            "5 14 Epoch: 44 | ANN: trainLoss: 0.1045 | trainAcc: 98.9583% (380/384)\n",
            "6 14 Epoch: 44 | ANN: trainLoss: 0.1048 | trainAcc: 98.8839% (443/448)\n",
            "7 14 Epoch: 44 | ANN: trainLoss: 0.1100 | trainAcc: 98.4375% (504/512)\n",
            "8 14 Epoch: 44 | ANN: trainLoss: 0.1137 | trainAcc: 98.0903% (565/576)\n",
            "9 14 Epoch: 44 | ANN: trainLoss: 0.1170 | trainAcc: 98.2812% (629/640)\n",
            "10 14 Epoch: 44 | ANN: trainLoss: 0.1162 | trainAcc: 98.4375% (693/704)\n",
            "11 14 Epoch: 44 | ANN: trainLoss: 0.1221 | trainAcc: 98.0469% (753/768)\n",
            "12 14 Epoch: 44 | ANN: trainLoss: 0.1213 | trainAcc: 97.9567% (815/832)\n",
            "13 14 Epoch: 44 | ANN: trainLoss: 0.1188 | trainAcc: 98.0415% (851/868)\n",
            "0 2 Epoch: 44 | ANN: testLoss: 0.7551 | testAcc: 68.7500% (44/64)\n",
            "1 2 Epoch: 44 | ANN: testLoss: 0.7367 | testAcc: 67.0103% (65/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 45 | ANN: trainLoss: 0.0639 | trainAcc: 100.0000% (64/64)\n",
            "1 14 Epoch: 45 | ANN: trainLoss: 0.0872 | trainAcc: 99.2188% (127/128)\n",
            "2 14 Epoch: 45 | ANN: trainLoss: 0.0854 | trainAcc: 99.4792% (191/192)\n",
            "3 14 Epoch: 45 | ANN: trainLoss: 0.0928 | trainAcc: 98.8281% (253/256)\n",
            "4 14 Epoch: 45 | ANN: trainLoss: 0.1009 | trainAcc: 99.0625% (317/320)\n",
            "5 14 Epoch: 45 | ANN: trainLoss: 0.0992 | trainAcc: 98.9583% (380/384)\n",
            "6 14 Epoch: 45 | ANN: trainLoss: 0.1029 | trainAcc: 99.1071% (444/448)\n",
            "7 14 Epoch: 45 | ANN: trainLoss: 0.1076 | trainAcc: 98.8281% (506/512)\n",
            "8 14 Epoch: 45 | ANN: trainLoss: 0.1110 | trainAcc: 98.6111% (568/576)\n",
            "9 14 Epoch: 45 | ANN: trainLoss: 0.1102 | trainAcc: 98.5938% (631/640)\n",
            "10 14 Epoch: 45 | ANN: trainLoss: 0.1108 | trainAcc: 98.5795% (694/704)\n",
            "11 14 Epoch: 45 | ANN: trainLoss: 0.1115 | trainAcc: 98.5677% (757/768)\n",
            "12 14 Epoch: 45 | ANN: trainLoss: 0.1129 | trainAcc: 98.5577% (820/832)\n",
            "13 14 Epoch: 45 | ANN: trainLoss: 0.1135 | trainAcc: 98.6175% (856/868)\n",
            "0 2 Epoch: 45 | ANN: testLoss: 0.7467 | testAcc: 67.1875% (43/64)\n",
            "1 2 Epoch: 45 | ANN: testLoss: 0.7499 | testAcc: 68.0412% (66/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 46 | ANN: trainLoss: 0.1384 | trainAcc: 96.8750% (62/64)\n",
            "1 14 Epoch: 46 | ANN: trainLoss: 0.1117 | trainAcc: 97.6562% (125/128)\n",
            "2 14 Epoch: 46 | ANN: trainLoss: 0.1096 | trainAcc: 98.4375% (189/192)\n",
            "3 14 Epoch: 46 | ANN: trainLoss: 0.1122 | trainAcc: 98.0469% (251/256)\n",
            "4 14 Epoch: 46 | ANN: trainLoss: 0.1115 | trainAcc: 98.1250% (314/320)\n",
            "5 14 Epoch: 46 | ANN: trainLoss: 0.1166 | trainAcc: 98.4375% (378/384)\n",
            "6 14 Epoch: 46 | ANN: trainLoss: 0.1152 | trainAcc: 98.6607% (442/448)\n",
            "7 14 Epoch: 46 | ANN: trainLoss: 0.1142 | trainAcc: 98.6328% (505/512)\n",
            "8 14 Epoch: 46 | ANN: trainLoss: 0.1105 | trainAcc: 98.7847% (569/576)\n",
            "9 14 Epoch: 46 | ANN: trainLoss: 0.1097 | trainAcc: 98.9062% (633/640)\n",
            "10 14 Epoch: 46 | ANN: trainLoss: 0.1082 | trainAcc: 98.8636% (696/704)\n",
            "11 14 Epoch: 46 | ANN: trainLoss: 0.1109 | trainAcc: 98.8281% (759/768)\n",
            "12 14 Epoch: 46 | ANN: trainLoss: 0.1140 | trainAcc: 98.3173% (818/832)\n",
            "13 14 Epoch: 46 | ANN: trainLoss: 0.1203 | trainAcc: 98.1567% (852/868)\n",
            "0 2 Epoch: 46 | ANN: testLoss: 0.8358 | testAcc: 65.6250% (42/64)\n",
            "1 2 Epoch: 46 | ANN: testLoss: 0.7526 | testAcc: 68.0412% (66/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 47 | ANN: trainLoss: 0.1123 | trainAcc: 98.4375% (63/64)\n",
            "1 14 Epoch: 47 | ANN: trainLoss: 0.1119 | trainAcc: 98.4375% (126/128)\n",
            "2 14 Epoch: 47 | ANN: trainLoss: 0.1051 | trainAcc: 98.9583% (190/192)\n",
            "3 14 Epoch: 47 | ANN: trainLoss: 0.1044 | trainAcc: 98.8281% (253/256)\n",
            "4 14 Epoch: 47 | ANN: trainLoss: 0.1033 | trainAcc: 98.7500% (316/320)\n",
            "5 14 Epoch: 47 | ANN: trainLoss: 0.1045 | trainAcc: 98.4375% (378/384)\n",
            "6 14 Epoch: 47 | ANN: trainLoss: 0.1094 | trainAcc: 98.4375% (441/448)\n",
            "7 14 Epoch: 47 | ANN: trainLoss: 0.1124 | trainAcc: 98.2422% (503/512)\n",
            "8 14 Epoch: 47 | ANN: trainLoss: 0.1135 | trainAcc: 97.9167% (564/576)\n",
            "9 14 Epoch: 47 | ANN: trainLoss: 0.1144 | trainAcc: 97.6562% (625/640)\n",
            "10 14 Epoch: 47 | ANN: trainLoss: 0.1143 | trainAcc: 97.8693% (689/704)\n",
            "11 14 Epoch: 47 | ANN: trainLoss: 0.1143 | trainAcc: 97.9167% (752/768)\n",
            "12 14 Epoch: 47 | ANN: trainLoss: 0.1120 | trainAcc: 98.0769% (816/832)\n",
            "13 14 Epoch: 47 | ANN: trainLoss: 0.1104 | trainAcc: 98.0415% (851/868)\n",
            "0 2 Epoch: 47 | ANN: testLoss: 0.7248 | testAcc: 68.7500% (44/64)\n",
            "1 2 Epoch: 47 | ANN: testLoss: 0.8271 | testAcc: 68.0412% (66/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 48 | ANN: trainLoss: 0.1235 | trainAcc: 98.4375% (63/64)\n",
            "1 14 Epoch: 48 | ANN: trainLoss: 0.1212 | trainAcc: 96.8750% (124/128)\n",
            "2 14 Epoch: 48 | ANN: trainLoss: 0.1138 | trainAcc: 96.8750% (186/192)\n",
            "3 14 Epoch: 48 | ANN: trainLoss: 0.1088 | trainAcc: 97.6562% (250/256)\n",
            "4 14 Epoch: 48 | ANN: trainLoss: 0.1019 | trainAcc: 98.1250% (314/320)\n",
            "5 14 Epoch: 48 | ANN: trainLoss: 0.1057 | trainAcc: 97.6562% (375/384)\n",
            "6 14 Epoch: 48 | ANN: trainLoss: 0.1035 | trainAcc: 97.7679% (438/448)\n",
            "7 14 Epoch: 48 | ANN: trainLoss: 0.1061 | trainAcc: 97.6562% (500/512)\n",
            "8 14 Epoch: 48 | ANN: trainLoss: 0.1111 | trainAcc: 97.3958% (561/576)\n",
            "9 14 Epoch: 48 | ANN: trainLoss: 0.1118 | trainAcc: 97.3438% (623/640)\n",
            "10 14 Epoch: 48 | ANN: trainLoss: 0.1098 | trainAcc: 97.4432% (686/704)\n",
            "11 14 Epoch: 48 | ANN: trainLoss: 0.1128 | trainAcc: 97.2656% (747/768)\n",
            "12 14 Epoch: 48 | ANN: trainLoss: 0.1114 | trainAcc: 97.3558% (810/832)\n",
            "13 14 Epoch: 48 | ANN: trainLoss: 0.1188 | trainAcc: 97.0046% (842/868)\n",
            "0 2 Epoch: 48 | ANN: testLoss: 0.8610 | testAcc: 65.6250% (42/64)\n",
            "1 2 Epoch: 48 | ANN: testLoss: 0.8233 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 49 | ANN: trainLoss: 0.0908 | trainAcc: 100.0000% (64/64)\n",
            "1 14 Epoch: 49 | ANN: trainLoss: 0.1125 | trainAcc: 97.6562% (125/128)\n",
            "2 14 Epoch: 49 | ANN: trainLoss: 0.1282 | trainAcc: 96.8750% (186/192)\n",
            "3 14 Epoch: 49 | ANN: trainLoss: 0.1182 | trainAcc: 97.2656% (249/256)\n",
            "4 14 Epoch: 49 | ANN: trainLoss: 0.1136 | trainAcc: 97.5000% (312/320)\n",
            "5 14 Epoch: 49 | ANN: trainLoss: 0.1131 | trainAcc: 97.6562% (375/384)\n",
            "6 14 Epoch: 49 | ANN: trainLoss: 0.1124 | trainAcc: 97.9911% (439/448)\n",
            "7 14 Epoch: 49 | ANN: trainLoss: 0.1099 | trainAcc: 98.0469% (502/512)\n",
            "8 14 Epoch: 49 | ANN: trainLoss: 0.1120 | trainAcc: 97.7431% (563/576)\n",
            "9 14 Epoch: 49 | ANN: trainLoss: 0.1093 | trainAcc: 97.9688% (627/640)\n",
            "10 14 Epoch: 49 | ANN: trainLoss: 0.1072 | trainAcc: 98.1534% (691/704)\n",
            "11 14 Epoch: 49 | ANN: trainLoss: 0.1106 | trainAcc: 97.9167% (752/768)\n",
            "12 14 Epoch: 49 | ANN: trainLoss: 0.1136 | trainAcc: 97.8365% (814/832)\n",
            "13 14 Epoch: 49 | ANN: trainLoss: 0.1155 | trainAcc: 97.8111% (849/868)\n",
            "0 2 Epoch: 49 | ANN: testLoss: 0.6800 | testAcc: 71.8750% (46/64)\n",
            "1 2 Epoch: 49 | ANN: testLoss: 0.8424 | testAcc: 70.1031% (68/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 50 | ANN: trainLoss: 0.0993 | trainAcc: 98.4375% (63/64)\n",
            "1 14 Epoch: 50 | ANN: trainLoss: 0.0936 | trainAcc: 98.4375% (126/128)\n",
            "2 14 Epoch: 50 | ANN: trainLoss: 0.0905 | trainAcc: 98.4375% (189/192)\n",
            "3 14 Epoch: 50 | ANN: trainLoss: 0.0898 | trainAcc: 98.8281% (253/256)\n",
            "4 14 Epoch: 50 | ANN: trainLoss: 0.0902 | trainAcc: 99.0625% (317/320)\n",
            "5 14 Epoch: 50 | ANN: trainLoss: 0.0943 | trainAcc: 98.6979% (379/384)\n",
            "6 14 Epoch: 50 | ANN: trainLoss: 0.0924 | trainAcc: 98.6607% (442/448)\n",
            "7 14 Epoch: 50 | ANN: trainLoss: 0.0961 | trainAcc: 98.4375% (504/512)\n",
            "8 14 Epoch: 50 | ANN: trainLoss: 0.0999 | trainAcc: 98.2639% (566/576)\n",
            "9 14 Epoch: 50 | ANN: trainLoss: 0.0988 | trainAcc: 98.4375% (630/640)\n",
            "10 14 Epoch: 50 | ANN: trainLoss: 0.0998 | trainAcc: 98.4375% (693/704)\n",
            "11 14 Epoch: 50 | ANN: trainLoss: 0.1009 | trainAcc: 98.4375% (756/768)\n",
            "12 14 Epoch: 50 | ANN: trainLoss: 0.1032 | trainAcc: 98.3173% (818/832)\n",
            "13 14 Epoch: 50 | ANN: trainLoss: 0.1023 | trainAcc: 98.3871% (854/868)\n",
            "0 2 Epoch: 50 | ANN: testLoss: 0.8582 | testAcc: 64.0625% (41/64)\n",
            "1 2 Epoch: 50 | ANN: testLoss: 0.7447 | testAcc: 67.0103% (65/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 51 | ANN: trainLoss: 0.0939 | trainAcc: 98.4375% (63/64)\n",
            "1 14 Epoch: 51 | ANN: trainLoss: 0.0817 | trainAcc: 99.2188% (127/128)\n",
            "2 14 Epoch: 51 | ANN: trainLoss: 0.0785 | trainAcc: 99.4792% (191/192)\n",
            "3 14 Epoch: 51 | ANN: trainLoss: 0.0790 | trainAcc: 99.6094% (255/256)\n",
            "4 14 Epoch: 51 | ANN: trainLoss: 0.0838 | trainAcc: 99.6875% (319/320)\n",
            "5 14 Epoch: 51 | ANN: trainLoss: 0.0879 | trainAcc: 99.4792% (382/384)\n",
            "6 14 Epoch: 51 | ANN: trainLoss: 0.0936 | trainAcc: 99.1071% (444/448)\n",
            "7 14 Epoch: 51 | ANN: trainLoss: 0.0938 | trainAcc: 98.8281% (506/512)\n",
            "8 14 Epoch: 51 | ANN: trainLoss: 0.0922 | trainAcc: 98.7847% (569/576)\n",
            "9 14 Epoch: 51 | ANN: trainLoss: 0.0917 | trainAcc: 98.9062% (633/640)\n",
            "10 14 Epoch: 51 | ANN: trainLoss: 0.0998 | trainAcc: 98.2955% (692/704)\n",
            "11 14 Epoch: 51 | ANN: trainLoss: 0.0996 | trainAcc: 98.4375% (756/768)\n",
            "12 14 Epoch: 51 | ANN: trainLoss: 0.1011 | trainAcc: 98.5577% (820/832)\n",
            "13 14 Epoch: 51 | ANN: trainLoss: 0.1038 | trainAcc: 98.6175% (856/868)\n",
            "0 2 Epoch: 51 | ANN: testLoss: 0.8610 | testAcc: 65.6250% (42/64)\n",
            "1 2 Epoch: 51 | ANN: testLoss: 0.7556 | testAcc: 67.0103% (65/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 52 | ANN: trainLoss: 0.1268 | trainAcc: 98.4375% (63/64)\n",
            "1 14 Epoch: 52 | ANN: trainLoss: 0.1068 | trainAcc: 99.2188% (127/128)\n",
            "2 14 Epoch: 52 | ANN: trainLoss: 0.0929 | trainAcc: 99.4792% (191/192)\n",
            "3 14 Epoch: 52 | ANN: trainLoss: 0.0912 | trainAcc: 99.2188% (254/256)\n",
            "4 14 Epoch: 52 | ANN: trainLoss: 0.0942 | trainAcc: 99.3750% (318/320)\n",
            "5 14 Epoch: 52 | ANN: trainLoss: 0.0927 | trainAcc: 99.2188% (381/384)\n",
            "6 14 Epoch: 52 | ANN: trainLoss: 0.0903 | trainAcc: 99.3304% (445/448)\n",
            "7 14 Epoch: 52 | ANN: trainLoss: 0.0995 | trainAcc: 98.8281% (506/512)\n",
            "8 14 Epoch: 52 | ANN: trainLoss: 0.1023 | trainAcc: 98.7847% (569/576)\n",
            "9 14 Epoch: 52 | ANN: trainLoss: 0.1015 | trainAcc: 98.7500% (632/640)\n",
            "10 14 Epoch: 52 | ANN: trainLoss: 0.0987 | trainAcc: 98.8636% (696/704)\n",
            "11 14 Epoch: 52 | ANN: trainLoss: 0.0998 | trainAcc: 98.8281% (759/768)\n",
            "12 14 Epoch: 52 | ANN: trainLoss: 0.1020 | trainAcc: 98.7981% (822/832)\n",
            "13 14 Epoch: 52 | ANN: trainLoss: 0.1021 | trainAcc: 98.8479% (858/868)\n",
            "0 2 Epoch: 52 | ANN: testLoss: 0.9097 | testAcc: 64.0625% (41/64)\n",
            "1 2 Epoch: 52 | ANN: testLoss: 0.7455 | testAcc: 68.0412% (66/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 53 | ANN: trainLoss: 0.0913 | trainAcc: 98.4375% (63/64)\n",
            "1 14 Epoch: 53 | ANN: trainLoss: 0.0976 | trainAcc: 98.4375% (126/128)\n",
            "2 14 Epoch: 53 | ANN: trainLoss: 0.1083 | trainAcc: 97.9167% (188/192)\n",
            "3 14 Epoch: 53 | ANN: trainLoss: 0.1041 | trainAcc: 98.0469% (251/256)\n",
            "4 14 Epoch: 53 | ANN: trainLoss: 0.1041 | trainAcc: 98.1250% (314/320)\n",
            "5 14 Epoch: 53 | ANN: trainLoss: 0.1032 | trainAcc: 98.4375% (378/384)\n",
            "6 14 Epoch: 53 | ANN: trainLoss: 0.1081 | trainAcc: 97.7679% (438/448)\n",
            "7 14 Epoch: 53 | ANN: trainLoss: 0.1058 | trainAcc: 97.8516% (501/512)\n",
            "8 14 Epoch: 53 | ANN: trainLoss: 0.1064 | trainAcc: 97.7431% (563/576)\n",
            "9 14 Epoch: 53 | ANN: trainLoss: 0.1062 | trainAcc: 97.8125% (626/640)\n",
            "10 14 Epoch: 53 | ANN: trainLoss: 0.1057 | trainAcc: 97.8693% (689/704)\n",
            "11 14 Epoch: 53 | ANN: trainLoss: 0.1112 | trainAcc: 97.9167% (752/768)\n",
            "12 14 Epoch: 53 | ANN: trainLoss: 0.1086 | trainAcc: 98.0769% (816/832)\n",
            "13 14 Epoch: 53 | ANN: trainLoss: 0.1075 | trainAcc: 98.0415% (851/868)\n",
            "0 2 Epoch: 53 | ANN: testLoss: 0.7371 | testAcc: 68.7500% (44/64)\n",
            "1 2 Epoch: 53 | ANN: testLoss: 0.8274 | testAcc: 68.0412% (66/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 54 | ANN: trainLoss: 0.0817 | trainAcc: 98.4375% (63/64)\n",
            "1 14 Epoch: 54 | ANN: trainLoss: 0.0900 | trainAcc: 97.6562% (125/128)\n",
            "2 14 Epoch: 54 | ANN: trainLoss: 0.1008 | trainAcc: 97.9167% (188/192)\n",
            "3 14 Epoch: 54 | ANN: trainLoss: 0.1164 | trainAcc: 97.2656% (249/256)\n",
            "4 14 Epoch: 54 | ANN: trainLoss: 0.1151 | trainAcc: 97.5000% (312/320)\n",
            "5 14 Epoch: 54 | ANN: trainLoss: 0.1203 | trainAcc: 97.3958% (374/384)\n",
            "6 14 Epoch: 54 | ANN: trainLoss: 0.1158 | trainAcc: 97.7679% (438/448)\n",
            "7 14 Epoch: 54 | ANN: trainLoss: 0.1192 | trainAcc: 97.6562% (500/512)\n",
            "8 14 Epoch: 54 | ANN: trainLoss: 0.1164 | trainAcc: 97.9167% (564/576)\n",
            "9 14 Epoch: 54 | ANN: trainLoss: 0.1165 | trainAcc: 97.8125% (626/640)\n",
            "10 14 Epoch: 54 | ANN: trainLoss: 0.1143 | trainAcc: 97.7273% (688/704)\n",
            "11 14 Epoch: 54 | ANN: trainLoss: 0.1141 | trainAcc: 97.6562% (750/768)\n",
            "12 14 Epoch: 54 | ANN: trainLoss: 0.1120 | trainAcc: 97.8365% (814/832)\n",
            "13 14 Epoch: 54 | ANN: trainLoss: 0.1111 | trainAcc: 97.8111% (849/868)\n",
            "0 2 Epoch: 54 | ANN: testLoss: 0.6670 | testAcc: 68.7500% (44/64)\n",
            "1 2 Epoch: 54 | ANN: testLoss: 0.8594 | testAcc: 68.0412% (66/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 55 | ANN: trainLoss: 0.0896 | trainAcc: 100.0000% (64/64)\n",
            "1 14 Epoch: 55 | ANN: trainLoss: 0.0965 | trainAcc: 99.2188% (127/128)\n",
            "2 14 Epoch: 55 | ANN: trainLoss: 0.0870 | trainAcc: 99.4792% (191/192)\n",
            "3 14 Epoch: 55 | ANN: trainLoss: 0.0867 | trainAcc: 99.2188% (254/256)\n",
            "4 14 Epoch: 55 | ANN: trainLoss: 0.0885 | trainAcc: 99.3750% (318/320)\n",
            "5 14 Epoch: 55 | ANN: trainLoss: 0.0939 | trainAcc: 99.2188% (381/384)\n",
            "6 14 Epoch: 55 | ANN: trainLoss: 0.0927 | trainAcc: 99.3304% (445/448)\n",
            "7 14 Epoch: 55 | ANN: trainLoss: 0.0940 | trainAcc: 99.0234% (507/512)\n",
            "8 14 Epoch: 55 | ANN: trainLoss: 0.0954 | trainAcc: 98.9583% (570/576)\n",
            "9 14 Epoch: 55 | ANN: trainLoss: 0.0964 | trainAcc: 98.9062% (633/640)\n",
            "10 14 Epoch: 55 | ANN: trainLoss: 0.0988 | trainAcc: 98.7216% (695/704)\n",
            "11 14 Epoch: 55 | ANN: trainLoss: 0.0971 | trainAcc: 98.6979% (758/768)\n",
            "12 14 Epoch: 55 | ANN: trainLoss: 0.1008 | trainAcc: 98.4375% (819/832)\n",
            "13 14 Epoch: 55 | ANN: trainLoss: 0.0973 | trainAcc: 98.5023% (855/868)\n",
            "0 2 Epoch: 55 | ANN: testLoss: 0.7234 | testAcc: 70.3125% (45/64)\n",
            "1 2 Epoch: 55 | ANN: testLoss: 0.8331 | testAcc: 68.0412% (66/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 56 | ANN: trainLoss: 0.0748 | trainAcc: 100.0000% (64/64)\n",
            "1 14 Epoch: 56 | ANN: trainLoss: 0.0853 | trainAcc: 100.0000% (128/128)\n",
            "2 14 Epoch: 56 | ANN: trainLoss: 0.0933 | trainAcc: 98.4375% (189/192)\n",
            "3 14 Epoch: 56 | ANN: trainLoss: 0.0892 | trainAcc: 98.8281% (253/256)\n",
            "4 14 Epoch: 56 | ANN: trainLoss: 0.0858 | trainAcc: 99.0625% (317/320)\n",
            "5 14 Epoch: 56 | ANN: trainLoss: 0.0877 | trainAcc: 98.9583% (380/384)\n",
            "6 14 Epoch: 56 | ANN: trainLoss: 0.0887 | trainAcc: 99.1071% (444/448)\n",
            "7 14 Epoch: 56 | ANN: trainLoss: 0.0864 | trainAcc: 99.2188% (508/512)\n",
            "8 14 Epoch: 56 | ANN: trainLoss: 0.0879 | trainAcc: 99.3056% (572/576)\n",
            "9 14 Epoch: 56 | ANN: trainLoss: 0.0917 | trainAcc: 99.3750% (636/640)\n",
            "10 14 Epoch: 56 | ANN: trainLoss: 0.0929 | trainAcc: 99.1477% (698/704)\n",
            "11 14 Epoch: 56 | ANN: trainLoss: 0.0933 | trainAcc: 99.0885% (761/768)\n",
            "12 14 Epoch: 56 | ANN: trainLoss: 0.0959 | trainAcc: 99.0385% (824/832)\n",
            "13 14 Epoch: 56 | ANN: trainLoss: 0.0931 | trainAcc: 99.0783% (860/868)\n",
            "0 2 Epoch: 56 | ANN: testLoss: 0.7511 | testAcc: 71.8750% (46/64)\n",
            "1 2 Epoch: 56 | ANN: testLoss: 0.8198 | testAcc: 69.0722% (67/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 57 | ANN: trainLoss: 0.1233 | trainAcc: 96.8750% (62/64)\n",
            "1 14 Epoch: 57 | ANN: trainLoss: 0.1037 | trainAcc: 97.6562% (125/128)\n",
            "2 14 Epoch: 57 | ANN: trainLoss: 0.0950 | trainAcc: 98.4375% (189/192)\n",
            "3 14 Epoch: 57 | ANN: trainLoss: 0.0965 | trainAcc: 98.4375% (252/256)\n",
            "4 14 Epoch: 57 | ANN: trainLoss: 0.0926 | trainAcc: 98.7500% (316/320)\n",
            "5 14 Epoch: 57 | ANN: trainLoss: 0.0943 | trainAcc: 98.1771% (377/384)\n",
            "6 14 Epoch: 57 | ANN: trainLoss: 0.0941 | trainAcc: 98.4375% (441/448)\n",
            "7 14 Epoch: 57 | ANN: trainLoss: 0.0969 | trainAcc: 98.6328% (505/512)\n",
            "8 14 Epoch: 57 | ANN: trainLoss: 0.0952 | trainAcc: 98.7847% (569/576)\n",
            "9 14 Epoch: 57 | ANN: trainLoss: 0.0955 | trainAcc: 98.7500% (632/640)\n",
            "10 14 Epoch: 57 | ANN: trainLoss: 0.0969 | trainAcc: 98.5795% (694/704)\n",
            "11 14 Epoch: 57 | ANN: trainLoss: 0.0982 | trainAcc: 98.5677% (757/768)\n",
            "12 14 Epoch: 57 | ANN: trainLoss: 0.0959 | trainAcc: 98.5577% (820/832)\n",
            "13 14 Epoch: 57 | ANN: trainLoss: 0.0994 | trainAcc: 98.5023% (855/868)\n",
            "0 2 Epoch: 57 | ANN: testLoss: 0.5707 | testAcc: 75.0000% (48/64)\n",
            "1 2 Epoch: 57 | ANN: testLoss: 0.9053 | testAcc: 69.0722% (67/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 58 | ANN: trainLoss: 0.1017 | trainAcc: 100.0000% (64/64)\n",
            "1 14 Epoch: 58 | ANN: trainLoss: 0.0948 | trainAcc: 100.0000% (128/128)\n",
            "2 14 Epoch: 58 | ANN: trainLoss: 0.0857 | trainAcc: 100.0000% (192/192)\n",
            "3 14 Epoch: 58 | ANN: trainLoss: 0.0861 | trainAcc: 100.0000% (256/256)\n",
            "4 14 Epoch: 58 | ANN: trainLoss: 0.0830 | trainAcc: 100.0000% (320/320)\n",
            "5 14 Epoch: 58 | ANN: trainLoss: 0.0849 | trainAcc: 100.0000% (384/384)\n",
            "6 14 Epoch: 58 | ANN: trainLoss: 0.0913 | trainAcc: 99.7768% (447/448)\n",
            "7 14 Epoch: 58 | ANN: trainLoss: 0.0911 | trainAcc: 99.8047% (511/512)\n",
            "8 14 Epoch: 58 | ANN: trainLoss: 0.0930 | trainAcc: 99.6528% (574/576)\n",
            "9 14 Epoch: 58 | ANN: trainLoss: 0.0954 | trainAcc: 99.6875% (638/640)\n",
            "10 14 Epoch: 58 | ANN: trainLoss: 0.0948 | trainAcc: 99.7159% (702/704)\n",
            "11 14 Epoch: 58 | ANN: trainLoss: 0.0940 | trainAcc: 99.6094% (765/768)\n",
            "12 14 Epoch: 58 | ANN: trainLoss: 0.0949 | trainAcc: 99.5192% (828/832)\n",
            "13 14 Epoch: 58 | ANN: trainLoss: 0.0964 | trainAcc: 99.5392% (864/868)\n",
            "0 2 Epoch: 58 | ANN: testLoss: 0.8697 | testAcc: 65.6250% (42/64)\n",
            "1 2 Epoch: 58 | ANN: testLoss: 0.7641 | testAcc: 67.0103% (65/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 59 | ANN: trainLoss: 0.1066 | trainAcc: 98.4375% (63/64)\n",
            "1 14 Epoch: 59 | ANN: trainLoss: 0.1006 | trainAcc: 98.4375% (126/128)\n",
            "2 14 Epoch: 59 | ANN: trainLoss: 0.0966 | trainAcc: 98.4375% (189/192)\n",
            "3 14 Epoch: 59 | ANN: trainLoss: 0.0964 | trainAcc: 98.4375% (252/256)\n",
            "4 14 Epoch: 59 | ANN: trainLoss: 0.0981 | trainAcc: 98.4375% (315/320)\n",
            "5 14 Epoch: 59 | ANN: trainLoss: 0.0953 | trainAcc: 98.4375% (378/384)\n",
            "6 14 Epoch: 59 | ANN: trainLoss: 0.0962 | trainAcc: 98.6607% (442/448)\n",
            "7 14 Epoch: 59 | ANN: trainLoss: 0.0951 | trainAcc: 98.8281% (506/512)\n",
            "8 14 Epoch: 59 | ANN: trainLoss: 0.0938 | trainAcc: 98.9583% (570/576)\n",
            "9 14 Epoch: 59 | ANN: trainLoss: 0.0935 | trainAcc: 98.9062% (633/640)\n",
            "10 14 Epoch: 59 | ANN: trainLoss: 0.0920 | trainAcc: 99.0057% (697/704)\n",
            "11 14 Epoch: 59 | ANN: trainLoss: 0.0950 | trainAcc: 98.8281% (759/768)\n",
            "12 14 Epoch: 59 | ANN: trainLoss: 0.0985 | trainAcc: 98.9183% (823/832)\n",
            "13 14 Epoch: 59 | ANN: trainLoss: 0.0973 | trainAcc: 98.9631% (859/868)\n",
            "0 2 Epoch: 59 | ANN: testLoss: 0.8803 | testAcc: 67.1875% (43/64)\n",
            "1 2 Epoch: 59 | ANN: testLoss: 0.7546 | testAcc: 68.0412% (66/97)\n",
            "---------------------------------------------\n",
            "Converting using MaxNorm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:00<00:00, 356.46it/s]\n",
            "100%|██████████| 14/14 [00:00<00:00, 363.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ANN accuracy: Test: 68.0400%\n",
            "SNN accuracy: max_norm: 68.0412%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NN Visualization\n",
        "\n",
        "# This block should be placed after the training and SNN conversion\n",
        "# of the model you wish to visualize. For example, after the\n",
        "# \"LENet to SNN Conversion Framework execution\" cell or\n",
        "# after the \"LENet_FCL to SNN Conversion Framework execution\" cell.\n",
        "# The `cnn_model_lenet` and `Snn_model_lenet` variables from that preceding cell\n",
        "# will be used for visualization.\n",
        "\n",
        "# Install nnviz if you haven't already (uncomment the line below if needed)\n",
        "# !pip install nnviz\n",
        "\n",
        "# Import necessary nnviz modules\n",
        "from nnviz import drawing, inspection\n",
        "import torch # Ensure torch is imported\n",
        "\n",
        "print(\"Starting Neural Network Visualization...\")\n",
        "\n",
        "# --- CNN Model Visualization ---\n",
        "if 'cnn_model_lenet' in locals() and isinstance(cnn_model_lenet, torch.nn.Module):\n",
        "    model_to_visualize_cnn = cnn_model_lenet\n",
        "    cnn_model_lenet_name = type(model_to_visualize_cnn).__name__\n",
        "    print(f\"\\nVisualizing LENet CNN model: {cnn_model_lenet_name}\")\n",
        "\n",
        "    try:\n",
        "        # Move model to CPU for inspection (safer for fx tracing)\n",
        "        cnn_model_lenet_cpu = model_to_visualize_cnn.to('cpu')\n",
        "\n",
        "        # Create an inspector\n",
        "        # TorchFxInspector uses torch.fx to trace the model graph\n",
        "        cnn_inspector = inspection.TorchFxInspector()\n",
        "\n",
        "        # Inspect the CNN model\n",
        "        # For models like LENet_FCL, training (as done in your script) ensures that\n",
        "        # dynamically created layers (e.g., self.fc) are initialized before inspection.\n",
        "        print(f\"Inspecting {cnn_model_lenet_name} on CPU...\")\n",
        "        cnn_graph = cnn_inspector.inspect(cnn_model_lenet_cpu)\n",
        "\n",
        "        # Create a drawer for saving the visualization (e.g., to a PDF file)\n",
        "        # You can change the output format by changing the extension (e.g., .png, .svg)\n",
        "        cnn_viz_filename = f\"{cnn_model_lenet_name}_LENet_cnn_architecture.png\"\n",
        "        cnn_drawer = drawing.GraphvizDrawer(cnn_viz_filename)\n",
        "\n",
        "        # Draw the graph and save it to the file\n",
        "        cnn_drawer.draw(cnn_graph)\n",
        "        print(f\"LENet CNN model visualization saved to: {cnn_viz_filename}\")\n",
        "        print(\n",
        "            f\"Note: If the output file is empty or shows an error, ensure Graphviz \"\n",
        "            f\"(specifically the 'dot' command) is installed and accessible in your system's PATH.\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\n",
        "            f\"An error occurred during CNN model ({cnn_model_lenet_name}) visualization: {e}\"\n",
        "        )\n",
        "        print(\"Troubleshooting tips:\")\n",
        "        print(\"- Ensure 'nnviz' is installed.\")\n",
        "        print(\n",
        "            \"- Ensure 'graphviz' (dot executable) is installed and in your system's PATH.\"\n",
        "        )\n",
        "        print(\n",
        "            \"- The model structure might contain operations not traceable by torch.fx. \"\n",
        "            \"Check nnviz documentation for advanced usage or alternative inspectors if needed.\"\n",
        "        )\n",
        "else:\n",
        "    print(\n",
        "        \"\\nCNN model ('cnn_model_lenet') not found in the current scope, or it's not a torch.nn.Module. \"\n",
        "        \"Skipping CNN visualization.\"\n",
        "    )\n",
        "print(\"\\nNeural Network Visualization process complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8p4PokzI0HX",
        "outputId": "cbfe2367-74a8-450a-8654-23c5ee940b4d",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Neural Network Visualization...\n",
            "\n",
            "CNN model ('cnn_model_lenet') not found in the current scope, or it's not a torch.nn.Module. Skipping CNN visualization.\n",
            "\n",
            "Neural Network Visualization process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LENet_FCL to SNN Conversion Framework execution\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 60\n",
        "BATCH_SIZE = 64\n",
        "TIME_STEPS = 100  # T for SNN\n",
        "TEST_SIZE = 0.1\n",
        "DROP_OUT = 0.2\n",
        "\n",
        "\n",
        "# Split the data\n",
        "print(f\"{100 - (TEST_SIZE * 100)}% of the dataset is used for training and {TEST_SIZE * 100}% is used for testing.\")\n",
        "train_data, test_data, train_label, test_label = train_test_split(datasetX, datasetY, test_size=TEST_SIZE, shuffle=True,\n",
        "                                                                  random_state=0)\n",
        "\n",
        "# Initialize model\n",
        "cnn_model_lenet_fcl = LENet_FCL(classes_num=3, channel_count=channel_count, drop_out = DROP_OUT).to(device)\n",
        "cnn_model_lenet_fcl.apply(initialize_weights)\n",
        "\n",
        "# Train CNN model\n",
        "train_acc, test_acc,  cnn_model_lenet_fcl = train_ann(cnn_model_lenet_fcl, train_data, train_label, test_data, test_label,\n",
        "                                              ep=EPOCHS, batch=BATCH_SIZE)\n",
        "max_norm_acc = anntosnn( cnn_model_lenet_fcl, train_data, train_label, test_data, test_label,\n",
        "                        batch=BATCH_SIZE, T=TIME_STEPS)\n",
        "snn_model_lenet_fcl = ann2snn.Converter(mode='max', dataloader=data_loader(train_data, train_label, batch=BATCH_SIZE))( cnn_model)\n",
        "\n",
        "print('\\n')\n",
        "print('ANN accuracy: Test: %.4f%%' % (test_acc * 100))\n",
        "print('SNN accuracy: max_norm: %.4f%%' % (max_norm_acc[-1] * 100))"
      ],
      "metadata": {
        "id": "AeiiLk4TQp-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "93531e47-65ea-4e24-ff50-c78b8d235e29"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90.0% of the dataset is used for training and 10.0% is used for testing.\n",
            "\n",
            "\n",
            "0 14 Epoch: 0 | ANN: trainLoss: 13.5148 | trainAcc: 40.6250% (26/64)\n",
            "1 14 Epoch: 0 | ANN: trainLoss: 13.2169 | trainAcc: 35.9375% (46/128)\n",
            "2 14 Epoch: 0 | ANN: trainLoss: 12.9593 | trainAcc: 36.4583% (70/192)\n",
            "3 14 Epoch: 0 | ANN: trainLoss: 12.7848 | trainAcc: 36.7188% (94/256)\n",
            "4 14 Epoch: 0 | ANN: trainLoss: 11.7772 | trainAcc: 36.8750% (118/320)\n",
            "5 14 Epoch: 0 | ANN: trainLoss: 11.2119 | trainAcc: 38.8021% (149/384)\n",
            "6 14 Epoch: 0 | ANN: trainLoss: 10.7812 | trainAcc: 38.1696% (171/448)\n",
            "7 14 Epoch: 0 | ANN: trainLoss: 10.3474 | trainAcc: 37.8906% (194/512)\n",
            "8 14 Epoch: 0 | ANN: trainLoss: 10.1498 | trainAcc: 37.5000% (216/576)\n",
            "9 14 Epoch: 0 | ANN: trainLoss: 10.1368 | trainAcc: 36.5625% (234/640)\n",
            "10 14 Epoch: 0 | ANN: trainLoss: 9.7865 | trainAcc: 37.0739% (261/704)\n",
            "11 14 Epoch: 0 | ANN: trainLoss: 9.5182 | trainAcc: 37.1094% (285/768)\n",
            "12 14 Epoch: 0 | ANN: trainLoss: 9.1674 | trainAcc: 37.3798% (311/832)\n",
            "13 14 Epoch: 0 | ANN: trainLoss: 9.0471 | trainAcc: 37.6728% (327/868)\n",
            "0 2 Epoch: 0 | ANN: testLoss: 7.0605 | testAcc: 42.1875% (27/64)\n",
            "1 2 Epoch: 0 | ANN: testLoss: 6.8803 | testAcc: 41.2371% (40/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 1 | ANN: trainLoss: 4.8011 | trainAcc: 50.0000% (32/64)\n",
            "1 14 Epoch: 1 | ANN: trainLoss: 5.3142 | trainAcc: 46.0938% (59/128)\n",
            "2 14 Epoch: 1 | ANN: trainLoss: 5.9422 | trainAcc: 45.3125% (87/192)\n",
            "3 14 Epoch: 1 | ANN: trainLoss: 5.7165 | trainAcc: 43.3594% (111/256)\n",
            "4 14 Epoch: 1 | ANN: trainLoss: 5.3901 | trainAcc: 44.3750% (142/320)\n",
            "5 14 Epoch: 1 | ANN: trainLoss: 5.1648 | trainAcc: 43.4896% (167/384)\n",
            "6 14 Epoch: 1 | ANN: trainLoss: 5.2000 | trainAcc: 43.0804% (193/448)\n",
            "7 14 Epoch: 1 | ANN: trainLoss: 5.1932 | trainAcc: 41.9922% (215/512)\n",
            "8 14 Epoch: 1 | ANN: trainLoss: 5.1412 | trainAcc: 42.1875% (243/576)\n",
            "9 14 Epoch: 1 | ANN: trainLoss: 5.2964 | trainAcc: 41.5625% (266/640)\n",
            "10 14 Epoch: 1 | ANN: trainLoss: 5.3749 | trainAcc: 41.6193% (293/704)\n",
            "11 14 Epoch: 1 | ANN: trainLoss: 5.3026 | trainAcc: 41.6667% (320/768)\n",
            "12 14 Epoch: 1 | ANN: trainLoss: 5.2572 | trainAcc: 41.3462% (344/832)\n",
            "13 14 Epoch: 1 | ANN: trainLoss: 5.3617 | trainAcc: 40.7834% (354/868)\n",
            "0 2 Epoch: 1 | ANN: testLoss: 6.9499 | testAcc: 32.8125% (21/64)\n",
            "1 2 Epoch: 1 | ANN: testLoss: 6.7511 | testAcc: 37.1134% (36/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 2 | ANN: trainLoss: 7.1995 | trainAcc: 32.8125% (21/64)\n",
            "1 14 Epoch: 2 | ANN: trainLoss: 5.6260 | trainAcc: 39.0625% (50/128)\n",
            "2 14 Epoch: 2 | ANN: trainLoss: 5.3874 | trainAcc: 39.0625% (75/192)\n",
            "3 14 Epoch: 2 | ANN: trainLoss: 4.9626 | trainAcc: 38.6719% (99/256)\n",
            "4 14 Epoch: 2 | ANN: trainLoss: 4.8110 | trainAcc: 39.6875% (127/320)\n",
            "5 14 Epoch: 2 | ANN: trainLoss: 4.7016 | trainAcc: 40.1042% (154/384)\n",
            "6 14 Epoch: 2 | ANN: trainLoss: 4.3206 | trainAcc: 41.5179% (186/448)\n",
            "7 14 Epoch: 2 | ANN: trainLoss: 4.3295 | trainAcc: 40.2344% (206/512)\n",
            "8 14 Epoch: 2 | ANN: trainLoss: 4.4093 | trainAcc: 40.4514% (233/576)\n",
            "9 14 Epoch: 2 | ANN: trainLoss: 4.3968 | trainAcc: 40.4688% (259/640)\n",
            "10 14 Epoch: 2 | ANN: trainLoss: 4.2815 | trainAcc: 40.9091% (288/704)\n",
            "11 14 Epoch: 2 | ANN: trainLoss: 4.1285 | trainAcc: 41.2760% (317/768)\n",
            "12 14 Epoch: 2 | ANN: trainLoss: 4.0489 | trainAcc: 42.1875% (351/832)\n",
            "13 14 Epoch: 2 | ANN: trainLoss: 4.0174 | trainAcc: 42.3963% (368/868)\n",
            "0 2 Epoch: 2 | ANN: testLoss: 3.4963 | testAcc: 40.6250% (26/64)\n",
            "1 2 Epoch: 2 | ANN: testLoss: 2.7787 | testAcc: 44.3299% (43/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 3 | ANN: trainLoss: 3.7223 | trainAcc: 40.6250% (26/64)\n",
            "1 14 Epoch: 3 | ANN: trainLoss: 3.4132 | trainAcc: 40.6250% (52/128)\n",
            "2 14 Epoch: 3 | ANN: trainLoss: 3.5217 | trainAcc: 43.7500% (84/192)\n",
            "3 14 Epoch: 3 | ANN: trainLoss: 3.7447 | trainAcc: 44.9219% (115/256)\n",
            "4 14 Epoch: 3 | ANN: trainLoss: 3.5272 | trainAcc: 44.6875% (143/320)\n",
            "5 14 Epoch: 3 | ANN: trainLoss: 3.2373 | trainAcc: 47.1354% (181/384)\n",
            "6 14 Epoch: 3 | ANN: trainLoss: 3.1802 | trainAcc: 47.5446% (213/448)\n",
            "7 14 Epoch: 3 | ANN: trainLoss: 3.0879 | trainAcc: 47.2656% (242/512)\n",
            "8 14 Epoch: 3 | ANN: trainLoss: 3.2042 | trainAcc: 47.0486% (271/576)\n",
            "9 14 Epoch: 3 | ANN: trainLoss: 3.1847 | trainAcc: 47.1875% (302/640)\n",
            "10 14 Epoch: 3 | ANN: trainLoss: 3.2570 | trainAcc: 46.7330% (329/704)\n",
            "11 14 Epoch: 3 | ANN: trainLoss: 3.1940 | trainAcc: 46.3542% (356/768)\n",
            "12 14 Epoch: 3 | ANN: trainLoss: 3.1309 | trainAcc: 46.8750% (390/832)\n",
            "13 14 Epoch: 3 | ANN: trainLoss: 3.1296 | trainAcc: 47.1198% (409/868)\n",
            "0 2 Epoch: 3 | ANN: testLoss: 2.7371 | testAcc: 43.7500% (28/64)\n",
            "1 2 Epoch: 3 | ANN: testLoss: 2.1865 | testAcc: 43.2990% (42/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 4 | ANN: trainLoss: 2.5457 | trainAcc: 39.0625% (25/64)\n",
            "1 14 Epoch: 4 | ANN: trainLoss: 2.3642 | trainAcc: 41.4062% (53/128)\n",
            "2 14 Epoch: 4 | ANN: trainLoss: 2.3738 | trainAcc: 43.2292% (83/192)\n",
            "3 14 Epoch: 4 | ANN: trainLoss: 2.4660 | trainAcc: 44.1406% (113/256)\n",
            "4 14 Epoch: 4 | ANN: trainLoss: 2.6518 | trainAcc: 44.0625% (141/320)\n",
            "5 14 Epoch: 4 | ANN: trainLoss: 2.3899 | trainAcc: 46.0938% (177/384)\n",
            "6 14 Epoch: 4 | ANN: trainLoss: 2.2883 | trainAcc: 46.4286% (208/448)\n",
            "7 14 Epoch: 4 | ANN: trainLoss: 2.3069 | trainAcc: 47.6562% (244/512)\n",
            "8 14 Epoch: 4 | ANN: trainLoss: 2.3145 | trainAcc: 48.2639% (278/576)\n",
            "9 14 Epoch: 4 | ANN: trainLoss: 2.3492 | trainAcc: 48.5938% (311/640)\n",
            "10 14 Epoch: 4 | ANN: trainLoss: 2.2879 | trainAcc: 48.7216% (343/704)\n",
            "11 14 Epoch: 4 | ANN: trainLoss: 2.2888 | trainAcc: 48.9583% (376/768)\n",
            "12 14 Epoch: 4 | ANN: trainLoss: 2.3123 | trainAcc: 49.2788% (410/832)\n",
            "13 14 Epoch: 4 | ANN: trainLoss: 2.2251 | trainAcc: 49.6544% (431/868)\n",
            "0 2 Epoch: 4 | ANN: testLoss: 1.5571 | testAcc: 45.3125% (29/64)\n",
            "1 2 Epoch: 4 | ANN: testLoss: 1.8125 | testAcc: 49.4845% (48/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 5 | ANN: trainLoss: 1.6401 | trainAcc: 48.4375% (31/64)\n",
            "1 14 Epoch: 5 | ANN: trainLoss: 2.1339 | trainAcc: 48.4375% (62/128)\n",
            "2 14 Epoch: 5 | ANN: trainLoss: 2.0331 | trainAcc: 51.5625% (99/192)\n",
            "3 14 Epoch: 5 | ANN: trainLoss: 1.9942 | trainAcc: 51.1719% (131/256)\n",
            "4 14 Epoch: 5 | ANN: trainLoss: 1.9435 | trainAcc: 51.5625% (165/320)\n",
            "5 14 Epoch: 5 | ANN: trainLoss: 1.9452 | trainAcc: 51.3021% (197/384)\n",
            "6 14 Epoch: 5 | ANN: trainLoss: 1.9777 | trainAcc: 50.4464% (226/448)\n",
            "7 14 Epoch: 5 | ANN: trainLoss: 1.8642 | trainAcc: 52.1484% (267/512)\n",
            "8 14 Epoch: 5 | ANN: trainLoss: 1.7836 | trainAcc: 52.7778% (304/576)\n",
            "9 14 Epoch: 5 | ANN: trainLoss: 1.8000 | trainAcc: 52.3438% (335/640)\n",
            "10 14 Epoch: 5 | ANN: trainLoss: 1.8539 | trainAcc: 51.7045% (364/704)\n",
            "11 14 Epoch: 5 | ANN: trainLoss: 1.8374 | trainAcc: 52.2135% (401/768)\n",
            "12 14 Epoch: 5 | ANN: trainLoss: 1.8357 | trainAcc: 53.2452% (443/832)\n",
            "13 14 Epoch: 5 | ANN: trainLoss: 1.8433 | trainAcc: 53.2258% (462/868)\n",
            "0 2 Epoch: 5 | ANN: testLoss: 1.3655 | testAcc: 48.4375% (31/64)\n",
            "1 2 Epoch: 5 | ANN: testLoss: 1.6273 | testAcc: 48.4536% (47/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 6 | ANN: trainLoss: 2.2524 | trainAcc: 43.7500% (28/64)\n",
            "1 14 Epoch: 6 | ANN: trainLoss: 1.8567 | trainAcc: 49.2188% (63/128)\n",
            "2 14 Epoch: 6 | ANN: trainLoss: 1.8092 | trainAcc: 50.0000% (96/192)\n",
            "3 14 Epoch: 6 | ANN: trainLoss: 1.6996 | trainAcc: 52.3438% (134/256)\n",
            "4 14 Epoch: 6 | ANN: trainLoss: 1.5862 | trainAcc: 52.8125% (169/320)\n",
            "5 14 Epoch: 6 | ANN: trainLoss: 1.6140 | trainAcc: 52.3438% (201/384)\n",
            "6 14 Epoch: 6 | ANN: trainLoss: 1.6385 | trainAcc: 52.2321% (234/448)\n",
            "7 14 Epoch: 6 | ANN: trainLoss: 1.5719 | trainAcc: 53.5156% (274/512)\n",
            "8 14 Epoch: 6 | ANN: trainLoss: 1.5508 | trainAcc: 54.1667% (312/576)\n",
            "9 14 Epoch: 6 | ANN: trainLoss: 1.5400 | trainAcc: 54.6875% (350/640)\n",
            "10 14 Epoch: 6 | ANN: trainLoss: 1.5769 | trainAcc: 53.8352% (379/704)\n",
            "11 14 Epoch: 6 | ANN: trainLoss: 1.6100 | trainAcc: 53.3854% (410/768)\n",
            "12 14 Epoch: 6 | ANN: trainLoss: 1.5837 | trainAcc: 53.6058% (446/832)\n",
            "13 14 Epoch: 6 | ANN: trainLoss: 1.6079 | trainAcc: 52.8802% (459/868)\n",
            "0 2 Epoch: 6 | ANN: testLoss: 1.2265 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 6 | ANN: testLoss: 1.5392 | testAcc: 53.6082% (52/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 7 | ANN: trainLoss: 1.6189 | trainAcc: 51.5625% (33/64)\n",
            "1 14 Epoch: 7 | ANN: trainLoss: 1.6948 | trainAcc: 50.7812% (65/128)\n",
            "2 14 Epoch: 7 | ANN: trainLoss: 1.4754 | trainAcc: 49.4792% (95/192)\n",
            "3 14 Epoch: 7 | ANN: trainLoss: 1.5483 | trainAcc: 49.2188% (126/256)\n",
            "4 14 Epoch: 7 | ANN: trainLoss: 1.4654 | trainAcc: 49.6875% (159/320)\n",
            "5 14 Epoch: 7 | ANN: trainLoss: 1.3827 | trainAcc: 52.3438% (201/384)\n",
            "6 14 Epoch: 7 | ANN: trainLoss: 1.3379 | trainAcc: 54.6875% (245/448)\n",
            "7 14 Epoch: 7 | ANN: trainLoss: 1.3284 | trainAcc: 54.6875% (280/512)\n",
            "8 14 Epoch: 7 | ANN: trainLoss: 1.3278 | trainAcc: 55.5556% (320/576)\n",
            "9 14 Epoch: 7 | ANN: trainLoss: 1.3163 | trainAcc: 55.4688% (355/640)\n",
            "10 14 Epoch: 7 | ANN: trainLoss: 1.3216 | trainAcc: 54.6875% (385/704)\n",
            "11 14 Epoch: 7 | ANN: trainLoss: 1.3309 | trainAcc: 54.2969% (417/768)\n",
            "12 14 Epoch: 7 | ANN: trainLoss: 1.3325 | trainAcc: 54.4471% (453/832)\n",
            "13 14 Epoch: 7 | ANN: trainLoss: 1.3614 | trainAcc: 54.3779% (472/868)\n",
            "0 2 Epoch: 7 | ANN: testLoss: 1.5843 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 7 | ANN: testLoss: 1.2567 | testAcc: 54.6392% (53/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 8 | ANN: trainLoss: 1.5665 | trainAcc: 43.7500% (28/64)\n",
            "1 14 Epoch: 8 | ANN: trainLoss: 1.5314 | trainAcc: 48.4375% (62/128)\n",
            "2 14 Epoch: 8 | ANN: trainLoss: 1.5157 | trainAcc: 50.5208% (97/192)\n",
            "3 14 Epoch: 8 | ANN: trainLoss: 1.3828 | trainAcc: 52.7344% (135/256)\n",
            "4 14 Epoch: 8 | ANN: trainLoss: 1.3893 | trainAcc: 52.5000% (168/320)\n",
            "5 14 Epoch: 8 | ANN: trainLoss: 1.3858 | trainAcc: 52.3438% (201/384)\n",
            "6 14 Epoch: 8 | ANN: trainLoss: 1.3896 | trainAcc: 52.6786% (236/448)\n",
            "7 14 Epoch: 8 | ANN: trainLoss: 1.4388 | trainAcc: 51.1719% (262/512)\n",
            "8 14 Epoch: 8 | ANN: trainLoss: 1.4432 | trainAcc: 51.0417% (294/576)\n",
            "9 14 Epoch: 8 | ANN: trainLoss: 1.4045 | trainAcc: 52.0312% (333/640)\n",
            "10 14 Epoch: 8 | ANN: trainLoss: 1.3749 | trainAcc: 52.2727% (368/704)\n",
            "11 14 Epoch: 8 | ANN: trainLoss: 1.3896 | trainAcc: 52.2135% (401/768)\n",
            "12 14 Epoch: 8 | ANN: trainLoss: 1.3808 | trainAcc: 52.0433% (433/832)\n",
            "13 14 Epoch: 8 | ANN: trainLoss: 1.3611 | trainAcc: 52.1889% (453/868)\n",
            "0 2 Epoch: 8 | ANN: testLoss: 1.2464 | testAcc: 46.8750% (30/64)\n",
            "1 2 Epoch: 8 | ANN: testLoss: 1.0823 | testAcc: 50.5155% (49/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 9 | ANN: trainLoss: 0.9789 | trainAcc: 53.1250% (34/64)\n",
            "1 14 Epoch: 9 | ANN: trainLoss: 1.0390 | trainAcc: 53.1250% (68/128)\n",
            "2 14 Epoch: 9 | ANN: trainLoss: 1.0271 | trainAcc: 57.2917% (110/192)\n",
            "3 14 Epoch: 9 | ANN: trainLoss: 1.0661 | trainAcc: 55.4688% (142/256)\n",
            "4 14 Epoch: 9 | ANN: trainLoss: 1.1252 | trainAcc: 56.5625% (181/320)\n",
            "5 14 Epoch: 9 | ANN: trainLoss: 1.1158 | trainAcc: 56.7708% (218/384)\n",
            "6 14 Epoch: 9 | ANN: trainLoss: 1.1210 | trainAcc: 57.3661% (257/448)\n",
            "7 14 Epoch: 9 | ANN: trainLoss: 1.1192 | trainAcc: 56.8359% (291/512)\n",
            "8 14 Epoch: 9 | ANN: trainLoss: 1.1261 | trainAcc: 57.2917% (330/576)\n",
            "9 14 Epoch: 9 | ANN: trainLoss: 1.1479 | trainAcc: 57.3438% (367/640)\n",
            "10 14 Epoch: 9 | ANN: trainLoss: 1.1383 | trainAcc: 56.3920% (397/704)\n",
            "11 14 Epoch: 9 | ANN: trainLoss: 1.1171 | trainAcc: 57.1615% (439/768)\n",
            "12 14 Epoch: 9 | ANN: trainLoss: 1.1142 | trainAcc: 56.7308% (472/832)\n",
            "13 14 Epoch: 9 | ANN: trainLoss: 1.1335 | trainAcc: 55.8756% (485/868)\n",
            "0 2 Epoch: 9 | ANN: testLoss: 1.0783 | testAcc: 51.5625% (33/64)\n",
            "1 2 Epoch: 9 | ANN: testLoss: 1.0989 | testAcc: 52.5773% (51/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 10 | ANN: trainLoss: 0.9640 | trainAcc: 57.8125% (37/64)\n",
            "1 14 Epoch: 10 | ANN: trainLoss: 0.9745 | trainAcc: 61.7188% (79/128)\n",
            "2 14 Epoch: 10 | ANN: trainLoss: 0.9871 | trainAcc: 59.3750% (114/192)\n",
            "3 14 Epoch: 10 | ANN: trainLoss: 0.9420 | trainAcc: 60.5469% (155/256)\n",
            "4 14 Epoch: 10 | ANN: trainLoss: 0.9643 | trainAcc: 59.3750% (190/320)\n",
            "5 14 Epoch: 10 | ANN: trainLoss: 0.9877 | trainAcc: 59.1146% (227/384)\n",
            "6 14 Epoch: 10 | ANN: trainLoss: 0.9687 | trainAcc: 58.9286% (264/448)\n",
            "7 14 Epoch: 10 | ANN: trainLoss: 0.9777 | trainAcc: 58.7891% (301/512)\n",
            "8 14 Epoch: 10 | ANN: trainLoss: 0.9576 | trainAcc: 59.2014% (341/576)\n",
            "9 14 Epoch: 10 | ANN: trainLoss: 0.9795 | trainAcc: 58.9062% (377/640)\n",
            "10 14 Epoch: 10 | ANN: trainLoss: 0.9860 | trainAcc: 58.8068% (414/704)\n",
            "11 14 Epoch: 10 | ANN: trainLoss: 0.9925 | trainAcc: 57.4219% (441/768)\n",
            "12 14 Epoch: 10 | ANN: trainLoss: 0.9901 | trainAcc: 56.9712% (474/832)\n",
            "13 14 Epoch: 10 | ANN: trainLoss: 1.0025 | trainAcc: 56.5668% (491/868)\n",
            "0 2 Epoch: 10 | ANN: testLoss: 1.0555 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 10 | ANN: testLoss: 1.0405 | testAcc: 52.5773% (51/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 11 | ANN: trainLoss: 0.9495 | trainAcc: 57.8125% (37/64)\n",
            "1 14 Epoch: 11 | ANN: trainLoss: 0.9311 | trainAcc: 54.6875% (70/128)\n",
            "2 14 Epoch: 11 | ANN: trainLoss: 0.9446 | trainAcc: 54.1667% (104/192)\n",
            "3 14 Epoch: 11 | ANN: trainLoss: 0.9944 | trainAcc: 55.0781% (141/256)\n",
            "4 14 Epoch: 11 | ANN: trainLoss: 0.9686 | trainAcc: 55.6250% (178/320)\n",
            "5 14 Epoch: 11 | ANN: trainLoss: 0.9829 | trainAcc: 55.9896% (215/384)\n",
            "6 14 Epoch: 11 | ANN: trainLoss: 0.9856 | trainAcc: 56.0268% (251/448)\n",
            "7 14 Epoch: 11 | ANN: trainLoss: 0.9712 | trainAcc: 57.6172% (295/512)\n",
            "8 14 Epoch: 11 | ANN: trainLoss: 0.9678 | trainAcc: 56.9444% (328/576)\n",
            "9 14 Epoch: 11 | ANN: trainLoss: 0.9789 | trainAcc: 56.0938% (359/640)\n",
            "10 14 Epoch: 11 | ANN: trainLoss: 0.9654 | trainAcc: 56.9602% (401/704)\n",
            "11 14 Epoch: 11 | ANN: trainLoss: 0.9674 | trainAcc: 57.6823% (443/768)\n",
            "12 14 Epoch: 11 | ANN: trainLoss: 0.9699 | trainAcc: 57.5721% (479/832)\n",
            "13 14 Epoch: 11 | ANN: trainLoss: 0.9641 | trainAcc: 57.4885% (499/868)\n",
            "0 2 Epoch: 11 | ANN: testLoss: 0.9661 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 11 | ANN: testLoss: 1.0670 | testAcc: 54.6392% (53/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 12 | ANN: trainLoss: 0.9358 | trainAcc: 56.2500% (36/64)\n",
            "1 14 Epoch: 12 | ANN: trainLoss: 0.9523 | trainAcc: 54.6875% (70/128)\n",
            "2 14 Epoch: 12 | ANN: trainLoss: 0.8805 | trainAcc: 59.3750% (114/192)\n",
            "3 14 Epoch: 12 | ANN: trainLoss: 0.8544 | trainAcc: 60.1562% (154/256)\n",
            "4 14 Epoch: 12 | ANN: trainLoss: 0.9175 | trainAcc: 57.5000% (184/320)\n",
            "5 14 Epoch: 12 | ANN: trainLoss: 0.9195 | trainAcc: 57.2917% (220/384)\n",
            "6 14 Epoch: 12 | ANN: trainLoss: 0.9357 | trainAcc: 58.0357% (260/448)\n",
            "7 14 Epoch: 12 | ANN: trainLoss: 0.9289 | trainAcc: 58.5938% (300/512)\n",
            "8 14 Epoch: 12 | ANN: trainLoss: 0.9260 | trainAcc: 58.8542% (339/576)\n",
            "9 14 Epoch: 12 | ANN: trainLoss: 0.9125 | trainAcc: 59.2188% (379/640)\n",
            "10 14 Epoch: 12 | ANN: trainLoss: 0.9052 | trainAcc: 60.0852% (423/704)\n",
            "11 14 Epoch: 12 | ANN: trainLoss: 0.9072 | trainAcc: 59.3750% (456/768)\n",
            "12 14 Epoch: 12 | ANN: trainLoss: 0.9101 | trainAcc: 59.1346% (492/832)\n",
            "13 14 Epoch: 12 | ANN: trainLoss: 0.9134 | trainAcc: 58.9862% (512/868)\n",
            "0 2 Epoch: 12 | ANN: testLoss: 1.0461 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 12 | ANN: testLoss: 0.9585 | testAcc: 55.6701% (54/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 13 | ANN: trainLoss: 0.9220 | trainAcc: 60.9375% (39/64)\n",
            "1 14 Epoch: 13 | ANN: trainLoss: 0.9730 | trainAcc: 58.5938% (75/128)\n",
            "2 14 Epoch: 13 | ANN: trainLoss: 0.9357 | trainAcc: 58.3333% (112/192)\n",
            "3 14 Epoch: 13 | ANN: trainLoss: 0.9555 | trainAcc: 58.9844% (151/256)\n",
            "4 14 Epoch: 13 | ANN: trainLoss: 0.9635 | trainAcc: 58.1250% (186/320)\n",
            "5 14 Epoch: 13 | ANN: trainLoss: 0.9406 | trainAcc: 58.3333% (224/384)\n",
            "6 14 Epoch: 13 | ANN: trainLoss: 0.9339 | trainAcc: 58.9286% (264/448)\n",
            "7 14 Epoch: 13 | ANN: trainLoss: 0.9253 | trainAcc: 58.7891% (301/512)\n",
            "8 14 Epoch: 13 | ANN: trainLoss: 0.9200 | trainAcc: 57.9861% (334/576)\n",
            "9 14 Epoch: 13 | ANN: trainLoss: 0.9233 | trainAcc: 58.4375% (374/640)\n",
            "10 14 Epoch: 13 | ANN: trainLoss: 0.9154 | trainAcc: 59.0909% (416/704)\n",
            "11 14 Epoch: 13 | ANN: trainLoss: 0.9067 | trainAcc: 59.5052% (457/768)\n",
            "12 14 Epoch: 13 | ANN: trainLoss: 0.9098 | trainAcc: 59.3750% (494/832)\n",
            "13 14 Epoch: 13 | ANN: trainLoss: 0.8874 | trainAcc: 59.9078% (520/868)\n",
            "0 2 Epoch: 13 | ANN: testLoss: 0.8899 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 13 | ANN: testLoss: 0.9797 | testAcc: 56.7010% (55/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 14 | ANN: trainLoss: 1.0501 | trainAcc: 53.1250% (34/64)\n",
            "1 14 Epoch: 14 | ANN: trainLoss: 1.0261 | trainAcc: 55.4688% (71/128)\n",
            "2 14 Epoch: 14 | ANN: trainLoss: 0.8778 | trainAcc: 64.0625% (123/192)\n",
            "3 14 Epoch: 14 | ANN: trainLoss: 0.9104 | trainAcc: 60.9375% (156/256)\n",
            "4 14 Epoch: 14 | ANN: trainLoss: 0.8888 | trainAcc: 61.8750% (198/320)\n",
            "5 14 Epoch: 14 | ANN: trainLoss: 0.8936 | trainAcc: 59.6354% (229/384)\n",
            "6 14 Epoch: 14 | ANN: trainLoss: 0.8805 | trainAcc: 59.5982% (267/448)\n",
            "7 14 Epoch: 14 | ANN: trainLoss: 0.8710 | trainAcc: 61.1328% (313/512)\n",
            "8 14 Epoch: 14 | ANN: trainLoss: 0.8710 | trainAcc: 61.1111% (352/576)\n",
            "9 14 Epoch: 14 | ANN: trainLoss: 0.8647 | trainAcc: 61.2500% (392/640)\n",
            "10 14 Epoch: 14 | ANN: trainLoss: 0.8567 | trainAcc: 61.7898% (435/704)\n",
            "11 14 Epoch: 14 | ANN: trainLoss: 0.8551 | trainAcc: 62.2396% (478/768)\n",
            "12 14 Epoch: 14 | ANN: trainLoss: 0.8549 | trainAcc: 62.0192% (516/832)\n",
            "13 14 Epoch: 14 | ANN: trainLoss: 0.8610 | trainAcc: 61.5207% (534/868)\n",
            "0 2 Epoch: 14 | ANN: testLoss: 0.9163 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 14 | ANN: testLoss: 0.9919 | testAcc: 59.7938% (58/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 15 | ANN: trainLoss: 0.8758 | trainAcc: 57.8125% (37/64)\n",
            "1 14 Epoch: 15 | ANN: trainLoss: 0.8396 | trainAcc: 61.7188% (79/128)\n",
            "2 14 Epoch: 15 | ANN: trainLoss: 0.8247 | trainAcc: 59.8958% (115/192)\n",
            "3 14 Epoch: 15 | ANN: trainLoss: 0.8365 | trainAcc: 58.9844% (151/256)\n",
            "4 14 Epoch: 15 | ANN: trainLoss: 0.8144 | trainAcc: 61.5625% (197/320)\n",
            "5 14 Epoch: 15 | ANN: trainLoss: 0.8160 | trainAcc: 61.9792% (238/384)\n",
            "6 14 Epoch: 15 | ANN: trainLoss: 0.8404 | trainAcc: 60.7143% (272/448)\n",
            "7 14 Epoch: 15 | ANN: trainLoss: 0.8341 | trainAcc: 61.9141% (317/512)\n",
            "8 14 Epoch: 15 | ANN: trainLoss: 0.8317 | trainAcc: 62.5000% (360/576)\n",
            "9 14 Epoch: 15 | ANN: trainLoss: 0.8346 | trainAcc: 62.8125% (402/640)\n",
            "10 14 Epoch: 15 | ANN: trainLoss: 0.8284 | trainAcc: 63.0682% (444/704)\n",
            "11 14 Epoch: 15 | ANN: trainLoss: 0.8403 | trainAcc: 63.2812% (486/768)\n",
            "12 14 Epoch: 15 | ANN: trainLoss: 0.8337 | trainAcc: 63.8221% (531/832)\n",
            "13 14 Epoch: 15 | ANN: trainLoss: 0.8984 | trainAcc: 63.5945% (552/868)\n",
            "0 2 Epoch: 15 | ANN: testLoss: 0.9836 | testAcc: 53.1250% (34/64)\n",
            "1 2 Epoch: 15 | ANN: testLoss: 0.8575 | testAcc: 56.7010% (55/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 16 | ANN: trainLoss: 0.8668 | trainAcc: 67.1875% (43/64)\n",
            "1 14 Epoch: 16 | ANN: trainLoss: 0.8193 | trainAcc: 65.6250% (84/128)\n",
            "2 14 Epoch: 16 | ANN: trainLoss: 0.8296 | trainAcc: 66.1458% (127/192)\n",
            "3 14 Epoch: 16 | ANN: trainLoss: 0.8322 | trainAcc: 65.2344% (167/256)\n",
            "4 14 Epoch: 16 | ANN: trainLoss: 0.8234 | trainAcc: 66.5625% (213/320)\n",
            "5 14 Epoch: 16 | ANN: trainLoss: 0.8068 | trainAcc: 66.9271% (257/384)\n",
            "6 14 Epoch: 16 | ANN: trainLoss: 0.8202 | trainAcc: 66.2946% (297/448)\n",
            "7 14 Epoch: 16 | ANN: trainLoss: 0.8314 | trainAcc: 65.8203% (337/512)\n",
            "8 14 Epoch: 16 | ANN: trainLoss: 0.8111 | trainAcc: 66.6667% (384/576)\n",
            "9 14 Epoch: 16 | ANN: trainLoss: 0.8187 | trainAcc: 66.0938% (423/640)\n",
            "10 14 Epoch: 16 | ANN: trainLoss: 0.8277 | trainAcc: 65.1989% (459/704)\n",
            "11 14 Epoch: 16 | ANN: trainLoss: 0.8349 | trainAcc: 65.2344% (501/768)\n",
            "12 14 Epoch: 16 | ANN: trainLoss: 0.8266 | trainAcc: 65.6250% (546/832)\n",
            "13 14 Epoch: 16 | ANN: trainLoss: 0.8401 | trainAcc: 65.2074% (566/868)\n",
            "0 2 Epoch: 16 | ANN: testLoss: 0.8951 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 16 | ANN: testLoss: 0.9247 | testAcc: 58.7629% (57/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 17 | ANN: trainLoss: 0.7975 | trainAcc: 67.1875% (43/64)\n",
            "1 14 Epoch: 17 | ANN: trainLoss: 0.8584 | trainAcc: 66.4062% (85/128)\n",
            "2 14 Epoch: 17 | ANN: trainLoss: 0.8776 | trainAcc: 66.6667% (128/192)\n",
            "3 14 Epoch: 17 | ANN: trainLoss: 0.8774 | trainAcc: 66.0156% (169/256)\n",
            "4 14 Epoch: 17 | ANN: trainLoss: 0.8912 | trainAcc: 63.7500% (204/320)\n",
            "5 14 Epoch: 17 | ANN: trainLoss: 0.8835 | trainAcc: 63.8021% (245/384)\n",
            "6 14 Epoch: 17 | ANN: trainLoss: 0.8669 | trainAcc: 63.1696% (283/448)\n",
            "7 14 Epoch: 17 | ANN: trainLoss: 0.8546 | trainAcc: 63.6719% (326/512)\n",
            "8 14 Epoch: 17 | ANN: trainLoss: 0.8430 | trainAcc: 63.8889% (368/576)\n",
            "9 14 Epoch: 17 | ANN: trainLoss: 0.8301 | trainAcc: 64.2188% (411/640)\n",
            "10 14 Epoch: 17 | ANN: trainLoss: 0.8200 | trainAcc: 64.9148% (457/704)\n",
            "11 14 Epoch: 17 | ANN: trainLoss: 0.8185 | trainAcc: 64.8438% (498/768)\n",
            "12 14 Epoch: 17 | ANN: trainLoss: 0.8252 | trainAcc: 64.4231% (536/832)\n",
            "13 14 Epoch: 17 | ANN: trainLoss: 0.8340 | trainAcc: 64.5161% (560/868)\n",
            "0 2 Epoch: 17 | ANN: testLoss: 0.8565 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 17 | ANN: testLoss: 0.9066 | testAcc: 59.7938% (58/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 18 | ANN: trainLoss: 0.8574 | trainAcc: 54.6875% (35/64)\n",
            "1 14 Epoch: 18 | ANN: trainLoss: 0.8263 | trainAcc: 58.5938% (75/128)\n",
            "2 14 Epoch: 18 | ANN: trainLoss: 0.8039 | trainAcc: 60.4167% (116/192)\n",
            "3 14 Epoch: 18 | ANN: trainLoss: 0.8034 | trainAcc: 61.3281% (157/256)\n",
            "4 14 Epoch: 18 | ANN: trainLoss: 0.8135 | trainAcc: 61.8750% (198/320)\n",
            "5 14 Epoch: 18 | ANN: trainLoss: 0.7996 | trainAcc: 63.2812% (243/384)\n",
            "6 14 Epoch: 18 | ANN: trainLoss: 0.8123 | trainAcc: 63.1696% (283/448)\n",
            "7 14 Epoch: 18 | ANN: trainLoss: 0.8128 | trainAcc: 63.0859% (323/512)\n",
            "8 14 Epoch: 18 | ANN: trainLoss: 0.7989 | trainAcc: 64.2361% (370/576)\n",
            "9 14 Epoch: 18 | ANN: trainLoss: 0.8018 | trainAcc: 64.8438% (415/640)\n",
            "10 14 Epoch: 18 | ANN: trainLoss: 0.7834 | trainAcc: 66.0511% (465/704)\n",
            "11 14 Epoch: 18 | ANN: trainLoss: 0.7876 | trainAcc: 65.6250% (504/768)\n",
            "12 14 Epoch: 18 | ANN: trainLoss: 0.7871 | trainAcc: 66.3462% (552/832)\n",
            "13 14 Epoch: 18 | ANN: trainLoss: 0.7996 | trainAcc: 65.8986% (572/868)\n",
            "0 2 Epoch: 18 | ANN: testLoss: 0.8187 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 18 | ANN: testLoss: 0.8925 | testAcc: 62.8866% (61/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 19 | ANN: trainLoss: 0.7285 | trainAcc: 70.3125% (45/64)\n",
            "1 14 Epoch: 19 | ANN: trainLoss: 0.7548 | trainAcc: 71.0938% (91/128)\n",
            "2 14 Epoch: 19 | ANN: trainLoss: 0.7899 | trainAcc: 66.6667% (128/192)\n",
            "3 14 Epoch: 19 | ANN: trainLoss: 0.8334 | trainAcc: 65.6250% (168/256)\n",
            "4 14 Epoch: 19 | ANN: trainLoss: 0.8185 | trainAcc: 65.6250% (210/320)\n",
            "5 14 Epoch: 19 | ANN: trainLoss: 0.8022 | trainAcc: 66.9271% (257/384)\n",
            "6 14 Epoch: 19 | ANN: trainLoss: 0.7966 | trainAcc: 66.9643% (300/448)\n",
            "7 14 Epoch: 19 | ANN: trainLoss: 0.8033 | trainAcc: 66.0156% (338/512)\n",
            "8 14 Epoch: 19 | ANN: trainLoss: 0.8129 | trainAcc: 65.4514% (377/576)\n",
            "9 14 Epoch: 19 | ANN: trainLoss: 0.8133 | trainAcc: 65.3125% (418/640)\n",
            "10 14 Epoch: 19 | ANN: trainLoss: 0.8014 | trainAcc: 65.3409% (460/704)\n",
            "11 14 Epoch: 19 | ANN: trainLoss: 0.7985 | trainAcc: 65.3646% (502/768)\n",
            "12 14 Epoch: 19 | ANN: trainLoss: 0.8015 | trainAcc: 65.1442% (542/832)\n",
            "13 14 Epoch: 19 | ANN: trainLoss: 0.7927 | trainAcc: 65.4378% (568/868)\n",
            "0 2 Epoch: 19 | ANN: testLoss: 0.8732 | testAcc: 68.7500% (44/64)\n",
            "1 2 Epoch: 19 | ANN: testLoss: 0.8775 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 20 | ANN: trainLoss: 0.7321 | trainAcc: 60.9375% (39/64)\n",
            "1 14 Epoch: 20 | ANN: trainLoss: 0.7349 | trainAcc: 65.6250% (84/128)\n",
            "2 14 Epoch: 20 | ANN: trainLoss: 0.7681 | trainAcc: 66.6667% (128/192)\n",
            "3 14 Epoch: 20 | ANN: trainLoss: 0.7909 | trainAcc: 66.0156% (169/256)\n",
            "4 14 Epoch: 20 | ANN: trainLoss: 0.8208 | trainAcc: 63.4375% (203/320)\n",
            "5 14 Epoch: 20 | ANN: trainLoss: 0.8178 | trainAcc: 64.3229% (247/384)\n",
            "6 14 Epoch: 20 | ANN: trainLoss: 0.8159 | trainAcc: 64.9554% (291/448)\n",
            "7 14 Epoch: 20 | ANN: trainLoss: 0.7939 | trainAcc: 65.6250% (336/512)\n",
            "8 14 Epoch: 20 | ANN: trainLoss: 0.7851 | trainAcc: 65.4514% (377/576)\n",
            "9 14 Epoch: 20 | ANN: trainLoss: 0.7790 | trainAcc: 65.6250% (420/640)\n",
            "10 14 Epoch: 20 | ANN: trainLoss: 0.7899 | trainAcc: 64.0625% (451/704)\n",
            "11 14 Epoch: 20 | ANN: trainLoss: 0.7888 | trainAcc: 64.0625% (492/768)\n",
            "12 14 Epoch: 20 | ANN: trainLoss: 0.7911 | trainAcc: 63.8221% (531/832)\n",
            "13 14 Epoch: 20 | ANN: trainLoss: 0.7879 | trainAcc: 64.0553% (556/868)\n",
            "0 2 Epoch: 20 | ANN: testLoss: 0.9055 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 20 | ANN: testLoss: 0.8453 | testAcc: 63.9175% (62/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 21 | ANN: trainLoss: 0.6991 | trainAcc: 71.8750% (46/64)\n",
            "1 14 Epoch: 21 | ANN: trainLoss: 0.7596 | trainAcc: 68.7500% (88/128)\n",
            "2 14 Epoch: 21 | ANN: trainLoss: 0.7564 | trainAcc: 70.3125% (135/192)\n",
            "3 14 Epoch: 21 | ANN: trainLoss: 0.7463 | trainAcc: 69.9219% (179/256)\n",
            "4 14 Epoch: 21 | ANN: trainLoss: 0.7392 | trainAcc: 69.6875% (223/320)\n",
            "5 14 Epoch: 21 | ANN: trainLoss: 0.7458 | trainAcc: 69.2708% (266/384)\n",
            "6 14 Epoch: 21 | ANN: trainLoss: 0.7572 | trainAcc: 68.5268% (307/448)\n",
            "7 14 Epoch: 21 | ANN: trainLoss: 0.7654 | trainAcc: 67.5781% (346/512)\n",
            "8 14 Epoch: 21 | ANN: trainLoss: 0.7649 | trainAcc: 68.2292% (393/576)\n",
            "9 14 Epoch: 21 | ANN: trainLoss: 0.7601 | trainAcc: 67.3438% (431/640)\n",
            "10 14 Epoch: 21 | ANN: trainLoss: 0.7710 | trainAcc: 66.1932% (466/704)\n",
            "11 14 Epoch: 21 | ANN: trainLoss: 0.7701 | trainAcc: 66.4062% (510/768)\n",
            "12 14 Epoch: 21 | ANN: trainLoss: 0.7635 | trainAcc: 66.4663% (553/832)\n",
            "13 14 Epoch: 21 | ANN: trainLoss: 0.7578 | trainAcc: 66.9355% (581/868)\n",
            "0 2 Epoch: 21 | ANN: testLoss: 0.9302 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 21 | ANN: testLoss: 0.8865 | testAcc: 61.8557% (60/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 22 | ANN: trainLoss: 0.7150 | trainAcc: 68.7500% (44/64)\n",
            "1 14 Epoch: 22 | ANN: trainLoss: 0.7567 | trainAcc: 69.5312% (89/128)\n",
            "2 14 Epoch: 22 | ANN: trainLoss: 0.7193 | trainAcc: 70.3125% (135/192)\n",
            "3 14 Epoch: 22 | ANN: trainLoss: 0.7198 | trainAcc: 71.0938% (182/256)\n",
            "4 14 Epoch: 22 | ANN: trainLoss: 0.7625 | trainAcc: 68.7500% (220/320)\n",
            "5 14 Epoch: 22 | ANN: trainLoss: 0.7345 | trainAcc: 71.6146% (275/384)\n",
            "6 14 Epoch: 22 | ANN: trainLoss: 0.7612 | trainAcc: 69.6429% (312/448)\n",
            "7 14 Epoch: 22 | ANN: trainLoss: 0.7680 | trainAcc: 68.5547% (351/512)\n",
            "8 14 Epoch: 22 | ANN: trainLoss: 0.7541 | trainAcc: 69.2708% (399/576)\n",
            "9 14 Epoch: 22 | ANN: trainLoss: 0.7439 | trainAcc: 69.8438% (447/640)\n",
            "10 14 Epoch: 22 | ANN: trainLoss: 0.7411 | trainAcc: 69.7443% (491/704)\n",
            "11 14 Epoch: 22 | ANN: trainLoss: 0.7361 | trainAcc: 70.3125% (540/768)\n",
            "12 14 Epoch: 22 | ANN: trainLoss: 0.7178 | trainAcc: 71.3942% (594/832)\n",
            "13 14 Epoch: 22 | ANN: trainLoss: 0.7168 | trainAcc: 71.0829% (617/868)\n",
            "0 2 Epoch: 22 | ANN: testLoss: 0.9536 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 22 | ANN: testLoss: 0.8545 | testAcc: 61.8557% (60/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 23 | ANN: trainLoss: 0.7327 | trainAcc: 68.7500% (44/64)\n",
            "1 14 Epoch: 23 | ANN: trainLoss: 0.7203 | trainAcc: 69.5312% (89/128)\n",
            "2 14 Epoch: 23 | ANN: trainLoss: 0.7224 | trainAcc: 70.8333% (136/192)\n",
            "3 14 Epoch: 23 | ANN: trainLoss: 0.7060 | trainAcc: 69.5312% (178/256)\n",
            "4 14 Epoch: 23 | ANN: trainLoss: 0.6911 | trainAcc: 69.6875% (223/320)\n",
            "5 14 Epoch: 23 | ANN: trainLoss: 0.6955 | trainAcc: 70.0521% (269/384)\n",
            "6 14 Epoch: 23 | ANN: trainLoss: 0.6908 | trainAcc: 70.5357% (316/448)\n",
            "7 14 Epoch: 23 | ANN: trainLoss: 0.7315 | trainAcc: 68.7500% (352/512)\n",
            "8 14 Epoch: 23 | ANN: trainLoss: 0.7279 | trainAcc: 69.0972% (398/576)\n",
            "9 14 Epoch: 23 | ANN: trainLoss: 0.7234 | trainAcc: 69.2188% (443/640)\n",
            "10 14 Epoch: 23 | ANN: trainLoss: 0.7234 | trainAcc: 69.6023% (490/704)\n",
            "11 14 Epoch: 23 | ANN: trainLoss: 0.7117 | trainAcc: 70.1823% (539/768)\n",
            "12 14 Epoch: 23 | ANN: trainLoss: 0.7085 | trainAcc: 70.3125% (585/832)\n",
            "13 14 Epoch: 23 | ANN: trainLoss: 0.7169 | trainAcc: 70.0461% (608/868)\n",
            "0 2 Epoch: 23 | ANN: testLoss: 0.9335 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 23 | ANN: testLoss: 0.8299 | testAcc: 61.8557% (60/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 24 | ANN: trainLoss: 0.5440 | trainAcc: 73.4375% (47/64)\n",
            "1 14 Epoch: 24 | ANN: trainLoss: 0.5567 | trainAcc: 74.2188% (95/128)\n",
            "2 14 Epoch: 24 | ANN: trainLoss: 0.5687 | trainAcc: 72.9167% (140/192)\n",
            "3 14 Epoch: 24 | ANN: trainLoss: 0.6267 | trainAcc: 71.4844% (183/256)\n",
            "4 14 Epoch: 24 | ANN: trainLoss: 0.6401 | trainAcc: 72.5000% (232/320)\n",
            "5 14 Epoch: 24 | ANN: trainLoss: 0.6739 | trainAcc: 72.1354% (277/384)\n",
            "6 14 Epoch: 24 | ANN: trainLoss: 0.6602 | trainAcc: 72.5446% (325/448)\n",
            "7 14 Epoch: 24 | ANN: trainLoss: 0.6594 | trainAcc: 72.6562% (372/512)\n",
            "8 14 Epoch: 24 | ANN: trainLoss: 0.6613 | trainAcc: 72.5694% (418/576)\n",
            "9 14 Epoch: 24 | ANN: trainLoss: 0.6567 | trainAcc: 73.2812% (469/640)\n",
            "10 14 Epoch: 24 | ANN: trainLoss: 0.6726 | trainAcc: 72.3011% (509/704)\n",
            "11 14 Epoch: 24 | ANN: trainLoss: 0.6840 | trainAcc: 71.7448% (551/768)\n",
            "12 14 Epoch: 24 | ANN: trainLoss: 0.6830 | trainAcc: 71.7548% (597/832)\n",
            "13 14 Epoch: 24 | ANN: trainLoss: 0.6932 | trainAcc: 71.6590% (622/868)\n",
            "0 2 Epoch: 24 | ANN: testLoss: 0.9072 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 24 | ANN: testLoss: 0.8985 | testAcc: 58.7629% (57/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 25 | ANN: trainLoss: 0.6310 | trainAcc: 68.7500% (44/64)\n",
            "1 14 Epoch: 25 | ANN: trainLoss: 0.6924 | trainAcc: 71.8750% (92/128)\n",
            "2 14 Epoch: 25 | ANN: trainLoss: 0.7302 | trainAcc: 69.2708% (133/192)\n",
            "3 14 Epoch: 25 | ANN: trainLoss: 0.7173 | trainAcc: 69.9219% (179/256)\n",
            "4 14 Epoch: 25 | ANN: trainLoss: 0.6979 | trainAcc: 71.5625% (229/320)\n",
            "5 14 Epoch: 25 | ANN: trainLoss: 0.7023 | trainAcc: 70.0521% (269/384)\n",
            "6 14 Epoch: 25 | ANN: trainLoss: 0.6903 | trainAcc: 69.8661% (313/448)\n",
            "7 14 Epoch: 25 | ANN: trainLoss: 0.6967 | trainAcc: 69.3359% (355/512)\n",
            "8 14 Epoch: 25 | ANN: trainLoss: 0.6930 | trainAcc: 68.9236% (397/576)\n",
            "9 14 Epoch: 25 | ANN: trainLoss: 0.6764 | trainAcc: 69.8438% (447/640)\n",
            "10 14 Epoch: 25 | ANN: trainLoss: 0.6703 | trainAcc: 70.7386% (498/704)\n",
            "11 14 Epoch: 25 | ANN: trainLoss: 0.6679 | trainAcc: 70.4427% (541/768)\n",
            "12 14 Epoch: 25 | ANN: trainLoss: 0.6744 | trainAcc: 69.9519% (582/832)\n",
            "13 14 Epoch: 25 | ANN: trainLoss: 0.6939 | trainAcc: 69.3548% (602/868)\n",
            "0 2 Epoch: 25 | ANN: testLoss: 0.9509 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 25 | ANN: testLoss: 0.8896 | testAcc: 59.7938% (58/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 26 | ANN: trainLoss: 0.7259 | trainAcc: 70.3125% (45/64)\n",
            "1 14 Epoch: 26 | ANN: trainLoss: 0.7868 | trainAcc: 63.2812% (81/128)\n",
            "2 14 Epoch: 26 | ANN: trainLoss: 0.8030 | trainAcc: 63.0208% (121/192)\n",
            "3 14 Epoch: 26 | ANN: trainLoss: 0.7507 | trainAcc: 66.0156% (169/256)\n",
            "4 14 Epoch: 26 | ANN: trainLoss: 0.7303 | trainAcc: 67.8125% (217/320)\n",
            "5 14 Epoch: 26 | ANN: trainLoss: 0.7415 | trainAcc: 66.4062% (255/384)\n",
            "6 14 Epoch: 26 | ANN: trainLoss: 0.7470 | trainAcc: 66.5179% (298/448)\n",
            "7 14 Epoch: 26 | ANN: trainLoss: 0.7497 | trainAcc: 66.9922% (343/512)\n",
            "8 14 Epoch: 26 | ANN: trainLoss: 0.7422 | trainAcc: 67.8819% (391/576)\n",
            "9 14 Epoch: 26 | ANN: trainLoss: 0.7395 | trainAcc: 68.2812% (437/640)\n",
            "10 14 Epoch: 26 | ANN: trainLoss: 0.7356 | trainAcc: 67.8977% (478/704)\n",
            "11 14 Epoch: 26 | ANN: trainLoss: 0.7197 | trainAcc: 68.6198% (527/768)\n",
            "12 14 Epoch: 26 | ANN: trainLoss: 0.7215 | trainAcc: 68.3894% (569/832)\n",
            "13 14 Epoch: 26 | ANN: trainLoss: 0.7149 | trainAcc: 68.6636% (596/868)\n",
            "0 2 Epoch: 26 | ANN: testLoss: 0.9310 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 26 | ANN: testLoss: 0.8294 | testAcc: 63.9175% (62/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 27 | ANN: trainLoss: 0.5658 | trainAcc: 73.4375% (47/64)\n",
            "1 14 Epoch: 27 | ANN: trainLoss: 0.6588 | trainAcc: 73.4375% (94/128)\n",
            "2 14 Epoch: 27 | ANN: trainLoss: 0.6579 | trainAcc: 72.3958% (139/192)\n",
            "3 14 Epoch: 27 | ANN: trainLoss: 0.6373 | trainAcc: 73.8281% (189/256)\n",
            "4 14 Epoch: 27 | ANN: trainLoss: 0.6399 | trainAcc: 73.7500% (236/320)\n",
            "5 14 Epoch: 27 | ANN: trainLoss: 0.6605 | trainAcc: 73.1771% (281/384)\n",
            "6 14 Epoch: 27 | ANN: trainLoss: 0.6398 | trainAcc: 74.5536% (334/448)\n",
            "7 14 Epoch: 27 | ANN: trainLoss: 0.6275 | trainAcc: 75.9766% (389/512)\n",
            "8 14 Epoch: 27 | ANN: trainLoss: 0.6232 | trainAcc: 76.0417% (438/576)\n",
            "9 14 Epoch: 27 | ANN: trainLoss: 0.6255 | trainAcc: 75.1562% (481/640)\n",
            "10 14 Epoch: 27 | ANN: trainLoss: 0.6286 | trainAcc: 74.8580% (527/704)\n",
            "11 14 Epoch: 27 | ANN: trainLoss: 0.6413 | trainAcc: 74.3490% (571/768)\n",
            "12 14 Epoch: 27 | ANN: trainLoss: 0.6406 | trainAcc: 74.5192% (620/832)\n",
            "13 14 Epoch: 27 | ANN: trainLoss: 0.6383 | trainAcc: 74.4240% (646/868)\n",
            "0 2 Epoch: 27 | ANN: testLoss: 0.8172 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 27 | ANN: testLoss: 0.9192 | testAcc: 60.8247% (59/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 28 | ANN: trainLoss: 0.4891 | trainAcc: 82.8125% (53/64)\n",
            "1 14 Epoch: 28 | ANN: trainLoss: 0.6137 | trainAcc: 76.5625% (98/128)\n",
            "2 14 Epoch: 28 | ANN: trainLoss: 0.6475 | trainAcc: 74.4792% (143/192)\n",
            "3 14 Epoch: 28 | ANN: trainLoss: 0.6110 | trainAcc: 76.1719% (195/256)\n",
            "4 14 Epoch: 28 | ANN: trainLoss: 0.5864 | trainAcc: 76.2500% (244/320)\n",
            "5 14 Epoch: 28 | ANN: trainLoss: 0.5858 | trainAcc: 76.8229% (295/384)\n",
            "6 14 Epoch: 28 | ANN: trainLoss: 0.5951 | trainAcc: 76.7857% (344/448)\n",
            "7 14 Epoch: 28 | ANN: trainLoss: 0.5897 | trainAcc: 76.5625% (392/512)\n",
            "8 14 Epoch: 28 | ANN: trainLoss: 0.5894 | trainAcc: 76.5625% (441/576)\n",
            "9 14 Epoch: 28 | ANN: trainLoss: 0.6021 | trainAcc: 75.6250% (484/640)\n",
            "10 14 Epoch: 28 | ANN: trainLoss: 0.5923 | trainAcc: 76.1364% (536/704)\n",
            "11 14 Epoch: 28 | ANN: trainLoss: 0.5891 | trainAcc: 75.9115% (583/768)\n",
            "12 14 Epoch: 28 | ANN: trainLoss: 0.5844 | trainAcc: 76.2019% (634/832)\n",
            "13 14 Epoch: 28 | ANN: trainLoss: 0.5902 | trainAcc: 75.6912% (657/868)\n",
            "0 2 Epoch: 28 | ANN: testLoss: 0.9722 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 28 | ANN: testLoss: 0.8133 | testAcc: 60.8247% (59/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 29 | ANN: trainLoss: 0.5341 | trainAcc: 78.1250% (50/64)\n",
            "1 14 Epoch: 29 | ANN: trainLoss: 0.5881 | trainAcc: 74.2188% (95/128)\n",
            "2 14 Epoch: 29 | ANN: trainLoss: 0.5927 | trainAcc: 73.4375% (141/192)\n",
            "3 14 Epoch: 29 | ANN: trainLoss: 0.5769 | trainAcc: 74.6094% (191/256)\n",
            "4 14 Epoch: 29 | ANN: trainLoss: 0.5629 | trainAcc: 75.6250% (242/320)\n",
            "5 14 Epoch: 29 | ANN: trainLoss: 0.5581 | trainAcc: 75.7812% (291/384)\n",
            "6 14 Epoch: 29 | ANN: trainLoss: 0.5666 | trainAcc: 75.4464% (338/448)\n",
            "7 14 Epoch: 29 | ANN: trainLoss: 0.5642 | trainAcc: 75.3906% (386/512)\n",
            "8 14 Epoch: 29 | ANN: trainLoss: 0.5669 | trainAcc: 75.5208% (435/576)\n",
            "9 14 Epoch: 29 | ANN: trainLoss: 0.5798 | trainAcc: 75.1562% (481/640)\n",
            "10 14 Epoch: 29 | ANN: trainLoss: 0.5842 | trainAcc: 74.4318% (524/704)\n",
            "11 14 Epoch: 29 | ANN: trainLoss: 0.6018 | trainAcc: 73.1771% (562/768)\n",
            "12 14 Epoch: 29 | ANN: trainLoss: 0.5980 | trainAcc: 73.5577% (612/832)\n",
            "13 14 Epoch: 29 | ANN: trainLoss: 0.6004 | trainAcc: 73.8479% (641/868)\n",
            "0 2 Epoch: 29 | ANN: testLoss: 0.8285 | testAcc: 67.1875% (43/64)\n",
            "1 2 Epoch: 29 | ANN: testLoss: 0.8842 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 30 | ANN: trainLoss: 0.5458 | trainAcc: 76.5625% (49/64)\n",
            "1 14 Epoch: 30 | ANN: trainLoss: 0.5701 | trainAcc: 76.5625% (98/128)\n",
            "2 14 Epoch: 30 | ANN: trainLoss: 0.5997 | trainAcc: 77.0833% (148/192)\n",
            "3 14 Epoch: 30 | ANN: trainLoss: 0.5538 | trainAcc: 79.6875% (204/256)\n",
            "4 14 Epoch: 30 | ANN: trainLoss: 0.5530 | trainAcc: 79.0625% (253/320)\n",
            "5 14 Epoch: 30 | ANN: trainLoss: 0.5297 | trainAcc: 80.2083% (308/384)\n",
            "6 14 Epoch: 30 | ANN: trainLoss: 0.5492 | trainAcc: 78.7946% (353/448)\n",
            "7 14 Epoch: 30 | ANN: trainLoss: 0.5590 | trainAcc: 78.1250% (400/512)\n",
            "8 14 Epoch: 30 | ANN: trainLoss: 0.5705 | trainAcc: 77.6042% (447/576)\n",
            "9 14 Epoch: 30 | ANN: trainLoss: 0.5730 | trainAcc: 76.8750% (492/640)\n",
            "10 14 Epoch: 30 | ANN: trainLoss: 0.5791 | trainAcc: 76.4205% (538/704)\n",
            "11 14 Epoch: 30 | ANN: trainLoss: 0.5819 | trainAcc: 76.8229% (590/768)\n",
            "12 14 Epoch: 30 | ANN: trainLoss: 0.5741 | trainAcc: 77.1635% (642/832)\n",
            "13 14 Epoch: 30 | ANN: trainLoss: 0.5855 | trainAcc: 76.8433% (667/868)\n",
            "0 2 Epoch: 30 | ANN: testLoss: 0.9269 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 30 | ANN: testLoss: 0.8411 | testAcc: 58.7629% (57/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 31 | ANN: trainLoss: 0.6411 | trainAcc: 75.0000% (48/64)\n",
            "1 14 Epoch: 31 | ANN: trainLoss: 0.6271 | trainAcc: 74.2188% (95/128)\n",
            "2 14 Epoch: 31 | ANN: trainLoss: 0.6785 | trainAcc: 72.9167% (140/192)\n",
            "3 14 Epoch: 31 | ANN: trainLoss: 0.7141 | trainAcc: 73.4375% (188/256)\n",
            "4 14 Epoch: 31 | ANN: trainLoss: 0.6888 | trainAcc: 74.0625% (237/320)\n",
            "5 14 Epoch: 31 | ANN: trainLoss: 0.6588 | trainAcc: 75.5208% (290/384)\n",
            "6 14 Epoch: 31 | ANN: trainLoss: 0.6512 | trainAcc: 75.2232% (337/448)\n",
            "7 14 Epoch: 31 | ANN: trainLoss: 0.6597 | trainAcc: 74.8047% (383/512)\n",
            "8 14 Epoch: 31 | ANN: trainLoss: 0.6354 | trainAcc: 75.5208% (435/576)\n",
            "9 14 Epoch: 31 | ANN: trainLoss: 0.6324 | trainAcc: 75.4688% (483/640)\n",
            "10 14 Epoch: 31 | ANN: trainLoss: 0.6266 | trainAcc: 75.7102% (533/704)\n",
            "11 14 Epoch: 31 | ANN: trainLoss: 0.6223 | trainAcc: 75.6510% (581/768)\n",
            "12 14 Epoch: 31 | ANN: trainLoss: 0.6233 | trainAcc: 75.0000% (624/832)\n",
            "13 14 Epoch: 31 | ANN: trainLoss: 0.6388 | trainAcc: 75.1152% (652/868)\n",
            "0 2 Epoch: 31 | ANN: testLoss: 0.9314 | testAcc: 57.8125% (37/64)\n",
            "1 2 Epoch: 31 | ANN: testLoss: 0.8390 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 32 | ANN: trainLoss: 0.5743 | trainAcc: 79.6875% (51/64)\n",
            "1 14 Epoch: 32 | ANN: trainLoss: 0.6098 | trainAcc: 79.6875% (102/128)\n",
            "2 14 Epoch: 32 | ANN: trainLoss: 0.6240 | trainAcc: 75.5208% (145/192)\n",
            "3 14 Epoch: 32 | ANN: trainLoss: 0.5751 | trainAcc: 78.1250% (200/256)\n",
            "4 14 Epoch: 32 | ANN: trainLoss: 0.5755 | trainAcc: 77.1875% (247/320)\n",
            "5 14 Epoch: 32 | ANN: trainLoss: 0.5735 | trainAcc: 77.3438% (297/384)\n",
            "6 14 Epoch: 32 | ANN: trainLoss: 0.5891 | trainAcc: 76.3393% (342/448)\n",
            "7 14 Epoch: 32 | ANN: trainLoss: 0.5937 | trainAcc: 76.3672% (391/512)\n",
            "8 14 Epoch: 32 | ANN: trainLoss: 0.5850 | trainAcc: 76.5625% (441/576)\n",
            "9 14 Epoch: 32 | ANN: trainLoss: 0.5776 | trainAcc: 76.5625% (490/640)\n",
            "10 14 Epoch: 32 | ANN: trainLoss: 0.5792 | trainAcc: 76.8466% (541/704)\n",
            "11 14 Epoch: 32 | ANN: trainLoss: 0.5878 | trainAcc: 76.5625% (588/768)\n",
            "12 14 Epoch: 32 | ANN: trainLoss: 0.5868 | trainAcc: 76.8029% (639/832)\n",
            "13 14 Epoch: 32 | ANN: trainLoss: 0.5814 | trainAcc: 76.9585% (668/868)\n",
            "0 2 Epoch: 32 | ANN: testLoss: 0.8909 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 32 | ANN: testLoss: 0.8733 | testAcc: 60.8247% (59/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 33 | ANN: trainLoss: 0.5473 | trainAcc: 78.1250% (50/64)\n",
            "1 14 Epoch: 33 | ANN: trainLoss: 0.5550 | trainAcc: 78.1250% (100/128)\n",
            "2 14 Epoch: 33 | ANN: trainLoss: 0.5709 | trainAcc: 76.5625% (147/192)\n",
            "3 14 Epoch: 33 | ANN: trainLoss: 0.5328 | trainAcc: 78.9062% (202/256)\n",
            "4 14 Epoch: 33 | ANN: trainLoss: 0.5351 | trainAcc: 79.3750% (254/320)\n",
            "5 14 Epoch: 33 | ANN: trainLoss: 0.5394 | trainAcc: 79.1667% (304/384)\n",
            "6 14 Epoch: 33 | ANN: trainLoss: 0.5525 | trainAcc: 79.4643% (356/448)\n",
            "7 14 Epoch: 33 | ANN: trainLoss: 0.5495 | trainAcc: 79.1016% (405/512)\n",
            "8 14 Epoch: 33 | ANN: trainLoss: 0.5661 | trainAcc: 78.2986% (451/576)\n",
            "9 14 Epoch: 33 | ANN: trainLoss: 0.5627 | trainAcc: 78.1250% (500/640)\n",
            "10 14 Epoch: 33 | ANN: trainLoss: 0.5534 | trainAcc: 77.8409% (548/704)\n",
            "11 14 Epoch: 33 | ANN: trainLoss: 0.5517 | trainAcc: 77.7344% (597/768)\n",
            "12 14 Epoch: 33 | ANN: trainLoss: 0.5556 | trainAcc: 77.5240% (645/832)\n",
            "13 14 Epoch: 33 | ANN: trainLoss: 0.5574 | trainAcc: 77.6498% (674/868)\n",
            "0 2 Epoch: 33 | ANN: testLoss: 0.8905 | testAcc: 57.8125% (37/64)\n",
            "1 2 Epoch: 33 | ANN: testLoss: 0.8367 | testAcc: 61.8557% (60/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 34 | ANN: trainLoss: 0.7610 | trainAcc: 78.1250% (50/64)\n",
            "1 14 Epoch: 34 | ANN: trainLoss: 0.6410 | trainAcc: 77.3438% (99/128)\n",
            "2 14 Epoch: 34 | ANN: trainLoss: 0.6247 | trainAcc: 75.0000% (144/192)\n",
            "3 14 Epoch: 34 | ANN: trainLoss: 0.6038 | trainAcc: 75.0000% (192/256)\n",
            "4 14 Epoch: 34 | ANN: trainLoss: 0.5964 | trainAcc: 73.7500% (236/320)\n",
            "5 14 Epoch: 34 | ANN: trainLoss: 0.5875 | trainAcc: 74.2188% (285/384)\n",
            "6 14 Epoch: 34 | ANN: trainLoss: 0.5535 | trainAcc: 75.8929% (340/448)\n",
            "7 14 Epoch: 34 | ANN: trainLoss: 0.5573 | trainAcc: 75.9766% (389/512)\n",
            "8 14 Epoch: 34 | ANN: trainLoss: 0.5654 | trainAcc: 75.6944% (436/576)\n",
            "9 14 Epoch: 34 | ANN: trainLoss: 0.5788 | trainAcc: 75.0000% (480/640)\n",
            "10 14 Epoch: 34 | ANN: trainLoss: 0.5818 | trainAcc: 75.2841% (530/704)\n",
            "11 14 Epoch: 34 | ANN: trainLoss: 0.5675 | trainAcc: 76.1719% (585/768)\n",
            "12 14 Epoch: 34 | ANN: trainLoss: 0.5744 | trainAcc: 75.7212% (630/832)\n",
            "13 14 Epoch: 34 | ANN: trainLoss: 0.5730 | trainAcc: 75.8065% (658/868)\n",
            "0 2 Epoch: 34 | ANN: testLoss: 0.8582 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 34 | ANN: testLoss: 0.8831 | testAcc: 60.8247% (59/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 35 | ANN: trainLoss: 0.5204 | trainAcc: 78.1250% (50/64)\n",
            "1 14 Epoch: 35 | ANN: trainLoss: 0.5424 | trainAcc: 75.7812% (97/128)\n",
            "2 14 Epoch: 35 | ANN: trainLoss: 0.5264 | trainAcc: 77.0833% (148/192)\n",
            "3 14 Epoch: 35 | ANN: trainLoss: 0.5200 | trainAcc: 77.7344% (199/256)\n",
            "4 14 Epoch: 35 | ANN: trainLoss: 0.5032 | trainAcc: 80.3125% (257/320)\n",
            "5 14 Epoch: 35 | ANN: trainLoss: 0.5131 | trainAcc: 79.4271% (305/384)\n",
            "6 14 Epoch: 35 | ANN: trainLoss: 0.5487 | trainAcc: 77.9018% (349/448)\n",
            "7 14 Epoch: 35 | ANN: trainLoss: 0.5315 | trainAcc: 78.5156% (402/512)\n",
            "8 14 Epoch: 35 | ANN: trainLoss: 0.5244 | trainAcc: 78.4722% (452/576)\n",
            "9 14 Epoch: 35 | ANN: trainLoss: 0.5437 | trainAcc: 76.8750% (492/640)\n",
            "10 14 Epoch: 35 | ANN: trainLoss: 0.5451 | trainAcc: 76.8466% (541/704)\n",
            "11 14 Epoch: 35 | ANN: trainLoss: 0.5399 | trainAcc: 76.8229% (590/768)\n",
            "12 14 Epoch: 35 | ANN: trainLoss: 0.5403 | trainAcc: 77.1635% (642/832)\n",
            "13 14 Epoch: 35 | ANN: trainLoss: 0.5538 | trainAcc: 76.9585% (668/868)\n",
            "0 2 Epoch: 35 | ANN: testLoss: 0.8799 | testAcc: 64.0625% (41/64)\n",
            "1 2 Epoch: 35 | ANN: testLoss: 0.8086 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 36 | ANN: trainLoss: 0.5159 | trainAcc: 84.3750% (54/64)\n",
            "1 14 Epoch: 36 | ANN: trainLoss: 0.5108 | trainAcc: 82.8125% (106/128)\n",
            "2 14 Epoch: 36 | ANN: trainLoss: 0.5670 | trainAcc: 78.6458% (151/192)\n",
            "3 14 Epoch: 36 | ANN: trainLoss: 0.5091 | trainAcc: 81.2500% (208/256)\n",
            "4 14 Epoch: 36 | ANN: trainLoss: 0.5107 | trainAcc: 81.5625% (261/320)\n",
            "5 14 Epoch: 36 | ANN: trainLoss: 0.5147 | trainAcc: 81.2500% (312/384)\n",
            "6 14 Epoch: 36 | ANN: trainLoss: 0.5234 | trainAcc: 80.3571% (360/448)\n",
            "7 14 Epoch: 36 | ANN: trainLoss: 0.5226 | trainAcc: 79.6875% (408/512)\n",
            "8 14 Epoch: 36 | ANN: trainLoss: 0.5213 | trainAcc: 79.8611% (460/576)\n",
            "9 14 Epoch: 36 | ANN: trainLoss: 0.5407 | trainAcc: 79.5312% (509/640)\n",
            "10 14 Epoch: 36 | ANN: trainLoss: 0.5477 | trainAcc: 78.4091% (552/704)\n",
            "11 14 Epoch: 36 | ANN: trainLoss: 0.5354 | trainAcc: 79.2969% (609/768)\n",
            "12 14 Epoch: 36 | ANN: trainLoss: 0.5462 | trainAcc: 78.3654% (652/832)\n",
            "13 14 Epoch: 36 | ANN: trainLoss: 0.5594 | trainAcc: 77.8802% (676/868)\n",
            "0 2 Epoch: 36 | ANN: testLoss: 0.9574 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 36 | ANN: testLoss: 0.8149 | testAcc: 63.9175% (62/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 37 | ANN: trainLoss: 0.4378 | trainAcc: 87.5000% (56/64)\n",
            "1 14 Epoch: 37 | ANN: trainLoss: 0.4750 | trainAcc: 85.1562% (109/128)\n",
            "2 14 Epoch: 37 | ANN: trainLoss: 0.4830 | trainAcc: 83.8542% (161/192)\n",
            "3 14 Epoch: 37 | ANN: trainLoss: 0.5239 | trainAcc: 80.8594% (207/256)\n",
            "4 14 Epoch: 37 | ANN: trainLoss: 0.5261 | trainAcc: 80.6250% (258/320)\n",
            "5 14 Epoch: 37 | ANN: trainLoss: 0.5651 | trainAcc: 77.8646% (299/384)\n",
            "6 14 Epoch: 37 | ANN: trainLoss: 0.5647 | trainAcc: 77.6786% (348/448)\n",
            "7 14 Epoch: 37 | ANN: trainLoss: 0.5657 | trainAcc: 77.9297% (399/512)\n",
            "8 14 Epoch: 37 | ANN: trainLoss: 0.5693 | trainAcc: 77.7778% (448/576)\n",
            "9 14 Epoch: 37 | ANN: trainLoss: 0.5629 | trainAcc: 77.8125% (498/640)\n",
            "10 14 Epoch: 37 | ANN: trainLoss: 0.5562 | trainAcc: 77.8409% (548/704)\n",
            "11 14 Epoch: 37 | ANN: trainLoss: 0.5624 | trainAcc: 77.4740% (595/768)\n",
            "12 14 Epoch: 37 | ANN: trainLoss: 0.5610 | trainAcc: 77.4038% (644/832)\n",
            "13 14 Epoch: 37 | ANN: trainLoss: 0.5557 | trainAcc: 77.5346% (673/868)\n",
            "0 2 Epoch: 37 | ANN: testLoss: 0.8843 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 37 | ANN: testLoss: 0.9093 | testAcc: 61.8557% (60/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 38 | ANN: trainLoss: 0.6345 | trainAcc: 67.1875% (43/64)\n",
            "1 14 Epoch: 38 | ANN: trainLoss: 0.6823 | trainAcc: 67.9688% (87/128)\n",
            "2 14 Epoch: 38 | ANN: trainLoss: 0.6244 | trainAcc: 71.3542% (137/192)\n",
            "3 14 Epoch: 38 | ANN: trainLoss: 0.5971 | trainAcc: 73.4375% (188/256)\n",
            "4 14 Epoch: 38 | ANN: trainLoss: 0.5798 | trainAcc: 75.6250% (242/320)\n",
            "5 14 Epoch: 38 | ANN: trainLoss: 0.5888 | trainAcc: 76.0417% (292/384)\n",
            "6 14 Epoch: 38 | ANN: trainLoss: 0.5897 | trainAcc: 77.2321% (346/448)\n",
            "7 14 Epoch: 38 | ANN: trainLoss: 0.6008 | trainAcc: 77.3438% (396/512)\n",
            "8 14 Epoch: 38 | ANN: trainLoss: 0.5950 | trainAcc: 77.0833% (444/576)\n",
            "9 14 Epoch: 38 | ANN: trainLoss: 0.5774 | trainAcc: 77.8125% (498/640)\n",
            "10 14 Epoch: 38 | ANN: trainLoss: 0.5708 | trainAcc: 77.9830% (549/704)\n",
            "11 14 Epoch: 38 | ANN: trainLoss: 0.5554 | trainAcc: 78.2552% (601/768)\n",
            "12 14 Epoch: 38 | ANN: trainLoss: 0.5526 | trainAcc: 78.3654% (652/832)\n",
            "13 14 Epoch: 38 | ANN: trainLoss: 0.5424 | trainAcc: 78.5714% (682/868)\n",
            "0 2 Epoch: 38 | ANN: testLoss: 0.9765 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 38 | ANN: testLoss: 0.8936 | testAcc: 62.8866% (61/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 39 | ANN: trainLoss: 0.4710 | trainAcc: 84.3750% (54/64)\n",
            "1 14 Epoch: 39 | ANN: trainLoss: 0.4464 | trainAcc: 83.5938% (107/128)\n",
            "2 14 Epoch: 39 | ANN: trainLoss: 0.4311 | trainAcc: 84.3750% (162/192)\n",
            "3 14 Epoch: 39 | ANN: trainLoss: 0.4403 | trainAcc: 83.9844% (215/256)\n",
            "4 14 Epoch: 39 | ANN: trainLoss: 0.4503 | trainAcc: 83.4375% (267/320)\n",
            "5 14 Epoch: 39 | ANN: trainLoss: 0.4452 | trainAcc: 83.8542% (322/384)\n",
            "6 14 Epoch: 39 | ANN: trainLoss: 0.4554 | trainAcc: 83.0357% (372/448)\n",
            "7 14 Epoch: 39 | ANN: trainLoss: 0.4708 | trainAcc: 82.2266% (421/512)\n",
            "8 14 Epoch: 39 | ANN: trainLoss: 0.4695 | trainAcc: 82.1181% (473/576)\n",
            "9 14 Epoch: 39 | ANN: trainLoss: 0.4882 | trainAcc: 82.0312% (525/640)\n",
            "10 14 Epoch: 39 | ANN: trainLoss: 0.4954 | trainAcc: 81.2500% (572/704)\n",
            "11 14 Epoch: 39 | ANN: trainLoss: 0.5055 | trainAcc: 80.4688% (618/768)\n",
            "12 14 Epoch: 39 | ANN: trainLoss: 0.5208 | trainAcc: 79.6875% (663/832)\n",
            "13 14 Epoch: 39 | ANN: trainLoss: 0.5296 | trainAcc: 79.3779% (689/868)\n",
            "0 2 Epoch: 39 | ANN: testLoss: 0.9669 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 39 | ANN: testLoss: 0.8814 | testAcc: 59.7938% (58/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 40 | ANN: trainLoss: 0.5690 | trainAcc: 79.6875% (51/64)\n",
            "1 14 Epoch: 40 | ANN: trainLoss: 0.5195 | trainAcc: 78.9062% (101/128)\n",
            "2 14 Epoch: 40 | ANN: trainLoss: 0.5710 | trainAcc: 76.5625% (147/192)\n",
            "3 14 Epoch: 40 | ANN: trainLoss: 0.5455 | trainAcc: 77.7344% (199/256)\n",
            "4 14 Epoch: 40 | ANN: trainLoss: 0.5169 | trainAcc: 79.3750% (254/320)\n",
            "5 14 Epoch: 40 | ANN: trainLoss: 0.5266 | trainAcc: 79.1667% (304/384)\n",
            "6 14 Epoch: 40 | ANN: trainLoss: 0.5339 | trainAcc: 78.3482% (351/448)\n",
            "7 14 Epoch: 40 | ANN: trainLoss: 0.5439 | trainAcc: 77.7344% (398/512)\n",
            "8 14 Epoch: 40 | ANN: trainLoss: 0.5448 | trainAcc: 77.0833% (444/576)\n",
            "9 14 Epoch: 40 | ANN: trainLoss: 0.5452 | trainAcc: 77.1875% (494/640)\n",
            "10 14 Epoch: 40 | ANN: trainLoss: 0.5270 | trainAcc: 78.2670% (551/704)\n",
            "11 14 Epoch: 40 | ANN: trainLoss: 0.5387 | trainAcc: 77.6042% (596/768)\n",
            "12 14 Epoch: 40 | ANN: trainLoss: 0.5391 | trainAcc: 77.2837% (643/832)\n",
            "13 14 Epoch: 40 | ANN: trainLoss: 0.5353 | trainAcc: 77.5346% (673/868)\n",
            "0 2 Epoch: 40 | ANN: testLoss: 0.9213 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 40 | ANN: testLoss: 0.9084 | testAcc: 58.7629% (57/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 41 | ANN: trainLoss: 0.5662 | trainAcc: 73.4375% (47/64)\n",
            "1 14 Epoch: 41 | ANN: trainLoss: 0.4849 | trainAcc: 78.9062% (101/128)\n",
            "2 14 Epoch: 41 | ANN: trainLoss: 0.4936 | trainAcc: 79.6875% (153/192)\n",
            "3 14 Epoch: 41 | ANN: trainLoss: 0.5156 | trainAcc: 77.7344% (199/256)\n",
            "4 14 Epoch: 41 | ANN: trainLoss: 0.4945 | trainAcc: 79.0625% (253/320)\n",
            "5 14 Epoch: 41 | ANN: trainLoss: 0.5003 | trainAcc: 78.9062% (303/384)\n",
            "6 14 Epoch: 41 | ANN: trainLoss: 0.5032 | trainAcc: 78.5714% (352/448)\n",
            "7 14 Epoch: 41 | ANN: trainLoss: 0.5026 | trainAcc: 79.1016% (405/512)\n",
            "8 14 Epoch: 41 | ANN: trainLoss: 0.5074 | trainAcc: 79.1667% (456/576)\n",
            "9 14 Epoch: 41 | ANN: trainLoss: 0.5051 | trainAcc: 79.5312% (509/640)\n",
            "10 14 Epoch: 41 | ANN: trainLoss: 0.5110 | trainAcc: 78.8352% (555/704)\n",
            "11 14 Epoch: 41 | ANN: trainLoss: 0.5064 | trainAcc: 78.6458% (604/768)\n",
            "12 14 Epoch: 41 | ANN: trainLoss: 0.5130 | trainAcc: 78.7260% (655/832)\n",
            "13 14 Epoch: 41 | ANN: trainLoss: 0.5255 | trainAcc: 78.3410% (680/868)\n",
            "0 2 Epoch: 41 | ANN: testLoss: 0.9802 | testAcc: 54.6875% (35/64)\n",
            "1 2 Epoch: 41 | ANN: testLoss: 0.8672 | testAcc: 60.8247% (59/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 42 | ANN: trainLoss: 0.4448 | trainAcc: 79.6875% (51/64)\n",
            "1 14 Epoch: 42 | ANN: trainLoss: 0.4072 | trainAcc: 82.0312% (105/128)\n",
            "2 14 Epoch: 42 | ANN: trainLoss: 0.4242 | trainAcc: 82.8125% (159/192)\n",
            "3 14 Epoch: 42 | ANN: trainLoss: 0.4705 | trainAcc: 81.2500% (208/256)\n",
            "4 14 Epoch: 42 | ANN: trainLoss: 0.4717 | trainAcc: 80.3125% (257/320)\n",
            "5 14 Epoch: 42 | ANN: trainLoss: 0.4549 | trainAcc: 81.7708% (314/384)\n",
            "6 14 Epoch: 42 | ANN: trainLoss: 0.4587 | trainAcc: 81.6964% (366/448)\n",
            "7 14 Epoch: 42 | ANN: trainLoss: 0.4769 | trainAcc: 80.0781% (410/512)\n",
            "8 14 Epoch: 42 | ANN: trainLoss: 0.4796 | trainAcc: 79.8611% (460/576)\n",
            "9 14 Epoch: 42 | ANN: trainLoss: 0.4882 | trainAcc: 79.6875% (510/640)\n",
            "10 14 Epoch: 42 | ANN: trainLoss: 0.4886 | trainAcc: 79.6875% (561/704)\n",
            "11 14 Epoch: 42 | ANN: trainLoss: 0.4940 | trainAcc: 79.5573% (611/768)\n",
            "12 14 Epoch: 42 | ANN: trainLoss: 0.4913 | trainAcc: 79.5673% (662/832)\n",
            "13 14 Epoch: 42 | ANN: trainLoss: 0.4919 | trainAcc: 79.6083% (691/868)\n",
            "0 2 Epoch: 42 | ANN: testLoss: 0.9571 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 42 | ANN: testLoss: 0.9126 | testAcc: 61.8557% (60/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 43 | ANN: trainLoss: 0.6023 | trainAcc: 71.8750% (46/64)\n",
            "1 14 Epoch: 43 | ANN: trainLoss: 0.5492 | trainAcc: 75.0000% (96/128)\n",
            "2 14 Epoch: 43 | ANN: trainLoss: 0.5276 | trainAcc: 77.6042% (149/192)\n",
            "3 14 Epoch: 43 | ANN: trainLoss: 0.4946 | trainAcc: 78.1250% (200/256)\n",
            "4 14 Epoch: 43 | ANN: trainLoss: 0.4984 | trainAcc: 79.0625% (253/320)\n",
            "5 14 Epoch: 43 | ANN: trainLoss: 0.4892 | trainAcc: 79.1667% (304/384)\n",
            "6 14 Epoch: 43 | ANN: trainLoss: 0.5163 | trainAcc: 79.0179% (354/448)\n",
            "7 14 Epoch: 43 | ANN: trainLoss: 0.5267 | trainAcc: 79.1016% (405/512)\n",
            "8 14 Epoch: 43 | ANN: trainLoss: 0.5255 | trainAcc: 79.3403% (457/576)\n",
            "9 14 Epoch: 43 | ANN: trainLoss: 0.5273 | trainAcc: 79.2188% (507/640)\n",
            "10 14 Epoch: 43 | ANN: trainLoss: 0.5238 | trainAcc: 78.9773% (556/704)\n",
            "11 14 Epoch: 43 | ANN: trainLoss: 0.5133 | trainAcc: 79.4271% (610/768)\n",
            "12 14 Epoch: 43 | ANN: trainLoss: 0.5127 | trainAcc: 79.8077% (664/832)\n",
            "13 14 Epoch: 43 | ANN: trainLoss: 0.5094 | trainAcc: 79.9539% (694/868)\n",
            "0 2 Epoch: 43 | ANN: testLoss: 0.9002 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 43 | ANN: testLoss: 0.8961 | testAcc: 61.8557% (60/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 44 | ANN: trainLoss: 0.4724 | trainAcc: 82.8125% (53/64)\n",
            "1 14 Epoch: 44 | ANN: trainLoss: 0.4170 | trainAcc: 82.0312% (105/128)\n",
            "2 14 Epoch: 44 | ANN: trainLoss: 0.4429 | trainAcc: 81.7708% (157/192)\n",
            "3 14 Epoch: 44 | ANN: trainLoss: 0.4884 | trainAcc: 80.8594% (207/256)\n",
            "4 14 Epoch: 44 | ANN: trainLoss: 0.4793 | trainAcc: 81.5625% (261/320)\n",
            "5 14 Epoch: 44 | ANN: trainLoss: 0.4917 | trainAcc: 80.9896% (311/384)\n",
            "6 14 Epoch: 44 | ANN: trainLoss: 0.4781 | trainAcc: 81.0268% (363/448)\n",
            "7 14 Epoch: 44 | ANN: trainLoss: 0.4620 | trainAcc: 82.2266% (421/512)\n",
            "8 14 Epoch: 44 | ANN: trainLoss: 0.4590 | trainAcc: 81.9444% (472/576)\n",
            "9 14 Epoch: 44 | ANN: trainLoss: 0.4801 | trainAcc: 80.7812% (517/640)\n",
            "10 14 Epoch: 44 | ANN: trainLoss: 0.4853 | trainAcc: 80.1136% (564/704)\n",
            "11 14 Epoch: 44 | ANN: trainLoss: 0.4876 | trainAcc: 79.6875% (612/768)\n",
            "12 14 Epoch: 44 | ANN: trainLoss: 0.4814 | trainAcc: 80.0481% (666/832)\n",
            "13 14 Epoch: 44 | ANN: trainLoss: 0.4872 | trainAcc: 79.8387% (693/868)\n",
            "0 2 Epoch: 44 | ANN: testLoss: 0.8842 | testAcc: 67.1875% (43/64)\n",
            "1 2 Epoch: 44 | ANN: testLoss: 0.9301 | testAcc: 60.8247% (59/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 45 | ANN: trainLoss: 0.5904 | trainAcc: 78.1250% (50/64)\n",
            "1 14 Epoch: 45 | ANN: trainLoss: 0.5410 | trainAcc: 78.1250% (100/128)\n",
            "2 14 Epoch: 45 | ANN: trainLoss: 0.4925 | trainAcc: 80.7292% (155/192)\n",
            "3 14 Epoch: 45 | ANN: trainLoss: 0.4880 | trainAcc: 80.4688% (206/256)\n",
            "4 14 Epoch: 45 | ANN: trainLoss: 0.4543 | trainAcc: 81.8750% (262/320)\n",
            "5 14 Epoch: 45 | ANN: trainLoss: 0.4778 | trainAcc: 80.2083% (308/384)\n",
            "6 14 Epoch: 45 | ANN: trainLoss: 0.4921 | trainAcc: 79.4643% (356/448)\n",
            "7 14 Epoch: 45 | ANN: trainLoss: 0.4960 | trainAcc: 79.2969% (406/512)\n",
            "8 14 Epoch: 45 | ANN: trainLoss: 0.4931 | trainAcc: 79.5139% (458/576)\n",
            "9 14 Epoch: 45 | ANN: trainLoss: 0.4918 | trainAcc: 79.5312% (509/640)\n",
            "10 14 Epoch: 45 | ANN: trainLoss: 0.4837 | trainAcc: 80.2557% (565/704)\n",
            "11 14 Epoch: 45 | ANN: trainLoss: 0.4792 | trainAcc: 80.9896% (622/768)\n",
            "12 14 Epoch: 45 | ANN: trainLoss: 0.4814 | trainAcc: 80.8894% (673/832)\n",
            "13 14 Epoch: 45 | ANN: trainLoss: 0.4881 | trainAcc: 80.6452% (700/868)\n",
            "0 2 Epoch: 45 | ANN: testLoss: 0.9974 | testAcc: 65.6250% (42/64)\n",
            "1 2 Epoch: 45 | ANN: testLoss: 0.8796 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 46 | ANN: trainLoss: 0.3615 | trainAcc: 84.3750% (54/64)\n",
            "1 14 Epoch: 46 | ANN: trainLoss: 0.3949 | trainAcc: 82.8125% (106/128)\n",
            "2 14 Epoch: 46 | ANN: trainLoss: 0.4394 | trainAcc: 80.7292% (155/192)\n",
            "3 14 Epoch: 46 | ANN: trainLoss: 0.4436 | trainAcc: 81.2500% (208/256)\n",
            "4 14 Epoch: 46 | ANN: trainLoss: 0.4182 | trainAcc: 83.4375% (267/320)\n",
            "5 14 Epoch: 46 | ANN: trainLoss: 0.4178 | trainAcc: 83.8542% (322/384)\n",
            "6 14 Epoch: 46 | ANN: trainLoss: 0.4275 | trainAcc: 83.7054% (375/448)\n",
            "7 14 Epoch: 46 | ANN: trainLoss: 0.4336 | trainAcc: 83.5938% (428/512)\n",
            "8 14 Epoch: 46 | ANN: trainLoss: 0.4379 | trainAcc: 83.3333% (480/576)\n",
            "9 14 Epoch: 46 | ANN: trainLoss: 0.4354 | trainAcc: 83.5938% (535/640)\n",
            "10 14 Epoch: 46 | ANN: trainLoss: 0.4487 | trainAcc: 83.0966% (585/704)\n",
            "11 14 Epoch: 46 | ANN: trainLoss: 0.4508 | trainAcc: 83.0729% (638/768)\n",
            "12 14 Epoch: 46 | ANN: trainLoss: 0.4560 | trainAcc: 82.6923% (688/832)\n",
            "13 14 Epoch: 46 | ANN: trainLoss: 0.4707 | trainAcc: 82.1429% (713/868)\n",
            "0 2 Epoch: 46 | ANN: testLoss: 0.9300 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 46 | ANN: testLoss: 0.9304 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 47 | ANN: trainLoss: 0.5487 | trainAcc: 75.0000% (48/64)\n",
            "1 14 Epoch: 47 | ANN: trainLoss: 0.5405 | trainAcc: 77.3438% (99/128)\n",
            "2 14 Epoch: 47 | ANN: trainLoss: 0.4659 | trainAcc: 81.2500% (156/192)\n",
            "3 14 Epoch: 47 | ANN: trainLoss: 0.4729 | trainAcc: 82.0312% (210/256)\n",
            "4 14 Epoch: 47 | ANN: trainLoss: 0.4873 | trainAcc: 81.5625% (261/320)\n",
            "5 14 Epoch: 47 | ANN: trainLoss: 0.5244 | trainAcc: 80.2083% (308/384)\n",
            "6 14 Epoch: 47 | ANN: trainLoss: 0.5323 | trainAcc: 78.7946% (353/448)\n",
            "7 14 Epoch: 47 | ANN: trainLoss: 0.5276 | trainAcc: 78.7109% (403/512)\n",
            "8 14 Epoch: 47 | ANN: trainLoss: 0.5257 | trainAcc: 78.9931% (455/576)\n",
            "9 14 Epoch: 47 | ANN: trainLoss: 0.5128 | trainAcc: 80.0000% (512/640)\n",
            "10 14 Epoch: 47 | ANN: trainLoss: 0.5046 | trainAcc: 80.2557% (565/704)\n",
            "11 14 Epoch: 47 | ANN: trainLoss: 0.4989 | trainAcc: 80.4688% (618/768)\n",
            "12 14 Epoch: 47 | ANN: trainLoss: 0.5072 | trainAcc: 80.1683% (667/832)\n",
            "13 14 Epoch: 47 | ANN: trainLoss: 0.5068 | trainAcc: 80.1843% (696/868)\n",
            "0 2 Epoch: 47 | ANN: testLoss: 0.8925 | testAcc: 64.0625% (41/64)\n",
            "1 2 Epoch: 47 | ANN: testLoss: 0.9370 | testAcc: 63.9175% (62/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 48 | ANN: trainLoss: 0.3784 | trainAcc: 85.9375% (55/64)\n",
            "1 14 Epoch: 48 | ANN: trainLoss: 0.4169 | trainAcc: 84.3750% (108/128)\n",
            "2 14 Epoch: 48 | ANN: trainLoss: 0.4417 | trainAcc: 82.8125% (159/192)\n",
            "3 14 Epoch: 48 | ANN: trainLoss: 0.4525 | trainAcc: 82.0312% (210/256)\n",
            "4 14 Epoch: 48 | ANN: trainLoss: 0.4552 | trainAcc: 82.5000% (264/320)\n",
            "5 14 Epoch: 48 | ANN: trainLoss: 0.4668 | trainAcc: 81.2500% (312/384)\n",
            "6 14 Epoch: 48 | ANN: trainLoss: 0.4792 | trainAcc: 80.5804% (361/448)\n",
            "7 14 Epoch: 48 | ANN: trainLoss: 0.4622 | trainAcc: 81.4453% (417/512)\n",
            "8 14 Epoch: 48 | ANN: trainLoss: 0.4550 | trainAcc: 81.4236% (469/576)\n",
            "9 14 Epoch: 48 | ANN: trainLoss: 0.4644 | trainAcc: 81.0938% (519/640)\n",
            "10 14 Epoch: 48 | ANN: trainLoss: 0.4558 | trainAcc: 81.5341% (574/704)\n",
            "11 14 Epoch: 48 | ANN: trainLoss: 0.4510 | trainAcc: 81.6406% (627/768)\n",
            "12 14 Epoch: 48 | ANN: trainLoss: 0.4508 | trainAcc: 81.4904% (678/832)\n",
            "13 14 Epoch: 48 | ANN: trainLoss: 0.4562 | trainAcc: 81.3364% (706/868)\n",
            "0 2 Epoch: 48 | ANN: testLoss: 0.7623 | testAcc: 67.1875% (43/64)\n",
            "1 2 Epoch: 48 | ANN: testLoss: 1.0220 | testAcc: 61.8557% (60/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 49 | ANN: trainLoss: 0.4391 | trainAcc: 76.5625% (49/64)\n",
            "1 14 Epoch: 49 | ANN: trainLoss: 0.3912 | trainAcc: 82.0312% (105/128)\n",
            "2 14 Epoch: 49 | ANN: trainLoss: 0.4095 | trainAcc: 80.2083% (154/192)\n",
            "3 14 Epoch: 49 | ANN: trainLoss: 0.3932 | trainAcc: 81.2500% (208/256)\n",
            "4 14 Epoch: 49 | ANN: trainLoss: 0.3826 | trainAcc: 82.8125% (265/320)\n",
            "5 14 Epoch: 49 | ANN: trainLoss: 0.3729 | trainAcc: 83.5938% (321/384)\n",
            "6 14 Epoch: 49 | ANN: trainLoss: 0.3904 | trainAcc: 82.8125% (371/448)\n",
            "7 14 Epoch: 49 | ANN: trainLoss: 0.3797 | trainAcc: 83.9844% (430/512)\n",
            "8 14 Epoch: 49 | ANN: trainLoss: 0.4024 | trainAcc: 83.1597% (479/576)\n",
            "9 14 Epoch: 49 | ANN: trainLoss: 0.4152 | trainAcc: 82.3438% (527/640)\n",
            "10 14 Epoch: 49 | ANN: trainLoss: 0.4119 | trainAcc: 82.6705% (582/704)\n",
            "11 14 Epoch: 49 | ANN: trainLoss: 0.4012 | trainAcc: 83.7240% (643/768)\n",
            "12 14 Epoch: 49 | ANN: trainLoss: 0.4186 | trainAcc: 83.1731% (692/832)\n",
            "13 14 Epoch: 49 | ANN: trainLoss: 0.4261 | trainAcc: 83.0645% (721/868)\n",
            "0 2 Epoch: 49 | ANN: testLoss: 0.9543 | testAcc: 62.5000% (40/64)\n",
            "1 2 Epoch: 49 | ANN: testLoss: 0.9175 | testAcc: 63.9175% (62/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 50 | ANN: trainLoss: 0.3886 | trainAcc: 81.2500% (52/64)\n",
            "1 14 Epoch: 50 | ANN: trainLoss: 0.4135 | trainAcc: 82.8125% (106/128)\n",
            "2 14 Epoch: 50 | ANN: trainLoss: 0.4280 | trainAcc: 82.8125% (159/192)\n",
            "3 14 Epoch: 50 | ANN: trainLoss: 0.4749 | trainAcc: 79.6875% (204/256)\n",
            "4 14 Epoch: 50 | ANN: trainLoss: 0.5107 | trainAcc: 78.7500% (252/320)\n",
            "5 14 Epoch: 50 | ANN: trainLoss: 0.5207 | trainAcc: 78.3854% (301/384)\n",
            "6 14 Epoch: 50 | ANN: trainLoss: 0.5195 | trainAcc: 78.3482% (351/448)\n",
            "7 14 Epoch: 50 | ANN: trainLoss: 0.5119 | trainAcc: 78.9062% (404/512)\n",
            "8 14 Epoch: 50 | ANN: trainLoss: 0.5031 | trainAcc: 79.3403% (457/576)\n",
            "9 14 Epoch: 50 | ANN: trainLoss: 0.5125 | trainAcc: 78.9062% (505/640)\n",
            "10 14 Epoch: 50 | ANN: trainLoss: 0.5083 | trainAcc: 79.1193% (557/704)\n",
            "11 14 Epoch: 50 | ANN: trainLoss: 0.5187 | trainAcc: 78.5156% (603/768)\n",
            "12 14 Epoch: 50 | ANN: trainLoss: 0.5216 | trainAcc: 78.2452% (651/832)\n",
            "13 14 Epoch: 50 | ANN: trainLoss: 0.5215 | trainAcc: 78.3410% (680/868)\n",
            "0 2 Epoch: 50 | ANN: testLoss: 0.7371 | testAcc: 70.3125% (45/64)\n",
            "1 2 Epoch: 50 | ANN: testLoss: 0.9945 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 51 | ANN: trainLoss: 0.3174 | trainAcc: 84.3750% (54/64)\n",
            "1 14 Epoch: 51 | ANN: trainLoss: 0.3985 | trainAcc: 82.8125% (106/128)\n",
            "2 14 Epoch: 51 | ANN: trainLoss: 0.4005 | trainAcc: 84.3750% (162/192)\n",
            "3 14 Epoch: 51 | ANN: trainLoss: 0.4394 | trainAcc: 82.0312% (210/256)\n",
            "4 14 Epoch: 51 | ANN: trainLoss: 0.4450 | trainAcc: 82.1875% (263/320)\n",
            "5 14 Epoch: 51 | ANN: trainLoss: 0.4412 | trainAcc: 83.3333% (320/384)\n",
            "6 14 Epoch: 51 | ANN: trainLoss: 0.4498 | trainAcc: 83.7054% (375/448)\n",
            "7 14 Epoch: 51 | ANN: trainLoss: 0.4448 | trainAcc: 84.3750% (432/512)\n",
            "8 14 Epoch: 51 | ANN: trainLoss: 0.4400 | trainAcc: 84.3750% (486/576)\n",
            "9 14 Epoch: 51 | ANN: trainLoss: 0.4238 | trainAcc: 85.1562% (545/640)\n",
            "10 14 Epoch: 51 | ANN: trainLoss: 0.4192 | trainAcc: 85.2273% (600/704)\n",
            "11 14 Epoch: 51 | ANN: trainLoss: 0.4354 | trainAcc: 83.5938% (642/768)\n",
            "12 14 Epoch: 51 | ANN: trainLoss: 0.4311 | trainAcc: 83.5337% (695/832)\n",
            "13 14 Epoch: 51 | ANN: trainLoss: 0.4186 | trainAcc: 83.7558% (727/868)\n",
            "0 2 Epoch: 51 | ANN: testLoss: 0.8202 | testAcc: 73.4375% (47/64)\n",
            "1 2 Epoch: 51 | ANN: testLoss: 0.9589 | testAcc: 65.9794% (64/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 52 | ANN: trainLoss: 0.6649 | trainAcc: 73.4375% (47/64)\n",
            "1 14 Epoch: 52 | ANN: trainLoss: 0.5999 | trainAcc: 75.7812% (97/128)\n",
            "2 14 Epoch: 52 | ANN: trainLoss: 0.5440 | trainAcc: 78.6458% (151/192)\n",
            "3 14 Epoch: 52 | ANN: trainLoss: 0.4948 | trainAcc: 80.0781% (205/256)\n",
            "4 14 Epoch: 52 | ANN: trainLoss: 0.4784 | trainAcc: 80.6250% (258/320)\n",
            "5 14 Epoch: 52 | ANN: trainLoss: 0.4678 | trainAcc: 80.7292% (310/384)\n",
            "6 14 Epoch: 52 | ANN: trainLoss: 0.4455 | trainAcc: 81.6964% (366/448)\n",
            "7 14 Epoch: 52 | ANN: trainLoss: 0.4569 | trainAcc: 81.0547% (415/512)\n",
            "8 14 Epoch: 52 | ANN: trainLoss: 0.4614 | trainAcc: 81.0764% (467/576)\n",
            "9 14 Epoch: 52 | ANN: trainLoss: 0.4910 | trainAcc: 79.6875% (510/640)\n",
            "10 14 Epoch: 52 | ANN: trainLoss: 0.4827 | trainAcc: 80.2557% (565/704)\n",
            "11 14 Epoch: 52 | ANN: trainLoss: 0.4891 | trainAcc: 80.4688% (618/768)\n",
            "12 14 Epoch: 52 | ANN: trainLoss: 0.4941 | trainAcc: 80.4087% (669/832)\n",
            "13 14 Epoch: 52 | ANN: trainLoss: 0.4946 | trainAcc: 80.6452% (700/868)\n",
            "0 2 Epoch: 52 | ANN: testLoss: 0.8338 | testAcc: 60.9375% (39/64)\n",
            "1 2 Epoch: 52 | ANN: testLoss: 0.9572 | testAcc: 63.9175% (62/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 53 | ANN: trainLoss: 0.3915 | trainAcc: 84.3750% (54/64)\n",
            "1 14 Epoch: 53 | ANN: trainLoss: 0.4112 | trainAcc: 86.7188% (111/128)\n",
            "2 14 Epoch: 53 | ANN: trainLoss: 0.4302 | trainAcc: 84.8958% (163/192)\n",
            "3 14 Epoch: 53 | ANN: trainLoss: 0.4333 | trainAcc: 83.9844% (215/256)\n",
            "4 14 Epoch: 53 | ANN: trainLoss: 0.4537 | trainAcc: 81.2500% (260/320)\n",
            "5 14 Epoch: 53 | ANN: trainLoss: 0.4606 | trainAcc: 80.4688% (309/384)\n",
            "6 14 Epoch: 53 | ANN: trainLoss: 0.4382 | trainAcc: 81.6964% (366/448)\n",
            "7 14 Epoch: 53 | ANN: trainLoss: 0.4542 | trainAcc: 80.8594% (414/512)\n",
            "8 14 Epoch: 53 | ANN: trainLoss: 0.4732 | trainAcc: 80.3819% (463/576)\n",
            "9 14 Epoch: 53 | ANN: trainLoss: 0.4756 | trainAcc: 80.6250% (516/640)\n",
            "10 14 Epoch: 53 | ANN: trainLoss: 0.4795 | trainAcc: 80.2557% (565/704)\n",
            "11 14 Epoch: 53 | ANN: trainLoss: 0.4847 | trainAcc: 80.0781% (615/768)\n",
            "12 14 Epoch: 53 | ANN: trainLoss: 0.4818 | trainAcc: 79.9279% (665/832)\n",
            "13 14 Epoch: 53 | ANN: trainLoss: 0.4744 | trainAcc: 80.0691% (695/868)\n",
            "0 2 Epoch: 53 | ANN: testLoss: 0.8929 | testAcc: 67.1875% (43/64)\n",
            "1 2 Epoch: 53 | ANN: testLoss: 0.9356 | testAcc: 63.9175% (62/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 54 | ANN: trainLoss: 0.4947 | trainAcc: 82.8125% (53/64)\n",
            "1 14 Epoch: 54 | ANN: trainLoss: 0.5580 | trainAcc: 79.6875% (102/128)\n",
            "2 14 Epoch: 54 | ANN: trainLoss: 0.5558 | trainAcc: 77.6042% (149/192)\n",
            "3 14 Epoch: 54 | ANN: trainLoss: 0.5114 | trainAcc: 80.0781% (205/256)\n",
            "4 14 Epoch: 54 | ANN: trainLoss: 0.4886 | trainAcc: 81.2500% (260/320)\n",
            "5 14 Epoch: 54 | ANN: trainLoss: 0.4842 | trainAcc: 80.7292% (310/384)\n",
            "6 14 Epoch: 54 | ANN: trainLoss: 0.4672 | trainAcc: 81.9196% (367/448)\n",
            "7 14 Epoch: 54 | ANN: trainLoss: 0.4690 | trainAcc: 81.8359% (419/512)\n",
            "8 14 Epoch: 54 | ANN: trainLoss: 0.4746 | trainAcc: 81.0764% (467/576)\n",
            "9 14 Epoch: 54 | ANN: trainLoss: 0.4621 | trainAcc: 82.0312% (525/640)\n",
            "10 14 Epoch: 54 | ANN: trainLoss: 0.4535 | trainAcc: 81.6761% (575/704)\n",
            "11 14 Epoch: 54 | ANN: trainLoss: 0.4544 | trainAcc: 81.6406% (627/768)\n",
            "12 14 Epoch: 54 | ANN: trainLoss: 0.4462 | trainAcc: 82.2115% (684/832)\n",
            "13 14 Epoch: 54 | ANN: trainLoss: 0.4458 | trainAcc: 82.1429% (713/868)\n",
            "0 2 Epoch: 54 | ANN: testLoss: 1.0344 | testAcc: 64.0625% (41/64)\n",
            "1 2 Epoch: 54 | ANN: testLoss: 0.8795 | testAcc: 63.9175% (62/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 55 | ANN: trainLoss: 0.4263 | trainAcc: 79.6875% (51/64)\n",
            "1 14 Epoch: 55 | ANN: trainLoss: 0.5103 | trainAcc: 78.1250% (100/128)\n",
            "2 14 Epoch: 55 | ANN: trainLoss: 0.4843 | trainAcc: 80.2083% (154/192)\n",
            "3 14 Epoch: 55 | ANN: trainLoss: 0.4429 | trainAcc: 82.0312% (210/256)\n",
            "4 14 Epoch: 55 | ANN: trainLoss: 0.4437 | trainAcc: 81.5625% (261/320)\n",
            "5 14 Epoch: 55 | ANN: trainLoss: 0.4265 | trainAcc: 83.3333% (320/384)\n",
            "6 14 Epoch: 55 | ANN: trainLoss: 0.4242 | trainAcc: 83.2589% (373/448)\n",
            "7 14 Epoch: 55 | ANN: trainLoss: 0.4323 | trainAcc: 82.8125% (424/512)\n",
            "8 14 Epoch: 55 | ANN: trainLoss: 0.4387 | trainAcc: 82.9861% (478/576)\n",
            "9 14 Epoch: 55 | ANN: trainLoss: 0.4389 | trainAcc: 83.2812% (533/640)\n",
            "10 14 Epoch: 55 | ANN: trainLoss: 0.4279 | trainAcc: 83.5227% (588/704)\n",
            "11 14 Epoch: 55 | ANN: trainLoss: 0.4294 | trainAcc: 83.3333% (640/768)\n",
            "12 14 Epoch: 55 | ANN: trainLoss: 0.4276 | trainAcc: 83.1731% (692/832)\n",
            "13 14 Epoch: 55 | ANN: trainLoss: 0.4219 | trainAcc: 83.2949% (723/868)\n",
            "0 2 Epoch: 55 | ANN: testLoss: 1.0088 | testAcc: 56.2500% (36/64)\n",
            "1 2 Epoch: 55 | ANN: testLoss: 0.8905 | testAcc: 63.9175% (62/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 56 | ANN: trainLoss: 0.5552 | trainAcc: 75.0000% (48/64)\n",
            "1 14 Epoch: 56 | ANN: trainLoss: 0.4791 | trainAcc: 78.9062% (101/128)\n",
            "2 14 Epoch: 56 | ANN: trainLoss: 0.4478 | trainAcc: 82.2917% (158/192)\n",
            "3 14 Epoch: 56 | ANN: trainLoss: 0.4554 | trainAcc: 80.4688% (206/256)\n",
            "4 14 Epoch: 56 | ANN: trainLoss: 0.4371 | trainAcc: 81.2500% (260/320)\n",
            "5 14 Epoch: 56 | ANN: trainLoss: 0.4551 | trainAcc: 81.5104% (313/384)\n",
            "6 14 Epoch: 56 | ANN: trainLoss: 0.4443 | trainAcc: 81.6964% (366/448)\n",
            "7 14 Epoch: 56 | ANN: trainLoss: 0.4334 | trainAcc: 82.8125% (424/512)\n",
            "8 14 Epoch: 56 | ANN: trainLoss: 0.4153 | trainAcc: 83.5069% (481/576)\n",
            "9 14 Epoch: 56 | ANN: trainLoss: 0.4168 | trainAcc: 83.5938% (535/640)\n",
            "10 14 Epoch: 56 | ANN: trainLoss: 0.4194 | trainAcc: 83.0966% (585/704)\n",
            "11 14 Epoch: 56 | ANN: trainLoss: 0.4259 | trainAcc: 82.9427% (637/768)\n",
            "12 14 Epoch: 56 | ANN: trainLoss: 0.4311 | trainAcc: 82.8125% (689/832)\n",
            "13 14 Epoch: 56 | ANN: trainLoss: 0.4343 | trainAcc: 82.9493% (720/868)\n",
            "0 2 Epoch: 56 | ANN: testLoss: 0.9895 | testAcc: 64.0625% (41/64)\n",
            "1 2 Epoch: 56 | ANN: testLoss: 0.8963 | testAcc: 63.9175% (62/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 57 | ANN: trainLoss: 0.4078 | trainAcc: 85.9375% (55/64)\n",
            "1 14 Epoch: 57 | ANN: trainLoss: 0.4332 | trainAcc: 83.5938% (107/128)\n",
            "2 14 Epoch: 57 | ANN: trainLoss: 0.4841 | trainAcc: 80.2083% (154/192)\n",
            "3 14 Epoch: 57 | ANN: trainLoss: 0.4593 | trainAcc: 82.0312% (210/256)\n",
            "4 14 Epoch: 57 | ANN: trainLoss: 0.4574 | trainAcc: 82.1875% (263/320)\n",
            "5 14 Epoch: 57 | ANN: trainLoss: 0.4609 | trainAcc: 81.5104% (313/384)\n",
            "6 14 Epoch: 57 | ANN: trainLoss: 0.4686 | trainAcc: 80.1339% (359/448)\n",
            "7 14 Epoch: 57 | ANN: trainLoss: 0.4817 | trainAcc: 78.5156% (402/512)\n",
            "8 14 Epoch: 57 | ANN: trainLoss: 0.4796 | trainAcc: 78.6458% (453/576)\n",
            "9 14 Epoch: 57 | ANN: trainLoss: 0.4726 | trainAcc: 79.0625% (506/640)\n",
            "10 14 Epoch: 57 | ANN: trainLoss: 0.4631 | trainAcc: 79.5455% (560/704)\n",
            "11 14 Epoch: 57 | ANN: trainLoss: 0.4713 | trainAcc: 78.7760% (605/768)\n",
            "12 14 Epoch: 57 | ANN: trainLoss: 0.4714 | trainAcc: 78.4856% (653/832)\n",
            "13 14 Epoch: 57 | ANN: trainLoss: 0.4693 | trainAcc: 78.3410% (680/868)\n",
            "0 2 Epoch: 57 | ANN: testLoss: 0.7900 | testAcc: 67.1875% (43/64)\n",
            "1 2 Epoch: 57 | ANN: testLoss: 0.9939 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 58 | ANN: trainLoss: 0.7295 | trainAcc: 71.8750% (46/64)\n",
            "1 14 Epoch: 58 | ANN: trainLoss: 0.6101 | trainAcc: 77.3438% (99/128)\n",
            "2 14 Epoch: 58 | ANN: trainLoss: 0.5393 | trainAcc: 78.1250% (150/192)\n",
            "3 14 Epoch: 58 | ANN: trainLoss: 0.5015 | trainAcc: 80.4688% (206/256)\n",
            "4 14 Epoch: 58 | ANN: trainLoss: 0.5063 | trainAcc: 79.3750% (254/320)\n",
            "5 14 Epoch: 58 | ANN: trainLoss: 0.4894 | trainAcc: 80.4688% (309/384)\n",
            "6 14 Epoch: 58 | ANN: trainLoss: 0.4796 | trainAcc: 80.5804% (361/448)\n",
            "7 14 Epoch: 58 | ANN: trainLoss: 0.4695 | trainAcc: 80.6641% (413/512)\n",
            "8 14 Epoch: 58 | ANN: trainLoss: 0.4644 | trainAcc: 80.9028% (466/576)\n",
            "9 14 Epoch: 58 | ANN: trainLoss: 0.4867 | trainAcc: 79.5312% (509/640)\n",
            "10 14 Epoch: 58 | ANN: trainLoss: 0.4865 | trainAcc: 79.8295% (562/704)\n",
            "11 14 Epoch: 58 | ANN: trainLoss: 0.4872 | trainAcc: 79.9479% (614/768)\n",
            "12 14 Epoch: 58 | ANN: trainLoss: 0.4884 | trainAcc: 79.6875% (663/832)\n",
            "13 14 Epoch: 58 | ANN: trainLoss: 0.4880 | trainAcc: 79.3779% (689/868)\n",
            "0 2 Epoch: 58 | ANN: testLoss: 0.8769 | testAcc: 64.0625% (41/64)\n",
            "1 2 Epoch: 58 | ANN: testLoss: 0.9585 | testAcc: 64.9485% (63/97)\n",
            "\n",
            "\n",
            "0 14 Epoch: 59 | ANN: trainLoss: 0.3782 | trainAcc: 87.5000% (56/64)\n",
            "1 14 Epoch: 59 | ANN: trainLoss: 0.3779 | trainAcc: 85.9375% (110/128)\n",
            "2 14 Epoch: 59 | ANN: trainLoss: 0.3664 | trainAcc: 88.0208% (169/192)\n",
            "3 14 Epoch: 59 | ANN: trainLoss: 0.3928 | trainAcc: 86.7188% (222/256)\n",
            "4 14 Epoch: 59 | ANN: trainLoss: 0.3826 | trainAcc: 87.5000% (280/320)\n",
            "5 14 Epoch: 59 | ANN: trainLoss: 0.4037 | trainAcc: 85.9375% (330/384)\n",
            "6 14 Epoch: 59 | ANN: trainLoss: 0.3963 | trainAcc: 85.2679% (382/448)\n",
            "7 14 Epoch: 59 | ANN: trainLoss: 0.3923 | trainAcc: 85.5469% (438/512)\n",
            "8 14 Epoch: 59 | ANN: trainLoss: 0.4055 | trainAcc: 85.2431% (491/576)\n",
            "9 14 Epoch: 59 | ANN: trainLoss: 0.4121 | trainAcc: 84.8438% (543/640)\n",
            "10 14 Epoch: 59 | ANN: trainLoss: 0.4115 | trainAcc: 85.0852% (599/704)\n",
            "11 14 Epoch: 59 | ANN: trainLoss: 0.4130 | trainAcc: 84.6354% (650/768)\n",
            "12 14 Epoch: 59 | ANN: trainLoss: 0.4181 | trainAcc: 84.3750% (702/832)\n",
            "13 14 Epoch: 59 | ANN: trainLoss: 0.4301 | trainAcc: 84.1014% (730/868)\n",
            "0 2 Epoch: 59 | ANN: testLoss: 1.0722 | testAcc: 59.3750% (38/64)\n",
            "1 2 Epoch: 59 | ANN: testLoss: 0.8753 | testAcc: 63.9175% (62/97)\n",
            "---------------------------------------------\n",
            "Converting using MaxNorm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:00<00:00, 379.40it/s]\n",
            "100%|██████████| 14/14 [00:00<00:00, 264.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ANN accuracy: Test: 63.9200%\n",
            "SNN accuracy: max_norm: 63.9175%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7o4VT4r0r-xf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9de2af79-b204-4cdc-e175-4810058961d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating LENet CNN...\n",
            "Evaluating LENet SNN...\n",
            "Evaluating LENet_FCL CNN...\n",
            "Evaluating LENet_FCL SNN...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAHHCAYAAADqJrG+AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUGRJREFUeJzt3XlYVGX7B/DvADKsM4iigCCiIIIKLqk/cgFzQSvTtEzTAlPLUlNcs9xLMS23MvR9M7e0MktN3MIUXFArFXdREBV3U1mVxZnn94cvkyOgs3EYxu+n61yXc+Y559yHMefmfpYjE0IIEBERERnAqqIDICIiosqLiQQREREZjIkEERERGYyJBBERERmMiQQREREZjIkEERERGYyJBBERERmMiQQREREZjIkEERERGYyJBFVq586dQ+fOnaFUKiGTybBhwwaTnv/ChQuQyWRYvny5Sc9bmYWHhyM8PLyiwyAiM8FEgoyWlpaG9957D3Xr1oWdnR0UCgVat26NBQsW4P79++V67cjISBw/fhwzZszAqlWr8Nxzz5Xr9aQUFRUFmUwGhUJR6s/x3LlzkMlkkMlk+OKLL/Q+/9WrVzF16lQkJyebIFpp1KlTBy+//PIT2xT/3Erb7OzsNO0SEhI0+w8dOlTqeZycnAyKc8uWLZg6darex61fvx5du3ZF9erVYWtrC09PT/Tu3Rs7d+40Ku7w8HDIZDJ069atRPviZNmQv0NEAGBT0QFQ5bZ582a8/vrrkMvlePvtt9GoUSMUFhZi7969GDt2LE6ePIn//Oc/5XLt+/fvY//+/fjkk08wbNiwcrmGj48P7t+/jypVqpTL+Z/GxsYG9+7dw6ZNm9C7d2+t91avXg07Ozvk5+cbdO6rV69i2rRpqFOnDpo0aaLzcb///rtB15OSXC7Ht99+W2K/tbV1qe2nTp2KTZs2mez6W7ZswaJFi3ROJoQQeOedd7B8+XI0bdoUo0aNgru7O65du4b169ejQ4cO2LdvH55//nmj4o6Li8OhQ4fQvHlzfW6H6ImYSJDB0tPT0adPH/j4+GDnzp3w8PDQvDd06FCkpqZi8+bN5Xb9W7duAQBcXFzK7RqP/xYrNblcjtatW+OHH34okUisWbMGL730En755RdJYrl37x4cHBxga2sryfWMYWNjg/79++vUtkmTJoiLi8Phw4fRrFmzco6sdF9++SWWL1+OkSNHYu7cuZDJZJr3PvnkE6xatQo2Ntr/XOsbd+3atZGTk4Np06bht99+M/k90LOLXRtksNmzZyM3NxdLly7VSiKK+fn5YcSIEZrXDx48wKeffop69epBLpejTp06+Pjjj1FQUKB1XHH5eu/evWjZsiXs7OxQt25drFy5UtNm6tSp8PHxAQCMHTsWMpkMderUAfCwtFv850dNnTpV6x9oAIiPj0ebNm3g4uICJycnBAQE4OOPP9a8X9YYiZ07d6Jt27ZwdHSEi4sLunfvjtOnT5d6vdTUVERFRcHFxQVKpRIDBgzAvXv3yv7BPubNN9/E1q1bkZmZqdn3119/4dy5c3jzzTdLtL9z5w7GjBmDxo0bw8nJCQqFAl27dsXRo0c1bRISEtCiRQsAwIABAzSl8uL7DA8PR6NGjXDo0CG0a9cODg4Omp/L42MkIiMjYWdnV+L+IyIiULVqVVy9elXne60Iw4cPR9WqVXWuHmzdulXz2Ts7O+Oll17CyZMnNe9HRUVh0aJFAKDVrVKW+/fvIyYmBg0aNMAXX3xRatu33noLLVu2NCpuZ2dnREdHY9OmTTh8+LBOxxDpgokEGWzTpk2oW7duiXJrWQYNGoTJkyejWbNmmDdvHsLCwhATE4M+ffqUaJuamorXXnsNnTp1wpdffomqVasiKipK8w92z549MW/ePABA3759sWrVKsyfP1+v+E+ePImXX34ZBQUFmD59Or788ku88sor2Ldv3xOP27FjByIiInDz5k1MnToVo0aNQlJSElq3bo0LFy6UaN+7d2/k5OQgJiYGvXv3xvLlyzFt2jSd4+zZsydkMhl+/fVXzb41a9agQYMGpf4mev78eWzYsAEvv/wy5s6di7Fjx+L48eMICwvTfKkHBgZi+vTpAIB3330Xq1atwqpVq9CuXTvNeW7fvo2uXbuiSZMmmD9/Ptq3b19qfAsWLICbmxsiIyOhUqkAAEuWLMHvv/+Or776Cp6enjrfqyn9888/Jbbs7OwS7RQKhc5fsKtWrcJLL70EJycnfP7555g0aRJOnTqFNm3aaD779957D506ddK0L97KsnfvXty5cwdvvvlmmV0vpdEn7mIjRozQK/kg0okgMkBWVpYAILp3765T++TkZAFADBo0SGv/mDFjBACxc+dOzT4fHx8BQOzevVuz7+bNm0Iul4vRo0dr9qWnpwsAYs6cOVrnjIyMFD4+PiVimDJlinj0r/y8efMEAHHr1q0y4y6+xrJlyzT7mjRpImrUqCFu376t2Xf06FFhZWUl3n777RLXe+edd7TO+eqrr4pq1aqVec1H78PR0VEIIcRrr70mOnToIIQQQqVSCXd3dzFt2rRSfwb5+flCpVKVuA+5XC6mT5+u2ffXX3+VuLdiYWFhAoBYvHhxqe+FhYVp7du+fbsAID777DNx/vx54eTkJHr06PHUe9SXj4+PeOmll57YJjIyUgAodYuIiNC027VrlwAgfv75Z5GZmSmqVq0qXnnlFa3zFP/8hRAiJydHuLi4iMGDB2td7/r160KpVGrtHzp0qND1n9cFCxYIAGL9+vU6tdc3biEefmYNGzYUQggxbdo0AUAcOnRICFH2/0dEumJFggxS/Juds7OzTu23bNkCABg1apTW/tGjRwNAibEUQUFBaNu2rea1m5sbAgICcP78eYNjflzx2IqNGzdCrVbrdMy1a9eQnJyMqKgouLq6avYHBwejU6dOmvt81JAhQ7Ret23bFrdv3y71t+OyvPnmm0hISMD169exc+dOXL9+vdRuDeDhuAorq4f/a6tUKty+fVvTbaNPSVsul2PAgAE6te3cuTPee+89TJ8+HT179oSdnR2WLFmi87VMzc7ODvHx8SW2WbNmldpeqVRi5MiR+O2333DkyJFS28THxyMzMxN9+/bVqnJYW1ujVatW2LVrl0Gx6vv/kr5xP664KqFPVYzoSZhIkEEUCgUAICcnR6f2Fy9ehJWVFfz8/LT2u7u7w8XFBRcvXtTaX7t27RLnqFq1Ku7evWtgxCW98cYbaN26NQYNGoSaNWuiT58+WLt27ROTiuI4AwICSrwXGBiIf/75B3l5eVr7H7+XqlWrAoBe9/Liiy/C2dkZP/30E1avXo0WLVqU+FkWU6vVmDdvHvz9/SGXy1G9enW4ubnh2LFjyMrK0vmatWrV0mtg5RdffAFXV1ckJydj4cKFqFGjxlOPuXXrFq5fv67ZcnNzdb7ek1hbW6Njx44ltifNThkxYgRcXFzKLPufO3cOAPDCCy/Azc1Na/v9999x8+ZNg2LV9/8lfeN+nCHJB9GTMJEggygUCnh6euLEiRN6HfekQWePKquvWAhh8DWK+++L2dvbY/fu3dixYwfeeustHDt2DG+88QY6depUoq0xjLmXYnK5HD179sSKFSuwfv36MqsRADBz5kyMGjUK7dq1w/fff4/t27cjPj4eDRs21LnyAjz8+ejjyJEjmi/T48eP63RMixYt4OHhodkqci2Dp33BFv/sVq1aVWq1Y+PGjQZdt0GDBgB0/5npG3dpipMPViXIFJhIkMFefvllpKWlYf/+/U9t6+PjA7VarfmtrtiNGzeQmZmpmYFhClWrVtWa4VDs8aoHAFhZWaFDhw6YO3cuTp06hRkzZmDnzp1llqmL40xJSSnx3pkzZ1C9enU4OjoadwNlePPNN3HkyBHk5OSUOkC12Lp169C+fXssXboUffr0QefOndGxY8cSPxNdkzpd5OXlYcCAAQgKCsK7776L2bNn46+//nrqcatXr9b6Mn777bdNFpMhRo4cWeYXbL169QAANWrUKLXa8ehMFn1+tm3atEHVqlXxww8/GJzAPinu0hQnHxs3bmRVgozGRIIMNm7cODg6OmLQoEG4ceNGiffT0tKwYMECAA9L8wBKzKyYO3cuAOCll14yWVz16tVDVlYWjh07ptlXvLDPo+7cuVPi2OLS9+NTUot5eHigSZMmWLFihdYX84kTJ/D7779r7rM8tG/fHp9++im+/vpruLu7l9nO2tq6RLXj559/xpUrV7T2FSc8pSVd+ho/fjwuXbqEFStWYO7cuahTpw4iIyPL/DkWa926tdaXcd26dY2OxRiPfsE+vuJnREQEFAoFZs6ciaKiohLHFq9rAuj3s3VwcMD48eNx+vRpjB8/vtRK1ffff48///zToLjLUpx8FM/eITIUF6Qig9WrVw9r1qzBG2+8gcDAQK2VLZOSkvDzzz8jKioKABASEoLIyEj85z//QWZmJsLCwvDnn39ixYoV6NGjR5lTCw3Rp08fjB8/Hq+++io+/PBD3Lt3D7Gxsahfv77WYMPp06dj9+7deOmll+Dj44ObN2/im2++gZeXF9q0aVPm+efMmYOuXbsiNDQUAwcOxP379/HVV19BqVSW67Q6KysrTJw48antXn75ZUyfPh0DBgzA888/j+PHj2P16tUlvqTr1asHFxcXLF68GM7OznB0dESrVq3g6+urV1w7d+7EN998gylTpmimoy5btgzh4eGYNGkSZs+erdf5niY1NRWfffZZif1NmzbVJKQPHjzA999/X+rxr7766hOrRiNGjMC8efNw9OhRrXYKhQKxsbF466230KxZM/Tp0wdubm64dOkSNm/ejNatW+Prr78GAM3KkR9++CEiIiJgbW39xCpS8SqwX375JXbt2oXXXnsN7u7uuH79OjZs2IA///wTSUlJT/y5lBV3WZRKJUaMGMHuDTJexU4aIUtw9uxZMXjwYFGnTh1ha2srnJ2dRevWrcVXX30l8vPzNe2KiorEtGnThK+vr6hSpYrw9vYWEyZM0GojRNlT/B6fdvikaWu///67aNSokbC1tRUBAQHi+++/LzH9848//hDdu3cXnp6ewtbWVnh6eoq+ffuKs2fPlrjG41Mkd+zYIVq3bi3s7e2FQqEQ3bp1E6dOndJqU3y9x6eXLlu2TAAQ6enpZf5MhSh9Gt/jypr+OXr0aOHh4SHs7e1F69atxf79+0udtrlx40YRFBQkbGxstO7z0emCj3v0PNnZ2cLHx0c0a9ZMFBUVabWLjo4WVlZWYv/+/U+8B30UTw0ubRs4cKAQ4snTPx/9uT86jfJxxZ9daT//Xbt2iYiICKFUKoWdnZ2oV6+eiIqKEn///bemzYMHD8Tw4cOFm5ubkMlkOk8FXbdunejcubNwdXUVNjY2wsPDQ7zxxhsiISFB6/r6xl3W53n37l2hVCo5/ZOMIhNCjxFfRERERI/gGAkiIiIyGBMJIiIiMhgTCSIiIjIYEwkiIiIyGBMJIiIiMhgTCSIiIjIYF6QyklqtxtWrV+Hs7GzSJYeJiKj8CSGQk5MDT09PzVNzy0N+fj4KCwtNci5bW1vY2dmZ5FymwETCSFevXoW3t3dFh0FEREbIyMiAl5dXuZw7Pz8fvj5OuH7TNA8DdHd3R3p6utkkE0wkjOTs7AwAuHi4DhRO7CmydK/Wb1zRIRCRCT1AEfZii+bf8vJQWFiI6zdVuHioDhTOxn1PZOeo4dP8AgoLC5lIWIri7gyFk5XRf0HI/NnIqlR0CERkSv9b21mKrmknZxmcnI27jhrm14XORIKIiEgCKqGGysiHUqiE2jTBmBATCSIiIgmoIaCGcZmEsceXB9biiYiIyGBMJIiIiCSgNtF/uoqNjUVwcDAUCgUUCgVCQ0OxdetWzfvh4eGQyWRa25AhQ/S+L3ZtEBERSUAlBFTCuK4JfY738vLCrFmz4O/vDyEEVqxYge7du+PIkSNo2LAhAGDw4MGYPn265hgHBwe9Y2IiQUREZIG6deum9XrGjBmIjY3FgQMHNImEg4MD3N3djboOuzaIiIgkUDzY0tjNECqVCj/++CPy8vIQGhqq2b969WpUr14djRo1woQJE3Dv3j29z82KBBERkQTUEFCZaNZGdna21n65XA65XF6i/fHjxxEaGor8/Hw4OTlh/fr1CAoKAgC8+eab8PHxgaenJ44dO4bx48cjJSUFv/76q14xMZEgIiKqZB5/NMOUKVMwderUEu0CAgKQnJyMrKwsrFu3DpGRkUhMTERQUBDeffddTbvGjRvDw8MDHTp0QFpaGurVq6dzLEwkiIiIJGDKdSQyMjKgUCg0+0urRgAPH/Dl5+cHAGjevDn++usvLFiwAEuWLCnRtlWrVgCA1NRUJhJERETmxpSzNoqndOpLrVajoKCg1PeSk5MBAB4eHnqdk4kEERGRBZowYQK6du2K2rVrIycnB2vWrEFCQgK2b9+OtLQ0rFmzBi+++CKqVauGY8eOITo6Gu3atUNwcLBe12EiQUREJAH1/zZjz6Grmzdv4u2338a1a9egVCoRHByM7du3o1OnTsjIyMCOHTswf/585OXlwdvbG7169cLEiRP1jomJBBERkQRUJpi1oc/xS5cuLfM9b29vJCYmGhVLMSYSREREElAJmODpn6aJxZS4IBUREREZjBUJIiIiCUg9RkIqTCSIiIgkoIYMKsiMPoe5YdcGERERGYwVCSIiIgmoxcPN2HOYGyYSREREElCZoGvD2OPLA7s2iIiIyGCsSBAREUnAUisSTCSIiIgkoBYyqIWRszaMPL48sGuDiIiIDMaKBBERkQTYtUFEREQGU8EKKiM7AlQmisWUmEgQERFJQJhgjITgGAkiIiKyJKxIEBERSYBjJIiIiMhgKmEFlTByjIQZLpHNrg0iIiIyGCsSREREElBDBrWRv7+rYX4lCSYSREREErDUMRLs2iAiIiKDsSJBREQkAdMMtmTXBhER0TPp4RgJIx/axa4NIiIisiSsSBAREUlAbYJnbXDWBhER0TOKYySIiIjIYGpYWeQ6EhwjQURERAZjRYKIiEgCKiGDysjHgBt7fHlgIkFERCQBlQkGW6rYtUFERESWhBUJIiIiCaiFFdRGztpQc9YGERHRs4ldG0RERESPYUWCiIhIAmoYP+tCbZpQTIqJBBERkQRMsyCV+XUkmF9EREREVGmwIkFERCQB0zxrw/x+/2ciQUREJAE1ZFDD2DESXNmSKpFNK6ph88rquJFhCwDwCchHv+jraPFCjqbNqb8dsPxzD5w57ABra6Buw/uYuSYNcnvzm6JE+mnUKhevf3AL/o3voZr7A0x9pw72b1NWdFhUDvhZS8NSKxLmF1EZoqKiIJPJIJPJUKVKFfj6+mLcuHHIz883+twXLlyATCZDcnKy8YFaEDePIrzz8VV8vS0FX209i5DWOZg6wBcXUuwAPEwiPulXD83b5WDhlnNYuOUsXhnwD2SV5m8VPYmdgxrnT9rh64+9KjoUKmf8rMkYlaoi0aVLFyxbtgxFRUU4dOgQIiMjIZPJ8Pnnn1d0aBbp/zpna70e8NF1xK2sjjOHHFAnIB9LptZCj4G38Mbwm5o23n4FUodJ5eTvXQr8vUtR0WGQBPhZS8M0C1KZ329q5hfRE8jlcri7u8Pb2xs9evRAx44dER8fDwBQq9WIiYmBr68v7O3tERISgnXr1mmOvXv3Lvr16wc3NzfY29vD398fy5YtAwD4+voCAJo2bQqZTIbw8HDJ783cqVRAwgYXFNyzQuBzecj8xwZnDjvCpdoDjOzmjzeCG2JMTz+cOOhY0aESEZkltZCZZNNVbGwsgoODoVAooFAoEBoaiq1bt2rez8/Px9ChQ1GtWjU4OTmhV69euHHjht73VakqEo86ceIEkpKS4OPjAwCIiYnB999/j8WLF8Pf3x+7d+9G//794ebmhrCwMEyaNAmnTp3C1q1bUb16daSmpuL+/fsAgD///BMtW7bEjh070LBhQ9ja2lbkrZmV9NN2GNnNH4UFVrB3VGPy0nT41C/A6UMOAIBVc90xeNJV1Gt4HzvWVcVHb9TDkp1nUKtuYQVHTkT0bPPy8sKsWbPg7+8PIQRWrFiB7t2748iRI2jYsCGio6OxefNm/Pzzz1AqlRg2bBh69uyJffv26XWdSpVIxMXFwcnJCQ8ePEBBQQGsrKzw9ddfo6CgADNnzsSOHTsQGhoKAKhbty727t2LJUuWICwsDJcuXULTpk3x3HPPAQDq1KmjOa+bmxsAoFq1anB3d39iDAUFBSgo+Ld8n52d/YTWlZ9XvQJ8E5+CeznW2BPngi9G+GDOr+eg/t/yai/2v42IPncAAH6N7yN5rzO2/1gN73x8rQKjJiIyP2oTdG3osyBVt27dtF7PmDEDsbGxOHDgALy8vLB06VKsWbMGL7zwAgBg2bJlCAwMxIEDB/B///d/Ol+nUiUS7du3R2xsLPLy8jBv3jzY2NigV69eOHnyJO7du4dOnTpptS8sLETTpk0BAO+//z569eqFw4cPo3PnzujRoweef/55vWOIiYnBtGnTTHI/lUEVW4Favg+rC/7B95GS7IAN37rhjWEPx0X41Nce7Ortl4+bV6pIHicRkbkzzdM/Hx7/+C+xcrkccrm8zONUKhV+/vln5OXlITQ0FIcOHUJRURE6duyoadOgQQPUrl0b+/fv1yuRqFRjJBwdHeHn54eQkBB89913OHjwIJYuXYrc3FwAwObNm5GcnKzZTp06pRkn0bVrV1y8eBHR0dG4evUqOnTogDFjxugdw4QJE5CVlaXZMjIyTHqP5k4IoKjQCjW9C1HNvRCX07T/4l45L0cNr6IKio6I6Nng7e0NpVKp2WJiYkptd/z4cTg5OUEul2PIkCFYv349goKCcP36ddja2sLFxUWrfc2aNXH9+nW9YqlUFYlHWVlZ4eOPP8aoUaNw9uxZyOVyXLp0CWFhYWUe4+bmhsjISERGRqJt27YYO3YsvvjiC82YCJVK9dTrPi3rsyTfzfRAixey4VarCPdzrbBrfVUcS3LCjDVpkMmA196/hVVfuKNu0H3UbXgfO352RUaaHSb+90JFh04mYOeggqfvv2Nd3L0LUbfhfeRkWuPWFY4jsiT8rKWhggwqIxeUKj4+IyMDCsW/M23K+l4KCAhAcnIysrKysG7dOkRGRiIxMdGoGB5XaRMJAHj99dcxduxYLFmyBGPGjEF0dDTUajXatGmDrKws7Nu3DwqFApGRkZg8eTKaN2+Ohg0boqCgAHFxcQgMDAQA1KhRA/b29ti2bRu8vLxgZ2cHpZKLsWT+Y4M5H/rgzk0bODir4BuYjxlr0tA87GEFqOfgWyjKl2HxlFrIybRG3aB8xPyQBs86HGhpCeqH3MecX9I0r4dMuwoA+P2nqvgyunZFhUXlgJ+1NEzZtVE8E+NpbG1t4efnBwBo3rw5/vrrLyxYsABvvPEGCgsLkZmZqVWVuHHjxlPHCj6uUicSNjY2GDZsGGbPno309HS4ubkhJiYG58+fh4uLC5o1a4aPP/4YwMMf5oQJE3DhwgXY29ujbdu2+PHHHzXnWbhwIaZPn47Jkyejbdu2SEhIqMA7Mw+j5j692+aN4Te11pEgy3FsvxMiPEMqOgySAD/rZ4darUZBQQGaN2+OKlWq4I8//kCvXr0AACkpKbh06ZJm0oKuZEIIrmVshOzsbCiVStw9WxcK50o15IQMEOHZpKJDICITeiCKkICNyMrK0uk3fEMUf09MPtgRdk7GDUbPzy3C9FY7dIp3woQJ6Nq1K2rXro2cnBysWbMGn3/+ObZv345OnTrh/fffx5YtW7B8+XIoFAoMHz4cAJCUlKRXTJW6IkFERFRZmLJrQxc3b97E22+/jWvXrkGpVCI4OFiTRADAvHnzYGVlhV69eqGgoAARERH45ptv9I6JiQQREZEEpH5o19KlS5/4vp2dHRYtWoRFixYZFRNr8URERGQwViSIiIgkICCD2sjpn8LI48sDEwkiIiIJSN21IRXzi4iIiIgqDVYkiIiIJKDvY8DLOoe5YSJBREQkAZUJnv5p7PHlwfwiIiIiokqDFQkiIiIJsGuDiIiIDKaGFdRGdgQYe3x5ML+IiIiIqNJgRYKIiEgCKiGDysiuCWOPLw9MJIiIiCTAMRJERERkMGGCp38KrmxJREREloQVCSIiIgmoIIPKyIduGXt8eWAiQUREJAG1MH6Mg1qYKBgTYtcGERERGYwVCSIiIgmoTTDY0tjjywMTCSIiIgmoIYPayDEOxh5fHswvtSEiIqJKgxUJIiIiCXBlSyIiIjKYpY6RML+IiIiIqNJgRYKIiEgCapjgWRtmONiSiQQREZEEhAlmbQgmEkRERM8mS336J8dIEBERkcFYkSAiIpKApc7aYCJBREQkAXZtEBERET2GFQkiIiIJWOqzNphIEBERSYBdG0RERESPYUWCiIhIApZakWAiQUREJAFLTSTYtUFEREQGY0WCiIhIApZakWAiQUREJAEB46dvCtOEYlJMJIiIiCRgqRUJjpEgIiIig7EiQUREJAFLrUgwkSAiIpKApSYS7NogIiKyQDExMWjRogWcnZ1Ro0YN9OjRAykpKVptwsPDIZPJtLYhQ4bodR0mEkRERBIorkgYu+kqMTERQ4cOxYEDBxAfH4+ioiJ07twZeXl5Wu0GDx6Ma9euabbZs2frdV/s2iAiIpKAEDIII7sm9Dl+27ZtWq+XL1+OGjVq4NChQ2jXrp1mv4ODA9zd3Q2OiRUJIiKiSiY7O1trKygoeOoxWVlZAABXV1et/atXr0b16tXRqFEjTJgwAffu3dMrFlYkiIiIJKCGzOgFqYqP9/b21to/ZcoUTJ06tezj1GqMHDkSrVu3RqNGjTT733zzTfj4+MDT0xPHjh3D+PHjkZKSgl9//VXnmJhIEBERScCUszYyMjKgUCg0++Vy+ROPGzp0KE6cOIG9e/dq7X/33Xc1f27cuDE8PDzQoUMHpKWloV69ejrFxESCiIioklEoFFqJxJMMGzYMcXFx2L17N7y8vJ7YtlWrVgCA1NRUJhJERETmROrBlkIIDB8+HOvXr0dCQgJ8fX2fekxycjIAwMPDQ+frMJEgIiKSgNQLUg0dOhRr1qzBxo0b4ezsjOvXrwMAlEol7O3tkZaWhjVr1uDFF19EtWrVcOzYMURHR6Ndu3YIDg7W+TpMJIiIiCQgdUUiNjYWwMNFpx61bNkyREVFwdbWFjt27MD8+fORl5cHb29v9OrVCxMnTtQrJiYSREREFkiIJz903NvbG4mJiUZfh4mEibxavzFsZFUqOgwqZ1fGP1/RIZCEbLOe/A8xVX6qwnxg6UZJriVM0LVhbEWjPDCRICIikoAA8JQigU7nMDdc2ZKIiIgMxooEERGRBNSQQWailS3NCRMJIiIiCUg9a0Mq7NogIiIig7EiQUREJAG1kEEm4YJUUmEiQUREJAEhTDBrwwynbbBrg4iIiAzGigQREZEELHWwJRMJIiIiCTCRICIiIoNZ6mBLjpEgIiIig7EiQUREJAFLnbXBRIKIiEgCDxMJY8dImCgYE2LXBhERERmMFQkiIiIJcNYGERERGUz8bzP2HOaGXRtERERkMFYkiIiIJMCuDSIiIjKchfZtMJEgIiKSggkqEjDDigTHSBAREZHBWJEgIiKSAFe2JCIiIoNZ6mBLdm0QERGRwViRICIikoKQGT9Y0gwrEkwkiIiIJGCpYyTYtUFEREQGY0WCiIhICs/yglS//fabzid85ZVXDA6GiIjIUlnqrA2dEokePXrodDKZTAaVSmVMPERERFSJ6JRIqNXq8o6DiIjI8plh14SxjBojkZ+fDzs7O1PFQkREZLEstWtD71kbKpUKn376KWrVqgUnJyecP38eADBp0iQsXbrU5AESERFZBGGizczonUjMmDEDy5cvx+zZs2Fra6vZ36hRI3z77bcmDY6IiIjMm96JxMqVK/Gf//wH/fr1g7W1tWZ/SEgIzpw5Y9LgiIiILIfMRJt50XuMxJUrV+Dn51div1qtRlFRkUmCIiIisjgWuo6E3hWJoKAg7Nmzp8T+devWoWnTpiYJioiIiCoHvSsSkydPRmRkJK5cuQK1Wo1ff/0VKSkpWLlyJeLi4sojRiIiosqPFYmHunfvjk2bNmHHjh1wdHTE5MmTcfr0aWzatAmdOnUqjxiJiIgqv+Knfxq7mRmD1pFo27Yt4uPjTR0LERERVTIGP/3z77//xqpVq7Bq1SocOnTIlDERERFZnOLHiBu76SomJgYtWrSAs7MzatSogR49eiAlJUWrTX5+PoYOHYpq1arByckJvXr1wo0bN/S6L70TicuXL6Nt27Zo2bIlRowYgREjRqBFixZo06YNLl++rO/piIiIng0SL0iVmJiIoUOH4sCBA4iPj0dRURE6d+6MvLw8TZvo6Ghs2rQJP//8MxITE3H16lX07NlTr9vSO5EYNGgQioqKcPr0ady5cwd37tzB6dOnoVarMWjQIH1PR0REROVg27ZtiIqKQsOGDRESEoLly5fj0qVLml6ErKwsLF26FHPnzsULL7yA5s2bY9myZUhKSsKBAwd0vo7eYyQSExORlJSEgIAAzb6AgAB89dVXaNu2rb6nIyIiejaYYrDk/47Pzs7W2i2XyyGXy594aFZWFgDA1dUVAHDo0CEUFRWhY8eOmjYNGjRA7dq1sX//fvzf//2fTiHpXZHw9vYudeEplUoFT09PfU9HRET0TJAJ02zAw+9ipVKp2WJiYp54bbVajZEjR6J169Zo1KgRAOD69euwtbWFi4uLVtuaNWvi+vXrOt+X3hWJOXPmYPjw4Vi0aBGee+45AA8HXo4YMQJffPGFvqcjIiJ6NphwHYmMjAwoFArN7qdVI4YOHYoTJ05g7969RgZQkk6JRNWqVSGT/VuOycvLQ6tWrWBj8/DwBw8ewMbGBu+88w569Ohh8iCJiIjoXwqFQiuReJJhw4YhLi4Ou3fvhpeXl2a/u7s7CgsLkZmZqVWVuHHjBtzd3XWORadEYv78+TqfkIiIiEphwjESOjUVAsOHD8f69euRkJAAX19frfebN2+OKlWq4I8//kCvXr0AACkpKbh06RJCQ0N1vo5OiURkZKTOJyQiIqJSSLxE9tChQ7FmzRps3LgRzs7OmnEPSqUS9vb2UCqVGDhwIEaNGgVXV1coFAoMHz4coaGhOg+0BAxc2bJYfn4+CgsLtfbpWmohIiKi8hMbGwsACA8P19q/bNkyREVFAQDmzZsHKysr9OrVCwUFBYiIiMA333yj13X0TiTy8vIwfvx4rF27Frdv3y7xvkql0veURERElk/iioTQYRlMOzs7LFq0CIsWLTI4JL2nf44bNw47d+5EbGws5HI5vv32W0ybNg2enp5YuXKlwYEQERFZNIlXtpSK3hWJTZs2YeXKlQgPD8eAAQPQtm1b+Pn5wcfHB6tXr0a/fv3KI04iIiIyQ3pXJO7cuYO6desCeDge4s6dOwCANm3aYPfu3aaNjoiIyFLwMeIP1a1bF+np6ahduzYaNGiAtWvXomXLlti0aVOJ1bHIsjRqlYvXP7gF/8b3UM39Aaa+Uwf7tykrOiwygeaeVzGgeTKCatxCDad7+HBTF+w8/+9Usc867USPIO2nBu694I0hG1+WOlQyUrPaV/H280cR6HELbs73MOqnCCSk/PtZvxf2Fzo3TIO7IhdFKiucvuaGRbta4sSVmhUYtWV4dGVKY85hbvSuSAwYMABHjx4FAHz00UdYtGgR7OzsEB0djbFjx5o0OJlMhg0bNgAALly4AJlMhuTkZJNeg3Rn56DG+ZN2+Ppjr6c3pkrFvkoRUv6phhkJZT8vZ88Fb4T9N1KzjdvWScIIyVTsbB/g7I1qmLWl9M/64m0XfL61DXov7o13lvfA1UxnLOq3GS4O9yWOlCoLvSsS0dHRmj937NgRZ86cwaFDh+Dn54fg4GC9zhUVFYUVK1aU2B8REYFt27bpGxqVs793KfD3Lk7vtUR7L/pg70WfJ7YpVFnj9j0HiSKi8pKUWhtJqbXLfH/bCX+t13N/fx6vNjuD+jVv4890/hJhFIlnbUjFqHUkAMDHxwc+Pk/+B+hJunTpgmXLlmnte9qa4UQkvRZeV5E4eBmyC+T4M6MWFu5vhax8u4oOi8qRjZUKPZufQk6+Lc5er1bR4ZCZ0imRWLhwoc4n/PDDD/UKQC6X67Wm95kzZ/DBBx/g8OHD8PPzw6JFixAWFqZ5PzExEWPHjsXRo0fh6uqKyMhIfPbZZ7CxsUFcXBz69++P27dvw9raGsnJyWjatCnGjx+PWbNmAQAGDRqE/Px8fP/993rdB5El23fRGztSfXElWwFvZTZGPH8Qi7tvRr+1r0It9O4hJTPX1v8iYnrFw67KA/yT44D3v38ZmfftKzqsSk8GE4yRMEkkpqVTIjFv3jydTiaTyfROJPQ1duxYzJ8/H0FBQZg7dy66deuG9PR0VKtWDVeuXMGLL76IqKgorFy5EmfOnMHgwYNhZ2eHqVOnom3btsjJycGRI0fw3HPPITExEdWrV0dCQoLm/ImJiRg/fnyZ1y8oKEBBQYHm9ePPhCeyRFvP/lvuPne7Gs7+Uw3bBqxGC6+rOJjBcrel+euCJ/oueR0uDvl4tdlpfN4rHm8v7Ym795hMUEk6/SqRnp6u03b+/Hm9A4iLi4OTk5PWNnPmzDLbDxs2DL169UJgYCBiY2OhVCqxdOlSAMA333wDb29vfP3112jQoAF69OiBadOm4csvv4RarYZSqUSTJk00iUNCQgKio6Nx5MgR5Obm4sqVK0hNTdWqcDwuJiZG6xnw3t7eet8zUWV3OVuBO/fsUFuZVdGhUDnIL6qCjLtKHL9SE9M3hUOllqFH09MVHVblZ6HTPyu8Jtm+fXskJydrbUOGDCmz/aNPJLOxscFzzz2H06cf/gU/ffo0QkNDtR553rp1a+Tm5uLy5csAgLCwMCQkJEAIgT179qBnz54IDAzE3r17kZiYCE9PT/j7aw82etSECROQlZWl2TIyMoz9ERBVOjWdcuFin49beRx8+SyQyQBbGz7+wGhc2bJ8ODo6ws/PT7LrhYeH47vvvsPRo0dRpUoVNGjQAOHh4UhISMDdu3efWI0AHo7peFYHg9o5qODp++9D2ty9C1G34X3kZFrj1hXbCoyMjGVfpUirulBLmY2A6v8gq0COrHw7fNDqL8Sn1sU/eQ7wdsnGqNb7cSlTiX2Xyh79T+bJvkoRvF0f+axdslG/5j/Ivi9H5n07DGp7GIkpdfBPrgNcHPLR+7kTqKHIQ/ypehUYNZmzCk8k9HXgwAG0a9cOAPDgwQMcOnQIw4YNAwAEBgbil19+gRBCU5XYt28fnJ2d4eX1sB+3eJzEvHnzNElDeHg4Zs2ahbt372L06NEVcFeVQ/2Q+5jzS5rm9ZBpVwEAv/9UFV9G8wulMmtU4yaWvfab5vX4dkkAgA2nAvDpznaoX/0OXglMgUJeiJt5jki66IWvD7REkcq6okImAwV53sR/IzdpXo+O2A8A+C25PmZuboc61TLx8uvb4eKQj6z7djh5tQYGLu+O87dcKypky8Hpn+WjoKBA84z0YjY2NqhevXqp7RctWgR/f38EBgZi3rx5uHv3Lt555x0AwAcffID58+dj+PDhGDZsGFJSUjBlyhSMGjUKVlYPe3GqVq2K4OBgrF69Gl9//TUAoF27dujduzeKioqeWpF4lh3b74QIz5CKDoPKwV9XaqHRgvfLfP+9DVzB0lIculgLzaaX3X085ucICaN5tljqypYVnkhs27YNHh4eWvsCAgJw5syZUtvPmjULs2bNQnJyMvz8/PDbb79pko5atWphy5YtGDt2LEJCQuDq6oqBAwdi4sSJWucICwtDcnKy5hntrq6uCAoKwo0bNxAQEGD6myQiIrJQMqHLA8sfs2fPHixZsgRpaWlYt24datWqhVWrVsHX1xdt2rQpjzjNVnZ2NpRKJcLRHTayKhUdDpWzK+Ofr+gQSEK2WWb46x+ZlKowHyeWfoKsrCwoFOWzcm/x90Sdz2bAys64RdzU+fm4MLF849WX3rM2fvnlF0RERMDe3h5HjhzRrKmQlZX1xGmbREREzzQLnbWhdyLx2WefYfHixfjvf/+LKlX+/Q28devWOHz4sEmDIyIiIvOm9xiJlJQUzayJRymVSmRmZpoiJiIiIotjqYMt9a5IuLu7IzU1tcT+vXv3om7duiYJioiIyOJwZcuHBg8ejBEjRuDgwYOQyWS4evUqVq9ejTFjxuD998uePkZERPRMs9AxEnp3bXz00UdQq9Xo0KED7t27h3bt2kEul2PMmDEYPnx4ecRIREREZkrvREImk+GTTz7B2LFjkZqaitzcXAQFBcHJyak84iMiIrIIljpGwuAFqWxtbREUFGTKWIiIiCwXl8h+qH379lpP13zczp07jQqIiIiIKg+9E4kmTZpovS4qKkJycjJOnDiByMhIU8VFRERkWUzQtWERFYl58+aVun/q1KnIzc01OiAiIiKLZKFdG3pP/yxL//798d1335nqdERERFQJmOzpn/v374edkQ8jISIislgWWpHQO5Ho2bOn1mshBK5du4a///4bkyZNMllgREREloTTP/9HqVRqvbayskJAQACmT5+Ozp07mywwIiIiMn96JRIqlQoDBgxA48aNUbVq1fKKiYiIiCoJvQZbWltbo3PnznzKJxERkb4s9Fkbes/aaNSoEc6fP18esRAREVms4jESxm7mRu9E4rPPPsOYMWMQFxeHa9euITs7W2sjIiKiZ4fOYySmT5+O0aNH48UXXwQAvPLKK1pLZQshIJPJoFKpTB8lERGRJTDDioKxdE4kpk2bhiFDhmDXrl3lGQ8REZFletbXkRDiYfRhYWHlFgwRERFVLnpN/3zSUz+JiIiobFyQCkD9+vWfmkzcuXPHqICIiIgs0rPetQE8HCfx+MqWRERE9OzSK5Ho06cPatSoUV6xEBERWSxL7drQeR0Jjo8gIiIyQgWsbLl7925069YNnp6ekMlk2LBhg9b7UVFRkMlkWluXLl30uobOiUTxrA0iIiKqHPLy8hASEoJFixaV2aZLly64du2aZvvhhx/0uobOXRtqtVqvExMREdEjKmCwZdeuXdG1a9cntpHL5XB3dzc4JL2XyCYiIiL9mfJZG48/nqKgoMDguBISElCjRg0EBATg/fffx+3bt/U6nokEERGRFEw4RsLb2xtKpVKzxcTEGBRSly5dsHLlSvzxxx/4/PPPkZiYiK5du+r1uAu9Zm0QERFRxcvIyIBCodC8lsvlBp2nT58+mj83btwYwcHBqFevHhISEtChQwedzsGKBBERkRRMWJFQKBRam6GJxOPq1q2L6tWrIzU1VedjWJEgIiKSQGVYR+Ly5cu4ffs2PDw8dD6GiQQREZGFys3N1aoupKenIzk5Ga6urnB1dcW0adPQq1cvuLu7Iy0tDePGjYOfnx8iIiJ0vgYTCSIiIilUwPTPv//+G+3bt9e8HjVqFAAgMjISsbGxOHbsGFasWIHMzEx4enqic+fO+PTTT/XqKmEiQUREJIGK6NoIDw9/4oKS27dvNy4gcLAlERERGYEVCSIiIinwMeJERERkMAtNJNi1QURERAZjRYKIiEgCsv9txp7D3DCRICIikoKFdm0wkSAiIpJAZVjZ0hAcI0FEREQGY0WCiIhICuzaICIiIqOYYSJgLHZtEBERkcFYkSAiIpKApQ62ZCJBREQkBQsdI8GuDSIiIjIYKxJEREQSYNcGERERGY5dG0RERETaWJEg0oN1QUVHQFIqVJrjI5LIlFQF0n3G7NogIiIiw1lo1wYTCSIiIilYaCLBMRJERERkMFYkiIiIJMAxEkRERGQ4dm0QERERaWNFgoiISAIyISATxpUUjD2+PDCRICIikgK7NoiIiIi0sSJBREQkAc7aICIiIsOxa4OIiIhIGysSREREEmDXBhERERnOQrs2mEgQERFJwFIrEhwjQURERAZjRYKIiEgK7NogIiIiY5hj14Sx2LVBREREBmNFgoiISApCPNyMPYeZYSJBREQkAc7aICIiInoMKxJERERSsNBZG6xIEBERSUCmNs2mj927d6Nbt27w9PSETCbDhg0btN4XQmDy5Mnw8PCAvb09OnbsiHPnzul1DSYSREREFiovLw8hISFYtGhRqe/Pnj0bCxcuxOLFi3Hw4EE4OjoiIiIC+fn5Ol+DXRtERERSqICuja5du6Jr166ln0oIzJ8/HxMnTkT37t0BACtXrkTNmjWxYcMG9OnTR6drsCJBREQkgeJZG8ZuppKeno7r16+jY8eOmn1KpRKtWrXC/v37dT4PKxJERERSMOE6EtnZ2Vq75XI55HK5Xqe6fv06AKBmzZpa+2vWrKl5TxesSBAREVUy3t7eUCqVmi0mJqbCYmFFgoiISAKmXJAqIyMDCoVCs1/fagQAuLu7AwBu3LgBDw8Pzf4bN26gSZMmOp+HFQkiIiIpCBNtABQKhdZmSCLh6+sLd3d3/PHHH5p92dnZOHjwIEJDQ3U+DysSREREFio3Nxepqama1+np6UhOToarqytq166NkSNH4rPPPoO/vz98fX0xadIkeHp6okePHjpfg4kEERGRBCriWRt///032rdvr3k9atQoAEBkZCSWL1+OcePGIS8vD++++y4yMzPRpk0bbNu2DXZ2djpfg4kEERGRFCrg6Z/h4eEQTzhGJpNh+vTpmD59usEhcYwEERERGYwVCSIiIglY6mPEmUgQERFJgU//JCIiItLGigQREZEE2LVBREREhlOLh5ux5zAzTCSIiIikwDESRERERNpYkSAiIpKADCYYI2GSSEyLiQQREZEUKmBlSymwa4OIiIgMxooEERGRBDj9k4iIiAzHWRtERERE2liRICIikoBMCMiMHCxp7PHlgYkEERGRFNT/24w9h5lh1wYREREZjBUJIiIiCbBrg4iIiAxnobM2mEgQERFJgStbEhEREWljRYJ01qhVLl7/4Bb8G99DNfcHmPpOHezfpqzosMgEmnldRVTLZAS630INp3sY+WsX7Er1LbXtxM6JeL3JKcz+43msPhQicaRkrOaeVzGgeTKCajz8rD/c1AU7z//7WX/WaSd6BKVoHbP3gjeGbHxZ6lAtDle2fEbIZDKsX78ePXr0qOhQzI6dgxrnT9ph+w+umPLdhYoOh0zIvkoRUm5Ww4bjDTDv1e1ltnvB/zwae9zAzRxHCaMjU7KvUoSUf6ph/akGWPBy6Z/1ngvemBj/guZ1kcpaqvAsm4V2bZhdIhEVFYXMzExs2LBBa39CQgLat2+Pu3fvwsXFpUJie9b9vUuBv3cpKjoMKgf70n2wL93niW1qOOXio4578f7al/HVa1skioxMbe9FH+y9+OTPulBljdv3HCSKiCo7s0skiMj8yCAw46U/sPzPJki77VrR4VA5a+F1FYmDlyG7QI4/M2ph4f5WyMq3q+iwKj2Z+uFm7DnMTaUcbHn79m307dsXtWrVgoODAxo3bowffvhBq014eDg+/PBDjBs3Dq6urnB3d8fUqVO12pw7dw7t2rWDnZ0dgoKCEB8fL+FdEFUeA1odgUpthTWHGld0KFTO9l30xsfbX8CgX1/BvL2heK7WNSzuvhlW5vgNVtkUd20Yu5mZSlmRyM/PR/PmzTF+/HgoFAps3rwZb731FurVq4eWLVtq2q1YsQKjRo3CwYMHsX//fkRFRaF169bo1KkT1Go1evbsiZo1a+LgwYPIysrCyJEjn3rtgoICFBQUaF5nZ2eXxy0SmY3AmrfQr/kx9Fn5OgBZRYdD5WzrWX/Nn8/droaz/1TDtgGr0cLrKg5meFVgZGSuzDKRiIuLg5OTk9Y+lUql+XOtWrUwZswYzevhw4dj+/btWLt2rVYiERwcjClTpgAA/P398fXXX+OPP/5Ap06dsGPHDpw5cwbbt2+Hp6cnAGDmzJno2rXrE2OLiYnBtGnTjL5HosqimddVuDrex7YhqzT7bKwERrffj37PHceLS/pXYHRU3i5nK3Dnnh1qK7OYSBiLC1JJp3379oiNjdXad/DgQfTv//AfLJVKhZkzZ2Lt2rW4cuUKCgsLUVBQAAcH7cFBwcHBWq89PDxw8+ZNAMDp06fh7e2tSSIAIDQ09KmxTZgwAaNGjdK8zs7Ohre3t343SFSJxJ0MwMGL2l8gsa9vRtzJ+thwIqCCoiKp1HTKhYt9Pm7lcfClsbhEtoQcHR3h5+ente/y5cuaP8+ZMwcLFizA/Pnz0bhxYzg6OmLkyJEoLCzUOqZKlSpar2UyGdRq4/r55HI55HK5UeeorOwcVPD0/fdn7O5diLoN7yMn0xq3rthWYGRkLPsqRahdNUvzupZLNgJq/IOs+3Jcz3EuMdCuSG2Ff/LscfFOValDJSPZVylCbeUjn7UyGwHV/0FWgRxZ+Xb4oNVfiE+ti3/yHODtko1RrffjUqYS+y7VrsCoyZyZZSLxNPv27UP37t01FQq1Wo2zZ88iKChI53MEBgYiIyMD165dg4eHBwDgwIED5RKvpagfch9zfknTvB4y7SoA4PefquLLaP4jU5k1dL+JpX1/07we+0ISAGDj8QBM3vpCWYdRJdSoxk0se+3fz3p8u4ef9YZTAfh0ZzvUr34HrwSmQCEvxM08RyRd9MLXB1pyLQlT4DoS5sPf3x/r1q1DUlISqlatirlz5+LGjRt6JRIdO3ZE/fr1ERkZiTlz5iA7OxuffPJJOUZd+R3b74QIT65kaIn+zqiFkNnv69ye4yIqr7+u1EKjBWV/1u9t4AqW5UYAMHbyi/nlEZVz+ufEiRPRrFkzREREIDw8HO7u7nqvRGllZYX169fj/v37aNmyJQYNGoQZM2aUT8BERPTMKx4jYexmbsyuIrF8+fJS94eHh0M88gN8fOXLxyUkJJTY9/gx9evXx549e7T2CTP8kIiIiMyV2SUSREREFknABGMkTBKJSTGRICIikoKFDraslGMkiIiIyDywIkFERCQFNYxfZd4MH3nCRIKIiEgClrqyJbs2iIiIyGCsSBAREUnBQgdbMpEgIiKSgoUmEuzaICIiskBTp06FTCbT2ho0aGDy67AiQUREJIUKqEg0bNgQO3bs0Ly2sTH91z4TCSIiIilUwPRPGxsbuLu7G3nRJ2PXBhERkQRM+dCu7Oxsra2goKDUa547dw6enp6oW7cu+vXrh0uXLpn8vphIEBERVTLe3t5QKpWaLSYmpkSbVq1aYfny5di2bRtiY2ORnp6Otm3bIicnx6SxsGuDiIhICiYcI5GRkQGFQqHZLZfLSzTt2rWr5s/BwcFo1aoVfHx8sHbtWgwcONC4OB7BRIKIiEgKagHIjEwk1A+PVygUWomELlxcXFC/fn2kpqYaF8Nj2LVBRET0DMjNzUVaWho8PDxMel4mEkRERFIo7towdtPRmDFjkJiYiAsXLiApKQmvvvoqrK2t0bdvX5PeFrs2iIiIJGGCMRLQ/fjLly+jb9++uH37Ntzc3NCmTRscOHAAbm5uRsagjYkEERGRBfrxxx8luQ4TCSIiIilY6LM2mEgQERFJQS2gT9dE2ecwLxxsSURERAZjRYKIiEgKQv1wM/YcZoaJBBERkRQ4RoKIiIgMxjESRERERNpYkSAiIpICuzaIiIjIYAImSCRMEolJsWuDiIiIDMaKBBERkRTYtUFEREQGU6sBGLkOhNr81pFg1wYREREZjBUJIiIiKbBrg4iIiAxmoYkEuzaIiIjIYKxIEBERScFCl8hmIkFERCQBIdQQRj6909jjywMTCSIiIikIYXxFgWMkiIiIyJKwIkFERCQFYYIxEmZYkWAiQUREJAW1GpAZOcbBDMdIsGuDiIiIDMaKBBERkRTYtUFERESGEmo1hJFdG+Y4/ZNdG0RERGQwViSIiIikwK4NIiIiMphaADLLSyTYtUFEREQGY0WCiIhICkIAMHYdCfOrSDCRICIikoBQCwgjuzYEEwkiIqJnlFDD+IoEp38SERGRBWFFgoiISALs2iAiIiLDWWjXBhMJIxVnhw9QZPQ6I2T+VAX5FR0CSUhV0QFQuSv+f1qK3/RN8T3xAEWmCcaEZMIc6ySVyOXLl+Ht7V3RYRARkREyMjLg5eVVLufOz8+Hr68vrl+/bpLzubu7Iz09HXZ2diY5n7GYSBhJrVbj6tWrcHZ2hkwmq+hwJJGdnQ1vb29kZGRAoVBUdDhUjvhZP1uexc9bCIGcnBx4enrCyqr85h/k5+ejsLDQJOeytbU1myQCYNeG0aysrMotizV3CoXimfnH5lnHz/rZ8qx93kqlstyvYWdnZ1Zf/qbE6Z9ERERkMCYSREREZDAmEqQ3uVyOKVOmQC6XV3QoVM74WT9b+HmTITjYkoiIiAzGigQREREZjIkEERERGYyJBBERERmMiQTRM0gmk2HDhg0AgAsXLkAmkyE5OblCY6LK79G/V/TsYCJBiIqKgkwmg0wmQ5UqVeDr64tx48YhP9/450rwS6piPPqZPrp16dKlokOjchQVFYUePXqU2J+QkACZTIbMzEzJYyLLx5UtCQDQpUsXLFu2DEVFRTh06BAiIyMhk8nw+eefV3RoZKDiz/RRnNZHRKbGigQBePgF4+7uDm9vb/To0QMdO3ZEfHw8gIfPE4mJiYGvry/s7e0REhKCdevWaY69e/cu+vXrBzc3N9jb28Pf31/zBebr6wsAaNq0KWQyGcLDwyW/t2dV8Wf66Fa1atUy2585cwbPP/887Ozs0KhRIyQmJmq9n5iYiJYtW0Iul8PDwwMfffQRHjx4AACIi4uDi4sLVKqHz8tMTk6GTCbDRx99pDl+0KBB6N+/fzncKenj9u3b6Nu3L2rVqgUHBwc0btwYP/zwg1ab8PBwfPjhhxg3bhxcXV3h7u6OqVOnarU5d+4c2rVrBzs7OwQFBWn+vaBnDxMJKuHEiRNISkqCra0tACAmJgYrV67E4sWLcfLkSURHR6N///6aL5pJkybh1KlT2Lp1K06fPo3Y2FhUr14dAPDnn38CAHbs2IFr167h119/rZiboqcaO3YsRo8ejSNHjiA0NBTdunXD7du3AQBXrlzBiy++iBYtWuDo0aOIjY3F0qVL8dlnnwEA2rZti5ycHBw5cgTAw6SjevXqSEhI0Jw/MTGRiaQZyM/PR/PmzbF582acOHEC7777Lt566y3N/6vFVqxYAUdHRxw8eBCzZ8/G9OnTtX656NmzJ2xtbXHw4EEsXrwY48ePr4jbIXMg6JkXGRkprK2thaOjo5DL5QKAsLKyEuvWrRP5+fnCwcFBJCUlaR0zcOBA0bdvXyGEEN26dRMDBgwo9dzp6ekCgDhy5Eh53wY94tHP9NFtxowZQgghAIj169cLIf79jGbNmqU5vqioSHh5eYnPP/9cCCHExx9/LAICAoRarda0WbRokXBychIqlUoIIUSzZs3EnDlzhBBC9OjRQ8yYMUPY2tqKnJwccfnyZQFAnD17Vorbf2aV9bnb2dkJAOLu3bulHvfSSy+J0aNHa16HhYWJNm3aaLVp0aKFGD9+vBBCiO3btwsbGxtx5coVzftbt27V+ntFzw6OkSAAQPv27REbG4u8vDzMmzcPNjY26NWrF06ePIl79+6hU6dOWu0LCwvRtGlTAMD777+PXr164fDhw+jcuTN69OiB559/viJugx5R/Jk+ytXVtcz2oaGhmj/b2Njgueeew+nTpwEAp0+fRmhoKGQymaZN69atkZubi8uXL6N27doICwtDQkICRo8ejT179iAmJgZr167F3r17cefOHXh6esLf39/Ed0mPK+1zP3jwoKZbSaVSYebMmVi7di2uXLmCwsJCFBQUwMHBQeuY4OBgrdceHh64efMmgId/H7y9veHp6al5/9G/P/RsYSJBAABHR0f4+fkBAL777juEhIRg6dKlaNSoEQBg8+bNqFWrltYxxQP3unbtiosXL2LLli2Ij49Hhw4dMHToUHzxxRfS3gRpefQzlUJ4eDi+++47HD16FFWqVEGDBg0QHh6OhIQE3L17F2FhYZLF8iwr7XO/fPmy5s9z5szBggULMH/+fDRu3BiOjo4YOXIkCgsLtY6pUqWK1muZTAa1Wl1+gVOlxTESVIKVlRU+/vhjTJw4EUFBQZDL5bh06RL8/Py0Nm9vb80xbm5uiIyMxPfff4/58+fjP//5DwBoxlkUD8Ij83XgwAHNnx88eIBDhw4hMDAQABAYGIj9+/dDPPJonn379sHZ2RleXl4A/h0nMW/ePE3SUJxIJCQkcHyEmdi3bx+6d++O/v37IyQkBHXr1sXZs2f1OkdgYCAyMjJw7do1zb5H//7Qs4WJBJXq9ddfh7W1NZYsWYIxY8YgOjoaK1asQFpaGg4fPoyvvvoKK1asAABMnjwZGzduRGpqKk6ePIm4uDjNF1CNGjVgb2+Pbdu24caNG8jKyqrI23qmFBQU4Pr161rbP//8U2b7RYsWYf369Thz5gyGDh2Ku3fv4p133gEAfPDBB8jIyMDw4cNx5swZbNy4EVOmTMGoUaNgZfXwn5GqVasiODgYq1ev1iQN7dq1w+HDh3H27FlWJMyEv78/4uPjkZSUhNOnT+O9997DjRs39DpHx44dUb9+fURGRuLo0aPYs2cPPvnkk3KKmMwdEwkqlY2NDYYNG4bZs2djwoQJmDRpEmJiYhAYGIguXbpg8+bNmqmdtra2mDBhAoKDg9GuXTtYW1vjxx9/1Jxn4cKFWLJkCTw9PdG9e/eKvK1nyrZt2+Dh4aG1tWnTpsz2s2bNwqxZsxASEoK9e/fit99+08y+qVWrFrZs2YI///wTISEhGDJkCAYOHIiJEydqnSMsLAwqlUqTSLi6uiIoKAju7u4ICAgot3sl3U2cOBHNmjVDREQEwsPD4e7uXuoiVk9iZWWF9evX4/79+2jZsiUGDRqEGTNmlE/AZPb4GHEiIiIyGCsSREREZDAmEkRERGQwJhJERERkMCYSREREZDAmEkRERGQwJhJERERkMCYSREREZDAmEkQWICoqSmtRofDwcIwcOVLyOBISEiCTyZCZmVlmG5lMhg0bNuh8zqlTp6JJkyZGxXXhwgXIZDIkJycbdR4iKomJBFE5iYqKgkwmg0wmg62tLfz8/DB9+nQ8ePCg3K/966+/4tNPP9WprS5f/kREZeHTP4nKUZcuXbBs2TIUFBRgy5YtGDp0KKpUqYIJEyaUaFtYWKh5yJmxnvS4cCIiU2JFgqgcyeVyuLu7w8fHB++//z46duyI3377DcC/3REzZsyAp6en5lkUGRkZ6N27N1xcXODq6oru3bvjwoULmnOqVCqMGjUKLi4uqFatGsaNG4fHV7p/vGujoKAA48ePh7e3N+RyOfz8/LB06VJcuHAB7du3B/DwoVsymQxRUVEAALVajZiYGPj6+sLe3h4hISFYt26d1nW2bNmC+vXrw97eHu3bt9eKU1fjx49H/fr14eDggLp162LSpEkoKioq0W7JkiXw9vaGg4MDevfuXeIBcN9++y0CAwNhZ2eHBg0a4JtvvtE7FiLSHxMJIgnZ29ujsLBQ8/qPP/5ASkoK4uPjERcXh6KiIkRERMDZ2Rl79uzBvn374OTkhC5dumiO+/LLL7F8+XJ899132Lt3L+7cuYP169c/8bpvv/02fvjhByxcuBCnT5/GkiVL4OTkBG9vb/zyyy8AgJSUFFy7dg0LFiwAAMTExGDlypVYvHgxTp48iejoaPTv3x+JiYkAHiY8PXv2RLdu3ZCcnIxBgwbho48+0vtn4uzsjOXLl+PUqVNYsGAB/vvf/2LevHlabVJTU7F27Vps2rQJ27Ztw5EjR/DBBx9o3l+9ejUmT56MGTNm4PTp05g5cyYmTZqkeUItEZUjQUTlIjIyUnTv3l0IIYRarRbx8fFCLpeLMWPGaN6vWbOmKCgo0ByzatUqERAQINRqtWZfQUGBsLe3F9u3bxdCCOHh4SFmz56teb+oqEh4eXlpriWEEGFhYWLEiBFCCCFSUlIEABEfH19qnLt27RIAxN27dzX78vPzhYODg0hKStJqO3DgQNG3b18hhBATJkwQQUFBWu+PHz++xLkeB0CsX7++zPfnzJkjmjdvrnk9ZcoUYW1tLS5fvqzZt3XrVmFlZSWuXbsmhBCiXr16Ys2aNVrn+fTTT0VoaKgQQoj09HQBQBw5cqTM6xKRYThGgqgcxcXFwcnJCUVFRVCr1XjzzTcxdepUzfuNGzfWGhdx9OhRpKamwtnZWes8+fn5SEtLQ1ZWFq5du4ZWrVpp3rOxscFzzz1XonujWHJyMqytrREWFqZz3Kmpqbh37x46deqktb+wsBBNmzYFAJw+fVorDgAIDQ3V+RrFfvrpJyxcuBBpaWnIzc3FgwcPoFAotNrUrl0btWrV0rqOWq1GSkoKnJ2dkZaWhoEDB2Lw4MGaNg8ePIBSqdQ7HiLSDxMJonLUvn17xMbGwtbWFp6enrCx0f5fztHRUet1bm4umjdvjtWrV5c4l5ubm0Ex2Nvb631Mbm4uAGDz5s1aX+DAw3EfprJ//37069cP06ZNQ0REBJRKJX788Ud8+eWXesf63//+t0RiY21tbbJYiah0TCSIypGjoyP8/Px0bt+sWTP89NNPqFGjRonfyot5eHjg4MGDaNeuHYCHv3kfOnQIzZo1K7V948aNoVarkZiYiI4dO5Z4v7giolKpNPuCgoIgl8tx6dKlMisZgYGBmoGjxQ4cOPD0m3xEUlISfHx88Mknn2j2Xbx4sUS7S5cu4erVq/D09NRcx8rKCgEBAahZsyY8PT1x/vx59OvXT6/rE5HxONiSyIz069cP1atXR/fu3bFnzx6kp6cjISEBH374IS5fvgwAGDFiBGbNmoUNGzbgzJkz+OCDD564BkSdOnUQGRmJd955Bxs2bNCcc+3atQAAHx8fyGQyxMXF4datW8jNzYWzszPGjBmD6OhorFixAmlpaTh8+DC++uorzQDGIUOG4Ny5cxg7dixSUlKwZs0aLF++XK/79ff3x6VLl/Djjz8iLS0NCxcuLHXgqJ2dHSIjI3H06FHs2bMHH374IXr37g13d3cAwLRp0xATE4OFCxfi7NmzOH78OJYtW4a5c+fqFQ8R6Y+JBJEZcXBwwO7du1G7dm307NkTgYGBGDhwIPLz8zUVitGjR+Ott95CZGQkQkND4ezsjFdfffWJ542NjcVrr72GDz74AA0aNMDgwYORl5cHAKhVqxamTZuGjz76CDVr1sSwYcMAAJ9++ikmTZqEmJgYBAYGokuXLti8eTN8fX0BPBy38Msvv2DDhg0ICQnB4sWLMXPmTL3u95VXXkF0dDSGDRuGJk2aICkpCZMmTSrRzs/PDz179sSLL76Izp07Izg4WGt656BBg/Dtt99i2bJlaNy4McLCwrB8+XJNrERUfmSirBFaRERERE/BigQREREZjIkEERERGYyJBBERERmMiQQREREZjIkEERERGYyJBBERERmMiQQREREZjIkEERERGYyJBBERERmMiQQREREZjIkEERERGYyJBBERERns/wFPEG0Yo4a2lQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Performance Summary ---\n",
            "| Model             | Overall Acc.    | Rest Acc.    | Elbow Acc.   | Hand Acc.    |\n",
            "|-------------------|-----------------|--------------|--------------|--------------|\n",
            "| LENet CNN         | 68.04%          | 94.74%       | 51.72%       | 50.00%       |\n",
            "| LENet SNN         | 68.04%          | 97.37%       | 51.72%       | 46.67%       |\n",
            "| LENet_FCL CNN     | 63.92%          | 78.95%       | 44.83%       | 63.33%       |\n",
            "| LENet_FCL SNN     | 64.95%          | 92.11%       | 62.07%       | 33.33%       |\n"
          ]
        }
      ],
      "source": [
        "# @title Evaluate models and visualize results\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "\n",
        "# Ensure all necessary variables from previous cells are available:\n",
        "# cnn_model_lenet, snn_model_lenet, cnn_model_lenet_fcl, snn_model_lenet_fcl\n",
        "# test_data, test_label (from the last train_test_split)\n",
        "# BATCH_SIZE, TIME_STEPS, device\n",
        "# data_loader function\n",
        "\n",
        "# 0. Prepare data loader for evaluation (using test_data from the last split)\n",
        "# This ensures all models are evaluated on the same test set.\n",
        "eval_test_loader = data_loader(test_data, test_label, batch=BATCH_SIZE, shuffle=False, drop=False)\n",
        "\n",
        "true_labels_list = []\n",
        "# Ensure targets are collected correctly from the loader\n",
        "for _, targets_batch in eval_test_loader:\n",
        "    true_labels_list.extend(targets_batch.cpu().numpy())\n",
        "true_labels_np = np.array(true_labels_list)\n",
        "\n",
        "# --- 1. Get predictions for LENet CNN ---\n",
        "print(\"Evaluating LENet CNN...\")\n",
        "cnn_model_lenet.eval().to(device)\n",
        "preds_lenet_cnn = []\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in eval_test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = cnn_model_lenet(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        preds_lenet_cnn.extend(predicted.cpu().numpy())\n",
        "\n",
        "# --- 2. Get predictions for LENet SNN ---\n",
        "print(\"Evaluating LENet SNN...\")\n",
        "snn_model_lenet.eval().to(device)\n",
        "preds_lenet_snn = []\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in eval_test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        for m_module in snn_model_lenet.modules():\n",
        "            if hasattr(m_module, 'reset'):\n",
        "                m_module.reset()\n",
        "        accumulated_outputs = None\n",
        "        for t in range(TIME_STEPS):\n",
        "            outputs_t = snn_model_lenet(inputs)\n",
        "            if accumulated_outputs is None:\n",
        "                accumulated_outputs = outputs_t.clone()\n",
        "            else:\n",
        "                accumulated_outputs += outputs_t\n",
        "        _, predicted = accumulated_outputs.max(1)\n",
        "        preds_lenet_snn.extend(predicted.cpu().numpy())\n",
        "\n",
        "# --- 3. Get predictions for LENet_FCL CNN ---\n",
        "print(\"Evaluating LENet_FCL CNN...\")\n",
        "cnn_model_lenet_fcl.eval().to(device)\n",
        "preds_lenet_fcl_cnn = []\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in eval_test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = cnn_model_lenet_fcl(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        preds_lenet_fcl_cnn.extend(predicted.cpu().numpy())\n",
        "\n",
        "# --- 4. Get predictions for LENet_FCL SNN ---\n",
        "print(\"Evaluating LENet_FCL SNN...\")\n",
        "snn_model_lenet_fcl.eval().to(device)\n",
        "preds_lenet_fcl_snn = []\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in eval_test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        for m_module in snn_model_lenet_fcl.modules():\n",
        "            if hasattr(m_module, 'reset'):\n",
        "                m_module.reset()\n",
        "        accumulated_outputs = None\n",
        "        for t in range(TIME_STEPS):\n",
        "            outputs_t = snn_model_lenet_fcl(inputs)\n",
        "            if accumulated_outputs is None:\n",
        "                accumulated_outputs = outputs_t.clone()\n",
        "            else:\n",
        "                accumulated_outputs += outputs_t\n",
        "        _, predicted = accumulated_outputs.max(1)\n",
        "        preds_lenet_fcl_snn.extend(predicted.cpu().numpy())\n",
        "\n",
        "# --- 5. Confusion Matrices ---\n",
        "class_names_display = ['Rest', 'Elbow', 'Hand'] # For display purposes\n",
        "\n",
        "# LENet CNN\n",
        "cm_lenet_cnn = confusion_matrix(true_labels_np, preds_lenet_cnn)\n",
        "disp_lenet_cnn = ConfusionMatrixDisplay(confusion_matrix=cm_lenet_cnn, display_labels=class_names_display)\n",
        "disp_lenet_cnn.plot()\n",
        "plt.title('Confusion Matrix - LENet CNN')\n",
        "plt.show()\n",
        "\n",
        "# LENet SNN\n",
        "cm_lenet_snn = confusion_matrix(true_labels_np, preds_lenet_snn)\n",
        "disp_lenet_snn = ConfusionMatrixDisplay(confusion_matrix=cm_lenet_snn, display_labels=class_names_display)\n",
        "disp_lenet_snn.plot()\n",
        "plt.title('Confusion Matrix - LENet SNN')\n",
        "plt.show()\n",
        "\n",
        "# LENet_FCL CNN\n",
        "cm_lenet_fcl_cnn = confusion_matrix(true_labels_np, preds_lenet_fcl_cnn)\n",
        "disp_lenet_fcl_cnn = ConfusionMatrixDisplay(confusion_matrix=cm_lenet_fcl_cnn, display_labels=class_names_display)\n",
        "disp_lenet_fcl_cnn.plot()\n",
        "plt.title('Confusion Matrix - LENet_FCL CNN')\n",
        "plt.show()\n",
        "\n",
        "# LENet_FCL SNN\n",
        "cm_lenet_fcl_snn = confusion_matrix(true_labels_np, preds_lenet_fcl_snn)\n",
        "disp_lenet_fcl_snn = ConfusionMatrixDisplay(confusion_matrix=cm_lenet_fcl_snn, display_labels=class_names_display)\n",
        "disp_lenet_fcl_snn.plot()\n",
        "plt.title('Confusion Matrix - LENet_FCL SNN')\n",
        "plt.show()\n",
        "\n",
        "# --- 6. Accuracy Table ---\n",
        "print(\"\\n--- Model Performance Summary ---\")\n",
        "\n",
        "model_predictions = {\n",
        "    \"LENet CNN\": preds_lenet_cnn,\n",
        "    \"LENet SNN\": preds_lenet_snn,\n",
        "    \"LENet_FCL CNN\": preds_lenet_fcl_cnn,\n",
        "    \"LENet_FCL SNN\": preds_lenet_fcl_snn\n",
        "}\n",
        "\n",
        "accuracy_results = {}\n",
        "class_names = ['Rest', 'Elbow', 'Hand'] # Corresponds to labels 0, 1, 2\n",
        "\n",
        "for model_name, predictions in model_predictions.items():\n",
        "    overall_acc = accuracy_score(true_labels_np, predictions)\n",
        "    accuracy_results[model_name] = {\"Overall\": overall_acc}\n",
        "    for class_idx, class_name in enumerate(class_names):\n",
        "        class_indices = np.where(true_labels_np == class_idx)[0]\n",
        "        if len(class_indices) > 0:\n",
        "            class_true = true_labels_np[class_indices]\n",
        "            class_pred = np.array(predictions)[class_indices]\n",
        "            class_acc = accuracy_score(class_true, class_pred)\n",
        "            accuracy_results[model_name][class_name] = class_acc\n",
        "        else:\n",
        "            accuracy_results[model_name][class_name] = np.nan # Or 0, or \"N/A\"\n",
        "\n",
        "# Print table\n",
        "header = f\"| {'Model':<17} | {'Overall Acc.':<15} | {class_names[0]+' Acc.':<12} | {class_names[1]+' Acc.':<12} | {class_names[2]+' Acc.':<12} |\"\n",
        "separator = \"|-------------------|-----------------|--------------|--------------|--------------|\"\n",
        "print(header)\n",
        "print(separator)\n",
        "\n",
        "for model_name in accuracy_results:\n",
        "    overall_str = f\"{accuracy_results[model_name]['Overall']*100:.2f}%\"\n",
        "    rest_str = f\"{accuracy_results[model_name][class_names[0]]*100:.2f}%\" if not np.isnan(accuracy_results[model_name][class_names[0]]) else \"N/A\"\n",
        "    elbow_str = f\"{accuracy_results[model_name][class_names[1]]*100:.2f}%\" if not np.isnan(accuracy_results[model_name][class_names[1]]) else \"N/A\"\n",
        "    hand_str = f\"{accuracy_results[model_name][class_names[2]]*100:.2f}%\" if not np.isnan(accuracy_results[model_name][class_names[2]]) else \"N/A\"\n",
        "\n",
        "    row = f\"| {model_name:<17} | {overall_str:<15} | {rest_str:<12} | {elbow_str:<12} | {hand_str:<12} |\"\n",
        "    print(row)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}