{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlonResearch/SNN-for-MI-EEG/blob/main/MI3_SNNforMIeeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xy9K5bJ9aB7D",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as da\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import scipy.io as scio\n",
        "#!pip install spikingjelly -q\n",
        "from spikingjelly.activation_based import ann2snn\n",
        "#!apt install -y graphviz graphviz-dev -q\n",
        "#!pip install nnviz -q\n",
        "from nnviz import drawing\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lAEV1BHaBp9",
        "outputId": "eba0b72a-be6b-4dbd-8613-acf20fa49cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wed May  7 11:22:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0             28W /   70W |    1372MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "CYNshZR5ZhDy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Data loader and other functions\n",
        "#Defining functions\n",
        "\n",
        "def data_loader(data, label, batch=64, shuffle=True, drop=False):\n",
        "    \"\"\"\n",
        "    Preprocess the data to fit model.\n",
        "    Feed data into data_loader.\n",
        "    input:\n",
        "        data (float): samples*length*ch (samples*ch*length).\n",
        "        label (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        batch (int): batch size\n",
        "        shuffle (bool): shuffle data before input into decoder\n",
        "        drop (bool): drop the last samples if True\n",
        "    output:\n",
        "        data loader\n",
        "    \"\"\"\n",
        "    label = torch.LongTensor(label.flatten()).to(device)\n",
        "    if data.shape[1] >= data.shape[2]:\n",
        "        data = torch.tensor(data.swapaxes(1, 2))\n",
        "    data = torch.unsqueeze(data, dim=1).type('torch.FloatTensor').to(device)\n",
        "    data = da.TensorDataset(data, label)\n",
        "    loader = da.DataLoader(dataset=data, batch_size=batch, shuffle=shuffle, drop_last=drop)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def val_snn(Dec, test_loader, T=None):\n",
        "    Dec.eval().to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if T is not None:\n",
        "        corrects = np.zeros(T)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            if T is None:\n",
        "                outputs = Dec(inputs)\n",
        "                correct += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            else:\n",
        "                for m in Dec.modules():\n",
        "                    if hasattr(m, 'reset'):\n",
        "                        m.reset()\n",
        "                for t in range(T):\n",
        "                    if t == 0:\n",
        "                        outputs = Dec(inputs)\n",
        "                    else:\n",
        "                        outputs += Dec(inputs)\n",
        "                    corrects[t] += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            total += targets.shape[0]\n",
        "    return correct / total if T is None else corrects / total\n",
        "\n",
        "\n",
        "def anntosnn(cnn_model, train_x, train_y, test_x, test_y, batch=64, T=None):\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    print('---------------------------------------------')\n",
        "    print('Converting using MaxNorm')\n",
        "    model_converter = ann2snn.Converter(mode='max', dataloader=train_loader)\n",
        "    snn_model = model_converter(cnn_model)\n",
        "    mode_max_accs = val_snn(snn_model, test_loader, T=T)\n",
        "\n",
        "    return mode_max_accs\n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def train_ann(cnn_model, train_x, train_y, test_x, test_y, ep=500, batch=64):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        train_x, test_x (float): samples*length*ch (samples*ch*length).\n",
        "        train_y, test_y (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        ep (int): total train and test epoch\n",
        "        batch (int): batch size\n",
        "    output:\n",
        "        train acc, test acc, weight_file\n",
        "    \"\"\"\n",
        "    # Define training configuration\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.01)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ep)\n",
        "\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "    for epoch in range(ep):\n",
        "        # Train ANN\n",
        "        cnn_model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        print('\\n')\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = cnn_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            print(batch_idx, len(train_loader), 'Epoch: %d | ANN: trainLoss: %.4f | trainAcc: %.4f%% (%d/%d)'\n",
        "                  % (epoch, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        train_acc.append(round(correct / total, 4))\n",
        "\n",
        "        # Test ANN\n",
        "        cnn_model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "                outputs = cnn_model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "                print(batch_idx, len(test_loader), 'Epoch: %d | ANN: testLoss: %.4f | testAcc: %.4f%% (%d/%d)'\n",
        "                      % (epoch, val_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        test_acc.append(round(correct / total, 4))\n",
        "\n",
        "    train_acc = np.asarray(train_acc[-1])\n",
        "    test_acc = np.asarray(test_acc[-1])\n",
        "    return train_acc, test_acc,cnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "EC8xjyPDaM6G",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Model definitions\n",
        "# Model 2a\n",
        "\n",
        "class LENet(nn.Module):\n",
        "    \"\"\"\n",
        "        LENet Model\n",
        "    input:\n",
        "         data shape as: batch_size*1*channel*length (64*1*22*1000) BCI IV-2a\n",
        "         batch_size：64\n",
        "         channel：22\n",
        "         length：1000\n",
        "    output:\n",
        "        classes_num\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classes_num=3, channel_count=22, drop_out = 0.5):\n",
        "        super(LENet, self).__init__()\n",
        "        self.drop_out = drop_out\n",
        "\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,64) #\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,32) #\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,16) #\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            # Temporal Convolution block fusion kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            # Spatial Convolution block kernel_size (channel,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(channel_count, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            # Feature Fusion Convolution block kernel_size (1,16) and (1,1) #\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),  #\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.CCB = nn.Sequential(\n",
        "            # Classification Convolution block kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=classes_num,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "        x = self.CCB(x)\n",
        "        return x\n",
        "\n",
        "class LENet_FCL(nn.Module):\n",
        "    def __init__(self, classes_num=3, channel_count=60, drop_out=0.5):\n",
        "        super(LENet_FCL, self).__init__()\n",
        "        self.drop_out = drop_out\n",
        "\n",
        "        # Keep all the convolutional layers the same\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(channel_count, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        # We'll determine the size of the FC layer in the forward pass\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = None\n",
        "        self.classes_num = classes_num\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Create the FC layer on first forward pass if it doesn't exist\n",
        "        if self.fc is None:\n",
        "            in_features = x.shape[1]\n",
        "            self.fc = nn.Linear(in_features, self.classes_num).to(x.device)\n",
        "            # Initialize weights for the new layer\n",
        "            nn.init.kaiming_normal_(self.fc.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            if self.fc.bias is not None:\n",
        "                nn.init.constant_(self.fc.bias, 0)\n",
        "\n",
        "        # Apply the FC layer\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T0hmMue2aMqA",
        "outputId": "73533a62-e426-4734-a693-2d7787f009d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channel count: 62\n",
            "Data shape: (965, 62, 360)\n",
            "Label shape: (965, 1)\n",
            "Class distribution: Rest: 365, Elbow: 300, Hand: 300\n",
            "Balanced class distribution: Rest: 365, Elbow: 300, Hand: 300\n",
            "Balanced data shape: (965, 62, 360)\n",
            "Balanced label shape: (965, 1)\n"
          ]
        }
      ],
      "source": [
        "# @title Loading the data\n",
        "\"\"\"\n",
        "Loading the data\n",
        "\"\"\"\n",
        "\n",
        "# Getting real samples\n",
        "#Locally load the dataset\n",
        "#file = scio.loadmat('Datasets\\BCICIV_2a_gdf\\Derivatives\\A01T.mat')\n",
        "\n",
        "# Google Colab load the dataset\n",
        "file = scio.loadmat('/content/sub-011_eeg90hz.mat')\n",
        "\n",
        "\n",
        "all_data = file['all_data']\n",
        "all_label = file['all_label']\n",
        "REDUCE_REST = 1\n",
        "\n",
        "# Print data information\n",
        "channel_count = all_data.shape[1]\n",
        "num_classes = len(np.unique(all_label.flatten()))\n",
        "data_length = all_data.shape[2] # Assuming data is samples*ch*length\n",
        "print(f\"Channel count: {channel_count}\")\n",
        "print(f\"Data shape: {all_data.shape}\")\n",
        "print(f\"Label shape: {all_label.shape}\")\n",
        "print(f\"Class distribution: Rest: {np.sum(all_label == 0)}, Elbow: {np.sum(all_label == 1)}, Hand: {np.sum(all_label == 2)}\")\n",
        "\n",
        "# Address class imbalance for 'Rest' class (label 0)\n",
        "label_0_indices = np.where(all_label.flatten() == 0)[0]\n",
        "label_other_indices = np.where(all_label.flatten() != 0)[0]\n",
        "\n",
        "# Randomly select \"REDUCE_REST\"% of label 0 indices\n",
        "num_label_0_to_keep = int(len(label_0_indices) * REDUCE_REST)\n",
        "#np.random.seed(42) # for reproducibility\n",
        "selected_label_0_indices = np.random.choice(label_0_indices, size=num_label_0_to_keep, replace=False)\n",
        "\n",
        "# Combine selected label 0 indices with all other labels\n",
        "balanced_indices = np.concatenate((selected_label_0_indices, label_other_indices))\n",
        "np.random.shuffle(balanced_indices) # Shuffle the combined dataset\n",
        "\n",
        "all_data_balanced = all_data[balanced_indices]\n",
        "all_label_balanced = all_label[balanced_indices]\n",
        "\n",
        "print(f\"Balanced class distribution: Rest: {np.sum(all_label_balanced == 0)}, Elbow: {np.sum(all_label_balanced == 1)}, Hand: {np.sum(all_label_balanced == 2)}\")\n",
        "print(f\"Balanced data shape: {all_data_balanced.shape}\")\n",
        "print(f\"Balanced label shape: {all_label_balanced.shape}\")\n",
        "\n",
        "datasetX = torch.tensor(all_data_balanced, dtype=torch.float32)\n",
        "datasetY = torch.tensor(all_label_balanced, dtype=torch.int64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LENet to SNN Conversion Framework execution\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "TIME_STEPS = 100  # T for SNN\n",
        "TEST_SIZE = 0.2\n",
        "DROP_OUT = 0.25\n",
        "\n",
        "\n",
        "# Split the data\n",
        "print(f\"{100 - (TEST_SIZE * 100)}% of the dataset is used for training and {TEST_SIZE * 100}% is used for testing.\")\n",
        "train_data, test_data, train_label, test_label = train_test_split(datasetX, datasetY, test_size=TEST_SIZE, shuffle=True,\n",
        "                                                                  random_state=0)\n",
        "\n",
        "# Initialize model\n",
        "cnn_model_lenet = LENet(classes_num=3, channel_count=channel_count, drop_out = DROP_OUT).to(device)\n",
        "cnn_model_lenet.apply(initialize_weights)\n",
        "\n",
        "# Train CNN model\n",
        "train_acc, test_acc,  cnn_model_lenet = train_ann(cnn_model_lenet, train_data, train_label, test_data, test_label,\n",
        "                                              ep=EPOCHS, batch=BATCH_SIZE)\n",
        "max_norm_acc = anntosnn( cnn_model_lenet, train_data, train_label, test_data, test_label,\n",
        "                        batch=BATCH_SIZE, T=TIME_STEPS)\n",
        "snn_model_lenet = ann2snn.Converter(mode='max', dataloader=data_loader(train_data, train_label, batch=BATCH_SIZE))( cnn_model_lenet)\n",
        "\n",
        "print('\\n')\n",
        "print('ANN accuracy: Test: %.4f%%' % (test_acc * 100))\n",
        "print('SNN accuracy: max_norm: %.4f%%' % (max_norm_acc[-1] * 100))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JiMa-ylsC2bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883b0233-fff0-4c32-f863-82dbdd998a4e"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80.0% of the dataset is used for training and 20.0% is used for testing.\n",
            "\n",
            "\n",
            "0 13 Epoch: 0 | ANN: trainLoss: 2.1795 | trainAcc: 29.6875% (19/64)\n",
            "1 13 Epoch: 0 | ANN: trainLoss: 2.0194 | trainAcc: 31.2500% (40/128)\n",
            "2 13 Epoch: 0 | ANN: trainLoss: 1.9567 | trainAcc: 33.8542% (65/192)\n",
            "3 13 Epoch: 0 | ANN: trainLoss: 1.9028 | trainAcc: 32.0312% (82/256)\n",
            "4 13 Epoch: 0 | ANN: trainLoss: 1.8013 | trainAcc: 31.8750% (102/320)\n",
            "5 13 Epoch: 0 | ANN: trainLoss: 1.7487 | trainAcc: 32.8125% (126/384)\n",
            "6 13 Epoch: 0 | ANN: trainLoss: 1.6683 | trainAcc: 33.2589% (149/448)\n",
            "7 13 Epoch: 0 | ANN: trainLoss: 1.6038 | trainAcc: 34.7656% (178/512)\n",
            "8 13 Epoch: 0 | ANN: trainLoss: 1.5373 | trainAcc: 35.2431% (203/576)\n",
            "9 13 Epoch: 0 | ANN: trainLoss: 1.4901 | trainAcc: 35.3125% (226/640)\n",
            "10 13 Epoch: 0 | ANN: trainLoss: 1.4533 | trainAcc: 34.9432% (246/704)\n",
            "11 13 Epoch: 0 | ANN: trainLoss: 1.4182 | trainAcc: 35.5469% (273/768)\n",
            "12 13 Epoch: 0 | ANN: trainLoss: 1.3807 | trainAcc: 35.7513% (276/772)\n",
            "0 4 Epoch: 0 | ANN: testLoss: 0.9601 | testAcc: 54.6875% (35/64)\n",
            "1 4 Epoch: 0 | ANN: testLoss: 0.9920 | testAcc: 54.6875% (70/128)\n",
            "2 4 Epoch: 0 | ANN: testLoss: 1.0139 | testAcc: 53.6458% (103/192)\n",
            "3 4 Epoch: 0 | ANN: testLoss: 0.7613 | testAcc: 53.8860% (104/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 1 | ANN: trainLoss: 1.0005 | trainAcc: 51.5625% (33/64)\n",
            "1 13 Epoch: 1 | ANN: trainLoss: 1.1088 | trainAcc: 46.8750% (60/128)\n",
            "2 13 Epoch: 1 | ANN: trainLoss: 1.1197 | trainAcc: 47.3958% (91/192)\n",
            "3 13 Epoch: 1 | ANN: trainLoss: 1.1345 | trainAcc: 46.8750% (120/256)\n",
            "4 13 Epoch: 1 | ANN: trainLoss: 1.1024 | trainAcc: 49.0625% (157/320)\n",
            "5 13 Epoch: 1 | ANN: trainLoss: 1.0584 | trainAcc: 51.8229% (199/384)\n",
            "6 13 Epoch: 1 | ANN: trainLoss: 1.0633 | trainAcc: 50.6696% (227/448)\n",
            "7 13 Epoch: 1 | ANN: trainLoss: 1.0379 | trainAcc: 51.1719% (262/512)\n",
            "8 13 Epoch: 1 | ANN: trainLoss: 1.0409 | trainAcc: 51.0417% (294/576)\n",
            "9 13 Epoch: 1 | ANN: trainLoss: 1.0434 | trainAcc: 51.4062% (329/640)\n",
            "10 13 Epoch: 1 | ANN: trainLoss: 1.0410 | trainAcc: 51.4205% (362/704)\n",
            "11 13 Epoch: 1 | ANN: trainLoss: 1.0311 | trainAcc: 51.4323% (395/768)\n",
            "12 13 Epoch: 1 | ANN: trainLoss: 1.0442 | trainAcc: 51.4249% (397/772)\n",
            "0 4 Epoch: 1 | ANN: testLoss: 0.9242 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 1 | ANN: testLoss: 0.9940 | testAcc: 54.6875% (70/128)\n",
            "2 4 Epoch: 1 | ANN: testLoss: 0.9790 | testAcc: 54.1667% (104/192)\n",
            "3 4 Epoch: 1 | ANN: testLoss: 1.1966 | testAcc: 53.8860% (104/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 2 | ANN: trainLoss: 1.0020 | trainAcc: 48.4375% (31/64)\n",
            "1 13 Epoch: 2 | ANN: trainLoss: 0.9471 | trainAcc: 53.9062% (69/128)\n",
            "2 13 Epoch: 2 | ANN: trainLoss: 0.9274 | trainAcc: 56.2500% (108/192)\n",
            "3 13 Epoch: 2 | ANN: trainLoss: 0.9187 | trainAcc: 55.0781% (141/256)\n",
            "4 13 Epoch: 2 | ANN: trainLoss: 0.8938 | trainAcc: 59.0625% (189/320)\n",
            "5 13 Epoch: 2 | ANN: trainLoss: 0.8964 | trainAcc: 58.0729% (223/384)\n",
            "6 13 Epoch: 2 | ANN: trainLoss: 0.9016 | trainAcc: 57.5893% (258/448)\n",
            "7 13 Epoch: 2 | ANN: trainLoss: 0.9089 | trainAcc: 55.8594% (286/512)\n",
            "8 13 Epoch: 2 | ANN: trainLoss: 0.9136 | trainAcc: 56.0764% (323/576)\n",
            "9 13 Epoch: 2 | ANN: trainLoss: 0.9110 | trainAcc: 56.8750% (364/640)\n",
            "10 13 Epoch: 2 | ANN: trainLoss: 0.9154 | trainAcc: 56.6761% (399/704)\n",
            "11 13 Epoch: 2 | ANN: trainLoss: 0.9198 | trainAcc: 56.1198% (431/768)\n",
            "12 13 Epoch: 2 | ANN: trainLoss: 0.9578 | trainAcc: 55.9585% (432/772)\n",
            "0 4 Epoch: 2 | ANN: testLoss: 0.9982 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 2 | ANN: testLoss: 1.0909 | testAcc: 50.0000% (64/128)\n",
            "2 4 Epoch: 2 | ANN: testLoss: 1.1120 | testAcc: 47.3958% (91/192)\n",
            "3 4 Epoch: 2 | ANN: testLoss: 0.9310 | testAcc: 47.6684% (92/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 3 | ANN: trainLoss: 0.8258 | trainAcc: 59.3750% (38/64)\n",
            "1 13 Epoch: 3 | ANN: trainLoss: 0.7890 | trainAcc: 65.6250% (84/128)\n",
            "2 13 Epoch: 3 | ANN: trainLoss: 0.8372 | trainAcc: 59.8958% (115/192)\n",
            "3 13 Epoch: 3 | ANN: trainLoss: 0.8287 | trainAcc: 60.5469% (155/256)\n",
            "4 13 Epoch: 3 | ANN: trainLoss: 0.8119 | trainAcc: 60.9375% (195/320)\n",
            "5 13 Epoch: 3 | ANN: trainLoss: 0.8017 | trainAcc: 60.9375% (234/384)\n",
            "6 13 Epoch: 3 | ANN: trainLoss: 0.8106 | trainAcc: 59.5982% (267/448)\n",
            "7 13 Epoch: 3 | ANN: trainLoss: 0.8197 | trainAcc: 58.7891% (301/512)\n",
            "8 13 Epoch: 3 | ANN: trainLoss: 0.8176 | trainAcc: 59.7222% (344/576)\n",
            "9 13 Epoch: 3 | ANN: trainLoss: 0.8150 | trainAcc: 60.0000% (384/640)\n",
            "10 13 Epoch: 3 | ANN: trainLoss: 0.8055 | trainAcc: 59.9432% (422/704)\n",
            "11 13 Epoch: 3 | ANN: trainLoss: 0.8018 | trainAcc: 59.8958% (460/768)\n",
            "12 13 Epoch: 3 | ANN: trainLoss: 0.7991 | trainAcc: 59.8446% (462/772)\n",
            "0 4 Epoch: 3 | ANN: testLoss: 0.8281 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 3 | ANN: testLoss: 0.7346 | testAcc: 60.1562% (77/128)\n",
            "2 4 Epoch: 3 | ANN: testLoss: 0.7993 | testAcc: 56.7708% (109/192)\n",
            "3 4 Epoch: 3 | ANN: testLoss: 0.6957 | testAcc: 56.9948% (110/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 4 | ANN: trainLoss: 0.8508 | trainAcc: 54.6875% (35/64)\n",
            "1 13 Epoch: 4 | ANN: trainLoss: 0.7693 | trainAcc: 60.1562% (77/128)\n",
            "2 13 Epoch: 4 | ANN: trainLoss: 0.7679 | trainAcc: 59.3750% (114/192)\n",
            "3 13 Epoch: 4 | ANN: trainLoss: 0.7789 | trainAcc: 61.3281% (157/256)\n",
            "4 13 Epoch: 4 | ANN: trainLoss: 0.7544 | trainAcc: 63.7500% (204/320)\n",
            "5 13 Epoch: 4 | ANN: trainLoss: 0.7616 | trainAcc: 63.2812% (243/384)\n",
            "6 13 Epoch: 4 | ANN: trainLoss: 0.7470 | trainAcc: 63.8393% (286/448)\n",
            "7 13 Epoch: 4 | ANN: trainLoss: 0.7419 | trainAcc: 64.4531% (330/512)\n",
            "8 13 Epoch: 4 | ANN: trainLoss: 0.7366 | trainAcc: 63.8889% (368/576)\n",
            "9 13 Epoch: 4 | ANN: trainLoss: 0.7381 | trainAcc: 63.4375% (406/640)\n",
            "10 13 Epoch: 4 | ANN: trainLoss: 0.7437 | trainAcc: 62.7841% (442/704)\n",
            "11 13 Epoch: 4 | ANN: trainLoss: 0.7437 | trainAcc: 62.7604% (482/768)\n",
            "12 13 Epoch: 4 | ANN: trainLoss: 0.7900 | trainAcc: 62.5648% (483/772)\n",
            "0 4 Epoch: 4 | ANN: testLoss: 0.8052 | testAcc: 51.5625% (33/64)\n",
            "1 4 Epoch: 4 | ANN: testLoss: 0.7745 | testAcc: 54.6875% (70/128)\n",
            "2 4 Epoch: 4 | ANN: testLoss: 0.7374 | testAcc: 57.8125% (111/192)\n",
            "3 4 Epoch: 4 | ANN: testLoss: 0.8455 | testAcc: 57.5130% (111/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 5 | ANN: trainLoss: 0.7587 | trainAcc: 59.3750% (38/64)\n",
            "1 13 Epoch: 5 | ANN: trainLoss: 0.7473 | trainAcc: 59.3750% (76/128)\n",
            "2 13 Epoch: 5 | ANN: trainLoss: 0.7379 | trainAcc: 61.4583% (118/192)\n",
            "3 13 Epoch: 5 | ANN: trainLoss: 0.7269 | trainAcc: 64.8438% (166/256)\n",
            "4 13 Epoch: 5 | ANN: trainLoss: 0.7101 | trainAcc: 66.2500% (212/320)\n",
            "5 13 Epoch: 5 | ANN: trainLoss: 0.6988 | trainAcc: 67.7083% (260/384)\n",
            "6 13 Epoch: 5 | ANN: trainLoss: 0.7072 | trainAcc: 67.4107% (302/448)\n",
            "7 13 Epoch: 5 | ANN: trainLoss: 0.7178 | trainAcc: 66.6016% (341/512)\n",
            "8 13 Epoch: 5 | ANN: trainLoss: 0.7108 | trainAcc: 66.8403% (385/576)\n",
            "9 13 Epoch: 5 | ANN: trainLoss: 0.7116 | trainAcc: 66.4062% (425/640)\n",
            "10 13 Epoch: 5 | ANN: trainLoss: 0.7100 | trainAcc: 66.1932% (466/704)\n",
            "11 13 Epoch: 5 | ANN: trainLoss: 0.7111 | trainAcc: 65.4948% (503/768)\n",
            "12 13 Epoch: 5 | ANN: trainLoss: 0.6992 | trainAcc: 65.5440% (506/772)\n",
            "0 4 Epoch: 5 | ANN: testLoss: 0.6655 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 5 | ANN: testLoss: 0.6909 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 5 | ANN: testLoss: 0.7179 | testAcc: 61.9792% (119/192)\n",
            "3 4 Epoch: 5 | ANN: testLoss: 0.7216 | testAcc: 62.1762% (120/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 6 | ANN: trainLoss: 0.8254 | trainAcc: 54.6875% (35/64)\n",
            "1 13 Epoch: 6 | ANN: trainLoss: 0.8036 | trainAcc: 52.3438% (67/128)\n",
            "2 13 Epoch: 6 | ANN: trainLoss: 0.7565 | trainAcc: 58.3333% (112/192)\n",
            "3 13 Epoch: 6 | ANN: trainLoss: 0.7160 | trainAcc: 60.9375% (156/256)\n",
            "4 13 Epoch: 6 | ANN: trainLoss: 0.7423 | trainAcc: 59.6875% (191/320)\n",
            "5 13 Epoch: 6 | ANN: trainLoss: 0.7405 | trainAcc: 60.4167% (232/384)\n",
            "6 13 Epoch: 6 | ANN: trainLoss: 0.7320 | trainAcc: 61.8304% (277/448)\n",
            "7 13 Epoch: 6 | ANN: trainLoss: 0.7138 | trainAcc: 63.4766% (325/512)\n",
            "8 13 Epoch: 6 | ANN: trainLoss: 0.7029 | trainAcc: 64.0625% (369/576)\n",
            "9 13 Epoch: 6 | ANN: trainLoss: 0.6964 | trainAcc: 63.9062% (409/640)\n",
            "10 13 Epoch: 6 | ANN: trainLoss: 0.6880 | trainAcc: 64.7727% (456/704)\n",
            "11 13 Epoch: 6 | ANN: trainLoss: 0.6852 | trainAcc: 65.3646% (502/768)\n",
            "12 13 Epoch: 6 | ANN: trainLoss: 0.6731 | trainAcc: 65.5440% (506/772)\n",
            "0 4 Epoch: 6 | ANN: testLoss: 0.6993 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 6 | ANN: testLoss: 0.6933 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 6 | ANN: testLoss: 0.7051 | testAcc: 61.4583% (118/192)\n",
            "3 4 Epoch: 6 | ANN: testLoss: 0.7407 | testAcc: 61.6580% (119/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 7 | ANN: trainLoss: 0.7403 | trainAcc: 59.3750% (38/64)\n",
            "1 13 Epoch: 7 | ANN: trainLoss: 0.7594 | trainAcc: 60.9375% (78/128)\n",
            "2 13 Epoch: 7 | ANN: trainLoss: 0.7223 | trainAcc: 64.0625% (123/192)\n",
            "3 13 Epoch: 7 | ANN: trainLoss: 0.6640 | trainAcc: 67.9688% (174/256)\n",
            "4 13 Epoch: 7 | ANN: trainLoss: 0.6564 | trainAcc: 67.8125% (217/320)\n",
            "5 13 Epoch: 7 | ANN: trainLoss: 0.6483 | trainAcc: 68.7500% (264/384)\n",
            "6 13 Epoch: 7 | ANN: trainLoss: 0.6333 | trainAcc: 69.6429% (312/448)\n",
            "7 13 Epoch: 7 | ANN: trainLoss: 0.6379 | trainAcc: 68.9453% (353/512)\n",
            "8 13 Epoch: 7 | ANN: trainLoss: 0.6493 | trainAcc: 68.2292% (393/576)\n",
            "9 13 Epoch: 7 | ANN: trainLoss: 0.6512 | trainAcc: 67.8125% (434/640)\n",
            "10 13 Epoch: 7 | ANN: trainLoss: 0.6555 | trainAcc: 67.6136% (476/704)\n",
            "11 13 Epoch: 7 | ANN: trainLoss: 0.6491 | trainAcc: 68.2292% (524/768)\n",
            "12 13 Epoch: 7 | ANN: trainLoss: 0.6416 | trainAcc: 68.2642% (527/772)\n",
            "0 4 Epoch: 7 | ANN: testLoss: 0.6857 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 7 | ANN: testLoss: 0.6634 | testAcc: 62.5000% (80/128)\n",
            "2 4 Epoch: 7 | ANN: testLoss: 0.6701 | testAcc: 63.5417% (122/192)\n",
            "3 4 Epoch: 7 | ANN: testLoss: 0.7236 | testAcc: 63.7306% (123/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 8 | ANN: trainLoss: 0.6664 | trainAcc: 68.7500% (44/64)\n",
            "1 13 Epoch: 8 | ANN: trainLoss: 0.6149 | trainAcc: 70.3125% (90/128)\n",
            "2 13 Epoch: 8 | ANN: trainLoss: 0.6227 | trainAcc: 70.8333% (136/192)\n",
            "3 13 Epoch: 8 | ANN: trainLoss: 0.6269 | trainAcc: 70.7031% (181/256)\n",
            "4 13 Epoch: 8 | ANN: trainLoss: 0.6364 | trainAcc: 69.3750% (222/320)\n",
            "5 13 Epoch: 8 | ANN: trainLoss: 0.6452 | trainAcc: 68.7500% (264/384)\n",
            "6 13 Epoch: 8 | ANN: trainLoss: 0.6375 | trainAcc: 68.9732% (309/448)\n",
            "7 13 Epoch: 8 | ANN: trainLoss: 0.6365 | trainAcc: 69.3359% (355/512)\n",
            "8 13 Epoch: 8 | ANN: trainLoss: 0.6325 | trainAcc: 68.9236% (397/576)\n",
            "9 13 Epoch: 8 | ANN: trainLoss: 0.6332 | trainAcc: 68.5938% (439/640)\n",
            "10 13 Epoch: 8 | ANN: trainLoss: 0.6200 | trainAcc: 69.6023% (490/704)\n",
            "11 13 Epoch: 8 | ANN: trainLoss: 0.6157 | trainAcc: 70.1823% (539/768)\n",
            "12 13 Epoch: 8 | ANN: trainLoss: 0.6005 | trainAcc: 70.2073% (542/772)\n",
            "0 4 Epoch: 8 | ANN: testLoss: 0.9894 | testAcc: 54.6875% (35/64)\n",
            "1 4 Epoch: 8 | ANN: testLoss: 0.9406 | testAcc: 53.9062% (69/128)\n",
            "2 4 Epoch: 8 | ANN: testLoss: 0.8596 | testAcc: 57.2917% (110/192)\n",
            "3 4 Epoch: 8 | ANN: testLoss: 1.1095 | testAcc: 56.9948% (110/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 9 | ANN: trainLoss: 0.6232 | trainAcc: 73.4375% (47/64)\n",
            "1 13 Epoch: 9 | ANN: trainLoss: 0.5400 | trainAcc: 78.9062% (101/128)\n",
            "2 13 Epoch: 9 | ANN: trainLoss: 0.5728 | trainAcc: 74.4792% (143/192)\n",
            "3 13 Epoch: 9 | ANN: trainLoss: 0.5958 | trainAcc: 71.8750% (184/256)\n",
            "4 13 Epoch: 9 | ANN: trainLoss: 0.5894 | trainAcc: 72.1875% (231/320)\n",
            "5 13 Epoch: 9 | ANN: trainLoss: 0.5901 | trainAcc: 72.1354% (277/384)\n",
            "6 13 Epoch: 9 | ANN: trainLoss: 0.5826 | trainAcc: 72.0982% (323/448)\n",
            "7 13 Epoch: 9 | ANN: trainLoss: 0.5948 | trainAcc: 71.6797% (367/512)\n",
            "8 13 Epoch: 9 | ANN: trainLoss: 0.6034 | trainAcc: 71.0069% (409/576)\n",
            "9 13 Epoch: 9 | ANN: trainLoss: 0.6006 | trainAcc: 70.7812% (453/640)\n",
            "10 13 Epoch: 9 | ANN: trainLoss: 0.5999 | trainAcc: 70.8807% (499/704)\n",
            "11 13 Epoch: 9 | ANN: trainLoss: 0.5920 | trainAcc: 71.6146% (550/768)\n",
            "12 13 Epoch: 9 | ANN: trainLoss: 0.6007 | trainAcc: 71.5026% (552/772)\n",
            "0 4 Epoch: 9 | ANN: testLoss: 0.6105 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 9 | ANN: testLoss: 0.6574 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 9 | ANN: testLoss: 0.6501 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 9 | ANN: testLoss: 0.7110 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 10 | ANN: trainLoss: 0.4587 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 10 | ANN: trainLoss: 0.5245 | trainAcc: 72.6562% (93/128)\n",
            "2 13 Epoch: 10 | ANN: trainLoss: 0.5587 | trainAcc: 70.8333% (136/192)\n",
            "3 13 Epoch: 10 | ANN: trainLoss: 0.5573 | trainAcc: 72.2656% (185/256)\n",
            "4 13 Epoch: 10 | ANN: trainLoss: 0.5769 | trainAcc: 71.5625% (229/320)\n",
            "5 13 Epoch: 10 | ANN: trainLoss: 0.5689 | trainAcc: 72.6562% (279/384)\n",
            "6 13 Epoch: 10 | ANN: trainLoss: 0.5749 | trainAcc: 72.9911% (327/448)\n",
            "7 13 Epoch: 10 | ANN: trainLoss: 0.5884 | trainAcc: 71.8750% (368/512)\n",
            "8 13 Epoch: 10 | ANN: trainLoss: 0.6085 | trainAcc: 70.4861% (406/576)\n",
            "9 13 Epoch: 10 | ANN: trainLoss: 0.6160 | trainAcc: 70.0000% (448/640)\n",
            "10 13 Epoch: 10 | ANN: trainLoss: 0.6099 | trainAcc: 70.4545% (496/704)\n",
            "11 13 Epoch: 10 | ANN: trainLoss: 0.6009 | trainAcc: 71.4844% (549/768)\n",
            "12 13 Epoch: 10 | ANN: trainLoss: 0.5772 | trainAcc: 71.6321% (553/772)\n",
            "0 4 Epoch: 10 | ANN: testLoss: 1.0014 | testAcc: 54.6875% (35/64)\n",
            "1 4 Epoch: 10 | ANN: testLoss: 0.9221 | testAcc: 59.3750% (76/128)\n",
            "2 4 Epoch: 10 | ANN: testLoss: 0.9405 | testAcc: 55.2083% (106/192)\n",
            "3 4 Epoch: 10 | ANN: testLoss: 1.1685 | testAcc: 54.9223% (106/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 11 | ANN: trainLoss: 0.8547 | trainAcc: 59.3750% (38/64)\n",
            "1 13 Epoch: 11 | ANN: trainLoss: 0.7003 | trainAcc: 67.9688% (87/128)\n",
            "2 13 Epoch: 11 | ANN: trainLoss: 0.6625 | trainAcc: 70.3125% (135/192)\n",
            "3 13 Epoch: 11 | ANN: trainLoss: 0.6298 | trainAcc: 72.2656% (185/256)\n",
            "4 13 Epoch: 11 | ANN: trainLoss: 0.6296 | trainAcc: 69.6875% (223/320)\n",
            "5 13 Epoch: 11 | ANN: trainLoss: 0.6158 | trainAcc: 70.3125% (270/384)\n",
            "6 13 Epoch: 11 | ANN: trainLoss: 0.6013 | trainAcc: 70.5357% (316/448)\n",
            "7 13 Epoch: 11 | ANN: trainLoss: 0.6107 | trainAcc: 70.7031% (362/512)\n",
            "8 13 Epoch: 11 | ANN: trainLoss: 0.6055 | trainAcc: 70.8333% (408/576)\n",
            "9 13 Epoch: 11 | ANN: trainLoss: 0.6041 | trainAcc: 70.6250% (452/640)\n",
            "10 13 Epoch: 11 | ANN: trainLoss: 0.6016 | trainAcc: 70.4545% (496/704)\n",
            "11 13 Epoch: 11 | ANN: trainLoss: 0.5981 | trainAcc: 70.8333% (544/768)\n",
            "12 13 Epoch: 11 | ANN: trainLoss: 0.5924 | trainAcc: 70.8549% (547/772)\n",
            "0 4 Epoch: 11 | ANN: testLoss: 0.6471 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 11 | ANN: testLoss: 0.6327 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 11 | ANN: testLoss: 0.6524 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 11 | ANN: testLoss: 0.4905 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 12 | ANN: trainLoss: 0.4476 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 12 | ANN: trainLoss: 0.4655 | trainAcc: 78.9062% (101/128)\n",
            "2 13 Epoch: 12 | ANN: trainLoss: 0.4339 | trainAcc: 82.2917% (158/192)\n",
            "3 13 Epoch: 12 | ANN: trainLoss: 0.4450 | trainAcc: 80.8594% (207/256)\n",
            "4 13 Epoch: 12 | ANN: trainLoss: 0.4554 | trainAcc: 79.6875% (255/320)\n",
            "5 13 Epoch: 12 | ANN: trainLoss: 0.4927 | trainAcc: 77.8646% (299/384)\n",
            "6 13 Epoch: 12 | ANN: trainLoss: 0.5107 | trainAcc: 77.6786% (348/448)\n",
            "7 13 Epoch: 12 | ANN: trainLoss: 0.5148 | trainAcc: 77.7344% (398/512)\n",
            "8 13 Epoch: 12 | ANN: trainLoss: 0.5217 | trainAcc: 76.9097% (443/576)\n",
            "9 13 Epoch: 12 | ANN: trainLoss: 0.5228 | trainAcc: 76.4062% (489/640)\n",
            "10 13 Epoch: 12 | ANN: trainLoss: 0.5266 | trainAcc: 75.8523% (534/704)\n",
            "11 13 Epoch: 12 | ANN: trainLoss: 0.5320 | trainAcc: 74.8698% (575/768)\n",
            "12 13 Epoch: 12 | ANN: trainLoss: 0.5201 | trainAcc: 74.8705% (578/772)\n",
            "0 4 Epoch: 12 | ANN: testLoss: 0.6665 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 12 | ANN: testLoss: 0.5899 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 12 | ANN: testLoss: 0.6098 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 12 | ANN: testLoss: 0.4624 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 13 | ANN: trainLoss: 0.5245 | trainAcc: 70.3125% (45/64)\n",
            "1 13 Epoch: 13 | ANN: trainLoss: 0.4726 | trainAcc: 75.7812% (97/128)\n",
            "2 13 Epoch: 13 | ANN: trainLoss: 0.5035 | trainAcc: 71.3542% (137/192)\n",
            "3 13 Epoch: 13 | ANN: trainLoss: 0.4672 | trainAcc: 75.7812% (194/256)\n",
            "4 13 Epoch: 13 | ANN: trainLoss: 0.4825 | trainAcc: 75.3125% (241/320)\n",
            "5 13 Epoch: 13 | ANN: trainLoss: 0.4953 | trainAcc: 74.7396% (287/384)\n",
            "6 13 Epoch: 13 | ANN: trainLoss: 0.4932 | trainAcc: 74.7768% (335/448)\n",
            "7 13 Epoch: 13 | ANN: trainLoss: 0.4893 | trainAcc: 75.3906% (386/512)\n",
            "8 13 Epoch: 13 | ANN: trainLoss: 0.4866 | trainAcc: 76.0417% (438/576)\n",
            "9 13 Epoch: 13 | ANN: trainLoss: 0.5017 | trainAcc: 75.1562% (481/640)\n",
            "10 13 Epoch: 13 | ANN: trainLoss: 0.5158 | trainAcc: 75.1420% (529/704)\n",
            "11 13 Epoch: 13 | ANN: trainLoss: 0.5168 | trainAcc: 74.7396% (574/768)\n",
            "12 13 Epoch: 13 | ANN: trainLoss: 0.6208 | trainAcc: 74.6114% (576/772)\n",
            "0 4 Epoch: 13 | ANN: testLoss: 0.5460 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 13 | ANN: testLoss: 0.6391 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 13 | ANN: testLoss: 0.6439 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 13 | ANN: testLoss: 0.4845 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 14 | ANN: trainLoss: 0.4726 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 14 | ANN: trainLoss: 0.4559 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 14 | ANN: trainLoss: 0.4906 | trainAcc: 78.1250% (150/192)\n",
            "3 13 Epoch: 14 | ANN: trainLoss: 0.4812 | trainAcc: 79.6875% (204/256)\n",
            "4 13 Epoch: 14 | ANN: trainLoss: 0.4884 | trainAcc: 78.4375% (251/320)\n",
            "5 13 Epoch: 14 | ANN: trainLoss: 0.4907 | trainAcc: 78.6458% (302/384)\n",
            "6 13 Epoch: 14 | ANN: trainLoss: 0.4960 | trainAcc: 79.2411% (355/448)\n",
            "7 13 Epoch: 14 | ANN: trainLoss: 0.4938 | trainAcc: 78.1250% (400/512)\n",
            "8 13 Epoch: 14 | ANN: trainLoss: 0.5008 | trainAcc: 77.4306% (446/576)\n",
            "9 13 Epoch: 14 | ANN: trainLoss: 0.5096 | trainAcc: 77.5000% (496/640)\n",
            "10 13 Epoch: 14 | ANN: trainLoss: 0.5080 | trainAcc: 77.5568% (546/704)\n",
            "11 13 Epoch: 14 | ANN: trainLoss: 0.5086 | trainAcc: 77.3438% (594/768)\n",
            "12 13 Epoch: 14 | ANN: trainLoss: 0.5156 | trainAcc: 77.3316% (597/772)\n",
            "0 4 Epoch: 14 | ANN: testLoss: 0.6462 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 14 | ANN: testLoss: 0.6064 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 14 | ANN: testLoss: 0.5812 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 14 | ANN: testLoss: 0.4421 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 15 | ANN: trainLoss: 0.4876 | trainAcc: 75.0000% (48/64)\n",
            "1 13 Epoch: 15 | ANN: trainLoss: 0.5328 | trainAcc: 69.5312% (89/128)\n",
            "2 13 Epoch: 15 | ANN: trainLoss: 0.5259 | trainAcc: 68.7500% (132/192)\n",
            "3 13 Epoch: 15 | ANN: trainLoss: 0.5337 | trainAcc: 67.9688% (174/256)\n",
            "4 13 Epoch: 15 | ANN: trainLoss: 0.5154 | trainAcc: 70.9375% (227/320)\n",
            "5 13 Epoch: 15 | ANN: trainLoss: 0.5122 | trainAcc: 72.6562% (279/384)\n",
            "6 13 Epoch: 15 | ANN: trainLoss: 0.5011 | trainAcc: 75.0000% (336/448)\n",
            "7 13 Epoch: 15 | ANN: trainLoss: 0.4961 | trainAcc: 75.9766% (389/512)\n",
            "8 13 Epoch: 15 | ANN: trainLoss: 0.4917 | trainAcc: 76.3889% (440/576)\n",
            "9 13 Epoch: 15 | ANN: trainLoss: 0.4815 | trainAcc: 76.8750% (492/640)\n",
            "10 13 Epoch: 15 | ANN: trainLoss: 0.4984 | trainAcc: 75.5682% (532/704)\n",
            "11 13 Epoch: 15 | ANN: trainLoss: 0.5000 | trainAcc: 75.2604% (578/768)\n",
            "12 13 Epoch: 15 | ANN: trainLoss: 0.5180 | trainAcc: 75.1295% (580/772)\n",
            "0 4 Epoch: 15 | ANN: testLoss: 0.5637 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 15 | ANN: testLoss: 0.5136 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 15 | ANN: testLoss: 0.5656 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 15 | ANN: testLoss: 0.5035 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 16 | ANN: trainLoss: 0.3949 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 16 | ANN: trainLoss: 0.4280 | trainAcc: 78.1250% (100/128)\n",
            "2 13 Epoch: 16 | ANN: trainLoss: 0.4057 | trainAcc: 81.7708% (157/192)\n",
            "3 13 Epoch: 16 | ANN: trainLoss: 0.4462 | trainAcc: 78.1250% (200/256)\n",
            "4 13 Epoch: 16 | ANN: trainLoss: 0.4405 | trainAcc: 80.0000% (256/320)\n",
            "5 13 Epoch: 16 | ANN: trainLoss: 0.4634 | trainAcc: 77.8646% (299/384)\n",
            "6 13 Epoch: 16 | ANN: trainLoss: 0.4651 | trainAcc: 77.6786% (348/448)\n",
            "7 13 Epoch: 16 | ANN: trainLoss: 0.4757 | trainAcc: 77.3438% (396/512)\n",
            "8 13 Epoch: 16 | ANN: trainLoss: 0.4727 | trainAcc: 77.9514% (449/576)\n",
            "9 13 Epoch: 16 | ANN: trainLoss: 0.4780 | trainAcc: 77.5000% (496/640)\n",
            "10 13 Epoch: 16 | ANN: trainLoss: 0.4763 | trainAcc: 77.4148% (545/704)\n",
            "11 13 Epoch: 16 | ANN: trainLoss: 0.4744 | trainAcc: 77.9948% (599/768)\n",
            "12 13 Epoch: 16 | ANN: trainLoss: 0.4710 | trainAcc: 77.9793% (602/772)\n",
            "0 4 Epoch: 16 | ANN: testLoss: 0.5983 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 16 | ANN: testLoss: 0.5682 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 16 | ANN: testLoss: 0.5567 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 16 | ANN: testLoss: 0.5022 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 17 | ANN: trainLoss: 0.4641 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 17 | ANN: trainLoss: 0.4383 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 17 | ANN: trainLoss: 0.4181 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 17 | ANN: trainLoss: 0.4517 | trainAcc: 82.8125% (212/256)\n",
            "4 13 Epoch: 17 | ANN: trainLoss: 0.4487 | trainAcc: 82.1875% (263/320)\n",
            "5 13 Epoch: 17 | ANN: trainLoss: 0.4515 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 17 | ANN: trainLoss: 0.4516 | trainAcc: 82.1429% (368/448)\n",
            "7 13 Epoch: 17 | ANN: trainLoss: 0.4569 | trainAcc: 81.4453% (417/512)\n",
            "8 13 Epoch: 17 | ANN: trainLoss: 0.4680 | trainAcc: 79.8611% (460/576)\n",
            "9 13 Epoch: 17 | ANN: trainLoss: 0.4591 | trainAcc: 80.0000% (512/640)\n",
            "10 13 Epoch: 17 | ANN: trainLoss: 0.4493 | trainAcc: 80.3977% (566/704)\n",
            "11 13 Epoch: 17 | ANN: trainLoss: 0.4647 | trainAcc: 79.5573% (611/768)\n",
            "12 13 Epoch: 17 | ANN: trainLoss: 0.4695 | trainAcc: 79.5337% (614/772)\n",
            "0 4 Epoch: 17 | ANN: testLoss: 0.7185 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 17 | ANN: testLoss: 0.6300 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 17 | ANN: testLoss: 0.6058 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 17 | ANN: testLoss: 0.4816 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 18 | ANN: trainLoss: 0.4954 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 18 | ANN: trainLoss: 0.5192 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 18 | ANN: trainLoss: 0.5189 | trainAcc: 75.5208% (145/192)\n",
            "3 13 Epoch: 18 | ANN: trainLoss: 0.4974 | trainAcc: 76.9531% (197/256)\n",
            "4 13 Epoch: 18 | ANN: trainLoss: 0.5175 | trainAcc: 75.9375% (243/320)\n",
            "5 13 Epoch: 18 | ANN: trainLoss: 0.5139 | trainAcc: 75.7812% (291/384)\n",
            "6 13 Epoch: 18 | ANN: trainLoss: 0.5215 | trainAcc: 74.7768% (335/448)\n",
            "7 13 Epoch: 18 | ANN: trainLoss: 0.5107 | trainAcc: 74.8047% (383/512)\n",
            "8 13 Epoch: 18 | ANN: trainLoss: 0.5011 | trainAcc: 75.8681% (437/576)\n",
            "9 13 Epoch: 18 | ANN: trainLoss: 0.4888 | trainAcc: 77.0312% (493/640)\n",
            "10 13 Epoch: 18 | ANN: trainLoss: 0.4995 | trainAcc: 76.9886% (542/704)\n",
            "11 13 Epoch: 18 | ANN: trainLoss: 0.4940 | trainAcc: 77.0833% (592/768)\n",
            "12 13 Epoch: 18 | ANN: trainLoss: 0.4939 | trainAcc: 77.2021% (596/772)\n",
            "0 4 Epoch: 18 | ANN: testLoss: 0.5369 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 18 | ANN: testLoss: 0.6354 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 18 | ANN: testLoss: 0.7096 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 18 | ANN: testLoss: 0.9894 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 19 | ANN: trainLoss: 0.4819 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 19 | ANN: trainLoss: 0.4443 | trainAcc: 78.9062% (101/128)\n",
            "2 13 Epoch: 19 | ANN: trainLoss: 0.4879 | trainAcc: 75.5208% (145/192)\n",
            "3 13 Epoch: 19 | ANN: trainLoss: 0.4822 | trainAcc: 75.3906% (193/256)\n",
            "4 13 Epoch: 19 | ANN: trainLoss: 0.4669 | trainAcc: 77.1875% (247/320)\n",
            "5 13 Epoch: 19 | ANN: trainLoss: 0.4808 | trainAcc: 77.0833% (296/384)\n",
            "6 13 Epoch: 19 | ANN: trainLoss: 0.4754 | trainAcc: 77.6786% (348/448)\n",
            "7 13 Epoch: 19 | ANN: trainLoss: 0.4791 | trainAcc: 77.5391% (397/512)\n",
            "8 13 Epoch: 19 | ANN: trainLoss: 0.4720 | trainAcc: 77.9514% (449/576)\n",
            "9 13 Epoch: 19 | ANN: trainLoss: 0.4848 | trainAcc: 77.6562% (497/640)\n",
            "10 13 Epoch: 19 | ANN: trainLoss: 0.4770 | trainAcc: 77.6989% (547/704)\n",
            "11 13 Epoch: 19 | ANN: trainLoss: 0.4687 | trainAcc: 78.3854% (602/768)\n",
            "12 13 Epoch: 19 | ANN: trainLoss: 0.5179 | trainAcc: 78.2383% (604/772)\n",
            "0 4 Epoch: 19 | ANN: testLoss: 0.5077 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 19 | ANN: testLoss: 0.5545 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 19 | ANN: testLoss: 0.6086 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 19 | ANN: testLoss: 1.1978 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 20 | ANN: trainLoss: 0.3909 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 20 | ANN: trainLoss: 0.3975 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 20 | ANN: trainLoss: 0.4078 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 20 | ANN: trainLoss: 0.4232 | trainAcc: 82.8125% (212/256)\n",
            "4 13 Epoch: 20 | ANN: trainLoss: 0.4295 | trainAcc: 81.8750% (262/320)\n",
            "5 13 Epoch: 20 | ANN: trainLoss: 0.4135 | trainAcc: 83.8542% (322/384)\n",
            "6 13 Epoch: 20 | ANN: trainLoss: 0.4073 | trainAcc: 83.9286% (376/448)\n",
            "7 13 Epoch: 20 | ANN: trainLoss: 0.4205 | trainAcc: 82.6172% (423/512)\n",
            "8 13 Epoch: 20 | ANN: trainLoss: 0.4300 | trainAcc: 81.7708% (471/576)\n",
            "9 13 Epoch: 20 | ANN: trainLoss: 0.4339 | trainAcc: 80.7812% (517/640)\n",
            "10 13 Epoch: 20 | ANN: trainLoss: 0.4380 | trainAcc: 80.6818% (568/704)\n",
            "11 13 Epoch: 20 | ANN: trainLoss: 0.4311 | trainAcc: 81.2500% (624/768)\n",
            "12 13 Epoch: 20 | ANN: trainLoss: 0.4498 | trainAcc: 81.2176% (627/772)\n",
            "0 4 Epoch: 20 | ANN: testLoss: 0.6028 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 20 | ANN: testLoss: 0.5457 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 20 | ANN: testLoss: 0.5645 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 20 | ANN: testLoss: 0.6127 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 21 | ANN: trainLoss: 0.4818 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 21 | ANN: trainLoss: 0.4242 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 21 | ANN: trainLoss: 0.4936 | trainAcc: 80.7292% (155/192)\n",
            "3 13 Epoch: 21 | ANN: trainLoss: 0.4774 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 21 | ANN: trainLoss: 0.5157 | trainAcc: 78.4375% (251/320)\n",
            "5 13 Epoch: 21 | ANN: trainLoss: 0.5033 | trainAcc: 78.3854% (301/384)\n",
            "6 13 Epoch: 21 | ANN: trainLoss: 0.5073 | trainAcc: 78.3482% (351/448)\n",
            "7 13 Epoch: 21 | ANN: trainLoss: 0.4983 | trainAcc: 78.9062% (404/512)\n",
            "8 13 Epoch: 21 | ANN: trainLoss: 0.4871 | trainAcc: 78.9931% (455/576)\n",
            "9 13 Epoch: 21 | ANN: trainLoss: 0.4838 | trainAcc: 78.5938% (503/640)\n",
            "10 13 Epoch: 21 | ANN: trainLoss: 0.4801 | trainAcc: 78.9773% (556/704)\n",
            "11 13 Epoch: 21 | ANN: trainLoss: 0.4721 | trainAcc: 79.4271% (610/768)\n",
            "12 13 Epoch: 21 | ANN: trainLoss: 0.4746 | trainAcc: 79.4041% (613/772)\n",
            "0 4 Epoch: 21 | ANN: testLoss: 0.6199 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 21 | ANN: testLoss: 0.6608 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 21 | ANN: testLoss: 0.6551 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 21 | ANN: testLoss: 0.6484 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 22 | ANN: trainLoss: 0.4513 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 22 | ANN: trainLoss: 0.4618 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 22 | ANN: trainLoss: 0.4710 | trainAcc: 82.8125% (159/192)\n",
            "3 13 Epoch: 22 | ANN: trainLoss: 0.5009 | trainAcc: 80.0781% (205/256)\n",
            "4 13 Epoch: 22 | ANN: trainLoss: 0.4807 | trainAcc: 80.6250% (258/320)\n",
            "5 13 Epoch: 22 | ANN: trainLoss: 0.4895 | trainAcc: 80.2083% (308/384)\n",
            "6 13 Epoch: 22 | ANN: trainLoss: 0.5099 | trainAcc: 78.1250% (350/448)\n",
            "7 13 Epoch: 22 | ANN: trainLoss: 0.4903 | trainAcc: 78.9062% (404/512)\n",
            "8 13 Epoch: 22 | ANN: trainLoss: 0.4840 | trainAcc: 78.9931% (455/576)\n",
            "9 13 Epoch: 22 | ANN: trainLoss: 0.4779 | trainAcc: 79.2188% (507/640)\n",
            "10 13 Epoch: 22 | ANN: trainLoss: 0.4844 | trainAcc: 78.8352% (555/704)\n",
            "11 13 Epoch: 22 | ANN: trainLoss: 0.4860 | trainAcc: 78.9062% (606/768)\n",
            "12 13 Epoch: 22 | ANN: trainLoss: 0.4722 | trainAcc: 79.0155% (610/772)\n",
            "0 4 Epoch: 22 | ANN: testLoss: 0.7094 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 22 | ANN: testLoss: 0.5968 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 22 | ANN: testLoss: 0.6030 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 22 | ANN: testLoss: 0.4604 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 23 | ANN: trainLoss: 0.4075 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 23 | ANN: trainLoss: 0.3699 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 23 | ANN: trainLoss: 0.4219 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 23 | ANN: trainLoss: 0.4346 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 23 | ANN: trainLoss: 0.4409 | trainAcc: 81.5625% (261/320)\n",
            "5 13 Epoch: 23 | ANN: trainLoss: 0.4465 | trainAcc: 81.7708% (314/384)\n",
            "6 13 Epoch: 23 | ANN: trainLoss: 0.4416 | trainAcc: 81.2500% (364/448)\n",
            "7 13 Epoch: 23 | ANN: trainLoss: 0.4278 | trainAcc: 81.4453% (417/512)\n",
            "8 13 Epoch: 23 | ANN: trainLoss: 0.4333 | trainAcc: 80.7292% (465/576)\n",
            "9 13 Epoch: 23 | ANN: trainLoss: 0.4569 | trainAcc: 79.0625% (506/640)\n",
            "10 13 Epoch: 23 | ANN: trainLoss: 0.4570 | trainAcc: 79.1193% (557/704)\n",
            "11 13 Epoch: 23 | ANN: trainLoss: 0.4605 | trainAcc: 78.7760% (605/768)\n",
            "12 13 Epoch: 23 | ANN: trainLoss: 0.4280 | trainAcc: 78.8860% (609/772)\n",
            "0 4 Epoch: 23 | ANN: testLoss: 0.7128 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 23 | ANN: testLoss: 0.5557 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 23 | ANN: testLoss: 0.6425 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 23 | ANN: testLoss: 0.5000 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 24 | ANN: trainLoss: 0.3767 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 24 | ANN: trainLoss: 0.3759 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 24 | ANN: trainLoss: 0.3794 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 24 | ANN: trainLoss: 0.3736 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 24 | ANN: trainLoss: 0.3681 | trainAcc: 81.5625% (261/320)\n",
            "5 13 Epoch: 24 | ANN: trainLoss: 0.3799 | trainAcc: 81.7708% (314/384)\n",
            "6 13 Epoch: 24 | ANN: trainLoss: 0.3783 | trainAcc: 81.2500% (364/448)\n",
            "7 13 Epoch: 24 | ANN: trainLoss: 0.3737 | trainAcc: 81.8359% (419/512)\n",
            "8 13 Epoch: 24 | ANN: trainLoss: 0.3757 | trainAcc: 82.4653% (475/576)\n",
            "9 13 Epoch: 24 | ANN: trainLoss: 0.3702 | trainAcc: 83.4375% (534/640)\n",
            "10 13 Epoch: 24 | ANN: trainLoss: 0.3732 | trainAcc: 83.2386% (586/704)\n",
            "11 13 Epoch: 24 | ANN: trainLoss: 0.3783 | trainAcc: 83.0729% (638/768)\n",
            "12 13 Epoch: 24 | ANN: trainLoss: 0.4578 | trainAcc: 82.9016% (640/772)\n",
            "0 4 Epoch: 24 | ANN: testLoss: 0.5460 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 24 | ANN: testLoss: 0.5017 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 24 | ANN: testLoss: 0.5505 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 24 | ANN: testLoss: 0.4140 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 25 | ANN: trainLoss: 0.3151 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 25 | ANN: trainLoss: 0.3212 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 25 | ANN: trainLoss: 0.3421 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 25 | ANN: trainLoss: 0.3405 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 25 | ANN: trainLoss: 0.3636 | trainAcc: 82.8125% (265/320)\n",
            "5 13 Epoch: 25 | ANN: trainLoss: 0.3643 | trainAcc: 83.3333% (320/384)\n",
            "6 13 Epoch: 25 | ANN: trainLoss: 0.3762 | trainAcc: 83.2589% (373/448)\n",
            "7 13 Epoch: 25 | ANN: trainLoss: 0.3809 | trainAcc: 83.0078% (425/512)\n",
            "8 13 Epoch: 25 | ANN: trainLoss: 0.3794 | trainAcc: 82.6389% (476/576)\n",
            "9 13 Epoch: 25 | ANN: trainLoss: 0.3845 | trainAcc: 82.6562% (529/640)\n",
            "10 13 Epoch: 25 | ANN: trainLoss: 0.3964 | trainAcc: 81.6761% (575/704)\n",
            "11 13 Epoch: 25 | ANN: trainLoss: 0.3921 | trainAcc: 82.2917% (632/768)\n",
            "12 13 Epoch: 25 | ANN: trainLoss: 0.3909 | trainAcc: 82.2539% (635/772)\n",
            "0 4 Epoch: 25 | ANN: testLoss: 0.4794 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 25 | ANN: testLoss: 0.5486 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 25 | ANN: testLoss: 0.5137 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 25 | ANN: testLoss: 0.6496 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 26 | ANN: trainLoss: 0.4014 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 26 | ANN: trainLoss: 0.3870 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 26 | ANN: trainLoss: 0.3893 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 26 | ANN: trainLoss: 0.3940 | trainAcc: 80.0781% (205/256)\n",
            "4 13 Epoch: 26 | ANN: trainLoss: 0.4046 | trainAcc: 80.3125% (257/320)\n",
            "5 13 Epoch: 26 | ANN: trainLoss: 0.3826 | trainAcc: 81.2500% (312/384)\n",
            "6 13 Epoch: 26 | ANN: trainLoss: 0.3753 | trainAcc: 82.1429% (368/448)\n",
            "7 13 Epoch: 26 | ANN: trainLoss: 0.3733 | trainAcc: 82.6172% (423/512)\n",
            "8 13 Epoch: 26 | ANN: trainLoss: 0.3776 | trainAcc: 82.9861% (478/576)\n",
            "9 13 Epoch: 26 | ANN: trainLoss: 0.3766 | trainAcc: 82.8125% (530/640)\n",
            "10 13 Epoch: 26 | ANN: trainLoss: 0.3806 | trainAcc: 82.9545% (584/704)\n",
            "11 13 Epoch: 26 | ANN: trainLoss: 0.3809 | trainAcc: 82.8125% (636/768)\n",
            "12 13 Epoch: 26 | ANN: trainLoss: 0.3839 | trainAcc: 82.7720% (639/772)\n",
            "0 4 Epoch: 26 | ANN: testLoss: 0.5693 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 26 | ANN: testLoss: 0.5851 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 26 | ANN: testLoss: 0.6031 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 26 | ANN: testLoss: 0.4546 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 27 | ANN: trainLoss: 0.3776 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 27 | ANN: trainLoss: 0.3517 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 27 | ANN: trainLoss: 0.3709 | trainAcc: 83.3333% (160/192)\n",
            "3 13 Epoch: 27 | ANN: trainLoss: 0.3831 | trainAcc: 82.4219% (211/256)\n",
            "4 13 Epoch: 27 | ANN: trainLoss: 0.3976 | trainAcc: 81.5625% (261/320)\n",
            "5 13 Epoch: 27 | ANN: trainLoss: 0.3987 | trainAcc: 82.2917% (316/384)\n",
            "6 13 Epoch: 27 | ANN: trainLoss: 0.3927 | trainAcc: 82.5893% (370/448)\n",
            "7 13 Epoch: 27 | ANN: trainLoss: 0.3934 | trainAcc: 82.4219% (422/512)\n",
            "8 13 Epoch: 27 | ANN: trainLoss: 0.3781 | trainAcc: 83.5069% (481/576)\n",
            "9 13 Epoch: 27 | ANN: trainLoss: 0.3762 | trainAcc: 83.7500% (536/640)\n",
            "10 13 Epoch: 27 | ANN: trainLoss: 0.3749 | trainAcc: 84.0909% (592/704)\n",
            "11 13 Epoch: 27 | ANN: trainLoss: 0.3753 | trainAcc: 83.8542% (644/768)\n",
            "12 13 Epoch: 27 | ANN: trainLoss: 0.4037 | trainAcc: 83.8083% (647/772)\n",
            "0 4 Epoch: 27 | ANN: testLoss: 0.5363 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 27 | ANN: testLoss: 0.6290 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 27 | ANN: testLoss: 0.6215 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 27 | ANN: testLoss: 1.1319 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 28 | ANN: trainLoss: 0.3642 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 28 | ANN: trainLoss: 0.4278 | trainAcc: 80.4688% (103/128)\n",
            "2 13 Epoch: 28 | ANN: trainLoss: 0.3942 | trainAcc: 81.7708% (157/192)\n",
            "3 13 Epoch: 28 | ANN: trainLoss: 0.3672 | trainAcc: 83.5938% (214/256)\n",
            "4 13 Epoch: 28 | ANN: trainLoss: 0.3667 | trainAcc: 83.7500% (268/320)\n",
            "5 13 Epoch: 28 | ANN: trainLoss: 0.3491 | trainAcc: 84.8958% (326/384)\n",
            "6 13 Epoch: 28 | ANN: trainLoss: 0.3528 | trainAcc: 84.3750% (378/448)\n",
            "7 13 Epoch: 28 | ANN: trainLoss: 0.3483 | trainAcc: 84.7656% (434/512)\n",
            "8 13 Epoch: 28 | ANN: trainLoss: 0.3638 | trainAcc: 84.0278% (484/576)\n",
            "9 13 Epoch: 28 | ANN: trainLoss: 0.3835 | trainAcc: 83.2812% (533/640)\n",
            "10 13 Epoch: 28 | ANN: trainLoss: 0.3742 | trainAcc: 83.9489% (591/704)\n",
            "11 13 Epoch: 28 | ANN: trainLoss: 0.3808 | trainAcc: 83.4635% (641/768)\n",
            "12 13 Epoch: 28 | ANN: trainLoss: 0.3675 | trainAcc: 83.5492% (645/772)\n",
            "0 4 Epoch: 28 | ANN: testLoss: 0.9622 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 28 | ANN: testLoss: 0.8889 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 28 | ANN: testLoss: 0.7665 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 28 | ANN: testLoss: 0.7179 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 29 | ANN: trainLoss: 0.3587 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 29 | ANN: trainLoss: 0.4090 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 29 | ANN: trainLoss: 0.4073 | trainAcc: 82.8125% (159/192)\n",
            "3 13 Epoch: 29 | ANN: trainLoss: 0.3813 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 29 | ANN: trainLoss: 0.3679 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 29 | ANN: trainLoss: 0.3629 | trainAcc: 85.6771% (329/384)\n",
            "6 13 Epoch: 29 | ANN: trainLoss: 0.3565 | trainAcc: 86.3839% (387/448)\n",
            "7 13 Epoch: 29 | ANN: trainLoss: 0.3445 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 29 | ANN: trainLoss: 0.3452 | trainAcc: 86.4583% (498/576)\n",
            "9 13 Epoch: 29 | ANN: trainLoss: 0.3471 | trainAcc: 85.4688% (547/640)\n",
            "10 13 Epoch: 29 | ANN: trainLoss: 0.3470 | trainAcc: 85.2273% (600/704)\n",
            "11 13 Epoch: 29 | ANN: trainLoss: 0.3517 | trainAcc: 84.8958% (652/768)\n",
            "12 13 Epoch: 29 | ANN: trainLoss: 0.3589 | trainAcc: 84.8446% (655/772)\n",
            "0 4 Epoch: 29 | ANN: testLoss: 0.6211 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 29 | ANN: testLoss: 0.5543 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 29 | ANN: testLoss: 0.6113 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 29 | ANN: testLoss: 0.5201 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 30 | ANN: trainLoss: 0.3082 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 30 | ANN: trainLoss: 0.3325 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 30 | ANN: trainLoss: 0.3634 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 30 | ANN: trainLoss: 0.3911 | trainAcc: 81.6406% (209/256)\n",
            "4 13 Epoch: 30 | ANN: trainLoss: 0.3814 | trainAcc: 82.5000% (264/320)\n",
            "5 13 Epoch: 30 | ANN: trainLoss: 0.3816 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 30 | ANN: trainLoss: 0.3663 | trainAcc: 83.2589% (373/448)\n",
            "7 13 Epoch: 30 | ANN: trainLoss: 0.3658 | trainAcc: 83.3984% (427/512)\n",
            "8 13 Epoch: 30 | ANN: trainLoss: 0.3641 | trainAcc: 83.6806% (482/576)\n",
            "9 13 Epoch: 30 | ANN: trainLoss: 0.3570 | trainAcc: 84.2188% (539/640)\n",
            "10 13 Epoch: 30 | ANN: trainLoss: 0.3616 | trainAcc: 84.3750% (594/704)\n",
            "11 13 Epoch: 30 | ANN: trainLoss: 0.3543 | trainAcc: 85.0260% (653/768)\n",
            "12 13 Epoch: 30 | ANN: trainLoss: 0.3314 | trainAcc: 85.1036% (657/772)\n",
            "0 4 Epoch: 30 | ANN: testLoss: 0.5308 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 30 | ANN: testLoss: 0.5580 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 30 | ANN: testLoss: 0.5815 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 30 | ANN: testLoss: 0.4385 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 31 | ANN: trainLoss: 0.2351 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 31 | ANN: trainLoss: 0.2627 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 31 | ANN: trainLoss: 0.2635 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 31 | ANN: trainLoss: 0.2538 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 31 | ANN: trainLoss: 0.2594 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 31 | ANN: trainLoss: 0.2585 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 31 | ANN: trainLoss: 0.2778 | trainAcc: 87.9464% (394/448)\n",
            "7 13 Epoch: 31 | ANN: trainLoss: 0.2774 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 31 | ANN: trainLoss: 0.2798 | trainAcc: 87.5000% (504/576)\n",
            "9 13 Epoch: 31 | ANN: trainLoss: 0.2885 | trainAcc: 87.0312% (557/640)\n",
            "10 13 Epoch: 31 | ANN: trainLoss: 0.2866 | trainAcc: 87.2159% (614/704)\n",
            "11 13 Epoch: 31 | ANN: trainLoss: 0.2849 | trainAcc: 87.2396% (670/768)\n",
            "12 13 Epoch: 31 | ANN: trainLoss: 0.2860 | trainAcc: 87.1762% (673/772)\n",
            "0 4 Epoch: 31 | ANN: testLoss: 0.6397 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 31 | ANN: testLoss: 0.6387 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 31 | ANN: testLoss: 0.6092 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 31 | ANN: testLoss: 0.4569 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 32 | ANN: trainLoss: 0.2711 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 32 | ANN: trainLoss: 0.2605 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 32 | ANN: trainLoss: 0.3072 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 32 | ANN: trainLoss: 0.3045 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 32 | ANN: trainLoss: 0.3036 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 32 | ANN: trainLoss: 0.3046 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 32 | ANN: trainLoss: 0.3062 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 32 | ANN: trainLoss: 0.3095 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 32 | ANN: trainLoss: 0.3043 | trainAcc: 86.8056% (500/576)\n",
            "9 13 Epoch: 32 | ANN: trainLoss: 0.2932 | trainAcc: 87.8125% (562/640)\n",
            "10 13 Epoch: 32 | ANN: trainLoss: 0.2981 | trainAcc: 87.5000% (616/704)\n",
            "11 13 Epoch: 32 | ANN: trainLoss: 0.2946 | trainAcc: 87.8906% (675/768)\n",
            "12 13 Epoch: 32 | ANN: trainLoss: 0.3289 | trainAcc: 87.8238% (678/772)\n",
            "0 4 Epoch: 32 | ANN: testLoss: 0.6022 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 32 | ANN: testLoss: 0.6575 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 32 | ANN: testLoss: 0.6485 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 32 | ANN: testLoss: 0.4956 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 33 | ANN: trainLoss: 0.2281 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 33 | ANN: trainLoss: 0.2171 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 33 | ANN: trainLoss: 0.2354 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 33 | ANN: trainLoss: 0.2679 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 33 | ANN: trainLoss: 0.2655 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 33 | ANN: trainLoss: 0.2991 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 33 | ANN: trainLoss: 0.2925 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 33 | ANN: trainLoss: 0.3067 | trainAcc: 85.9375% (440/512)\n",
            "8 13 Epoch: 33 | ANN: trainLoss: 0.3051 | trainAcc: 85.7639% (494/576)\n",
            "9 13 Epoch: 33 | ANN: trainLoss: 0.3044 | trainAcc: 86.2500% (552/640)\n",
            "10 13 Epoch: 33 | ANN: trainLoss: 0.3067 | trainAcc: 85.9375% (605/704)\n",
            "11 13 Epoch: 33 | ANN: trainLoss: 0.2983 | trainAcc: 86.4583% (664/768)\n",
            "12 13 Epoch: 33 | ANN: trainLoss: 0.3258 | trainAcc: 86.2694% (666/772)\n",
            "0 4 Epoch: 33 | ANN: testLoss: 0.5574 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 33 | ANN: testLoss: 0.5472 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 33 | ANN: testLoss: 0.5688 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 33 | ANN: testLoss: 0.7374 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 34 | ANN: trainLoss: 0.3496 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 34 | ANN: trainLoss: 0.3079 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 34 | ANN: trainLoss: 0.2947 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 34 | ANN: trainLoss: 0.2925 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 34 | ANN: trainLoss: 0.2806 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 34 | ANN: trainLoss: 0.2646 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 34 | ANN: trainLoss: 0.2809 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 34 | ANN: trainLoss: 0.2932 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 34 | ANN: trainLoss: 0.3017 | trainAcc: 86.8056% (500/576)\n",
            "9 13 Epoch: 34 | ANN: trainLoss: 0.3029 | trainAcc: 86.5625% (554/640)\n",
            "10 13 Epoch: 34 | ANN: trainLoss: 0.2985 | trainAcc: 86.6477% (610/704)\n",
            "11 13 Epoch: 34 | ANN: trainLoss: 0.2980 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 34 | ANN: trainLoss: 0.2821 | trainAcc: 86.7876% (670/772)\n",
            "0 4 Epoch: 34 | ANN: testLoss: 0.4938 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 34 | ANN: testLoss: 0.5126 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 34 | ANN: testLoss: 0.5633 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 34 | ANN: testLoss: 0.4267 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 35 | ANN: trainLoss: 0.2681 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 35 | ANN: trainLoss: 0.2906 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 35 | ANN: trainLoss: 0.3131 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 35 | ANN: trainLoss: 0.2986 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 35 | ANN: trainLoss: 0.2908 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 35 | ANN: trainLoss: 0.2861 | trainAcc: 86.7188% (333/384)\n",
            "6 13 Epoch: 35 | ANN: trainLoss: 0.2964 | trainAcc: 86.3839% (387/448)\n",
            "7 13 Epoch: 35 | ANN: trainLoss: 0.2939 | trainAcc: 86.7188% (444/512)\n",
            "8 13 Epoch: 35 | ANN: trainLoss: 0.2806 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 35 | ANN: trainLoss: 0.2788 | trainAcc: 87.9688% (563/640)\n",
            "10 13 Epoch: 35 | ANN: trainLoss: 0.2742 | trainAcc: 88.2102% (621/704)\n",
            "11 13 Epoch: 35 | ANN: trainLoss: 0.2750 | trainAcc: 88.0208% (676/768)\n",
            "12 13 Epoch: 35 | ANN: trainLoss: 0.3336 | trainAcc: 87.8238% (678/772)\n",
            "0 4 Epoch: 35 | ANN: testLoss: 0.7180 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 35 | ANN: testLoss: 0.5701 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 35 | ANN: testLoss: 0.5864 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 35 | ANN: testLoss: 0.6421 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 36 | ANN: trainLoss: 0.4071 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 36 | ANN: trainLoss: 0.3597 | trainAcc: 80.4688% (103/128)\n",
            "2 13 Epoch: 36 | ANN: trainLoss: 0.3474 | trainAcc: 80.7292% (155/192)\n",
            "3 13 Epoch: 36 | ANN: trainLoss: 0.3624 | trainAcc: 80.4688% (206/256)\n",
            "4 13 Epoch: 36 | ANN: trainLoss: 0.3731 | trainAcc: 80.3125% (257/320)\n",
            "5 13 Epoch: 36 | ANN: trainLoss: 0.3676 | trainAcc: 81.2500% (312/384)\n",
            "6 13 Epoch: 36 | ANN: trainLoss: 0.3629 | trainAcc: 81.9196% (367/448)\n",
            "7 13 Epoch: 36 | ANN: trainLoss: 0.3593 | trainAcc: 82.4219% (422/512)\n",
            "8 13 Epoch: 36 | ANN: trainLoss: 0.3440 | trainAcc: 83.5069% (481/576)\n",
            "9 13 Epoch: 36 | ANN: trainLoss: 0.3319 | trainAcc: 84.0625% (538/640)\n",
            "10 13 Epoch: 36 | ANN: trainLoss: 0.3336 | trainAcc: 84.2330% (593/704)\n",
            "11 13 Epoch: 36 | ANN: trainLoss: 0.3451 | trainAcc: 83.9844% (645/768)\n",
            "12 13 Epoch: 36 | ANN: trainLoss: 0.3538 | trainAcc: 83.9378% (648/772)\n",
            "0 4 Epoch: 36 | ANN: testLoss: 0.7165 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 36 | ANN: testLoss: 0.7057 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 36 | ANN: testLoss: 0.6297 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 36 | ANN: testLoss: 0.4733 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 37 | ANN: trainLoss: 0.2737 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 37 | ANN: trainLoss: 0.2642 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 37 | ANN: trainLoss: 0.3375 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 37 | ANN: trainLoss: 0.3648 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 37 | ANN: trainLoss: 0.3545 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 37 | ANN: trainLoss: 0.3546 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 37 | ANN: trainLoss: 0.3633 | trainAcc: 85.9375% (385/448)\n",
            "7 13 Epoch: 37 | ANN: trainLoss: 0.3644 | trainAcc: 86.1328% (441/512)\n",
            "8 13 Epoch: 37 | ANN: trainLoss: 0.3658 | trainAcc: 85.9375% (495/576)\n",
            "9 13 Epoch: 37 | ANN: trainLoss: 0.3607 | trainAcc: 86.4062% (553/640)\n",
            "10 13 Epoch: 37 | ANN: trainLoss: 0.3547 | trainAcc: 86.7898% (611/704)\n",
            "11 13 Epoch: 37 | ANN: trainLoss: 0.3477 | trainAcc: 86.7188% (666/768)\n",
            "12 13 Epoch: 37 | ANN: trainLoss: 0.4046 | trainAcc: 86.5285% (668/772)\n",
            "0 4 Epoch: 37 | ANN: testLoss: 0.5145 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 37 | ANN: testLoss: 0.6378 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 37 | ANN: testLoss: 0.6538 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 37 | ANN: testLoss: 0.7019 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 38 | ANN: trainLoss: 0.3466 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 38 | ANN: trainLoss: 0.3301 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 38 | ANN: trainLoss: 0.3751 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 38 | ANN: trainLoss: 0.3891 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 38 | ANN: trainLoss: 0.3648 | trainAcc: 85.3125% (273/320)\n",
            "5 13 Epoch: 38 | ANN: trainLoss: 0.3531 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 38 | ANN: trainLoss: 0.3576 | trainAcc: 84.3750% (378/448)\n",
            "7 13 Epoch: 38 | ANN: trainLoss: 0.3442 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 38 | ANN: trainLoss: 0.3352 | trainAcc: 85.5903% (493/576)\n",
            "9 13 Epoch: 38 | ANN: trainLoss: 0.3567 | trainAcc: 84.5312% (541/640)\n",
            "10 13 Epoch: 38 | ANN: trainLoss: 0.3524 | trainAcc: 84.6591% (596/704)\n",
            "11 13 Epoch: 38 | ANN: trainLoss: 0.3499 | trainAcc: 84.3750% (648/768)\n",
            "12 13 Epoch: 38 | ANN: trainLoss: 0.3434 | trainAcc: 84.4560% (652/772)\n",
            "0 4 Epoch: 38 | ANN: testLoss: 0.6762 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 38 | ANN: testLoss: 0.6762 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 38 | ANN: testLoss: 0.7254 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 38 | ANN: testLoss: 0.9834 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 39 | ANN: trainLoss: 0.3026 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 39 | ANN: trainLoss: 0.2915 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 39 | ANN: trainLoss: 0.2664 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 39 | ANN: trainLoss: 0.3109 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 39 | ANN: trainLoss: 0.3029 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 39 | ANN: trainLoss: 0.2959 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 39 | ANN: trainLoss: 0.3058 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 39 | ANN: trainLoss: 0.3175 | trainAcc: 86.7188% (444/512)\n",
            "8 13 Epoch: 39 | ANN: trainLoss: 0.3177 | trainAcc: 86.4583% (498/576)\n",
            "9 13 Epoch: 39 | ANN: trainLoss: 0.3065 | trainAcc: 86.8750% (556/640)\n",
            "10 13 Epoch: 39 | ANN: trainLoss: 0.3009 | trainAcc: 87.2159% (614/704)\n",
            "11 13 Epoch: 39 | ANN: trainLoss: 0.3011 | trainAcc: 87.1094% (669/768)\n",
            "12 13 Epoch: 39 | ANN: trainLoss: 0.3264 | trainAcc: 86.9171% (671/772)\n",
            "0 4 Epoch: 39 | ANN: testLoss: 0.5682 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 39 | ANN: testLoss: 0.6436 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 39 | ANN: testLoss: 0.7017 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 39 | ANN: testLoss: 0.6102 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 40 | ANN: trainLoss: 0.3139 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 40 | ANN: trainLoss: 0.2581 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 40 | ANN: trainLoss: 0.2560 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 40 | ANN: trainLoss: 0.2605 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 40 | ANN: trainLoss: 0.2887 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 40 | ANN: trainLoss: 0.2817 | trainAcc: 90.3646% (347/384)\n",
            "6 13 Epoch: 40 | ANN: trainLoss: 0.2763 | trainAcc: 90.4018% (405/448)\n",
            "7 13 Epoch: 40 | ANN: trainLoss: 0.2904 | trainAcc: 89.0625% (456/512)\n",
            "8 13 Epoch: 40 | ANN: trainLoss: 0.2883 | trainAcc: 89.0625% (513/576)\n",
            "9 13 Epoch: 40 | ANN: trainLoss: 0.2815 | trainAcc: 89.2188% (571/640)\n",
            "10 13 Epoch: 40 | ANN: trainLoss: 0.2782 | trainAcc: 89.2045% (628/704)\n",
            "11 13 Epoch: 40 | ANN: trainLoss: 0.2804 | trainAcc: 88.8021% (682/768)\n",
            "12 13 Epoch: 40 | ANN: trainLoss: 0.2938 | trainAcc: 88.7306% (685/772)\n",
            "0 4 Epoch: 40 | ANN: testLoss: 0.4964 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 40 | ANN: testLoss: 0.5822 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 40 | ANN: testLoss: 0.5624 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 40 | ANN: testLoss: 0.4229 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 41 | ANN: trainLoss: 0.2333 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 41 | ANN: trainLoss: 0.2619 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 41 | ANN: trainLoss: 0.2486 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 41 | ANN: trainLoss: 0.2637 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 41 | ANN: trainLoss: 0.2747 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 41 | ANN: trainLoss: 0.2748 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 41 | ANN: trainLoss: 0.2805 | trainAcc: 87.2768% (391/448)\n",
            "7 13 Epoch: 41 | ANN: trainLoss: 0.2844 | trainAcc: 86.9141% (445/512)\n",
            "8 13 Epoch: 41 | ANN: trainLoss: 0.2805 | trainAcc: 87.1528% (502/576)\n",
            "9 13 Epoch: 41 | ANN: trainLoss: 0.2813 | trainAcc: 87.3438% (559/640)\n",
            "10 13 Epoch: 41 | ANN: trainLoss: 0.2860 | trainAcc: 86.9318% (612/704)\n",
            "11 13 Epoch: 41 | ANN: trainLoss: 0.2823 | trainAcc: 87.2396% (670/768)\n",
            "12 13 Epoch: 41 | ANN: trainLoss: 0.2722 | trainAcc: 87.3057% (674/772)\n",
            "0 4 Epoch: 41 | ANN: testLoss: 0.5004 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 41 | ANN: testLoss: 0.5743 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 41 | ANN: testLoss: 0.6686 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 41 | ANN: testLoss: 1.5586 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 42 | ANN: trainLoss: 0.2688 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 42 | ANN: trainLoss: 0.2898 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 42 | ANN: trainLoss: 0.2930 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 42 | ANN: trainLoss: 0.2842 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 42 | ANN: trainLoss: 0.2839 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 42 | ANN: trainLoss: 0.2820 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 42 | ANN: trainLoss: 0.2822 | trainAcc: 88.3929% (396/448)\n",
            "7 13 Epoch: 42 | ANN: trainLoss: 0.2734 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 42 | ANN: trainLoss: 0.2836 | trainAcc: 88.7153% (511/576)\n",
            "9 13 Epoch: 42 | ANN: trainLoss: 0.2892 | trainAcc: 87.9688% (563/640)\n",
            "10 13 Epoch: 42 | ANN: trainLoss: 0.2883 | trainAcc: 88.2102% (621/704)\n",
            "11 13 Epoch: 42 | ANN: trainLoss: 0.2923 | trainAcc: 87.3698% (671/768)\n",
            "12 13 Epoch: 42 | ANN: trainLoss: 0.5262 | trainAcc: 86.9171% (671/772)\n",
            "0 4 Epoch: 42 | ANN: testLoss: 0.8583 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 42 | ANN: testLoss: 0.7262 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 42 | ANN: testLoss: 0.7394 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 42 | ANN: testLoss: 0.5639 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 43 | ANN: trainLoss: 0.1800 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 43 | ANN: trainLoss: 0.2151 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 43 | ANN: trainLoss: 0.2238 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 43 | ANN: trainLoss: 0.2176 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 43 | ANN: trainLoss: 0.2312 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 43 | ANN: trainLoss: 0.2433 | trainAcc: 90.6250% (348/384)\n",
            "6 13 Epoch: 43 | ANN: trainLoss: 0.2541 | trainAcc: 89.5089% (401/448)\n",
            "7 13 Epoch: 43 | ANN: trainLoss: 0.2524 | trainAcc: 89.8438% (460/512)\n",
            "8 13 Epoch: 43 | ANN: trainLoss: 0.2629 | trainAcc: 89.4097% (515/576)\n",
            "9 13 Epoch: 43 | ANN: trainLoss: 0.2532 | trainAcc: 90.3125% (578/640)\n",
            "10 13 Epoch: 43 | ANN: trainLoss: 0.2555 | trainAcc: 90.1989% (635/704)\n",
            "11 13 Epoch: 43 | ANN: trainLoss: 0.2548 | trainAcc: 90.3646% (694/768)\n",
            "12 13 Epoch: 43 | ANN: trainLoss: 0.2627 | trainAcc: 90.2850% (697/772)\n",
            "0 4 Epoch: 43 | ANN: testLoss: 0.5665 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 43 | ANN: testLoss: 0.5628 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 43 | ANN: testLoss: 0.6054 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 43 | ANN: testLoss: 0.6452 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 44 | ANN: trainLoss: 0.2443 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 44 | ANN: trainLoss: 0.1999 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 44 | ANN: trainLoss: 0.2125 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 44 | ANN: trainLoss: 0.2830 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 44 | ANN: trainLoss: 0.2894 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 44 | ANN: trainLoss: 0.2974 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 44 | ANN: trainLoss: 0.3008 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 44 | ANN: trainLoss: 0.2920 | trainAcc: 87.6953% (449/512)\n",
            "8 13 Epoch: 44 | ANN: trainLoss: 0.2968 | trainAcc: 87.3264% (503/576)\n",
            "9 13 Epoch: 44 | ANN: trainLoss: 0.2889 | trainAcc: 87.9688% (563/640)\n",
            "10 13 Epoch: 44 | ANN: trainLoss: 0.2825 | trainAcc: 88.4943% (623/704)\n",
            "11 13 Epoch: 44 | ANN: trainLoss: 0.2771 | trainAcc: 88.9323% (683/768)\n",
            "12 13 Epoch: 44 | ANN: trainLoss: 0.2736 | trainAcc: 88.9896% (687/772)\n",
            "0 4 Epoch: 44 | ANN: testLoss: 0.8211 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 44 | ANN: testLoss: 0.7253 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 44 | ANN: testLoss: 0.7114 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 44 | ANN: testLoss: 0.5336 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 45 | ANN: trainLoss: 0.2486 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 45 | ANN: trainLoss: 0.2881 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 45 | ANN: trainLoss: 0.2711 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 45 | ANN: trainLoss: 0.2447 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 45 | ANN: trainLoss: 0.2394 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 45 | ANN: trainLoss: 0.2509 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 45 | ANN: trainLoss: 0.2562 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 45 | ANN: trainLoss: 0.2602 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 45 | ANN: trainLoss: 0.2454 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 45 | ANN: trainLoss: 0.2471 | trainAcc: 89.8438% (575/640)\n",
            "10 13 Epoch: 45 | ANN: trainLoss: 0.2485 | trainAcc: 89.9148% (633/704)\n",
            "11 13 Epoch: 45 | ANN: trainLoss: 0.2461 | trainAcc: 89.9740% (691/768)\n",
            "12 13 Epoch: 45 | ANN: trainLoss: 0.2409 | trainAcc: 90.0259% (695/772)\n",
            "0 4 Epoch: 45 | ANN: testLoss: 0.4590 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 45 | ANN: testLoss: 0.5776 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 45 | ANN: testLoss: 0.6450 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 45 | ANN: testLoss: 0.4837 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 46 | ANN: trainLoss: 0.2748 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 46 | ANN: trainLoss: 0.3304 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 46 | ANN: trainLoss: 0.3393 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 46 | ANN: trainLoss: 0.3386 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 46 | ANN: trainLoss: 0.3051 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 46 | ANN: trainLoss: 0.2788 | trainAcc: 89.5833% (344/384)\n",
            "6 13 Epoch: 46 | ANN: trainLoss: 0.2680 | trainAcc: 90.1786% (404/448)\n",
            "7 13 Epoch: 46 | ANN: trainLoss: 0.2595 | trainAcc: 90.4297% (463/512)\n",
            "8 13 Epoch: 46 | ANN: trainLoss: 0.2567 | trainAcc: 90.4514% (521/576)\n",
            "9 13 Epoch: 46 | ANN: trainLoss: 0.2506 | trainAcc: 90.9375% (582/640)\n",
            "10 13 Epoch: 46 | ANN: trainLoss: 0.2550 | trainAcc: 90.4830% (637/704)\n",
            "11 13 Epoch: 46 | ANN: trainLoss: 0.2609 | trainAcc: 90.2344% (693/768)\n",
            "12 13 Epoch: 46 | ANN: trainLoss: 0.2652 | trainAcc: 90.2850% (697/772)\n",
            "0 4 Epoch: 46 | ANN: testLoss: 0.8691 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 46 | ANN: testLoss: 0.7545 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 46 | ANN: testLoss: 0.7441 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 46 | ANN: testLoss: 0.7520 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 47 | ANN: trainLoss: 0.3081 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 47 | ANN: trainLoss: 0.2804 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 47 | ANN: trainLoss: 0.2736 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 47 | ANN: trainLoss: 0.2572 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 47 | ANN: trainLoss: 0.2382 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 47 | ANN: trainLoss: 0.2256 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 47 | ANN: trainLoss: 0.2316 | trainAcc: 91.2946% (409/448)\n",
            "7 13 Epoch: 47 | ANN: trainLoss: 0.2246 | trainAcc: 91.9922% (471/512)\n",
            "8 13 Epoch: 47 | ANN: trainLoss: 0.2245 | trainAcc: 92.0139% (530/576)\n",
            "9 13 Epoch: 47 | ANN: trainLoss: 0.2274 | trainAcc: 91.8750% (588/640)\n",
            "10 13 Epoch: 47 | ANN: trainLoss: 0.2293 | trainAcc: 91.3352% (643/704)\n",
            "11 13 Epoch: 47 | ANN: trainLoss: 0.2290 | trainAcc: 91.0156% (699/768)\n",
            "12 13 Epoch: 47 | ANN: trainLoss: 0.2331 | trainAcc: 90.9326% (702/772)\n",
            "0 4 Epoch: 47 | ANN: testLoss: 0.5985 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 47 | ANN: testLoss: 0.5787 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 47 | ANN: testLoss: 0.6234 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 47 | ANN: testLoss: 0.4712 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 48 | ANN: trainLoss: 0.1525 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 48 | ANN: trainLoss: 0.1555 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 48 | ANN: trainLoss: 0.2021 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 48 | ANN: trainLoss: 0.2037 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 48 | ANN: trainLoss: 0.1947 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 48 | ANN: trainLoss: 0.1923 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 48 | ANN: trainLoss: 0.2012 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 48 | ANN: trainLoss: 0.2040 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 48 | ANN: trainLoss: 0.2152 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 48 | ANN: trainLoss: 0.2184 | trainAcc: 91.2500% (584/640)\n",
            "10 13 Epoch: 48 | ANN: trainLoss: 0.2227 | trainAcc: 90.4830% (637/704)\n",
            "11 13 Epoch: 48 | ANN: trainLoss: 0.2152 | trainAcc: 90.8854% (698/768)\n",
            "12 13 Epoch: 48 | ANN: trainLoss: 0.2401 | trainAcc: 90.8031% (701/772)\n",
            "0 4 Epoch: 48 | ANN: testLoss: 0.5015 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 48 | ANN: testLoss: 0.6360 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 48 | ANN: testLoss: 0.6682 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 48 | ANN: testLoss: 0.8848 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 49 | ANN: trainLoss: 0.1946 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 49 | ANN: trainLoss: 0.2032 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 49 | ANN: trainLoss: 0.2310 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 49 | ANN: trainLoss: 0.2309 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 49 | ANN: trainLoss: 0.2233 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 49 | ANN: trainLoss: 0.2322 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 49 | ANN: trainLoss: 0.2244 | trainAcc: 89.7321% (402/448)\n",
            "7 13 Epoch: 49 | ANN: trainLoss: 0.2393 | trainAcc: 89.0625% (456/512)\n",
            "8 13 Epoch: 49 | ANN: trainLoss: 0.2380 | trainAcc: 89.2361% (514/576)\n",
            "9 13 Epoch: 49 | ANN: trainLoss: 0.2334 | trainAcc: 89.6875% (574/640)\n",
            "10 13 Epoch: 49 | ANN: trainLoss: 0.2289 | trainAcc: 90.0568% (634/704)\n",
            "11 13 Epoch: 49 | ANN: trainLoss: 0.2281 | trainAcc: 89.9740% (691/768)\n",
            "12 13 Epoch: 49 | ANN: trainLoss: 0.2145 | trainAcc: 90.0259% (695/772)\n",
            "0 4 Epoch: 49 | ANN: testLoss: 0.5568 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 49 | ANN: testLoss: 0.6943 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 49 | ANN: testLoss: 0.7318 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 49 | ANN: testLoss: 0.5491 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 50 | ANN: trainLoss: 0.1907 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 50 | ANN: trainLoss: 0.1775 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 50 | ANN: trainLoss: 0.1801 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 50 | ANN: trainLoss: 0.1717 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 50 | ANN: trainLoss: 0.1881 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 50 | ANN: trainLoss: 0.1850 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 50 | ANN: trainLoss: 0.1976 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 50 | ANN: trainLoss: 0.1894 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 50 | ANN: trainLoss: 0.2172 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 50 | ANN: trainLoss: 0.2170 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 50 | ANN: trainLoss: 0.2196 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 50 | ANN: trainLoss: 0.2180 | trainAcc: 93.0990% (715/768)\n",
            "12 13 Epoch: 50 | ANN: trainLoss: 0.2139 | trainAcc: 93.1347% (719/772)\n",
            "0 4 Epoch: 50 | ANN: testLoss: 0.6411 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 50 | ANN: testLoss: 0.7290 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 50 | ANN: testLoss: 0.7397 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 50 | ANN: testLoss: 0.5887 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 51 | ANN: trainLoss: 0.1994 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 51 | ANN: trainLoss: 0.2024 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 51 | ANN: trainLoss: 0.1841 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 51 | ANN: trainLoss: 0.1759 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 51 | ANN: trainLoss: 0.1785 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 51 | ANN: trainLoss: 0.1807 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 51 | ANN: trainLoss: 0.1808 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 51 | ANN: trainLoss: 0.1822 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 51 | ANN: trainLoss: 0.1821 | trainAcc: 92.8819% (535/576)\n",
            "9 13 Epoch: 51 | ANN: trainLoss: 0.1842 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 51 | ANN: trainLoss: 0.1846 | trainAcc: 92.6136% (652/704)\n",
            "11 13 Epoch: 51 | ANN: trainLoss: 0.1859 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 51 | ANN: trainLoss: 0.2057 | trainAcc: 92.7461% (716/772)\n",
            "0 4 Epoch: 51 | ANN: testLoss: 0.8321 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 51 | ANN: testLoss: 0.7389 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 51 | ANN: testLoss: 0.6886 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 51 | ANN: testLoss: 1.0150 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 52 | ANN: trainLoss: 0.2270 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 52 | ANN: trainLoss: 0.1846 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 52 | ANN: trainLoss: 0.1749 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 52 | ANN: trainLoss: 0.1832 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 52 | ANN: trainLoss: 0.1807 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 52 | ANN: trainLoss: 0.1850 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 52 | ANN: trainLoss: 0.1888 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 52 | ANN: trainLoss: 0.1903 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 52 | ANN: trainLoss: 0.1978 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 52 | ANN: trainLoss: 0.2054 | trainAcc: 92.0312% (589/640)\n",
            "10 13 Epoch: 52 | ANN: trainLoss: 0.2072 | trainAcc: 91.6193% (645/704)\n",
            "11 13 Epoch: 52 | ANN: trainLoss: 0.2026 | trainAcc: 91.7969% (705/768)\n",
            "12 13 Epoch: 52 | ANN: trainLoss: 0.2272 | trainAcc: 91.7098% (708/772)\n",
            "0 4 Epoch: 52 | ANN: testLoss: 0.8335 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 52 | ANN: testLoss: 0.8213 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 52 | ANN: testLoss: 0.7926 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 52 | ANN: testLoss: 0.6606 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 53 | ANN: trainLoss: 0.2009 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 53 | ANN: trainLoss: 0.1431 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 53 | ANN: trainLoss: 0.1670 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 53 | ANN: trainLoss: 0.1748 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 53 | ANN: trainLoss: 0.1848 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 53 | ANN: trainLoss: 0.1770 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 53 | ANN: trainLoss: 0.1753 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 53 | ANN: trainLoss: 0.1686 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 53 | ANN: trainLoss: 0.1726 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 53 | ANN: trainLoss: 0.1774 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 53 | ANN: trainLoss: 0.1756 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 53 | ANN: trainLoss: 0.1734 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 53 | ANN: trainLoss: 0.1766 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 53 | ANN: testLoss: 0.7633 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 53 | ANN: testLoss: 0.8398 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 53 | ANN: testLoss: 0.8439 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 53 | ANN: testLoss: 0.6329 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 54 | ANN: trainLoss: 0.2294 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 54 | ANN: trainLoss: 0.2052 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 54 | ANN: trainLoss: 0.1823 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 54 | ANN: trainLoss: 0.1881 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 54 | ANN: trainLoss: 0.1861 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 54 | ANN: trainLoss: 0.1884 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 54 | ANN: trainLoss: 0.1913 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 54 | ANN: trainLoss: 0.1916 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 54 | ANN: trainLoss: 0.1851 | trainAcc: 93.5764% (539/576)\n",
            "9 13 Epoch: 54 | ANN: trainLoss: 0.1844 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 54 | ANN: trainLoss: 0.1836 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 54 | ANN: trainLoss: 0.1812 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 54 | ANN: trainLoss: 0.1688 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 54 | ANN: testLoss: 0.6936 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 54 | ANN: testLoss: 0.7821 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 54 | ANN: testLoss: 0.7475 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 54 | ANN: testLoss: 0.5606 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 55 | ANN: trainLoss: 0.1935 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 55 | ANN: trainLoss: 0.1749 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 55 | ANN: trainLoss: 0.1933 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 55 | ANN: trainLoss: 0.1824 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 55 | ANN: trainLoss: 0.1821 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 55 | ANN: trainLoss: 0.1795 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 55 | ANN: trainLoss: 0.1801 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 55 | ANN: trainLoss: 0.1765 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 55 | ANN: trainLoss: 0.1834 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 55 | ANN: trainLoss: 0.1832 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 55 | ANN: trainLoss: 0.1735 | trainAcc: 93.6080% (659/704)\n",
            "11 13 Epoch: 55 | ANN: trainLoss: 0.1717 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 55 | ANN: trainLoss: 0.1763 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 55 | ANN: testLoss: 0.7137 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 55 | ANN: testLoss: 0.6515 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 55 | ANN: testLoss: 0.6724 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 55 | ANN: testLoss: 0.5043 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 56 | ANN: trainLoss: 0.1710 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 56 | ANN: trainLoss: 0.2166 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 56 | ANN: trainLoss: 0.1848 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 56 | ANN: trainLoss: 0.1760 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 56 | ANN: trainLoss: 0.1874 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 56 | ANN: trainLoss: 0.1869 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 56 | ANN: trainLoss: 0.1762 | trainAcc: 92.4107% (414/448)\n",
            "7 13 Epoch: 56 | ANN: trainLoss: 0.1796 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 56 | ANN: trainLoss: 0.1816 | trainAcc: 92.8819% (535/576)\n",
            "9 13 Epoch: 56 | ANN: trainLoss: 0.1802 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 56 | ANN: trainLoss: 0.1741 | trainAcc: 93.6080% (659/704)\n",
            "11 13 Epoch: 56 | ANN: trainLoss: 0.1710 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 56 | ANN: trainLoss: 0.2221 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 56 | ANN: testLoss: 0.4859 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 56 | ANN: testLoss: 0.5962 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 56 | ANN: testLoss: 0.7126 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 56 | ANN: testLoss: 0.5372 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 57 | ANN: trainLoss: 0.1509 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 57 | ANN: trainLoss: 0.1470 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 57 | ANN: trainLoss: 0.1598 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 57 | ANN: trainLoss: 0.1864 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 57 | ANN: trainLoss: 0.1730 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 57 | ANN: trainLoss: 0.1760 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 57 | ANN: trainLoss: 0.1780 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 57 | ANN: trainLoss: 0.1786 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 57 | ANN: trainLoss: 0.1772 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 57 | ANN: trainLoss: 0.1817 | trainAcc: 94.3750% (604/640)\n",
            "10 13 Epoch: 57 | ANN: trainLoss: 0.1881 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 57 | ANN: trainLoss: 0.1882 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 57 | ANN: trainLoss: 0.1852 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 57 | ANN: testLoss: 0.6364 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 57 | ANN: testLoss: 0.6874 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 57 | ANN: testLoss: 0.7568 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 57 | ANN: testLoss: 0.5679 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 58 | ANN: trainLoss: 0.1451 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 58 | ANN: trainLoss: 0.1607 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 58 | ANN: trainLoss: 0.1749 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 58 | ANN: trainLoss: 0.1730 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 58 | ANN: trainLoss: 0.1825 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 58 | ANN: trainLoss: 0.1961 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 58 | ANN: trainLoss: 0.2040 | trainAcc: 90.6250% (406/448)\n",
            "7 13 Epoch: 58 | ANN: trainLoss: 0.1960 | trainAcc: 91.0156% (466/512)\n",
            "8 13 Epoch: 58 | ANN: trainLoss: 0.2025 | trainAcc: 90.9722% (524/576)\n",
            "9 13 Epoch: 58 | ANN: trainLoss: 0.2009 | trainAcc: 90.7812% (581/640)\n",
            "10 13 Epoch: 58 | ANN: trainLoss: 0.1998 | trainAcc: 90.9091% (640/704)\n",
            "11 13 Epoch: 58 | ANN: trainLoss: 0.1953 | trainAcc: 91.1458% (700/768)\n",
            "12 13 Epoch: 58 | ANN: trainLoss: 0.4040 | trainAcc: 90.8031% (701/772)\n",
            "0 4 Epoch: 58 | ANN: testLoss: 0.5699 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 58 | ANN: testLoss: 0.7022 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 58 | ANN: testLoss: 0.7008 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 58 | ANN: testLoss: 1.3605 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 59 | ANN: trainLoss: 0.1620 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 59 | ANN: trainLoss: 0.1254 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 59 | ANN: trainLoss: 0.1208 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 59 | ANN: trainLoss: 0.1239 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 59 | ANN: trainLoss: 0.1309 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 59 | ANN: trainLoss: 0.1336 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 59 | ANN: trainLoss: 0.1411 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 59 | ANN: trainLoss: 0.1591 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 59 | ANN: trainLoss: 0.1594 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 59 | ANN: trainLoss: 0.1674 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 59 | ANN: trainLoss: 0.1716 | trainAcc: 92.6136% (652/704)\n",
            "11 13 Epoch: 59 | ANN: trainLoss: 0.1803 | trainAcc: 92.1875% (708/768)\n",
            "12 13 Epoch: 59 | ANN: trainLoss: 0.2527 | trainAcc: 92.0984% (711/772)\n",
            "0 4 Epoch: 59 | ANN: testLoss: 0.8299 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 59 | ANN: testLoss: 0.7885 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 59 | ANN: testLoss: 0.7208 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 59 | ANN: testLoss: 0.5448 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 60 | ANN: trainLoss: 0.2012 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 60 | ANN: trainLoss: 0.1847 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 60 | ANN: trainLoss: 0.1918 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 60 | ANN: trainLoss: 0.1811 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 60 | ANN: trainLoss: 0.1694 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 60 | ANN: trainLoss: 0.1606 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 60 | ANN: trainLoss: 0.1626 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 60 | ANN: trainLoss: 0.1704 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 60 | ANN: trainLoss: 0.1689 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 60 | ANN: trainLoss: 0.1702 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 60 | ANN: trainLoss: 0.1783 | trainAcc: 92.6136% (652/704)\n",
            "11 13 Epoch: 60 | ANN: trainLoss: 0.1858 | trainAcc: 92.5781% (711/768)\n",
            "12 13 Epoch: 60 | ANN: trainLoss: 0.2578 | trainAcc: 92.3575% (713/772)\n",
            "0 4 Epoch: 60 | ANN: testLoss: 0.6739 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 60 | ANN: testLoss: 0.7013 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 60 | ANN: testLoss: 0.7369 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 60 | ANN: testLoss: 0.7042 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 61 | ANN: trainLoss: 0.1606 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 61 | ANN: trainLoss: 0.1540 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 61 | ANN: trainLoss: 0.1695 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 61 | ANN: trainLoss: 0.1640 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 61 | ANN: trainLoss: 0.1697 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 61 | ANN: trainLoss: 0.1680 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 61 | ANN: trainLoss: 0.1690 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 61 | ANN: trainLoss: 0.1696 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 61 | ANN: trainLoss: 0.1672 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 61 | ANN: trainLoss: 0.1663 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 61 | ANN: trainLoss: 0.1640 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 61 | ANN: trainLoss: 0.1700 | trainAcc: 94.2708% (724/768)\n",
            "12 13 Epoch: 61 | ANN: trainLoss: 0.1612 | trainAcc: 94.3005% (728/772)\n",
            "0 4 Epoch: 61 | ANN: testLoss: 0.6706 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 61 | ANN: testLoss: 0.9109 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 61 | ANN: testLoss: 0.8925 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 61 | ANN: testLoss: 1.0194 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 62 | ANN: trainLoss: 0.2732 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 62 | ANN: trainLoss: 0.1850 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 62 | ANN: trainLoss: 0.1717 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 62 | ANN: trainLoss: 0.1709 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 62 | ANN: trainLoss: 0.1805 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 62 | ANN: trainLoss: 0.1899 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 62 | ANN: trainLoss: 0.1867 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 62 | ANN: trainLoss: 0.1780 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 62 | ANN: trainLoss: 0.1748 | trainAcc: 94.0972% (542/576)\n",
            "9 13 Epoch: 62 | ANN: trainLoss: 0.1713 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 62 | ANN: trainLoss: 0.1743 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 62 | ANN: trainLoss: 0.1757 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 62 | ANN: trainLoss: 0.2435 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 62 | ANN: testLoss: 0.8379 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 62 | ANN: testLoss: 0.7299 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 62 | ANN: testLoss: 0.7908 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 62 | ANN: testLoss: 0.6798 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 63 | ANN: trainLoss: 0.1139 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 63 | ANN: trainLoss: 0.1201 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 63 | ANN: trainLoss: 0.1148 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 63 | ANN: trainLoss: 0.1573 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 63 | ANN: trainLoss: 0.1478 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 63 | ANN: trainLoss: 0.1700 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 63 | ANN: trainLoss: 0.1715 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 63 | ANN: trainLoss: 0.1838 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 63 | ANN: trainLoss: 0.1928 | trainAcc: 92.5347% (533/576)\n",
            "9 13 Epoch: 63 | ANN: trainLoss: 0.1973 | trainAcc: 92.5000% (592/640)\n",
            "10 13 Epoch: 63 | ANN: trainLoss: 0.1929 | trainAcc: 92.6136% (652/704)\n",
            "11 13 Epoch: 63 | ANN: trainLoss: 0.1854 | trainAcc: 93.0990% (715/768)\n",
            "12 13 Epoch: 63 | ANN: trainLoss: 0.1817 | trainAcc: 93.1347% (719/772)\n",
            "0 4 Epoch: 63 | ANN: testLoss: 0.8110 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 63 | ANN: testLoss: 0.7912 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 63 | ANN: testLoss: 0.7986 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 63 | ANN: testLoss: 0.6003 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 64 | ANN: trainLoss: 0.2154 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 64 | ANN: trainLoss: 0.1525 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 64 | ANN: trainLoss: 0.1523 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 64 | ANN: trainLoss: 0.1577 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 64 | ANN: trainLoss: 0.1612 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 64 | ANN: trainLoss: 0.1739 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 64 | ANN: trainLoss: 0.1786 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 64 | ANN: trainLoss: 0.1664 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 64 | ANN: trainLoss: 0.1627 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 64 | ANN: trainLoss: 0.1600 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 64 | ANN: trainLoss: 0.1611 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 64 | ANN: trainLoss: 0.1612 | trainAcc: 94.0104% (722/768)\n",
            "12 13 Epoch: 64 | ANN: trainLoss: 0.1855 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 64 | ANN: testLoss: 0.9605 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 64 | ANN: testLoss: 0.8448 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 64 | ANN: testLoss: 0.8144 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 64 | ANN: testLoss: 0.6113 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 65 | ANN: trainLoss: 0.0825 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 65 | ANN: trainLoss: 0.0815 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 65 | ANN: trainLoss: 0.0990 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 65 | ANN: trainLoss: 0.1182 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 65 | ANN: trainLoss: 0.1263 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 65 | ANN: trainLoss: 0.1337 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 65 | ANN: trainLoss: 0.1345 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 65 | ANN: trainLoss: 0.1402 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 65 | ANN: trainLoss: 0.1586 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 65 | ANN: trainLoss: 0.1529 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 65 | ANN: trainLoss: 0.1496 | trainAcc: 95.0284% (669/704)\n",
            "11 13 Epoch: 65 | ANN: trainLoss: 0.1529 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 65 | ANN: trainLoss: 0.2223 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 65 | ANN: testLoss: 0.6497 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 65 | ANN: testLoss: 0.7767 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 65 | ANN: testLoss: 0.7851 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 65 | ANN: testLoss: 0.6556 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 66 | ANN: trainLoss: 0.1318 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 66 | ANN: trainLoss: 0.1193 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 66 | ANN: trainLoss: 0.1126 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 66 | ANN: trainLoss: 0.1324 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 66 | ANN: trainLoss: 0.1545 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 66 | ANN: trainLoss: 0.1426 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 66 | ANN: trainLoss: 0.1536 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 66 | ANN: trainLoss: 0.1493 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 66 | ANN: trainLoss: 0.1460 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 66 | ANN: trainLoss: 0.1449 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 66 | ANN: trainLoss: 0.1499 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 66 | ANN: trainLoss: 0.1509 | trainAcc: 95.0521% (730/768)\n",
            "12 13 Epoch: 66 | ANN: trainLoss: 0.4103 | trainAcc: 94.5596% (730/772)\n",
            "0 4 Epoch: 66 | ANN: testLoss: 0.8474 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 66 | ANN: testLoss: 0.7972 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 66 | ANN: testLoss: 0.7473 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 66 | ANN: testLoss: 0.5662 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 67 | ANN: trainLoss: 0.0959 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 67 | ANN: trainLoss: 0.1107 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 67 | ANN: trainLoss: 0.1185 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 67 | ANN: trainLoss: 0.1276 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 67 | ANN: trainLoss: 0.1277 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 67 | ANN: trainLoss: 0.1418 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 67 | ANN: trainLoss: 0.1511 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 67 | ANN: trainLoss: 0.1518 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 67 | ANN: trainLoss: 0.1541 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 67 | ANN: trainLoss: 0.1568 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 67 | ANN: trainLoss: 0.1557 | trainAcc: 94.3182% (664/704)\n",
            "11 13 Epoch: 67 | ANN: trainLoss: 0.1548 | trainAcc: 94.4010% (725/768)\n",
            "12 13 Epoch: 67 | ANN: trainLoss: 0.1722 | trainAcc: 94.4301% (729/772)\n",
            "0 4 Epoch: 67 | ANN: testLoss: 0.7693 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 67 | ANN: testLoss: 0.7792 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 67 | ANN: testLoss: 0.8802 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 67 | ANN: testLoss: 0.7094 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 68 | ANN: trainLoss: 0.1481 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 68 | ANN: trainLoss: 0.1660 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 68 | ANN: trainLoss: 0.1675 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 68 | ANN: trainLoss: 0.1650 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 68 | ANN: trainLoss: 0.1619 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 68 | ANN: trainLoss: 0.1631 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 68 | ANN: trainLoss: 0.1660 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 68 | ANN: trainLoss: 0.1637 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 68 | ANN: trainLoss: 0.1537 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 68 | ANN: trainLoss: 0.1666 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 68 | ANN: trainLoss: 0.1660 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 68 | ANN: trainLoss: 0.1607 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 68 | ANN: trainLoss: 0.1511 | trainAcc: 94.1710% (727/772)\n",
            "0 4 Epoch: 68 | ANN: testLoss: 0.4963 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 68 | ANN: testLoss: 0.7640 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 68 | ANN: testLoss: 0.7493 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 68 | ANN: testLoss: 0.6567 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 69 | ANN: trainLoss: 0.1655 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 69 | ANN: trainLoss: 0.1211 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 69 | ANN: trainLoss: 0.1210 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 69 | ANN: trainLoss: 0.1363 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 69 | ANN: trainLoss: 0.1301 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 69 | ANN: trainLoss: 0.1360 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 69 | ANN: trainLoss: 0.1364 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 69 | ANN: trainLoss: 0.1406 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 69 | ANN: trainLoss: 0.1477 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 69 | ANN: trainLoss: 0.1456 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 69 | ANN: trainLoss: 0.1456 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 69 | ANN: trainLoss: 0.1485 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 69 | ANN: trainLoss: 0.1497 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 69 | ANN: testLoss: 0.7476 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 69 | ANN: testLoss: 0.7612 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 69 | ANN: testLoss: 0.7366 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 69 | ANN: testLoss: 0.5585 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 70 | ANN: trainLoss: 0.0875 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 70 | ANN: trainLoss: 0.1018 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 70 | ANN: trainLoss: 0.1204 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 70 | ANN: trainLoss: 0.1172 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 70 | ANN: trainLoss: 0.1227 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 70 | ANN: trainLoss: 0.1207 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 70 | ANN: trainLoss: 0.1168 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 70 | ANN: trainLoss: 0.1182 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 70 | ANN: trainLoss: 0.1187 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 70 | ANN: trainLoss: 0.1251 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 70 | ANN: trainLoss: 0.1295 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 70 | ANN: trainLoss: 0.1273 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 70 | ANN: trainLoss: 0.2015 | trainAcc: 96.7617% (747/772)\n",
            "0 4 Epoch: 70 | ANN: testLoss: 0.8000 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 70 | ANN: testLoss: 0.8919 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 70 | ANN: testLoss: 0.7814 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 70 | ANN: testLoss: 0.5964 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 71 | ANN: trainLoss: 0.1498 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 71 | ANN: trainLoss: 0.1411 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 71 | ANN: trainLoss: 0.1370 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 71 | ANN: trainLoss: 0.1368 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 71 | ANN: trainLoss: 0.1434 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 71 | ANN: trainLoss: 0.1508 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 71 | ANN: trainLoss: 0.1548 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 71 | ANN: trainLoss: 0.1530 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 71 | ANN: trainLoss: 0.1508 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 71 | ANN: trainLoss: 0.1489 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 71 | ANN: trainLoss: 0.1432 | trainAcc: 95.5966% (673/704)\n",
            "11 13 Epoch: 71 | ANN: trainLoss: 0.1407 | trainAcc: 95.7031% (735/768)\n",
            "12 13 Epoch: 71 | ANN: trainLoss: 0.1366 | trainAcc: 95.7254% (739/772)\n",
            "0 4 Epoch: 71 | ANN: testLoss: 0.5294 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 71 | ANN: testLoss: 0.7375 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 71 | ANN: testLoss: 0.8689 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 71 | ANN: testLoss: 0.9542 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 72 | ANN: trainLoss: 0.1236 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 72 | ANN: trainLoss: 0.1225 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 72 | ANN: trainLoss: 0.1292 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 72 | ANN: trainLoss: 0.1384 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 72 | ANN: trainLoss: 0.1423 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 72 | ANN: trainLoss: 0.1457 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 72 | ANN: trainLoss: 0.1495 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 72 | ANN: trainLoss: 0.1616 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 72 | ANN: trainLoss: 0.1584 | trainAcc: 94.0972% (542/576)\n",
            "9 13 Epoch: 72 | ANN: trainLoss: 0.1664 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 72 | ANN: trainLoss: 0.1644 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 72 | ANN: trainLoss: 0.1618 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 72 | ANN: trainLoss: 0.1514 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 72 | ANN: testLoss: 0.7737 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 72 | ANN: testLoss: 0.8333 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 72 | ANN: testLoss: 0.7983 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 72 | ANN: testLoss: 0.8296 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 73 | ANN: trainLoss: 0.1199 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 73 | ANN: trainLoss: 0.1096 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 73 | ANN: trainLoss: 0.1012 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 73 | ANN: trainLoss: 0.1015 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 73 | ANN: trainLoss: 0.1252 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 73 | ANN: trainLoss: 0.1332 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 73 | ANN: trainLoss: 0.1413 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 73 | ANN: trainLoss: 0.1392 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 73 | ANN: trainLoss: 0.1445 | trainAcc: 95.6597% (551/576)\n",
            "9 13 Epoch: 73 | ANN: trainLoss: 0.1547 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 73 | ANN: trainLoss: 0.1496 | trainAcc: 95.5966% (673/704)\n",
            "11 13 Epoch: 73 | ANN: trainLoss: 0.1465 | trainAcc: 95.7031% (735/768)\n",
            "12 13 Epoch: 73 | ANN: trainLoss: 0.1371 | trainAcc: 95.7254% (739/772)\n",
            "0 4 Epoch: 73 | ANN: testLoss: 0.7313 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 73 | ANN: testLoss: 0.7243 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 73 | ANN: testLoss: 0.7957 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 73 | ANN: testLoss: 1.4215 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 74 | ANN: trainLoss: 0.0645 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 74 | ANN: trainLoss: 0.0816 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 74 | ANN: trainLoss: 0.0896 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 74 | ANN: trainLoss: 0.0855 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 74 | ANN: trainLoss: 0.1109 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 74 | ANN: trainLoss: 0.1134 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 74 | ANN: trainLoss: 0.1178 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 74 | ANN: trainLoss: 0.1104 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 74 | ANN: trainLoss: 0.1209 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 74 | ANN: trainLoss: 0.1271 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 74 | ANN: trainLoss: 0.1274 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 74 | ANN: trainLoss: 0.1236 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 74 | ANN: trainLoss: 0.1207 | trainAcc: 95.9845% (741/772)\n",
            "0 4 Epoch: 74 | ANN: testLoss: 0.7424 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 74 | ANN: testLoss: 0.8240 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 74 | ANN: testLoss: 0.7855 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 74 | ANN: testLoss: 2.3383 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 75 | ANN: trainLoss: 0.1079 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 75 | ANN: trainLoss: 0.1200 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 75 | ANN: trainLoss: 0.1166 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 75 | ANN: trainLoss: 0.1340 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 75 | ANN: trainLoss: 0.1293 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 75 | ANN: trainLoss: 0.1246 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 75 | ANN: trainLoss: 0.1296 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 75 | ANN: trainLoss: 0.1367 | trainAcc: 95.3125% (488/512)\n",
            "8 13 Epoch: 75 | ANN: trainLoss: 0.1306 | trainAcc: 95.6597% (551/576)\n",
            "9 13 Epoch: 75 | ANN: trainLoss: 0.1325 | trainAcc: 95.4688% (611/640)\n",
            "10 13 Epoch: 75 | ANN: trainLoss: 0.1321 | trainAcc: 95.4545% (672/704)\n",
            "11 13 Epoch: 75 | ANN: trainLoss: 0.1340 | trainAcc: 95.3125% (732/768)\n",
            "12 13 Epoch: 75 | ANN: trainLoss: 0.2033 | trainAcc: 95.0777% (734/772)\n",
            "0 4 Epoch: 75 | ANN: testLoss: 0.8575 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 75 | ANN: testLoss: 0.9300 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 75 | ANN: testLoss: 0.8273 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 75 | ANN: testLoss: 0.6233 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 76 | ANN: trainLoss: 0.1877 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 76 | ANN: trainLoss: 0.1456 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 76 | ANN: trainLoss: 0.1349 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 76 | ANN: trainLoss: 0.1220 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 76 | ANN: trainLoss: 0.1289 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 76 | ANN: trainLoss: 0.1323 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 76 | ANN: trainLoss: 0.1301 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 76 | ANN: trainLoss: 0.1412 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 76 | ANN: trainLoss: 0.1346 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 76 | ANN: trainLoss: 0.1403 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 76 | ANN: trainLoss: 0.1378 | trainAcc: 95.4545% (672/704)\n",
            "11 13 Epoch: 76 | ANN: trainLoss: 0.1391 | trainAcc: 95.3125% (732/768)\n",
            "12 13 Epoch: 76 | ANN: trainLoss: 0.1729 | trainAcc: 95.2073% (735/772)\n",
            "0 4 Epoch: 76 | ANN: testLoss: 0.7431 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 76 | ANN: testLoss: 0.7142 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 76 | ANN: testLoss: 0.7662 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 76 | ANN: testLoss: 0.6214 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 77 | ANN: trainLoss: 0.1139 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 77 | ANN: trainLoss: 0.1093 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 77 | ANN: trainLoss: 0.1095 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 77 | ANN: trainLoss: 0.1102 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 77 | ANN: trainLoss: 0.1354 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 77 | ANN: trainLoss: 0.1374 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 77 | ANN: trainLoss: 0.1412 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 77 | ANN: trainLoss: 0.1432 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 77 | ANN: trainLoss: 0.1403 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 77 | ANN: trainLoss: 0.1395 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 77 | ANN: trainLoss: 0.1394 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 77 | ANN: trainLoss: 0.1407 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 77 | ANN: trainLoss: 0.1519 | trainAcc: 95.2073% (735/772)\n",
            "0 4 Epoch: 77 | ANN: testLoss: 0.6826 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 77 | ANN: testLoss: 0.7569 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 77 | ANN: testLoss: 0.7324 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 77 | ANN: testLoss: 0.6991 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 78 | ANN: trainLoss: 0.1209 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 78 | ANN: trainLoss: 0.1549 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 78 | ANN: trainLoss: 0.1498 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 78 | ANN: trainLoss: 0.1620 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 78 | ANN: trainLoss: 0.1715 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 78 | ANN: trainLoss: 0.1656 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 78 | ANN: trainLoss: 0.1497 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 78 | ANN: trainLoss: 0.1554 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 78 | ANN: trainLoss: 0.1573 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 78 | ANN: trainLoss: 0.1540 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 78 | ANN: trainLoss: 0.1548 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 78 | ANN: trainLoss: 0.1507 | trainAcc: 94.2708% (724/768)\n",
            "12 13 Epoch: 78 | ANN: trainLoss: 0.2152 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 78 | ANN: testLoss: 0.8144 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 78 | ANN: testLoss: 0.8616 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 78 | ANN: testLoss: 0.7879 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 78 | ANN: testLoss: 0.5934 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 79 | ANN: trainLoss: 0.0925 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 79 | ANN: trainLoss: 0.1183 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 79 | ANN: trainLoss: 0.1082 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 79 | ANN: trainLoss: 0.1133 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 79 | ANN: trainLoss: 0.1085 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 79 | ANN: trainLoss: 0.1034 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 79 | ANN: trainLoss: 0.1004 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 79 | ANN: trainLoss: 0.1043 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 79 | ANN: trainLoss: 0.1000 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 79 | ANN: trainLoss: 0.1204 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 79 | ANN: trainLoss: 0.1207 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 79 | ANN: trainLoss: 0.1220 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 79 | ANN: trainLoss: 0.1165 | trainAcc: 96.3731% (744/772)\n",
            "0 4 Epoch: 79 | ANN: testLoss: 0.9945 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 79 | ANN: testLoss: 0.7862 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 79 | ANN: testLoss: 0.7730 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 79 | ANN: testLoss: 0.5806 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 80 | ANN: trainLoss: 0.1407 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 80 | ANN: trainLoss: 0.1291 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 80 | ANN: trainLoss: 0.1165 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 80 | ANN: trainLoss: 0.1241 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 80 | ANN: trainLoss: 0.1345 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 80 | ANN: trainLoss: 0.1315 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 80 | ANN: trainLoss: 0.1343 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 80 | ANN: trainLoss: 0.1377 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 80 | ANN: trainLoss: 0.1340 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 80 | ANN: trainLoss: 0.1323 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 80 | ANN: trainLoss: 0.1373 | trainAcc: 95.3125% (671/704)\n",
            "11 13 Epoch: 80 | ANN: trainLoss: 0.1323 | trainAcc: 95.5729% (734/768)\n",
            "12 13 Epoch: 80 | ANN: trainLoss: 0.1308 | trainAcc: 95.5959% (738/772)\n",
            "0 4 Epoch: 80 | ANN: testLoss: 0.7380 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 80 | ANN: testLoss: 0.7399 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 80 | ANN: testLoss: 0.7830 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 80 | ANN: testLoss: 0.7871 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 81 | ANN: trainLoss: 0.0705 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 81 | ANN: trainLoss: 0.0917 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 81 | ANN: trainLoss: 0.1211 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 81 | ANN: trainLoss: 0.1102 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 81 | ANN: trainLoss: 0.1090 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 81 | ANN: trainLoss: 0.1123 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 81 | ANN: trainLoss: 0.1172 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 81 | ANN: trainLoss: 0.1170 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 81 | ANN: trainLoss: 0.1156 | trainAcc: 95.6597% (551/576)\n",
            "9 13 Epoch: 81 | ANN: trainLoss: 0.1126 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 81 | ANN: trainLoss: 0.1089 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 81 | ANN: trainLoss: 0.1104 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 81 | ANN: trainLoss: 0.1104 | trainAcc: 96.1140% (742/772)\n",
            "0 4 Epoch: 81 | ANN: testLoss: 0.7702 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 81 | ANN: testLoss: 0.7137 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 81 | ANN: testLoss: 0.7555 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 81 | ANN: testLoss: 0.5798 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 82 | ANN: trainLoss: 0.1309 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 82 | ANN: trainLoss: 0.1234 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 82 | ANN: trainLoss: 0.1164 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 82 | ANN: trainLoss: 0.1272 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 82 | ANN: trainLoss: 0.1347 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 82 | ANN: trainLoss: 0.1256 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 82 | ANN: trainLoss: 0.1353 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 82 | ANN: trainLoss: 0.1318 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 82 | ANN: trainLoss: 0.1281 | trainAcc: 95.6597% (551/576)\n",
            "9 13 Epoch: 82 | ANN: trainLoss: 0.1271 | trainAcc: 95.6250% (612/640)\n",
            "10 13 Epoch: 82 | ANN: trainLoss: 0.1278 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 82 | ANN: trainLoss: 0.1245 | trainAcc: 95.8333% (736/768)\n",
            "12 13 Epoch: 82 | ANN: trainLoss: 0.2254 | trainAcc: 95.5959% (738/772)\n",
            "0 4 Epoch: 82 | ANN: testLoss: 0.5549 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 82 | ANN: testLoss: 0.7649 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 82 | ANN: testLoss: 0.7896 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 82 | ANN: testLoss: 0.5922 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 83 | ANN: trainLoss: 0.0837 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 83 | ANN: trainLoss: 0.1033 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 83 | ANN: trainLoss: 0.1148 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 83 | ANN: trainLoss: 0.1175 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 83 | ANN: trainLoss: 0.1225 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 83 | ANN: trainLoss: 0.1225 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 83 | ANN: trainLoss: 0.1175 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 83 | ANN: trainLoss: 0.1195 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 83 | ANN: trainLoss: 0.1209 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 83 | ANN: trainLoss: 0.1203 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 83 | ANN: trainLoss: 0.1232 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 83 | ANN: trainLoss: 0.1221 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 83 | ANN: trainLoss: 0.1606 | trainAcc: 96.2435% (743/772)\n",
            "0 4 Epoch: 83 | ANN: testLoss: 0.7935 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 83 | ANN: testLoss: 0.9619 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 83 | ANN: testLoss: 0.8788 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 83 | ANN: testLoss: 0.6622 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 84 | ANN: trainLoss: 0.0719 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 84 | ANN: trainLoss: 0.0966 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 84 | ANN: trainLoss: 0.0941 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 84 | ANN: trainLoss: 0.1098 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 84 | ANN: trainLoss: 0.1044 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 84 | ANN: trainLoss: 0.1029 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 84 | ANN: trainLoss: 0.0995 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 84 | ANN: trainLoss: 0.1013 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 84 | ANN: trainLoss: 0.1042 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 84 | ANN: trainLoss: 0.1059 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 84 | ANN: trainLoss: 0.1102 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 84 | ANN: trainLoss: 0.1091 | trainAcc: 96.6146% (742/768)\n",
            "12 13 Epoch: 84 | ANN: trainLoss: 0.2000 | trainAcc: 96.5026% (745/772)\n",
            "0 4 Epoch: 84 | ANN: testLoss: 0.8383 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 84 | ANN: testLoss: 0.9069 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 84 | ANN: testLoss: 0.8213 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 84 | ANN: testLoss: 0.6216 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 85 | ANN: trainLoss: 0.1793 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 85 | ANN: trainLoss: 0.1280 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 85 | ANN: trainLoss: 0.1119 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 85 | ANN: trainLoss: 0.1100 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 85 | ANN: trainLoss: 0.1093 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 85 | ANN: trainLoss: 0.1017 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 85 | ANN: trainLoss: 0.1075 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 85 | ANN: trainLoss: 0.1089 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 85 | ANN: trainLoss: 0.1067 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 85 | ANN: trainLoss: 0.1064 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 85 | ANN: trainLoss: 0.1097 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 85 | ANN: trainLoss: 0.1096 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 85 | ANN: trainLoss: 0.1068 | trainAcc: 97.1503% (750/772)\n",
            "0 4 Epoch: 85 | ANN: testLoss: 0.7981 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 85 | ANN: testLoss: 0.8490 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 85 | ANN: testLoss: 0.8220 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 85 | ANN: testLoss: 0.6165 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 86 | ANN: trainLoss: 0.1156 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 86 | ANN: trainLoss: 0.1082 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 86 | ANN: trainLoss: 0.1289 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 86 | ANN: trainLoss: 0.1183 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 86 | ANN: trainLoss: 0.1122 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 86 | ANN: trainLoss: 0.0984 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 86 | ANN: trainLoss: 0.0954 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 86 | ANN: trainLoss: 0.0971 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 86 | ANN: trainLoss: 0.0968 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 86 | ANN: trainLoss: 0.0954 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 86 | ANN: trainLoss: 0.0968 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 86 | ANN: trainLoss: 0.0978 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 86 | ANN: trainLoss: 0.1178 | trainAcc: 97.2798% (751/772)\n",
            "0 4 Epoch: 86 | ANN: testLoss: 0.5929 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 86 | ANN: testLoss: 0.8650 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 86 | ANN: testLoss: 0.8196 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 86 | ANN: testLoss: 0.8786 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 87 | ANN: trainLoss: 0.1503 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 87 | ANN: trainLoss: 0.1250 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 87 | ANN: trainLoss: 0.1154 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 87 | ANN: trainLoss: 0.1174 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 87 | ANN: trainLoss: 0.1121 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 87 | ANN: trainLoss: 0.1090 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 87 | ANN: trainLoss: 0.1046 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 87 | ANN: trainLoss: 0.1132 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 87 | ANN: trainLoss: 0.1155 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 87 | ANN: trainLoss: 0.1149 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 87 | ANN: trainLoss: 0.1112 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 87 | ANN: trainLoss: 0.1102 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 87 | ANN: trainLoss: 0.1288 | trainAcc: 97.2798% (751/772)\n",
            "0 4 Epoch: 87 | ANN: testLoss: 0.7732 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 87 | ANN: testLoss: 0.7927 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 87 | ANN: testLoss: 0.8186 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 87 | ANN: testLoss: 0.6171 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 88 | ANN: trainLoss: 0.0719 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 88 | ANN: trainLoss: 0.0853 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 88 | ANN: trainLoss: 0.0783 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 88 | ANN: trainLoss: 0.0754 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 88 | ANN: trainLoss: 0.0839 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 88 | ANN: trainLoss: 0.0857 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 88 | ANN: trainLoss: 0.0833 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 88 | ANN: trainLoss: 0.0813 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 88 | ANN: trainLoss: 0.0863 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 88 | ANN: trainLoss: 0.0875 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 88 | ANN: trainLoss: 0.0881 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 88 | ANN: trainLoss: 0.0871 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 88 | ANN: trainLoss: 0.0876 | trainAcc: 96.8912% (748/772)\n",
            "0 4 Epoch: 88 | ANN: testLoss: 0.9111 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 88 | ANN: testLoss: 0.8820 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 88 | ANN: testLoss: 0.8425 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 88 | ANN: testLoss: 0.6342 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 89 | ANN: trainLoss: 0.0737 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 89 | ANN: trainLoss: 0.0895 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 89 | ANN: trainLoss: 0.1154 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 89 | ANN: trainLoss: 0.1063 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 89 | ANN: trainLoss: 0.1138 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 89 | ANN: trainLoss: 0.1211 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 89 | ANN: trainLoss: 0.1168 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 89 | ANN: trainLoss: 0.1144 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 89 | ANN: trainLoss: 0.1225 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 89 | ANN: trainLoss: 0.1203 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 89 | ANN: trainLoss: 0.1172 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 89 | ANN: trainLoss: 0.1193 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 89 | ANN: trainLoss: 0.3398 | trainAcc: 96.1140% (742/772)\n",
            "0 4 Epoch: 89 | ANN: testLoss: 0.7549 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 89 | ANN: testLoss: 0.7539 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 89 | ANN: testLoss: 0.8076 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 89 | ANN: testLoss: 1.6994 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 90 | ANN: trainLoss: 0.1736 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 90 | ANN: trainLoss: 0.1422 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 90 | ANN: trainLoss: 0.1268 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 90 | ANN: trainLoss: 0.1188 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 90 | ANN: trainLoss: 0.1119 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 90 | ANN: trainLoss: 0.1073 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 90 | ANN: trainLoss: 0.1037 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 90 | ANN: trainLoss: 0.1008 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 90 | ANN: trainLoss: 0.0979 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 90 | ANN: trainLoss: 0.0958 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 90 | ANN: trainLoss: 0.0999 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 90 | ANN: trainLoss: 0.1058 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 90 | ANN: trainLoss: 0.0981 | trainAcc: 96.2435% (743/772)\n",
            "0 4 Epoch: 90 | ANN: testLoss: 0.6372 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 90 | ANN: testLoss: 0.8004 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 90 | ANN: testLoss: 0.8264 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 90 | ANN: testLoss: 0.9637 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 91 | ANN: trainLoss: 0.0982 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 91 | ANN: trainLoss: 0.0734 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 91 | ANN: trainLoss: 0.0725 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 91 | ANN: trainLoss: 0.0716 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 91 | ANN: trainLoss: 0.0700 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 91 | ANN: trainLoss: 0.0757 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 91 | ANN: trainLoss: 0.0770 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 91 | ANN: trainLoss: 0.0748 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 91 | ANN: trainLoss: 0.0785 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 91 | ANN: trainLoss: 0.0858 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 91 | ANN: trainLoss: 0.0855 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 91 | ANN: trainLoss: 0.0898 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 91 | ANN: trainLoss: 0.1076 | trainAcc: 97.4093% (752/772)\n",
            "0 4 Epoch: 91 | ANN: testLoss: 1.1641 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 91 | ANN: testLoss: 0.8434 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 91 | ANN: testLoss: 0.8255 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 91 | ANN: testLoss: 0.6198 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 92 | ANN: trainLoss: 0.0906 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 92 | ANN: trainLoss: 0.1133 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 92 | ANN: trainLoss: 0.0944 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 92 | ANN: trainLoss: 0.1037 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 92 | ANN: trainLoss: 0.0963 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 92 | ANN: trainLoss: 0.0981 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 92 | ANN: trainLoss: 0.1027 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 92 | ANN: trainLoss: 0.1029 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 92 | ANN: trainLoss: 0.1033 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 92 | ANN: trainLoss: 0.1082 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 92 | ANN: trainLoss: 0.1088 | trainAcc: 96.8750% (682/704)\n",
            "11 13 Epoch: 92 | ANN: trainLoss: 0.1080 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 92 | ANN: trainLoss: 0.1122 | trainAcc: 96.8912% (748/772)\n",
            "0 4 Epoch: 92 | ANN: testLoss: 0.7467 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 92 | ANN: testLoss: 0.7351 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 92 | ANN: testLoss: 0.8049 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 92 | ANN: testLoss: 1.5690 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 93 | ANN: trainLoss: 0.1438 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 93 | ANN: trainLoss: 0.1182 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 93 | ANN: trainLoss: 0.1292 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 93 | ANN: trainLoss: 0.1272 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 93 | ANN: trainLoss: 0.1168 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 93 | ANN: trainLoss: 0.1106 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 93 | ANN: trainLoss: 0.1069 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 93 | ANN: trainLoss: 0.1015 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 93 | ANN: trainLoss: 0.1042 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 93 | ANN: trainLoss: 0.1011 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 93 | ANN: trainLoss: 0.0997 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 93 | ANN: trainLoss: 0.0994 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 93 | ANN: trainLoss: 0.1004 | trainAcc: 97.7979% (755/772)\n",
            "0 4 Epoch: 93 | ANN: testLoss: 0.6489 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 93 | ANN: testLoss: 0.7302 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 93 | ANN: testLoss: 0.8184 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 93 | ANN: testLoss: 0.6143 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 94 | ANN: trainLoss: 0.1071 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 94 | ANN: trainLoss: 0.1152 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 94 | ANN: trainLoss: 0.1159 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 94 | ANN: trainLoss: 0.1368 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 94 | ANN: trainLoss: 0.1256 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 94 | ANN: trainLoss: 0.1194 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 94 | ANN: trainLoss: 0.1230 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 94 | ANN: trainLoss: 0.1222 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 94 | ANN: trainLoss: 0.1152 | trainAcc: 95.6597% (551/576)\n",
            "9 13 Epoch: 94 | ANN: trainLoss: 0.1137 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 94 | ANN: trainLoss: 0.1113 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 94 | ANN: trainLoss: 0.1117 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 94 | ANN: trainLoss: 0.1131 | trainAcc: 96.1140% (742/772)\n",
            "0 4 Epoch: 94 | ANN: testLoss: 0.9702 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 94 | ANN: testLoss: 0.8420 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 94 | ANN: testLoss: 0.8291 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 94 | ANN: testLoss: 0.6218 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 95 | ANN: trainLoss: 0.1082 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 95 | ANN: trainLoss: 0.1190 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 95 | ANN: trainLoss: 0.1225 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 95 | ANN: trainLoss: 0.1176 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 95 | ANN: trainLoss: 0.1125 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 95 | ANN: trainLoss: 0.1080 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 95 | ANN: trainLoss: 0.1054 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 95 | ANN: trainLoss: 0.1055 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 95 | ANN: trainLoss: 0.1111 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 95 | ANN: trainLoss: 0.1116 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 95 | ANN: trainLoss: 0.1074 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 95 | ANN: trainLoss: 0.1062 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 95 | ANN: trainLoss: 0.1392 | trainAcc: 96.7617% (747/772)\n",
            "0 4 Epoch: 95 | ANN: testLoss: 0.6183 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 95 | ANN: testLoss: 0.7915 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 95 | ANN: testLoss: 0.8442 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 95 | ANN: testLoss: 0.6331 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 96 | ANN: trainLoss: 0.1614 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 96 | ANN: trainLoss: 0.1262 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 96 | ANN: trainLoss: 0.1211 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 96 | ANN: trainLoss: 0.1152 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 96 | ANN: trainLoss: 0.1129 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 96 | ANN: trainLoss: 0.1047 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 96 | ANN: trainLoss: 0.1144 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 96 | ANN: trainLoss: 0.1127 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 96 | ANN: trainLoss: 0.1098 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 96 | ANN: trainLoss: 0.1062 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 96 | ANN: trainLoss: 0.1019 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 96 | ANN: trainLoss: 0.1007 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 96 | ANN: trainLoss: 0.1006 | trainAcc: 96.8912% (748/772)\n",
            "0 4 Epoch: 96 | ANN: testLoss: 0.9098 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 96 | ANN: testLoss: 0.6845 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 96 | ANN: testLoss: 0.7997 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 96 | ANN: testLoss: 0.5998 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 97 | ANN: trainLoss: 0.1190 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 97 | ANN: trainLoss: 0.1052 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 97 | ANN: trainLoss: 0.1175 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 97 | ANN: trainLoss: 0.1290 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 97 | ANN: trainLoss: 0.1257 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 97 | ANN: trainLoss: 0.1212 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 97 | ANN: trainLoss: 0.1135 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 97 | ANN: trainLoss: 0.1148 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 97 | ANN: trainLoss: 0.1131 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 97 | ANN: trainLoss: 0.1094 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 97 | ANN: trainLoss: 0.1106 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 97 | ANN: trainLoss: 0.1108 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 97 | ANN: trainLoss: 0.1320 | trainAcc: 96.2435% (743/772)\n",
            "0 4 Epoch: 97 | ANN: testLoss: 0.6946 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 97 | ANN: testLoss: 0.7819 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 97 | ANN: testLoss: 0.8258 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 97 | ANN: testLoss: 0.8625 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 98 | ANN: trainLoss: 0.0815 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 98 | ANN: trainLoss: 0.0855 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 98 | ANN: trainLoss: 0.1007 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 98 | ANN: trainLoss: 0.0947 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 98 | ANN: trainLoss: 0.0917 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 98 | ANN: trainLoss: 0.0920 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 98 | ANN: trainLoss: 0.0876 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 98 | ANN: trainLoss: 0.0870 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 98 | ANN: trainLoss: 0.0862 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 98 | ANN: trainLoss: 0.0875 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 98 | ANN: trainLoss: 0.0868 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 98 | ANN: trainLoss: 0.0885 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 98 | ANN: trainLoss: 0.0914 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 98 | ANN: testLoss: 1.0553 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 98 | ANN: testLoss: 0.9207 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 98 | ANN: testLoss: 0.8246 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 98 | ANN: testLoss: 0.7930 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 99 | ANN: trainLoss: 0.1318 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 99 | ANN: trainLoss: 0.1209 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 99 | ANN: trainLoss: 0.1146 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 99 | ANN: trainLoss: 0.1065 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 99 | ANN: trainLoss: 0.1107 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 99 | ANN: trainLoss: 0.1053 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 99 | ANN: trainLoss: 0.1073 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 99 | ANN: trainLoss: 0.1059 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 99 | ANN: trainLoss: 0.1038 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 99 | ANN: trainLoss: 0.1036 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 99 | ANN: trainLoss: 0.1005 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 99 | ANN: trainLoss: 0.1003 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 99 | ANN: trainLoss: 0.1107 | trainAcc: 96.7617% (747/772)\n",
            "0 4 Epoch: 99 | ANN: testLoss: 0.7727 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 99 | ANN: testLoss: 0.7912 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 99 | ANN: testLoss: 0.8262 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 99 | ANN: testLoss: 1.3510 | testAcc: 69.4301% (134/193)\n",
            "---------------------------------------------\n",
            "Converting using MaxNorm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 379.14it/s]\n",
            "100%|██████████| 13/13 [00:00<00:00, 446.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ANN accuracy: Test: 69.4300%\n",
            "SNN accuracy: max_norm: 70.9845%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NN Visualization\n",
        "\n",
        "# This block should be placed after the training and SNN conversion\n",
        "# of the model you wish to visualize. For example, after the\n",
        "# \"LENet to SNN Conversion Framework execution\" cell or\n",
        "# after the \"LENet_FCL to SNN Conversion Framework execution\" cell.\n",
        "# The `cnn_model_lenet` and `Snn_model_lenet` variables from that preceding cell\n",
        "# will be used for visualization.\n",
        "\n",
        "# Install nnviz if you haven't already (uncomment the line below if needed)\n",
        "# !pip install nnviz\n",
        "\n",
        "# Import necessary nnviz modules\n",
        "from nnviz import drawing, inspection\n",
        "import torch # Ensure torch is imported\n",
        "\n",
        "print(\"Starting Neural Network Visualization...\")\n",
        "\n",
        "# --- CNN Model Visualization ---\n",
        "if 'cnn_model_lenet' in locals() and isinstance(cnn_model_lenet, torch.nn.Module):\n",
        "    model_to_visualize_cnn = cnn_model_lenet\n",
        "    cnn_model_lenet_name = type(model_to_visualize_cnn).__name__\n",
        "    print(f\"\\nVisualizing LENet CNN model: {cnn_model_lenet_name}\")\n",
        "\n",
        "    try:\n",
        "        # Move model to CPU for inspection (safer for fx tracing)\n",
        "        cnn_model_lenet_cpu = model_to_visualize_cnn.to('cpu')\n",
        "\n",
        "        # Create an inspector\n",
        "        # TorchFxInspector uses torch.fx to trace the model graph\n",
        "        cnn_inspector = inspection.TorchFxInspector()\n",
        "\n",
        "        # Inspect the CNN model\n",
        "        # For models like LENet_FCL, training (as done in your script) ensures that\n",
        "        # dynamically created layers (e.g., self.fc) are initialized before inspection.\n",
        "        print(f\"Inspecting {cnn_model_lenet_name} on CPU...\")\n",
        "        cnn_graph = cnn_inspector.inspect(cnn_model_lenet_cpu)\n",
        "\n",
        "        # Create a drawer for saving the visualization (e.g., to a PDF file)\n",
        "        # You can change the output format by changing the extension (e.g., .png, .svg)\n",
        "        cnn_viz_filename = f\"{cnn_model_lenet_name}_LENet_cnn_architecture.png\"\n",
        "        cnn_drawer = drawing.GraphvizDrawer(cnn_viz_filename)\n",
        "\n",
        "        # Draw the graph and save it to the file\n",
        "        cnn_drawer.draw(cnn_graph)\n",
        "        print(f\"LENet CNN model visualization saved to: {cnn_viz_filename}\")\n",
        "        print(\n",
        "            f\"Note: If the output file is empty or shows an error, ensure Graphviz \"\n",
        "            f\"(specifically the 'dot' command) is installed and accessible in your system's PATH.\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\n",
        "            f\"An error occurred during CNN model ({cnn_model_lenet_name}) visualization: {e}\"\n",
        "        )\n",
        "        print(\"Troubleshooting tips:\")\n",
        "        print(\"- Ensure 'nnviz' is installed.\")\n",
        "        print(\n",
        "            \"- Ensure 'graphviz' (dot executable) is installed and in your system's PATH.\"\n",
        "        )\n",
        "        print(\n",
        "            \"- The model structure might contain operations not traceable by torch.fx. \"\n",
        "            \"Check nnviz documentation for advanced usage or alternative inspectors if needed.\"\n",
        "        )\n",
        "else:\n",
        "    print(\n",
        "        \"\\nCNN model ('cnn_model_lenet') not found in the current scope, or it's not a torch.nn.Module. \"\n",
        "        \"Skipping CNN visualization.\"\n",
        "    )\n",
        "print(\"\\nNeural Network Visualization process complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8p4PokzI0HX",
        "outputId": "d80b0d23-6902-401d-b540-13dfeec95249",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Neural Network Visualization...\n",
            "\n",
            "Visualizing LENet CNN model: LENet\n",
            "Inspecting LENet on CPU...\n",
            "LENet CNN model visualization saved to: LENet_LENet_cnn_architecture.png\n",
            "Note: If the output file is empty or shows an error, ensure Graphviz (specifically the 'dot' command) is installed and accessible in your system's PATH.\n",
            "\n",
            "Neural Network Visualization process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LENet_FCL to SNN Conversion Framework execution\n",
        "\n",
        "# Hyperparameters\n",
        "#EPOCHS = 300\n",
        "#BATCH_SIZE = 128\n",
        "#TIME_STEPS = 100  # T for SNN\n",
        "#TEST_SIZE = 0.2\n",
        "#DROP_OUT = 0.5\n",
        "\n",
        "\n",
        "# Split the data\n",
        "print(f\"{100 - (TEST_SIZE * 100)}% of the dataset is used for training and {TEST_SIZE * 100}% is used for testing.\")\n",
        "train_data, test_data, train_label, test_label = train_test_split(datasetX, datasetY, test_size=TEST_SIZE, shuffle=True,\n",
        "                                                                  random_state=0)\n",
        "\n",
        "# Initialize model\n",
        "cnn_model_lenet_fcl = LENet_FCL(classes_num=3, channel_count=channel_count, drop_out = DROP_OUT).to(device)\n",
        "cnn_model_lenet_fcl.apply(initialize_weights)\n",
        "\n",
        "# Train CNN model\n",
        "train_acc, test_acc,  cnn_model_lenet_fcl = train_ann(cnn_model_lenet_fcl, train_data, train_label, test_data, test_label,\n",
        "                                              ep=EPOCHS, batch=BATCH_SIZE)\n",
        "max_norm_acc = anntosnn( cnn_model_lenet_fcl, train_data, train_label, test_data, test_label,\n",
        "                        batch=BATCH_SIZE, T=TIME_STEPS)\n",
        "snn_model_lenet_fcl = ann2snn.Converter(mode='max', dataloader=data_loader(train_data, train_label, batch=BATCH_SIZE))(cnn_model_lenet_fcl)\n",
        "\n",
        "print('\\n')\n",
        "print('ANN accuracy: Test: %.4f%%' % (test_acc * 100))\n",
        "print('SNN accuracy: max_norm: %.4f%%' % (max_norm_acc[-1] * 100))"
      ],
      "metadata": {
        "id": "AeiiLk4TQp-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "outputId": "f3801624-2e00-4831-92e3-9c1e675140a6"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80.0% of the dataset is used for training and 20.0% is used for testing.\n",
            "\n",
            "\n",
            "0 13 Epoch: 0 | ANN: trainLoss: 4.5163 | trainAcc: 43.7500% (28/64)\n",
            "1 13 Epoch: 0 | ANN: trainLoss: 4.6624 | trainAcc: 40.6250% (52/128)\n",
            "2 13 Epoch: 0 | ANN: trainLoss: 4.6817 | trainAcc: 39.5833% (76/192)\n",
            "3 13 Epoch: 0 | ANN: trainLoss: 4.5467 | trainAcc: 38.6719% (99/256)\n",
            "4 13 Epoch: 0 | ANN: trainLoss: 4.3232 | trainAcc: 39.3750% (126/320)\n",
            "5 13 Epoch: 0 | ANN: trainLoss: 4.0721 | trainAcc: 38.0208% (146/384)\n",
            "6 13 Epoch: 0 | ANN: trainLoss: 3.9171 | trainAcc: 38.8393% (174/448)\n",
            "7 13 Epoch: 0 | ANN: trainLoss: 3.8669 | trainAcc: 36.9141% (189/512)\n",
            "8 13 Epoch: 0 | ANN: trainLoss: 3.7063 | trainAcc: 38.3681% (221/576)\n",
            "9 13 Epoch: 0 | ANN: trainLoss: 3.7149 | trainAcc: 37.6562% (241/640)\n",
            "10 13 Epoch: 0 | ANN: trainLoss: 3.6788 | trainAcc: 37.6420% (265/704)\n",
            "11 13 Epoch: 0 | ANN: trainLoss: 3.5879 | trainAcc: 37.8906% (291/768)\n",
            "12 13 Epoch: 0 | ANN: trainLoss: 3.4714 | trainAcc: 37.9534% (293/772)\n",
            "0 4 Epoch: 0 | ANN: testLoss: 1.7018 | testAcc: 39.0625% (25/64)\n",
            "1 4 Epoch: 0 | ANN: testLoss: 1.8406 | testAcc: 35.9375% (46/128)\n",
            "2 4 Epoch: 0 | ANN: testLoss: 1.8678 | testAcc: 36.9792% (71/192)\n",
            "3 4 Epoch: 0 | ANN: testLoss: 1.4109 | testAcc: 37.3057% (72/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 1 | ANN: trainLoss: 1.7874 | trainAcc: 42.1875% (27/64)\n",
            "1 13 Epoch: 1 | ANN: trainLoss: 2.0165 | trainAcc: 46.0938% (59/128)\n",
            "2 13 Epoch: 1 | ANN: trainLoss: 2.1965 | trainAcc: 44.2708% (85/192)\n",
            "3 13 Epoch: 1 | ANN: trainLoss: 2.1900 | trainAcc: 43.7500% (112/256)\n",
            "4 13 Epoch: 1 | ANN: trainLoss: 2.3280 | trainAcc: 43.1250% (138/320)\n",
            "5 13 Epoch: 1 | ANN: trainLoss: 2.2942 | trainAcc: 43.2292% (166/384)\n",
            "6 13 Epoch: 1 | ANN: trainLoss: 2.2800 | trainAcc: 43.3036% (194/448)\n",
            "7 13 Epoch: 1 | ANN: trainLoss: 2.3229 | trainAcc: 41.9922% (215/512)\n",
            "8 13 Epoch: 1 | ANN: trainLoss: 2.2254 | trainAcc: 43.9236% (253/576)\n",
            "9 13 Epoch: 1 | ANN: trainLoss: 2.2282 | trainAcc: 43.7500% (280/640)\n",
            "10 13 Epoch: 1 | ANN: trainLoss: 2.1601 | trainAcc: 44.4602% (313/704)\n",
            "11 13 Epoch: 1 | ANN: trainLoss: 2.1270 | trainAcc: 44.9219% (345/768)\n",
            "12 13 Epoch: 1 | ANN: trainLoss: 2.0980 | trainAcc: 44.8187% (346/772)\n",
            "0 4 Epoch: 1 | ANN: testLoss: 1.3177 | testAcc: 43.7500% (28/64)\n",
            "1 4 Epoch: 1 | ANN: testLoss: 1.3291 | testAcc: 46.8750% (60/128)\n",
            "2 4 Epoch: 1 | ANN: testLoss: 1.3794 | testAcc: 44.2708% (85/192)\n",
            "3 4 Epoch: 1 | ANN: testLoss: 1.5907 | testAcc: 44.0415% (85/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 2 | ANN: trainLoss: 2.1065 | trainAcc: 50.0000% (32/64)\n",
            "1 13 Epoch: 2 | ANN: trainLoss: 2.2407 | trainAcc: 41.4062% (53/128)\n",
            "2 13 Epoch: 2 | ANN: trainLoss: 2.2538 | trainAcc: 41.1458% (79/192)\n",
            "3 13 Epoch: 2 | ANN: trainLoss: 2.1660 | trainAcc: 42.1875% (108/256)\n",
            "4 13 Epoch: 2 | ANN: trainLoss: 2.1134 | trainAcc: 40.9375% (131/320)\n",
            "5 13 Epoch: 2 | ANN: trainLoss: 1.9992 | trainAcc: 42.7083% (164/384)\n",
            "6 13 Epoch: 2 | ANN: trainLoss: 1.9995 | trainAcc: 41.9643% (188/448)\n",
            "7 13 Epoch: 2 | ANN: trainLoss: 2.0037 | trainAcc: 41.2109% (211/512)\n",
            "8 13 Epoch: 2 | ANN: trainLoss: 1.9751 | trainAcc: 40.9722% (236/576)\n",
            "9 13 Epoch: 2 | ANN: trainLoss: 1.9548 | trainAcc: 41.7188% (267/640)\n",
            "10 13 Epoch: 2 | ANN: trainLoss: 1.9189 | trainAcc: 42.1875% (297/704)\n",
            "11 13 Epoch: 2 | ANN: trainLoss: 1.9021 | trainAcc: 43.3594% (333/768)\n",
            "12 13 Epoch: 2 | ANN: trainLoss: 1.9255 | trainAcc: 43.3938% (335/772)\n",
            "0 4 Epoch: 2 | ANN: testLoss: 1.4005 | testAcc: 43.7500% (28/64)\n",
            "1 4 Epoch: 2 | ANN: testLoss: 1.3716 | testAcc: 44.5312% (57/128)\n",
            "2 4 Epoch: 2 | ANN: testLoss: 1.3967 | testAcc: 43.2292% (83/192)\n",
            "3 4 Epoch: 2 | ANN: testLoss: 1.0834 | testAcc: 43.5233% (84/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 3 | ANN: trainLoss: 1.4673 | trainAcc: 45.3125% (29/64)\n",
            "1 13 Epoch: 3 | ANN: trainLoss: 1.5791 | trainAcc: 42.1875% (54/128)\n",
            "2 13 Epoch: 3 | ANN: trainLoss: 1.3895 | trainAcc: 50.0000% (96/192)\n",
            "3 13 Epoch: 3 | ANN: trainLoss: 1.4596 | trainAcc: 48.0469% (123/256)\n",
            "4 13 Epoch: 3 | ANN: trainLoss: 1.4410 | trainAcc: 48.4375% (155/320)\n",
            "5 13 Epoch: 3 | ANN: trainLoss: 1.4255 | trainAcc: 48.9583% (188/384)\n",
            "6 13 Epoch: 3 | ANN: trainLoss: 1.4997 | trainAcc: 46.6518% (209/448)\n",
            "7 13 Epoch: 3 | ANN: trainLoss: 1.4683 | trainAcc: 47.4609% (243/512)\n",
            "8 13 Epoch: 3 | ANN: trainLoss: 1.4506 | trainAcc: 48.4375% (279/576)\n",
            "9 13 Epoch: 3 | ANN: trainLoss: 1.4280 | trainAcc: 48.7500% (312/640)\n",
            "10 13 Epoch: 3 | ANN: trainLoss: 1.4179 | trainAcc: 48.7216% (343/704)\n",
            "11 13 Epoch: 3 | ANN: trainLoss: 1.4165 | trainAcc: 49.2188% (378/768)\n",
            "12 13 Epoch: 3 | ANN: trainLoss: 1.5978 | trainAcc: 48.9637% (378/772)\n",
            "0 4 Epoch: 3 | ANN: testLoss: 1.1153 | testAcc: 48.4375% (31/64)\n",
            "1 4 Epoch: 3 | ANN: testLoss: 1.1193 | testAcc: 52.3438% (67/128)\n",
            "2 4 Epoch: 3 | ANN: testLoss: 1.1264 | testAcc: 52.0833% (100/192)\n",
            "3 4 Epoch: 3 | ANN: testLoss: 0.9015 | testAcc: 52.3316% (101/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 4 | ANN: trainLoss: 1.3081 | trainAcc: 48.4375% (31/64)\n",
            "1 13 Epoch: 4 | ANN: trainLoss: 1.1376 | trainAcc: 54.6875% (70/128)\n",
            "2 13 Epoch: 4 | ANN: trainLoss: 1.2585 | trainAcc: 50.0000% (96/192)\n",
            "3 13 Epoch: 4 | ANN: trainLoss: 1.3675 | trainAcc: 48.8281% (125/256)\n",
            "4 13 Epoch: 4 | ANN: trainLoss: 1.3274 | trainAcc: 49.6875% (159/320)\n",
            "5 13 Epoch: 4 | ANN: trainLoss: 1.4053 | trainAcc: 47.3958% (182/384)\n",
            "6 13 Epoch: 4 | ANN: trainLoss: 1.3264 | trainAcc: 48.6607% (218/448)\n",
            "7 13 Epoch: 4 | ANN: trainLoss: 1.3215 | trainAcc: 48.2422% (247/512)\n",
            "8 13 Epoch: 4 | ANN: trainLoss: 1.3156 | trainAcc: 48.4375% (279/576)\n",
            "9 13 Epoch: 4 | ANN: trainLoss: 1.3281 | trainAcc: 48.1250% (308/640)\n",
            "10 13 Epoch: 4 | ANN: trainLoss: 1.3274 | trainAcc: 48.2955% (340/704)\n",
            "11 13 Epoch: 4 | ANN: trainLoss: 1.3148 | trainAcc: 48.6979% (374/768)\n",
            "12 13 Epoch: 4 | ANN: trainLoss: 1.3467 | trainAcc: 48.5751% (375/772)\n",
            "0 4 Epoch: 4 | ANN: testLoss: 1.0525 | testAcc: 54.6875% (35/64)\n",
            "1 4 Epoch: 4 | ANN: testLoss: 1.0900 | testAcc: 51.5625% (66/128)\n",
            "2 4 Epoch: 4 | ANN: testLoss: 1.0555 | testAcc: 51.0417% (98/192)\n",
            "3 4 Epoch: 4 | ANN: testLoss: 0.8254 | testAcc: 51.2953% (99/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 5 | ANN: trainLoss: 1.1565 | trainAcc: 48.4375% (31/64)\n",
            "1 13 Epoch: 5 | ANN: trainLoss: 1.1479 | trainAcc: 50.7812% (65/128)\n",
            "2 13 Epoch: 5 | ANN: trainLoss: 1.2573 | trainAcc: 46.8750% (90/192)\n",
            "3 13 Epoch: 5 | ANN: trainLoss: 1.1880 | trainAcc: 50.7812% (130/256)\n",
            "4 13 Epoch: 5 | ANN: trainLoss: 1.2125 | trainAcc: 50.9375% (163/320)\n",
            "5 13 Epoch: 5 | ANN: trainLoss: 1.2146 | trainAcc: 51.0417% (196/384)\n",
            "6 13 Epoch: 5 | ANN: trainLoss: 1.2651 | trainAcc: 50.2232% (225/448)\n",
            "7 13 Epoch: 5 | ANN: trainLoss: 1.2584 | trainAcc: 49.8047% (255/512)\n",
            "8 13 Epoch: 5 | ANN: trainLoss: 1.2294 | trainAcc: 50.0000% (288/576)\n",
            "9 13 Epoch: 5 | ANN: trainLoss: 1.1970 | trainAcc: 51.0938% (327/640)\n",
            "10 13 Epoch: 5 | ANN: trainLoss: 1.1882 | trainAcc: 51.2784% (361/704)\n",
            "11 13 Epoch: 5 | ANN: trainLoss: 1.1920 | trainAcc: 51.1719% (393/768)\n",
            "12 13 Epoch: 5 | ANN: trainLoss: 1.2096 | trainAcc: 51.1658% (395/772)\n",
            "0 4 Epoch: 5 | ANN: testLoss: 1.1132 | testAcc: 54.6875% (35/64)\n",
            "1 4 Epoch: 5 | ANN: testLoss: 1.1037 | testAcc: 53.1250% (68/128)\n",
            "2 4 Epoch: 5 | ANN: testLoss: 1.0328 | testAcc: 54.6875% (105/192)\n",
            "3 4 Epoch: 5 | ANN: testLoss: 1.0354 | testAcc: 54.4041% (105/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 6 | ANN: trainLoss: 1.1119 | trainAcc: 46.8750% (30/64)\n",
            "1 13 Epoch: 6 | ANN: trainLoss: 1.0684 | trainAcc: 47.6562% (61/128)\n",
            "2 13 Epoch: 6 | ANN: trainLoss: 1.0975 | trainAcc: 46.8750% (90/192)\n",
            "3 13 Epoch: 6 | ANN: trainLoss: 1.1017 | trainAcc: 49.6094% (127/256)\n",
            "4 13 Epoch: 6 | ANN: trainLoss: 1.0680 | trainAcc: 50.6250% (162/320)\n",
            "5 13 Epoch: 6 | ANN: trainLoss: 1.0691 | trainAcc: 50.2604% (193/384)\n",
            "6 13 Epoch: 6 | ANN: trainLoss: 1.0605 | trainAcc: 50.8929% (228/448)\n",
            "7 13 Epoch: 6 | ANN: trainLoss: 1.0251 | trainAcc: 53.3203% (273/512)\n",
            "8 13 Epoch: 6 | ANN: trainLoss: 1.0029 | trainAcc: 54.3403% (313/576)\n",
            "9 13 Epoch: 6 | ANN: trainLoss: 1.0076 | trainAcc: 53.7500% (344/640)\n",
            "10 13 Epoch: 6 | ANN: trainLoss: 0.9989 | trainAcc: 53.9773% (380/704)\n",
            "11 13 Epoch: 6 | ANN: trainLoss: 1.0116 | trainAcc: 53.2552% (409/768)\n",
            "12 13 Epoch: 6 | ANN: trainLoss: 1.0076 | trainAcc: 53.1088% (410/772)\n",
            "0 4 Epoch: 6 | ANN: testLoss: 0.9048 | testAcc: 53.1250% (34/64)\n",
            "1 4 Epoch: 6 | ANN: testLoss: 0.8925 | testAcc: 53.9062% (69/128)\n",
            "2 4 Epoch: 6 | ANN: testLoss: 0.8946 | testAcc: 54.1667% (104/192)\n",
            "3 4 Epoch: 6 | ANN: testLoss: 1.0003 | testAcc: 53.8860% (104/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 7 | ANN: trainLoss: 0.9748 | trainAcc: 54.6875% (35/64)\n",
            "1 13 Epoch: 7 | ANN: trainLoss: 0.9858 | trainAcc: 50.7812% (65/128)\n",
            "2 13 Epoch: 7 | ANN: trainLoss: 0.9772 | trainAcc: 51.0417% (98/192)\n",
            "3 13 Epoch: 7 | ANN: trainLoss: 0.9496 | trainAcc: 51.9531% (133/256)\n",
            "4 13 Epoch: 7 | ANN: trainLoss: 0.9366 | trainAcc: 55.3125% (177/320)\n",
            "5 13 Epoch: 7 | ANN: trainLoss: 0.9337 | trainAcc: 55.9896% (215/384)\n",
            "6 13 Epoch: 7 | ANN: trainLoss: 0.9204 | trainAcc: 55.8036% (250/448)\n",
            "7 13 Epoch: 7 | ANN: trainLoss: 0.9205 | trainAcc: 56.8359% (291/512)\n",
            "8 13 Epoch: 7 | ANN: trainLoss: 0.9249 | trainAcc: 56.4236% (325/576)\n",
            "9 13 Epoch: 7 | ANN: trainLoss: 0.9226 | trainAcc: 57.5000% (368/640)\n",
            "10 13 Epoch: 7 | ANN: trainLoss: 0.9005 | trainAcc: 57.9545% (408/704)\n",
            "11 13 Epoch: 7 | ANN: trainLoss: 0.8973 | trainAcc: 58.7240% (451/768)\n",
            "12 13 Epoch: 7 | ANN: trainLoss: 0.8623 | trainAcc: 58.8083% (454/772)\n",
            "0 4 Epoch: 7 | ANN: testLoss: 0.8551 | testAcc: 53.1250% (34/64)\n",
            "1 4 Epoch: 7 | ANN: testLoss: 0.8972 | testAcc: 53.9062% (69/128)\n",
            "2 4 Epoch: 7 | ANN: testLoss: 0.8649 | testAcc: 56.2500% (108/192)\n",
            "3 4 Epoch: 7 | ANN: testLoss: 0.8234 | testAcc: 56.4767% (109/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 8 | ANN: trainLoss: 0.8335 | trainAcc: 59.3750% (38/64)\n",
            "1 13 Epoch: 8 | ANN: trainLoss: 0.8127 | trainAcc: 61.7188% (79/128)\n",
            "2 13 Epoch: 8 | ANN: trainLoss: 0.8601 | trainAcc: 60.9375% (117/192)\n",
            "3 13 Epoch: 8 | ANN: trainLoss: 0.8579 | trainAcc: 59.3750% (152/256)\n",
            "4 13 Epoch: 8 | ANN: trainLoss: 0.8524 | trainAcc: 59.0625% (189/320)\n",
            "5 13 Epoch: 8 | ANN: trainLoss: 0.8320 | trainAcc: 60.9375% (234/384)\n",
            "6 13 Epoch: 8 | ANN: trainLoss: 0.8536 | trainAcc: 59.3750% (266/448)\n",
            "7 13 Epoch: 8 | ANN: trainLoss: 0.8469 | trainAcc: 60.3516% (309/512)\n",
            "8 13 Epoch: 8 | ANN: trainLoss: 0.8862 | trainAcc: 59.0278% (340/576)\n",
            "9 13 Epoch: 8 | ANN: trainLoss: 0.8771 | trainAcc: 60.4688% (387/640)\n",
            "10 13 Epoch: 8 | ANN: trainLoss: 0.8846 | trainAcc: 60.0852% (423/704)\n",
            "11 13 Epoch: 8 | ANN: trainLoss: 0.8641 | trainAcc: 61.0677% (469/768)\n",
            "12 13 Epoch: 8 | ANN: trainLoss: 0.8640 | trainAcc: 61.0104% (471/772)\n",
            "0 4 Epoch: 8 | ANN: testLoss: 0.8034 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 8 | ANN: testLoss: 0.8408 | testAcc: 57.8125% (74/128)\n",
            "2 4 Epoch: 8 | ANN: testLoss: 0.8567 | testAcc: 56.2500% (108/192)\n",
            "3 4 Epoch: 8 | ANN: testLoss: 0.8505 | testAcc: 56.4767% (109/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 9 | ANN: trainLoss: 0.8604 | trainAcc: 59.3750% (38/64)\n",
            "1 13 Epoch: 9 | ANN: trainLoss: 0.8411 | trainAcc: 63.2812% (81/128)\n",
            "2 13 Epoch: 9 | ANN: trainLoss: 0.8534 | trainAcc: 61.9792% (119/192)\n",
            "3 13 Epoch: 9 | ANN: trainLoss: 0.8685 | trainAcc: 61.3281% (157/256)\n",
            "4 13 Epoch: 9 | ANN: trainLoss: 0.8704 | trainAcc: 60.0000% (192/320)\n",
            "5 13 Epoch: 9 | ANN: trainLoss: 0.8369 | trainAcc: 61.9792% (238/384)\n",
            "6 13 Epoch: 9 | ANN: trainLoss: 0.8610 | trainAcc: 61.6071% (276/448)\n",
            "7 13 Epoch: 9 | ANN: trainLoss: 0.8715 | trainAcc: 61.9141% (317/512)\n",
            "8 13 Epoch: 9 | ANN: trainLoss: 0.8445 | trainAcc: 63.5417% (366/576)\n",
            "9 13 Epoch: 9 | ANN: trainLoss: 0.8343 | trainAcc: 63.9062% (409/640)\n",
            "10 13 Epoch: 9 | ANN: trainLoss: 0.8312 | trainAcc: 63.3523% (446/704)\n",
            "11 13 Epoch: 9 | ANN: trainLoss: 0.8294 | trainAcc: 62.7604% (482/768)\n",
            "12 13 Epoch: 9 | ANN: trainLoss: 0.8268 | trainAcc: 62.6943% (484/772)\n",
            "0 4 Epoch: 9 | ANN: testLoss: 0.7880 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 9 | ANN: testLoss: 0.8634 | testAcc: 53.1250% (68/128)\n",
            "2 4 Epoch: 9 | ANN: testLoss: 0.8581 | testAcc: 55.7292% (107/192)\n",
            "3 4 Epoch: 9 | ANN: testLoss: 0.9213 | testAcc: 55.4404% (107/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 10 | ANN: trainLoss: 0.6850 | trainAcc: 70.3125% (45/64)\n",
            "1 13 Epoch: 10 | ANN: trainLoss: 0.7141 | trainAcc: 69.5312% (89/128)\n",
            "2 13 Epoch: 10 | ANN: trainLoss: 0.6953 | trainAcc: 71.8750% (138/192)\n",
            "3 13 Epoch: 10 | ANN: trainLoss: 0.7569 | trainAcc: 68.7500% (176/256)\n",
            "4 13 Epoch: 10 | ANN: trainLoss: 0.7876 | trainAcc: 66.8750% (214/320)\n",
            "5 13 Epoch: 10 | ANN: trainLoss: 0.8076 | trainAcc: 65.3646% (251/384)\n",
            "6 13 Epoch: 10 | ANN: trainLoss: 0.8099 | trainAcc: 65.8482% (295/448)\n",
            "7 13 Epoch: 10 | ANN: trainLoss: 0.7991 | trainAcc: 65.6250% (336/512)\n",
            "8 13 Epoch: 10 | ANN: trainLoss: 0.7907 | trainAcc: 65.6250% (378/576)\n",
            "9 13 Epoch: 10 | ANN: trainLoss: 0.7837 | trainAcc: 65.6250% (420/640)\n",
            "10 13 Epoch: 10 | ANN: trainLoss: 0.7919 | trainAcc: 65.3409% (460/704)\n",
            "11 13 Epoch: 10 | ANN: trainLoss: 0.7992 | trainAcc: 64.5833% (496/768)\n",
            "12 13 Epoch: 10 | ANN: trainLoss: 0.7767 | trainAcc: 64.6373% (499/772)\n",
            "0 4 Epoch: 10 | ANN: testLoss: 0.8249 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 10 | ANN: testLoss: 0.8424 | testAcc: 54.6875% (70/128)\n",
            "2 4 Epoch: 10 | ANN: testLoss: 0.8465 | testAcc: 54.1667% (104/192)\n",
            "3 4 Epoch: 10 | ANN: testLoss: 0.8871 | testAcc: 53.8860% (104/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 11 | ANN: trainLoss: 0.7900 | trainAcc: 67.1875% (43/64)\n",
            "1 13 Epoch: 11 | ANN: trainLoss: 0.8104 | trainAcc: 64.8438% (83/128)\n",
            "2 13 Epoch: 11 | ANN: trainLoss: 0.8107 | trainAcc: 64.5833% (124/192)\n",
            "3 13 Epoch: 11 | ANN: trainLoss: 0.7947 | trainAcc: 64.8438% (166/256)\n",
            "4 13 Epoch: 11 | ANN: trainLoss: 0.7992 | trainAcc: 65.3125% (209/320)\n",
            "5 13 Epoch: 11 | ANN: trainLoss: 0.8259 | trainAcc: 64.5833% (248/384)\n",
            "6 13 Epoch: 11 | ANN: trainLoss: 0.8138 | trainAcc: 64.9554% (291/448)\n",
            "7 13 Epoch: 11 | ANN: trainLoss: 0.8053 | trainAcc: 65.2344% (334/512)\n",
            "8 13 Epoch: 11 | ANN: trainLoss: 0.8000 | trainAcc: 65.6250% (378/576)\n",
            "9 13 Epoch: 11 | ANN: trainLoss: 0.7942 | trainAcc: 66.0938% (423/640)\n",
            "10 13 Epoch: 11 | ANN: trainLoss: 0.7801 | trainAcc: 66.6193% (469/704)\n",
            "11 13 Epoch: 11 | ANN: trainLoss: 0.7788 | trainAcc: 65.7552% (505/768)\n",
            "12 13 Epoch: 11 | ANN: trainLoss: 0.8294 | trainAcc: 65.5440% (506/772)\n",
            "0 4 Epoch: 11 | ANN: testLoss: 0.8897 | testAcc: 51.5625% (33/64)\n",
            "1 4 Epoch: 11 | ANN: testLoss: 0.8498 | testAcc: 53.1250% (68/128)\n",
            "2 4 Epoch: 11 | ANN: testLoss: 0.8070 | testAcc: 56.2500% (108/192)\n",
            "3 4 Epoch: 11 | ANN: testLoss: 1.0320 | testAcc: 55.9585% (108/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 12 | ANN: trainLoss: 0.6215 | trainAcc: 70.3125% (45/64)\n",
            "1 13 Epoch: 12 | ANN: trainLoss: 0.6085 | trainAcc: 71.0938% (91/128)\n",
            "2 13 Epoch: 12 | ANN: trainLoss: 0.6211 | trainAcc: 70.8333% (136/192)\n",
            "3 13 Epoch: 12 | ANN: trainLoss: 0.6587 | trainAcc: 68.3594% (175/256)\n",
            "4 13 Epoch: 12 | ANN: trainLoss: 0.6717 | trainAcc: 67.5000% (216/320)\n",
            "5 13 Epoch: 12 | ANN: trainLoss: 0.6925 | trainAcc: 66.4062% (255/384)\n",
            "6 13 Epoch: 12 | ANN: trainLoss: 0.7164 | trainAcc: 65.6250% (294/448)\n",
            "7 13 Epoch: 12 | ANN: trainLoss: 0.7269 | trainAcc: 65.4297% (335/512)\n",
            "8 13 Epoch: 12 | ANN: trainLoss: 0.7260 | trainAcc: 65.6250% (378/576)\n",
            "9 13 Epoch: 12 | ANN: trainLoss: 0.7386 | trainAcc: 65.0000% (416/640)\n",
            "10 13 Epoch: 12 | ANN: trainLoss: 0.7426 | trainAcc: 64.3466% (453/704)\n",
            "11 13 Epoch: 12 | ANN: trainLoss: 0.7408 | trainAcc: 64.5833% (496/768)\n",
            "12 13 Epoch: 12 | ANN: trainLoss: 0.7671 | trainAcc: 64.5078% (498/772)\n",
            "0 4 Epoch: 12 | ANN: testLoss: 0.9180 | testAcc: 54.6875% (35/64)\n",
            "1 4 Epoch: 12 | ANN: testLoss: 0.8408 | testAcc: 58.5938% (75/128)\n",
            "2 4 Epoch: 12 | ANN: testLoss: 0.8409 | testAcc: 55.7292% (107/192)\n",
            "3 4 Epoch: 12 | ANN: testLoss: 0.6890 | testAcc: 55.9585% (108/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 13 | ANN: trainLoss: 0.8120 | trainAcc: 60.9375% (39/64)\n",
            "1 13 Epoch: 13 | ANN: trainLoss: 0.8333 | trainAcc: 61.7188% (79/128)\n",
            "2 13 Epoch: 13 | ANN: trainLoss: 0.8478 | trainAcc: 60.4167% (116/192)\n",
            "3 13 Epoch: 13 | ANN: trainLoss: 0.7862 | trainAcc: 63.6719% (163/256)\n",
            "4 13 Epoch: 13 | ANN: trainLoss: 0.7876 | trainAcc: 64.0625% (205/320)\n",
            "5 13 Epoch: 13 | ANN: trainLoss: 0.7831 | trainAcc: 64.5833% (248/384)\n",
            "6 13 Epoch: 13 | ANN: trainLoss: 0.7681 | trainAcc: 65.1786% (292/448)\n",
            "7 13 Epoch: 13 | ANN: trainLoss: 0.7542 | trainAcc: 66.2109% (339/512)\n",
            "8 13 Epoch: 13 | ANN: trainLoss: 0.7444 | trainAcc: 66.8403% (385/576)\n",
            "9 13 Epoch: 13 | ANN: trainLoss: 0.7419 | trainAcc: 66.8750% (428/640)\n",
            "10 13 Epoch: 13 | ANN: trainLoss: 0.7382 | trainAcc: 66.9034% (471/704)\n",
            "11 13 Epoch: 13 | ANN: trainLoss: 0.7408 | trainAcc: 67.0573% (515/768)\n",
            "12 13 Epoch: 13 | ANN: trainLoss: 0.7210 | trainAcc: 67.0984% (518/772)\n",
            "0 4 Epoch: 13 | ANN: testLoss: 0.8022 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 13 | ANN: testLoss: 0.8333 | testAcc: 56.2500% (72/128)\n",
            "2 4 Epoch: 13 | ANN: testLoss: 0.7927 | testAcc: 57.2917% (110/192)\n",
            "3 4 Epoch: 13 | ANN: testLoss: 0.5956 | testAcc: 57.5130% (111/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 14 | ANN: trainLoss: 0.6702 | trainAcc: 70.3125% (45/64)\n",
            "1 13 Epoch: 14 | ANN: trainLoss: 0.7073 | trainAcc: 70.3125% (90/128)\n",
            "2 13 Epoch: 14 | ANN: trainLoss: 0.7329 | trainAcc: 69.7917% (134/192)\n",
            "3 13 Epoch: 14 | ANN: trainLoss: 0.7303 | trainAcc: 69.5312% (178/256)\n",
            "4 13 Epoch: 14 | ANN: trainLoss: 0.7156 | trainAcc: 69.3750% (222/320)\n",
            "5 13 Epoch: 14 | ANN: trainLoss: 0.7137 | trainAcc: 68.7500% (264/384)\n",
            "6 13 Epoch: 14 | ANN: trainLoss: 0.7162 | trainAcc: 68.7500% (308/448)\n",
            "7 13 Epoch: 14 | ANN: trainLoss: 0.7100 | trainAcc: 69.7266% (357/512)\n",
            "8 13 Epoch: 14 | ANN: trainLoss: 0.7006 | trainAcc: 70.3125% (405/576)\n",
            "9 13 Epoch: 14 | ANN: trainLoss: 0.6997 | trainAcc: 70.3125% (450/640)\n",
            "10 13 Epoch: 14 | ANN: trainLoss: 0.6998 | trainAcc: 69.8864% (492/704)\n",
            "11 13 Epoch: 14 | ANN: trainLoss: 0.6986 | trainAcc: 69.5312% (534/768)\n",
            "12 13 Epoch: 14 | ANN: trainLoss: 0.7040 | trainAcc: 69.5596% (537/772)\n",
            "0 4 Epoch: 14 | ANN: testLoss: 0.7492 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 14 | ANN: testLoss: 0.7939 | testAcc: 58.5938% (75/128)\n",
            "2 4 Epoch: 14 | ANN: testLoss: 0.7887 | testAcc: 58.8542% (113/192)\n",
            "3 4 Epoch: 14 | ANN: testLoss: 0.6185 | testAcc: 59.0674% (114/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 15 | ANN: trainLoss: 0.6773 | trainAcc: 68.7500% (44/64)\n",
            "1 13 Epoch: 15 | ANN: trainLoss: 0.6083 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 15 | ANN: trainLoss: 0.6383 | trainAcc: 71.3542% (137/192)\n",
            "3 13 Epoch: 15 | ANN: trainLoss: 0.6422 | trainAcc: 72.2656% (185/256)\n",
            "4 13 Epoch: 15 | ANN: trainLoss: 0.6713 | trainAcc: 69.3750% (222/320)\n",
            "5 13 Epoch: 15 | ANN: trainLoss: 0.6724 | trainAcc: 70.3125% (270/384)\n",
            "6 13 Epoch: 15 | ANN: trainLoss: 0.6914 | trainAcc: 68.9732% (309/448)\n",
            "7 13 Epoch: 15 | ANN: trainLoss: 0.7021 | trainAcc: 68.7500% (352/512)\n",
            "8 13 Epoch: 15 | ANN: trainLoss: 0.6853 | trainAcc: 69.4444% (400/576)\n",
            "9 13 Epoch: 15 | ANN: trainLoss: 0.7147 | trainAcc: 67.8125% (434/640)\n",
            "10 13 Epoch: 15 | ANN: trainLoss: 0.7177 | trainAcc: 67.4716% (475/704)\n",
            "11 13 Epoch: 15 | ANN: trainLoss: 0.7180 | trainAcc: 67.5781% (519/768)\n",
            "12 13 Epoch: 15 | ANN: trainLoss: 0.7712 | trainAcc: 67.4870% (521/772)\n",
            "0 4 Epoch: 15 | ANN: testLoss: 0.6611 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 15 | ANN: testLoss: 0.8042 | testAcc: 58.5938% (75/128)\n",
            "2 4 Epoch: 15 | ANN: testLoss: 0.8174 | testAcc: 58.3333% (112/192)\n",
            "3 4 Epoch: 15 | ANN: testLoss: 0.7618 | testAcc: 58.5492% (113/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 16 | ANN: trainLoss: 0.5275 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 16 | ANN: trainLoss: 0.6009 | trainAcc: 75.7812% (97/128)\n",
            "2 13 Epoch: 16 | ANN: trainLoss: 0.5980 | trainAcc: 75.5208% (145/192)\n",
            "3 13 Epoch: 16 | ANN: trainLoss: 0.6013 | trainAcc: 74.2188% (190/256)\n",
            "4 13 Epoch: 16 | ANN: trainLoss: 0.6252 | trainAcc: 71.5625% (229/320)\n",
            "5 13 Epoch: 16 | ANN: trainLoss: 0.6388 | trainAcc: 71.3542% (274/384)\n",
            "6 13 Epoch: 16 | ANN: trainLoss: 0.6475 | trainAcc: 70.7589% (317/448)\n",
            "7 13 Epoch: 16 | ANN: trainLoss: 0.6483 | trainAcc: 70.8984% (363/512)\n",
            "8 13 Epoch: 16 | ANN: trainLoss: 0.6474 | trainAcc: 70.8333% (408/576)\n",
            "9 13 Epoch: 16 | ANN: trainLoss: 0.6542 | trainAcc: 70.0000% (448/640)\n",
            "10 13 Epoch: 16 | ANN: trainLoss: 0.6464 | trainAcc: 70.4545% (496/704)\n",
            "11 13 Epoch: 16 | ANN: trainLoss: 0.6491 | trainAcc: 70.4427% (541/768)\n",
            "12 13 Epoch: 16 | ANN: trainLoss: 0.6704 | trainAcc: 70.3368% (543/772)\n",
            "0 4 Epoch: 16 | ANN: testLoss: 0.8807 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 16 | ANN: testLoss: 0.8505 | testAcc: 55.4688% (71/128)\n",
            "2 4 Epoch: 16 | ANN: testLoss: 0.8032 | testAcc: 58.3333% (112/192)\n",
            "3 4 Epoch: 16 | ANN: testLoss: 0.7565 | testAcc: 58.5492% (113/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 17 | ANN: trainLoss: 0.6667 | trainAcc: 70.3125% (45/64)\n",
            "1 13 Epoch: 17 | ANN: trainLoss: 0.6359 | trainAcc: 75.7812% (97/128)\n",
            "2 13 Epoch: 17 | ANN: trainLoss: 0.7028 | trainAcc: 69.2708% (133/192)\n",
            "3 13 Epoch: 17 | ANN: trainLoss: 0.7117 | trainAcc: 70.7031% (181/256)\n",
            "4 13 Epoch: 17 | ANN: trainLoss: 0.7049 | trainAcc: 70.3125% (225/320)\n",
            "5 13 Epoch: 17 | ANN: trainLoss: 0.6885 | trainAcc: 71.3542% (274/384)\n",
            "6 13 Epoch: 17 | ANN: trainLoss: 0.6919 | trainAcc: 71.4286% (320/448)\n",
            "7 13 Epoch: 17 | ANN: trainLoss: 0.6790 | trainAcc: 70.8984% (363/512)\n",
            "8 13 Epoch: 17 | ANN: trainLoss: 0.6843 | trainAcc: 70.4861% (406/576)\n",
            "9 13 Epoch: 17 | ANN: trainLoss: 0.6798 | trainAcc: 70.6250% (452/640)\n",
            "10 13 Epoch: 17 | ANN: trainLoss: 0.6861 | trainAcc: 70.1705% (494/704)\n",
            "11 13 Epoch: 17 | ANN: trainLoss: 0.6909 | trainAcc: 69.6615% (535/768)\n",
            "12 13 Epoch: 17 | ANN: trainLoss: 0.7130 | trainAcc: 69.5596% (537/772)\n",
            "0 4 Epoch: 17 | ANN: testLoss: 0.6841 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 17 | ANN: testLoss: 0.7804 | testAcc: 58.5938% (75/128)\n",
            "2 4 Epoch: 17 | ANN: testLoss: 0.7855 | testAcc: 57.8125% (111/192)\n",
            "3 4 Epoch: 17 | ANN: testLoss: 0.7186 | testAcc: 58.0311% (112/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 18 | ANN: trainLoss: 0.6075 | trainAcc: 68.7500% (44/64)\n",
            "1 13 Epoch: 18 | ANN: trainLoss: 0.5517 | trainAcc: 75.7812% (97/128)\n",
            "2 13 Epoch: 18 | ANN: trainLoss: 0.5889 | trainAcc: 75.5208% (145/192)\n",
            "3 13 Epoch: 18 | ANN: trainLoss: 0.5947 | trainAcc: 75.3906% (193/256)\n",
            "4 13 Epoch: 18 | ANN: trainLoss: 0.6089 | trainAcc: 73.7500% (236/320)\n",
            "5 13 Epoch: 18 | ANN: trainLoss: 0.6210 | trainAcc: 72.3958% (278/384)\n",
            "6 13 Epoch: 18 | ANN: trainLoss: 0.6402 | trainAcc: 72.3214% (324/448)\n",
            "7 13 Epoch: 18 | ANN: trainLoss: 0.6371 | trainAcc: 72.8516% (373/512)\n",
            "8 13 Epoch: 18 | ANN: trainLoss: 0.6441 | trainAcc: 72.9167% (420/576)\n",
            "9 13 Epoch: 18 | ANN: trainLoss: 0.6415 | trainAcc: 73.4375% (470/640)\n",
            "10 13 Epoch: 18 | ANN: trainLoss: 0.6453 | trainAcc: 73.0114% (514/704)\n",
            "11 13 Epoch: 18 | ANN: trainLoss: 0.6460 | trainAcc: 72.3958% (556/768)\n",
            "12 13 Epoch: 18 | ANN: trainLoss: 0.6286 | trainAcc: 72.4093% (559/772)\n",
            "0 4 Epoch: 18 | ANN: testLoss: 0.7109 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 18 | ANN: testLoss: 0.7441 | testAcc: 60.1562% (77/128)\n",
            "2 4 Epoch: 18 | ANN: testLoss: 0.7551 | testAcc: 58.3333% (112/192)\n",
            "3 4 Epoch: 18 | ANN: testLoss: 0.8426 | testAcc: 58.0311% (112/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 19 | ANN: trainLoss: 0.6015 | trainAcc: 65.6250% (42/64)\n",
            "1 13 Epoch: 19 | ANN: trainLoss: 0.5983 | trainAcc: 71.0938% (91/128)\n",
            "2 13 Epoch: 19 | ANN: trainLoss: 0.5908 | trainAcc: 70.3125% (135/192)\n",
            "3 13 Epoch: 19 | ANN: trainLoss: 0.5924 | trainAcc: 70.3125% (180/256)\n",
            "4 13 Epoch: 19 | ANN: trainLoss: 0.6326 | trainAcc: 68.7500% (220/320)\n",
            "5 13 Epoch: 19 | ANN: trainLoss: 0.6153 | trainAcc: 70.8333% (272/384)\n",
            "6 13 Epoch: 19 | ANN: trainLoss: 0.6454 | trainAcc: 69.4196% (311/448)\n",
            "7 13 Epoch: 19 | ANN: trainLoss: 0.6476 | trainAcc: 69.3359% (355/512)\n",
            "8 13 Epoch: 19 | ANN: trainLoss: 0.6463 | trainAcc: 69.6181% (401/576)\n",
            "9 13 Epoch: 19 | ANN: trainLoss: 0.6450 | trainAcc: 69.6875% (446/640)\n",
            "10 13 Epoch: 19 | ANN: trainLoss: 0.6443 | trainAcc: 70.0284% (493/704)\n",
            "11 13 Epoch: 19 | ANN: trainLoss: 0.6463 | trainAcc: 70.0521% (538/768)\n",
            "12 13 Epoch: 19 | ANN: trainLoss: 0.6236 | trainAcc: 70.0777% (541/772)\n",
            "0 4 Epoch: 19 | ANN: testLoss: 0.7471 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 19 | ANN: testLoss: 0.7216 | testAcc: 60.9375% (78/128)\n",
            "2 4 Epoch: 19 | ANN: testLoss: 0.7177 | testAcc: 58.3333% (112/192)\n",
            "3 4 Epoch: 19 | ANN: testLoss: 0.8066 | testAcc: 58.0311% (112/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 20 | ANN: trainLoss: 0.5247 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 20 | ANN: trainLoss: 0.6009 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 20 | ANN: trainLoss: 0.5795 | trainAcc: 75.0000% (144/192)\n",
            "3 13 Epoch: 20 | ANN: trainLoss: 0.5779 | trainAcc: 74.2188% (190/256)\n",
            "4 13 Epoch: 20 | ANN: trainLoss: 0.5789 | trainAcc: 73.1250% (234/320)\n",
            "5 13 Epoch: 20 | ANN: trainLoss: 0.5919 | trainAcc: 72.6562% (279/384)\n",
            "6 13 Epoch: 20 | ANN: trainLoss: 0.5858 | trainAcc: 72.0982% (323/448)\n",
            "7 13 Epoch: 20 | ANN: trainLoss: 0.5818 | trainAcc: 72.4609% (371/512)\n",
            "8 13 Epoch: 20 | ANN: trainLoss: 0.5778 | trainAcc: 72.5694% (418/576)\n",
            "9 13 Epoch: 20 | ANN: trainLoss: 0.5983 | trainAcc: 71.7188% (459/640)\n",
            "10 13 Epoch: 20 | ANN: trainLoss: 0.5943 | trainAcc: 71.7330% (505/704)\n",
            "11 13 Epoch: 20 | ANN: trainLoss: 0.5926 | trainAcc: 71.7448% (551/768)\n",
            "12 13 Epoch: 20 | ANN: trainLoss: 0.6399 | trainAcc: 71.6321% (553/772)\n",
            "0 4 Epoch: 20 | ANN: testLoss: 0.8091 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 20 | ANN: testLoss: 0.7255 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 20 | ANN: testLoss: 0.6928 | testAcc: 63.0208% (121/192)\n",
            "3 4 Epoch: 20 | ANN: testLoss: 0.9741 | testAcc: 62.6943% (121/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 21 | ANN: trainLoss: 0.5245 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 21 | ANN: trainLoss: 0.5164 | trainAcc: 80.4688% (103/128)\n",
            "2 13 Epoch: 21 | ANN: trainLoss: 0.5033 | trainAcc: 80.2083% (154/192)\n",
            "3 13 Epoch: 21 | ANN: trainLoss: 0.5642 | trainAcc: 77.3438% (198/256)\n",
            "4 13 Epoch: 21 | ANN: trainLoss: 0.5542 | trainAcc: 76.8750% (246/320)\n",
            "5 13 Epoch: 21 | ANN: trainLoss: 0.5739 | trainAcc: 75.5208% (290/384)\n",
            "6 13 Epoch: 21 | ANN: trainLoss: 0.5764 | trainAcc: 74.3304% (333/448)\n",
            "7 13 Epoch: 21 | ANN: trainLoss: 0.5754 | trainAcc: 74.8047% (383/512)\n",
            "8 13 Epoch: 21 | ANN: trainLoss: 0.5896 | trainAcc: 74.1319% (427/576)\n",
            "9 13 Epoch: 21 | ANN: trainLoss: 0.5954 | trainAcc: 73.4375% (470/640)\n",
            "10 13 Epoch: 21 | ANN: trainLoss: 0.6085 | trainAcc: 72.8693% (513/704)\n",
            "11 13 Epoch: 21 | ANN: trainLoss: 0.6120 | trainAcc: 71.8750% (552/768)\n",
            "12 13 Epoch: 21 | ANN: trainLoss: 0.6778 | trainAcc: 71.6321% (553/772)\n",
            "0 4 Epoch: 21 | ANN: testLoss: 0.6875 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 21 | ANN: testLoss: 0.7115 | testAcc: 59.3750% (76/128)\n",
            "2 4 Epoch: 21 | ANN: testLoss: 0.7170 | testAcc: 60.4167% (116/192)\n",
            "3 4 Epoch: 21 | ANN: testLoss: 0.7421 | testAcc: 60.6218% (117/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 22 | ANN: trainLoss: 0.5016 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 22 | ANN: trainLoss: 0.5546 | trainAcc: 78.1250% (100/128)\n",
            "2 13 Epoch: 22 | ANN: trainLoss: 0.5972 | trainAcc: 72.9167% (140/192)\n",
            "3 13 Epoch: 22 | ANN: trainLoss: 0.6357 | trainAcc: 73.0469% (187/256)\n",
            "4 13 Epoch: 22 | ANN: trainLoss: 0.6264 | trainAcc: 73.4375% (235/320)\n",
            "5 13 Epoch: 22 | ANN: trainLoss: 0.6144 | trainAcc: 74.4792% (286/384)\n",
            "6 13 Epoch: 22 | ANN: trainLoss: 0.5956 | trainAcc: 75.2232% (337/448)\n",
            "7 13 Epoch: 22 | ANN: trainLoss: 0.6000 | trainAcc: 74.8047% (383/512)\n",
            "8 13 Epoch: 22 | ANN: trainLoss: 0.6137 | trainAcc: 73.9583% (426/576)\n",
            "9 13 Epoch: 22 | ANN: trainLoss: 0.6101 | trainAcc: 74.0625% (474/640)\n",
            "10 13 Epoch: 22 | ANN: trainLoss: 0.5946 | trainAcc: 74.8580% (527/704)\n",
            "11 13 Epoch: 22 | ANN: trainLoss: 0.5950 | trainAcc: 74.7396% (574/768)\n",
            "12 13 Epoch: 22 | ANN: trainLoss: 0.6785 | trainAcc: 74.4819% (575/772)\n",
            "0 4 Epoch: 22 | ANN: testLoss: 0.8565 | testAcc: 50.0000% (32/64)\n",
            "1 4 Epoch: 22 | ANN: testLoss: 0.7608 | testAcc: 57.0312% (73/128)\n",
            "2 4 Epoch: 22 | ANN: testLoss: 0.7601 | testAcc: 57.8125% (111/192)\n",
            "3 4 Epoch: 22 | ANN: testLoss: 0.5714 | testAcc: 58.0311% (112/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 23 | ANN: trainLoss: 0.6763 | trainAcc: 71.8750% (46/64)\n",
            "1 13 Epoch: 23 | ANN: trainLoss: 0.6884 | trainAcc: 71.0938% (91/128)\n",
            "2 13 Epoch: 23 | ANN: trainLoss: 0.6519 | trainAcc: 73.9583% (142/192)\n",
            "3 13 Epoch: 23 | ANN: trainLoss: 0.6222 | trainAcc: 75.0000% (192/256)\n",
            "4 13 Epoch: 23 | ANN: trainLoss: 0.5868 | trainAcc: 75.9375% (243/320)\n",
            "5 13 Epoch: 23 | ANN: trainLoss: 0.6127 | trainAcc: 73.1771% (281/384)\n",
            "6 13 Epoch: 23 | ANN: trainLoss: 0.6162 | trainAcc: 72.7679% (326/448)\n",
            "7 13 Epoch: 23 | ANN: trainLoss: 0.6277 | trainAcc: 71.6797% (367/512)\n",
            "8 13 Epoch: 23 | ANN: trainLoss: 0.6209 | trainAcc: 72.5694% (418/576)\n",
            "9 13 Epoch: 23 | ANN: trainLoss: 0.6177 | trainAcc: 72.5000% (464/640)\n",
            "10 13 Epoch: 23 | ANN: trainLoss: 0.6080 | trainAcc: 72.7273% (512/704)\n",
            "11 13 Epoch: 23 | ANN: trainLoss: 0.6102 | trainAcc: 72.7865% (559/768)\n",
            "12 13 Epoch: 23 | ANN: trainLoss: 0.6332 | trainAcc: 72.7979% (562/772)\n",
            "0 4 Epoch: 23 | ANN: testLoss: 0.7519 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 23 | ANN: testLoss: 0.7875 | testAcc: 58.5938% (75/128)\n",
            "2 4 Epoch: 23 | ANN: testLoss: 0.7499 | testAcc: 59.8958% (115/192)\n",
            "3 4 Epoch: 23 | ANN: testLoss: 0.7110 | testAcc: 60.1036% (116/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 24 | ANN: trainLoss: 0.7221 | trainAcc: 60.9375% (39/64)\n",
            "1 13 Epoch: 24 | ANN: trainLoss: 0.7381 | trainAcc: 65.6250% (84/128)\n",
            "2 13 Epoch: 24 | ANN: trainLoss: 0.7185 | trainAcc: 66.6667% (128/192)\n",
            "3 13 Epoch: 24 | ANN: trainLoss: 0.6489 | trainAcc: 71.4844% (183/256)\n",
            "4 13 Epoch: 24 | ANN: trainLoss: 0.6485 | trainAcc: 71.5625% (229/320)\n",
            "5 13 Epoch: 24 | ANN: trainLoss: 0.6255 | trainAcc: 72.6562% (279/384)\n",
            "6 13 Epoch: 24 | ANN: trainLoss: 0.6300 | trainAcc: 72.3214% (324/448)\n",
            "7 13 Epoch: 24 | ANN: trainLoss: 0.6242 | trainAcc: 72.8516% (373/512)\n",
            "8 13 Epoch: 24 | ANN: trainLoss: 0.6214 | trainAcc: 73.4375% (423/576)\n",
            "9 13 Epoch: 24 | ANN: trainLoss: 0.6100 | trainAcc: 74.0625% (474/640)\n",
            "10 13 Epoch: 24 | ANN: trainLoss: 0.6019 | trainAcc: 73.8636% (520/704)\n",
            "11 13 Epoch: 24 | ANN: trainLoss: 0.5957 | trainAcc: 74.3490% (571/768)\n",
            "12 13 Epoch: 24 | ANN: trainLoss: 0.6287 | trainAcc: 74.3523% (574/772)\n",
            "0 4 Epoch: 24 | ANN: testLoss: 0.8926 | testAcc: 51.5625% (33/64)\n",
            "1 4 Epoch: 24 | ANN: testLoss: 0.7935 | testAcc: 56.2500% (72/128)\n",
            "2 4 Epoch: 24 | ANN: testLoss: 0.7467 | testAcc: 58.8542% (113/192)\n",
            "3 4 Epoch: 24 | ANN: testLoss: 0.7951 | testAcc: 58.5492% (113/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 25 | ANN: trainLoss: 0.5981 | trainAcc: 75.0000% (48/64)\n",
            "1 13 Epoch: 25 | ANN: trainLoss: 0.5891 | trainAcc: 75.0000% (96/128)\n",
            "2 13 Epoch: 25 | ANN: trainLoss: 0.5678 | trainAcc: 74.4792% (143/192)\n",
            "3 13 Epoch: 25 | ANN: trainLoss: 0.6170 | trainAcc: 71.8750% (184/256)\n",
            "4 13 Epoch: 25 | ANN: trainLoss: 0.6197 | trainAcc: 71.8750% (230/320)\n",
            "5 13 Epoch: 25 | ANN: trainLoss: 0.5902 | trainAcc: 73.9583% (284/384)\n",
            "6 13 Epoch: 25 | ANN: trainLoss: 0.6100 | trainAcc: 72.9911% (327/448)\n",
            "7 13 Epoch: 25 | ANN: trainLoss: 0.5880 | trainAcc: 74.6094% (382/512)\n",
            "8 13 Epoch: 25 | ANN: trainLoss: 0.6117 | trainAcc: 73.9583% (426/576)\n",
            "9 13 Epoch: 25 | ANN: trainLoss: 0.6180 | trainAcc: 73.4375% (470/640)\n",
            "10 13 Epoch: 25 | ANN: trainLoss: 0.6128 | trainAcc: 73.5795% (518/704)\n",
            "11 13 Epoch: 25 | ANN: trainLoss: 0.6075 | trainAcc: 74.0885% (569/768)\n",
            "12 13 Epoch: 25 | ANN: trainLoss: 0.6498 | trainAcc: 73.8342% (570/772)\n",
            "0 4 Epoch: 25 | ANN: testLoss: 0.7130 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 25 | ANN: testLoss: 0.7263 | testAcc: 60.9375% (78/128)\n",
            "2 4 Epoch: 25 | ANN: testLoss: 0.7290 | testAcc: 61.9792% (119/192)\n",
            "3 4 Epoch: 25 | ANN: testLoss: 0.6067 | testAcc: 62.1762% (120/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 26 | ANN: trainLoss: 0.6341 | trainAcc: 70.3125% (45/64)\n",
            "1 13 Epoch: 26 | ANN: trainLoss: 0.6961 | trainAcc: 64.8438% (83/128)\n",
            "2 13 Epoch: 26 | ANN: trainLoss: 0.6844 | trainAcc: 66.6667% (128/192)\n",
            "3 13 Epoch: 26 | ANN: trainLoss: 0.6419 | trainAcc: 68.3594% (175/256)\n",
            "4 13 Epoch: 26 | ANN: trainLoss: 0.6471 | trainAcc: 69.0625% (221/320)\n",
            "5 13 Epoch: 26 | ANN: trainLoss: 0.6597 | trainAcc: 69.2708% (266/384)\n",
            "6 13 Epoch: 26 | ANN: trainLoss: 0.6487 | trainAcc: 68.9732% (309/448)\n",
            "7 13 Epoch: 26 | ANN: trainLoss: 0.6344 | trainAcc: 69.7266% (357/512)\n",
            "8 13 Epoch: 26 | ANN: trainLoss: 0.6067 | trainAcc: 71.1806% (410/576)\n",
            "9 13 Epoch: 26 | ANN: trainLoss: 0.6006 | trainAcc: 72.5000% (464/640)\n",
            "10 13 Epoch: 26 | ANN: trainLoss: 0.6028 | trainAcc: 72.4432% (510/704)\n",
            "11 13 Epoch: 26 | ANN: trainLoss: 0.5989 | trainAcc: 72.6562% (558/768)\n",
            "12 13 Epoch: 26 | ANN: trainLoss: 0.6398 | trainAcc: 72.5389% (560/772)\n",
            "0 4 Epoch: 26 | ANN: testLoss: 0.6827 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 26 | ANN: testLoss: 0.7620 | testAcc: 59.3750% (76/128)\n",
            "2 4 Epoch: 26 | ANN: testLoss: 0.7448 | testAcc: 61.9792% (119/192)\n",
            "3 4 Epoch: 26 | ANN: testLoss: 0.6927 | testAcc: 62.1762% (120/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 27 | ANN: trainLoss: 0.4958 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 27 | ANN: trainLoss: 0.5221 | trainAcc: 78.9062% (101/128)\n",
            "2 13 Epoch: 27 | ANN: trainLoss: 0.4953 | trainAcc: 80.2083% (154/192)\n",
            "3 13 Epoch: 27 | ANN: trainLoss: 0.5134 | trainAcc: 79.2969% (203/256)\n",
            "4 13 Epoch: 27 | ANN: trainLoss: 0.5534 | trainAcc: 77.8125% (249/320)\n",
            "5 13 Epoch: 27 | ANN: trainLoss: 0.5544 | trainAcc: 77.0833% (296/384)\n",
            "6 13 Epoch: 27 | ANN: trainLoss: 0.5561 | trainAcc: 77.4554% (347/448)\n",
            "7 13 Epoch: 27 | ANN: trainLoss: 0.5561 | trainAcc: 78.1250% (400/512)\n",
            "8 13 Epoch: 27 | ANN: trainLoss: 0.5553 | trainAcc: 78.1250% (450/576)\n",
            "9 13 Epoch: 27 | ANN: trainLoss: 0.5646 | trainAcc: 77.3438% (495/640)\n",
            "10 13 Epoch: 27 | ANN: trainLoss: 0.5698 | trainAcc: 76.5625% (539/704)\n",
            "11 13 Epoch: 27 | ANN: trainLoss: 0.5765 | trainAcc: 76.0417% (584/768)\n",
            "12 13 Epoch: 27 | ANN: trainLoss: 0.6029 | trainAcc: 75.9067% (586/772)\n",
            "0 4 Epoch: 27 | ANN: testLoss: 0.7152 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 27 | ANN: testLoss: 0.7221 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 27 | ANN: testLoss: 0.7019 | testAcc: 65.1042% (125/192)\n",
            "3 4 Epoch: 27 | ANN: testLoss: 0.5758 | testAcc: 65.2850% (126/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 28 | ANN: trainLoss: 0.6212 | trainAcc: 75.0000% (48/64)\n",
            "1 13 Epoch: 28 | ANN: trainLoss: 0.5976 | trainAcc: 75.7812% (97/128)\n",
            "2 13 Epoch: 28 | ANN: trainLoss: 0.5731 | trainAcc: 76.5625% (147/192)\n",
            "3 13 Epoch: 28 | ANN: trainLoss: 0.5997 | trainAcc: 73.8281% (189/256)\n",
            "4 13 Epoch: 28 | ANN: trainLoss: 0.5830 | trainAcc: 74.6875% (239/320)\n",
            "5 13 Epoch: 28 | ANN: trainLoss: 0.5639 | trainAcc: 75.2604% (289/384)\n",
            "6 13 Epoch: 28 | ANN: trainLoss: 0.5801 | trainAcc: 74.5536% (334/448)\n",
            "7 13 Epoch: 28 | ANN: trainLoss: 0.5643 | trainAcc: 75.5859% (387/512)\n",
            "8 13 Epoch: 28 | ANN: trainLoss: 0.5535 | trainAcc: 76.0417% (438/576)\n",
            "9 13 Epoch: 28 | ANN: trainLoss: 0.5819 | trainAcc: 74.5312% (477/640)\n",
            "10 13 Epoch: 28 | ANN: trainLoss: 0.5895 | trainAcc: 74.5739% (525/704)\n",
            "11 13 Epoch: 28 | ANN: trainLoss: 0.5917 | trainAcc: 74.8698% (575/768)\n",
            "12 13 Epoch: 28 | ANN: trainLoss: 0.6467 | trainAcc: 74.6114% (576/772)\n",
            "0 4 Epoch: 28 | ANN: testLoss: 0.6070 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 28 | ANN: testLoss: 0.6779 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 28 | ANN: testLoss: 0.6652 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 28 | ANN: testLoss: 0.6386 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 29 | ANN: trainLoss: 0.4666 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 29 | ANN: trainLoss: 0.4932 | trainAcc: 79.6875% (102/128)\n",
            "2 13 Epoch: 29 | ANN: trainLoss: 0.4984 | trainAcc: 79.6875% (153/192)\n",
            "3 13 Epoch: 29 | ANN: trainLoss: 0.5106 | trainAcc: 79.2969% (203/256)\n",
            "4 13 Epoch: 29 | ANN: trainLoss: 0.5146 | trainAcc: 79.3750% (254/320)\n",
            "5 13 Epoch: 29 | ANN: trainLoss: 0.5397 | trainAcc: 77.0833% (296/384)\n",
            "6 13 Epoch: 29 | ANN: trainLoss: 0.5264 | trainAcc: 77.9018% (349/448)\n",
            "7 13 Epoch: 29 | ANN: trainLoss: 0.5338 | trainAcc: 77.9297% (399/512)\n",
            "8 13 Epoch: 29 | ANN: trainLoss: 0.5356 | trainAcc: 78.1250% (450/576)\n",
            "9 13 Epoch: 29 | ANN: trainLoss: 0.5464 | trainAcc: 77.5000% (496/640)\n",
            "10 13 Epoch: 29 | ANN: trainLoss: 0.5397 | trainAcc: 77.4148% (545/704)\n",
            "11 13 Epoch: 29 | ANN: trainLoss: 0.5386 | trainAcc: 77.3438% (594/768)\n",
            "12 13 Epoch: 29 | ANN: trainLoss: 0.5723 | trainAcc: 77.2021% (596/772)\n",
            "0 4 Epoch: 29 | ANN: testLoss: 0.7127 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 29 | ANN: testLoss: 0.6518 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 29 | ANN: testLoss: 0.6529 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 29 | ANN: testLoss: 0.5822 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 30 | ANN: trainLoss: 0.4434 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 30 | ANN: trainLoss: 0.4824 | trainAcc: 82.0312% (105/128)\n",
            "2 13 Epoch: 30 | ANN: trainLoss: 0.4969 | trainAcc: 80.7292% (155/192)\n",
            "3 13 Epoch: 30 | ANN: trainLoss: 0.5412 | trainAcc: 77.3438% (198/256)\n",
            "4 13 Epoch: 30 | ANN: trainLoss: 0.5581 | trainAcc: 75.6250% (242/320)\n",
            "5 13 Epoch: 30 | ANN: trainLoss: 0.5519 | trainAcc: 75.7812% (291/384)\n",
            "6 13 Epoch: 30 | ANN: trainLoss: 0.5327 | trainAcc: 76.5625% (343/448)\n",
            "7 13 Epoch: 30 | ANN: trainLoss: 0.5345 | trainAcc: 76.5625% (392/512)\n",
            "8 13 Epoch: 30 | ANN: trainLoss: 0.5335 | trainAcc: 76.3889% (440/576)\n",
            "9 13 Epoch: 30 | ANN: trainLoss: 0.5411 | trainAcc: 76.0938% (487/640)\n",
            "10 13 Epoch: 30 | ANN: trainLoss: 0.5398 | trainAcc: 75.8523% (534/704)\n",
            "11 13 Epoch: 30 | ANN: trainLoss: 0.5385 | trainAcc: 76.0417% (584/768)\n",
            "12 13 Epoch: 30 | ANN: trainLoss: 0.5163 | trainAcc: 76.0363% (587/772)\n",
            "0 4 Epoch: 30 | ANN: testLoss: 0.8654 | testAcc: 54.6875% (35/64)\n",
            "1 4 Epoch: 30 | ANN: testLoss: 0.8115 | testAcc: 57.8125% (74/128)\n",
            "2 4 Epoch: 30 | ANN: testLoss: 0.7899 | testAcc: 59.8958% (115/192)\n",
            "3 4 Epoch: 30 | ANN: testLoss: 0.6660 | testAcc: 60.1036% (116/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 31 | ANN: trainLoss: 0.4261 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 31 | ANN: trainLoss: 0.4974 | trainAcc: 78.1250% (100/128)\n",
            "2 13 Epoch: 31 | ANN: trainLoss: 0.4955 | trainAcc: 79.6875% (153/192)\n",
            "3 13 Epoch: 31 | ANN: trainLoss: 0.4734 | trainAcc: 79.2969% (203/256)\n",
            "4 13 Epoch: 31 | ANN: trainLoss: 0.4652 | trainAcc: 80.3125% (257/320)\n",
            "5 13 Epoch: 31 | ANN: trainLoss: 0.4545 | trainAcc: 81.2500% (312/384)\n",
            "6 13 Epoch: 31 | ANN: trainLoss: 0.4789 | trainAcc: 79.9107% (358/448)\n",
            "7 13 Epoch: 31 | ANN: trainLoss: 0.4861 | trainAcc: 78.9062% (404/512)\n",
            "8 13 Epoch: 31 | ANN: trainLoss: 0.4800 | trainAcc: 78.9931% (455/576)\n",
            "9 13 Epoch: 31 | ANN: trainLoss: 0.4807 | trainAcc: 78.5938% (503/640)\n",
            "10 13 Epoch: 31 | ANN: trainLoss: 0.4848 | trainAcc: 78.6932% (554/704)\n",
            "11 13 Epoch: 31 | ANN: trainLoss: 0.4823 | trainAcc: 79.1667% (608/768)\n",
            "12 13 Epoch: 31 | ANN: trainLoss: 0.4614 | trainAcc: 79.2746% (612/772)\n",
            "0 4 Epoch: 31 | ANN: testLoss: 0.6058 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 31 | ANN: testLoss: 0.5969 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 31 | ANN: testLoss: 0.6216 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 31 | ANN: testLoss: 0.9996 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 32 | ANN: trainLoss: 0.5320 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 32 | ANN: trainLoss: 0.5085 | trainAcc: 79.6875% (102/128)\n",
            "2 13 Epoch: 32 | ANN: trainLoss: 0.5179 | trainAcc: 77.0833% (148/192)\n",
            "3 13 Epoch: 32 | ANN: trainLoss: 0.4952 | trainAcc: 78.5156% (201/256)\n",
            "4 13 Epoch: 32 | ANN: trainLoss: 0.5119 | trainAcc: 76.8750% (246/320)\n",
            "5 13 Epoch: 32 | ANN: trainLoss: 0.5097 | trainAcc: 77.0833% (296/384)\n",
            "6 13 Epoch: 32 | ANN: trainLoss: 0.4948 | trainAcc: 77.4554% (347/448)\n",
            "7 13 Epoch: 32 | ANN: trainLoss: 0.5007 | trainAcc: 77.1484% (395/512)\n",
            "8 13 Epoch: 32 | ANN: trainLoss: 0.4948 | trainAcc: 77.0833% (444/576)\n",
            "9 13 Epoch: 32 | ANN: trainLoss: 0.4949 | trainAcc: 77.3438% (495/640)\n",
            "10 13 Epoch: 32 | ANN: trainLoss: 0.4913 | trainAcc: 77.6989% (547/704)\n",
            "11 13 Epoch: 32 | ANN: trainLoss: 0.4961 | trainAcc: 77.6042% (596/768)\n",
            "12 13 Epoch: 32 | ANN: trainLoss: 0.4607 | trainAcc: 77.7202% (600/772)\n",
            "0 4 Epoch: 32 | ANN: testLoss: 0.5681 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 32 | ANN: testLoss: 0.6321 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 32 | ANN: testLoss: 0.6440 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 32 | ANN: testLoss: 0.4972 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 33 | ANN: trainLoss: 0.4588 | trainAcc: 75.0000% (48/64)\n",
            "1 13 Epoch: 33 | ANN: trainLoss: 0.4649 | trainAcc: 77.3438% (99/128)\n",
            "2 13 Epoch: 33 | ANN: trainLoss: 0.4621 | trainAcc: 79.1667% (152/192)\n",
            "3 13 Epoch: 33 | ANN: trainLoss: 0.4927 | trainAcc: 77.3438% (198/256)\n",
            "4 13 Epoch: 33 | ANN: trainLoss: 0.4944 | trainAcc: 76.5625% (245/320)\n",
            "5 13 Epoch: 33 | ANN: trainLoss: 0.4747 | trainAcc: 78.6458% (302/384)\n",
            "6 13 Epoch: 33 | ANN: trainLoss: 0.4617 | trainAcc: 79.0179% (354/448)\n",
            "7 13 Epoch: 33 | ANN: trainLoss: 0.4486 | trainAcc: 80.4688% (412/512)\n",
            "8 13 Epoch: 33 | ANN: trainLoss: 0.4376 | trainAcc: 81.5972% (470/576)\n",
            "9 13 Epoch: 33 | ANN: trainLoss: 0.4342 | trainAcc: 81.8750% (524/640)\n",
            "10 13 Epoch: 33 | ANN: trainLoss: 0.4402 | trainAcc: 81.2500% (572/704)\n",
            "11 13 Epoch: 33 | ANN: trainLoss: 0.4469 | trainAcc: 80.9896% (622/768)\n",
            "12 13 Epoch: 33 | ANN: trainLoss: 0.4260 | trainAcc: 81.0881% (626/772)\n",
            "0 4 Epoch: 33 | ANN: testLoss: 0.5731 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 33 | ANN: testLoss: 0.6409 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 33 | ANN: testLoss: 0.6458 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 33 | ANN: testLoss: 0.5203 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 34 | ANN: trainLoss: 0.5806 | trainAcc: 75.0000% (48/64)\n",
            "1 13 Epoch: 34 | ANN: trainLoss: 0.5181 | trainAcc: 77.3438% (99/128)\n",
            "2 13 Epoch: 34 | ANN: trainLoss: 0.4831 | trainAcc: 79.6875% (153/192)\n",
            "3 13 Epoch: 34 | ANN: trainLoss: 0.4820 | trainAcc: 79.6875% (204/256)\n",
            "4 13 Epoch: 34 | ANN: trainLoss: 0.4722 | trainAcc: 80.6250% (258/320)\n",
            "5 13 Epoch: 34 | ANN: trainLoss: 0.4811 | trainAcc: 80.2083% (308/384)\n",
            "6 13 Epoch: 34 | ANN: trainLoss: 0.4751 | trainAcc: 80.5804% (361/448)\n",
            "7 13 Epoch: 34 | ANN: trainLoss: 0.4625 | trainAcc: 81.2500% (416/512)\n",
            "8 13 Epoch: 34 | ANN: trainLoss: 0.4580 | trainAcc: 81.4236% (469/576)\n",
            "9 13 Epoch: 34 | ANN: trainLoss: 0.4494 | trainAcc: 82.0312% (525/640)\n",
            "10 13 Epoch: 34 | ANN: trainLoss: 0.4373 | trainAcc: 82.6705% (582/704)\n",
            "11 13 Epoch: 34 | ANN: trainLoss: 0.4362 | trainAcc: 82.4219% (633/768)\n",
            "12 13 Epoch: 34 | ANN: trainLoss: 0.4195 | trainAcc: 82.5130% (637/772)\n",
            "0 4 Epoch: 34 | ANN: testLoss: 0.5615 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 34 | ANN: testLoss: 0.5987 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 34 | ANN: testLoss: 0.6327 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 34 | ANN: testLoss: 0.5785 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 35 | ANN: trainLoss: 0.6250 | trainAcc: 67.1875% (43/64)\n",
            "1 13 Epoch: 35 | ANN: trainLoss: 0.5095 | trainAcc: 76.5625% (98/128)\n",
            "2 13 Epoch: 35 | ANN: trainLoss: 0.4681 | trainAcc: 79.6875% (153/192)\n",
            "3 13 Epoch: 35 | ANN: trainLoss: 0.4394 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 35 | ANN: trainLoss: 0.4306 | trainAcc: 81.5625% (261/320)\n",
            "5 13 Epoch: 35 | ANN: trainLoss: 0.4211 | trainAcc: 82.5521% (317/384)\n",
            "6 13 Epoch: 35 | ANN: trainLoss: 0.4170 | trainAcc: 83.2589% (373/448)\n",
            "7 13 Epoch: 35 | ANN: trainLoss: 0.4228 | trainAcc: 82.8125% (424/512)\n",
            "8 13 Epoch: 35 | ANN: trainLoss: 0.4455 | trainAcc: 81.5972% (470/576)\n",
            "9 13 Epoch: 35 | ANN: trainLoss: 0.4462 | trainAcc: 81.2500% (520/640)\n",
            "10 13 Epoch: 35 | ANN: trainLoss: 0.4485 | trainAcc: 81.1080% (571/704)\n",
            "11 13 Epoch: 35 | ANN: trainLoss: 0.4405 | trainAcc: 81.5104% (626/768)\n",
            "12 13 Epoch: 35 | ANN: trainLoss: 0.4712 | trainAcc: 81.3472% (628/772)\n",
            "0 4 Epoch: 35 | ANN: testLoss: 0.6834 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 35 | ANN: testLoss: 0.6121 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 35 | ANN: testLoss: 0.6158 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 35 | ANN: testLoss: 0.6618 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 36 | ANN: trainLoss: 0.4056 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 36 | ANN: trainLoss: 0.3654 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 36 | ANN: trainLoss: 0.3921 | trainAcc: 83.8542% (161/192)\n",
            "3 13 Epoch: 36 | ANN: trainLoss: 0.4059 | trainAcc: 83.2031% (213/256)\n",
            "4 13 Epoch: 36 | ANN: trainLoss: 0.4292 | trainAcc: 81.8750% (262/320)\n",
            "5 13 Epoch: 36 | ANN: trainLoss: 0.4204 | trainAcc: 82.5521% (317/384)\n",
            "6 13 Epoch: 36 | ANN: trainLoss: 0.4294 | trainAcc: 82.1429% (368/448)\n",
            "7 13 Epoch: 36 | ANN: trainLoss: 0.4228 | trainAcc: 82.6172% (423/512)\n",
            "8 13 Epoch: 36 | ANN: trainLoss: 0.4378 | trainAcc: 82.2917% (474/576)\n",
            "9 13 Epoch: 36 | ANN: trainLoss: 0.4339 | trainAcc: 82.5000% (528/640)\n",
            "10 13 Epoch: 36 | ANN: trainLoss: 0.4254 | trainAcc: 82.9545% (584/704)\n",
            "11 13 Epoch: 36 | ANN: trainLoss: 0.4314 | trainAcc: 82.4219% (633/768)\n",
            "12 13 Epoch: 36 | ANN: trainLoss: 0.4287 | trainAcc: 82.3834% (636/772)\n",
            "0 4 Epoch: 36 | ANN: testLoss: 0.6661 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 36 | ANN: testLoss: 0.6563 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 36 | ANN: testLoss: 0.6812 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 36 | ANN: testLoss: 0.5163 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 37 | ANN: trainLoss: 0.2868 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 37 | ANN: trainLoss: 0.3881 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 37 | ANN: trainLoss: 0.3852 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 37 | ANN: trainLoss: 0.3787 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 37 | ANN: trainLoss: 0.3822 | trainAcc: 84.0625% (269/320)\n",
            "5 13 Epoch: 37 | ANN: trainLoss: 0.3982 | trainAcc: 83.0729% (319/384)\n",
            "6 13 Epoch: 37 | ANN: trainLoss: 0.4202 | trainAcc: 82.8125% (371/448)\n",
            "7 13 Epoch: 37 | ANN: trainLoss: 0.4001 | trainAcc: 84.5703% (433/512)\n",
            "8 13 Epoch: 37 | ANN: trainLoss: 0.3984 | trainAcc: 84.2014% (485/576)\n",
            "9 13 Epoch: 37 | ANN: trainLoss: 0.3953 | trainAcc: 84.6875% (542/640)\n",
            "10 13 Epoch: 37 | ANN: trainLoss: 0.3966 | trainAcc: 84.6591% (596/704)\n",
            "11 13 Epoch: 37 | ANN: trainLoss: 0.3927 | trainAcc: 84.5052% (649/768)\n",
            "12 13 Epoch: 37 | ANN: trainLoss: 0.4391 | trainAcc: 84.4560% (652/772)\n",
            "0 4 Epoch: 37 | ANN: testLoss: 0.8680 | testAcc: 57.8125% (37/64)\n",
            "1 4 Epoch: 37 | ANN: testLoss: 0.7988 | testAcc: 61.7188% (79/128)\n",
            "2 4 Epoch: 37 | ANN: testLoss: 0.8116 | testAcc: 62.5000% (120/192)\n",
            "3 4 Epoch: 37 | ANN: testLoss: 0.6936 | testAcc: 62.6943% (121/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 38 | ANN: trainLoss: 0.6107 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 38 | ANN: trainLoss: 0.6226 | trainAcc: 74.2188% (95/128)\n",
            "2 13 Epoch: 38 | ANN: trainLoss: 0.5259 | trainAcc: 79.1667% (152/192)\n",
            "3 13 Epoch: 38 | ANN: trainLoss: 0.4919 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 38 | ANN: trainLoss: 0.4709 | trainAcc: 80.9375% (259/320)\n",
            "5 13 Epoch: 38 | ANN: trainLoss: 0.4861 | trainAcc: 80.4688% (309/384)\n",
            "6 13 Epoch: 38 | ANN: trainLoss: 0.4745 | trainAcc: 80.5804% (361/448)\n",
            "7 13 Epoch: 38 | ANN: trainLoss: 0.4596 | trainAcc: 81.2500% (416/512)\n",
            "8 13 Epoch: 38 | ANN: trainLoss: 0.4608 | trainAcc: 80.7292% (465/576)\n",
            "9 13 Epoch: 38 | ANN: trainLoss: 0.4561 | trainAcc: 81.4062% (521/640)\n",
            "10 13 Epoch: 38 | ANN: trainLoss: 0.4434 | trainAcc: 82.1023% (578/704)\n",
            "11 13 Epoch: 38 | ANN: trainLoss: 0.4530 | trainAcc: 80.9896% (622/768)\n",
            "12 13 Epoch: 38 | ANN: trainLoss: 0.5505 | trainAcc: 80.8290% (624/772)\n",
            "0 4 Epoch: 38 | ANN: testLoss: 0.7551 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 38 | ANN: testLoss: 0.7481 | testAcc: 61.7188% (79/128)\n",
            "2 4 Epoch: 38 | ANN: testLoss: 0.6933 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 38 | ANN: testLoss: 0.8671 | testAcc: 64.2487% (124/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 39 | ANN: trainLoss: 0.3798 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 39 | ANN: trainLoss: 0.3483 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 39 | ANN: trainLoss: 0.4022 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 39 | ANN: trainLoss: 0.4084 | trainAcc: 83.5938% (214/256)\n",
            "4 13 Epoch: 39 | ANN: trainLoss: 0.3837 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 39 | ANN: trainLoss: 0.4171 | trainAcc: 83.3333% (320/384)\n",
            "6 13 Epoch: 39 | ANN: trainLoss: 0.4259 | trainAcc: 83.0357% (372/448)\n",
            "7 13 Epoch: 39 | ANN: trainLoss: 0.4221 | trainAcc: 83.5938% (428/512)\n",
            "8 13 Epoch: 39 | ANN: trainLoss: 0.4358 | trainAcc: 83.3333% (480/576)\n",
            "9 13 Epoch: 39 | ANN: trainLoss: 0.4476 | trainAcc: 82.8125% (530/640)\n",
            "10 13 Epoch: 39 | ANN: trainLoss: 0.4436 | trainAcc: 82.8125% (583/704)\n",
            "11 13 Epoch: 39 | ANN: trainLoss: 0.4470 | trainAcc: 82.5521% (634/768)\n",
            "12 13 Epoch: 39 | ANN: trainLoss: 0.4425 | trainAcc: 82.6425% (638/772)\n",
            "0 4 Epoch: 39 | ANN: testLoss: 0.6635 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 39 | ANN: testLoss: 0.7172 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 39 | ANN: testLoss: 0.7318 | testAcc: 64.0625% (123/192)\n",
            "3 4 Epoch: 39 | ANN: testLoss: 0.8116 | testAcc: 63.7306% (123/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 40 | ANN: trainLoss: 0.4228 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 40 | ANN: trainLoss: 0.4315 | trainAcc: 81.2500% (104/128)\n",
            "2 13 Epoch: 40 | ANN: trainLoss: 0.4761 | trainAcc: 79.6875% (153/192)\n",
            "3 13 Epoch: 40 | ANN: trainLoss: 0.4894 | trainAcc: 78.9062% (202/256)\n",
            "4 13 Epoch: 40 | ANN: trainLoss: 0.4818 | trainAcc: 78.4375% (251/320)\n",
            "5 13 Epoch: 40 | ANN: trainLoss: 0.4791 | trainAcc: 79.1667% (304/384)\n",
            "6 13 Epoch: 40 | ANN: trainLoss: 0.4682 | trainAcc: 80.3571% (360/448)\n",
            "7 13 Epoch: 40 | ANN: trainLoss: 0.4807 | trainAcc: 80.6641% (413/512)\n",
            "8 13 Epoch: 40 | ANN: trainLoss: 0.4762 | trainAcc: 81.0764% (467/576)\n",
            "9 13 Epoch: 40 | ANN: trainLoss: 0.4751 | trainAcc: 81.0938% (519/640)\n",
            "10 13 Epoch: 40 | ANN: trainLoss: 0.4702 | trainAcc: 81.2500% (572/704)\n",
            "11 13 Epoch: 40 | ANN: trainLoss: 0.4687 | trainAcc: 80.8594% (621/768)\n",
            "12 13 Epoch: 40 | ANN: trainLoss: 0.4493 | trainAcc: 80.9585% (625/772)\n",
            "0 4 Epoch: 40 | ANN: testLoss: 0.8022 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 40 | ANN: testLoss: 0.7669 | testAcc: 62.5000% (80/128)\n",
            "2 4 Epoch: 40 | ANN: testLoss: 0.7407 | testAcc: 62.5000% (120/192)\n",
            "3 4 Epoch: 40 | ANN: testLoss: 0.7604 | testAcc: 62.1762% (120/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 41 | ANN: trainLoss: 0.3031 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 41 | ANN: trainLoss: 0.4724 | trainAcc: 78.9062% (101/128)\n",
            "2 13 Epoch: 41 | ANN: trainLoss: 0.4614 | trainAcc: 81.2500% (156/192)\n",
            "3 13 Epoch: 41 | ANN: trainLoss: 0.4421 | trainAcc: 81.6406% (209/256)\n",
            "4 13 Epoch: 41 | ANN: trainLoss: 0.4701 | trainAcc: 80.9375% (259/320)\n",
            "5 13 Epoch: 41 | ANN: trainLoss: 0.4726 | trainAcc: 80.4688% (309/384)\n",
            "6 13 Epoch: 41 | ANN: trainLoss: 0.4665 | trainAcc: 80.8036% (362/448)\n",
            "7 13 Epoch: 41 | ANN: trainLoss: 0.4687 | trainAcc: 81.2500% (416/512)\n",
            "8 13 Epoch: 41 | ANN: trainLoss: 0.4737 | trainAcc: 80.7292% (465/576)\n",
            "9 13 Epoch: 41 | ANN: trainLoss: 0.4687 | trainAcc: 81.2500% (520/640)\n",
            "10 13 Epoch: 41 | ANN: trainLoss: 0.4758 | trainAcc: 80.9659% (570/704)\n",
            "11 13 Epoch: 41 | ANN: trainLoss: 0.4682 | trainAcc: 81.3802% (625/768)\n",
            "12 13 Epoch: 41 | ANN: trainLoss: 0.4824 | trainAcc: 81.3472% (628/772)\n",
            "0 4 Epoch: 41 | ANN: testLoss: 0.6069 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 41 | ANN: testLoss: 0.7040 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 41 | ANN: testLoss: 0.6977 | testAcc: 63.5417% (122/192)\n",
            "3 4 Epoch: 41 | ANN: testLoss: 0.5564 | testAcc: 63.7306% (123/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 42 | ANN: trainLoss: 0.3914 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 42 | ANN: trainLoss: 0.3654 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 42 | ANN: trainLoss: 0.3438 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 42 | ANN: trainLoss: 0.3538 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 42 | ANN: trainLoss: 0.3598 | trainAcc: 85.0000% (272/320)\n",
            "5 13 Epoch: 42 | ANN: trainLoss: 0.3928 | trainAcc: 84.3750% (324/384)\n",
            "6 13 Epoch: 42 | ANN: trainLoss: 0.3801 | trainAcc: 85.2679% (382/448)\n",
            "7 13 Epoch: 42 | ANN: trainLoss: 0.3803 | trainAcc: 84.9609% (435/512)\n",
            "8 13 Epoch: 42 | ANN: trainLoss: 0.3857 | trainAcc: 84.2014% (485/576)\n",
            "9 13 Epoch: 42 | ANN: trainLoss: 0.3845 | trainAcc: 84.5312% (541/640)\n",
            "10 13 Epoch: 42 | ANN: trainLoss: 0.3919 | trainAcc: 83.9489% (591/704)\n",
            "11 13 Epoch: 42 | ANN: trainLoss: 0.3989 | trainAcc: 83.7240% (643/768)\n",
            "12 13 Epoch: 42 | ANN: trainLoss: 0.4381 | trainAcc: 83.4197% (644/772)\n",
            "0 4 Epoch: 42 | ANN: testLoss: 0.6887 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 42 | ANN: testLoss: 0.6638 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 42 | ANN: testLoss: 0.6498 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 42 | ANN: testLoss: 0.6673 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 43 | ANN: trainLoss: 0.4533 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 43 | ANN: trainLoss: 0.4771 | trainAcc: 77.3438% (99/128)\n",
            "2 13 Epoch: 43 | ANN: trainLoss: 0.4612 | trainAcc: 79.6875% (153/192)\n",
            "3 13 Epoch: 43 | ANN: trainLoss: 0.4430 | trainAcc: 81.2500% (208/256)\n",
            "4 13 Epoch: 43 | ANN: trainLoss: 0.4250 | trainAcc: 82.5000% (264/320)\n",
            "5 13 Epoch: 43 | ANN: trainLoss: 0.4326 | trainAcc: 82.8125% (318/384)\n",
            "6 13 Epoch: 43 | ANN: trainLoss: 0.4476 | trainAcc: 82.3661% (369/448)\n",
            "7 13 Epoch: 43 | ANN: trainLoss: 0.4477 | trainAcc: 81.8359% (419/512)\n",
            "8 13 Epoch: 43 | ANN: trainLoss: 0.4560 | trainAcc: 81.0764% (467/576)\n",
            "9 13 Epoch: 43 | ANN: trainLoss: 0.4612 | trainAcc: 80.7812% (517/640)\n",
            "10 13 Epoch: 43 | ANN: trainLoss: 0.4617 | trainAcc: 80.9659% (570/704)\n",
            "11 13 Epoch: 43 | ANN: trainLoss: 0.4670 | trainAcc: 80.7292% (620/768)\n",
            "12 13 Epoch: 43 | ANN: trainLoss: 0.4405 | trainAcc: 80.8290% (624/772)\n",
            "0 4 Epoch: 43 | ANN: testLoss: 0.5567 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 43 | ANN: testLoss: 0.6192 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 43 | ANN: testLoss: 0.6158 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 43 | ANN: testLoss: 1.1785 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 44 | ANN: trainLoss: 0.5099 | trainAcc: 76.5625% (49/64)\n",
            "1 13 Epoch: 44 | ANN: trainLoss: 0.4487 | trainAcc: 79.6875% (102/128)\n",
            "2 13 Epoch: 44 | ANN: trainLoss: 0.4198 | trainAcc: 79.1667% (152/192)\n",
            "3 13 Epoch: 44 | ANN: trainLoss: 0.4076 | trainAcc: 80.0781% (205/256)\n",
            "4 13 Epoch: 44 | ANN: trainLoss: 0.3778 | trainAcc: 81.8750% (262/320)\n",
            "5 13 Epoch: 44 | ANN: trainLoss: 0.4178 | trainAcc: 80.7292% (310/384)\n",
            "6 13 Epoch: 44 | ANN: trainLoss: 0.4159 | trainAcc: 81.0268% (363/448)\n",
            "7 13 Epoch: 44 | ANN: trainLoss: 0.4018 | trainAcc: 82.0312% (420/512)\n",
            "8 13 Epoch: 44 | ANN: trainLoss: 0.4025 | trainAcc: 81.9444% (472/576)\n",
            "9 13 Epoch: 44 | ANN: trainLoss: 0.4095 | trainAcc: 81.8750% (524/640)\n",
            "10 13 Epoch: 44 | ANN: trainLoss: 0.4204 | trainAcc: 81.3920% (573/704)\n",
            "11 13 Epoch: 44 | ANN: trainLoss: 0.4181 | trainAcc: 81.2500% (624/768)\n",
            "12 13 Epoch: 44 | ANN: trainLoss: 0.3953 | trainAcc: 81.3472% (628/772)\n",
            "0 4 Epoch: 44 | ANN: testLoss: 0.6721 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 44 | ANN: testLoss: 0.5992 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 44 | ANN: testLoss: 0.6071 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 44 | ANN: testLoss: 0.4575 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 45 | ANN: trainLoss: 0.3417 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 45 | ANN: trainLoss: 0.3526 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 45 | ANN: trainLoss: 0.3491 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 45 | ANN: trainLoss: 0.3390 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 45 | ANN: trainLoss: 0.3884 | trainAcc: 83.4375% (267/320)\n",
            "5 13 Epoch: 45 | ANN: trainLoss: 0.4054 | trainAcc: 82.0312% (315/384)\n",
            "6 13 Epoch: 45 | ANN: trainLoss: 0.3998 | trainAcc: 82.5893% (370/448)\n",
            "7 13 Epoch: 45 | ANN: trainLoss: 0.4042 | trainAcc: 83.0078% (425/512)\n",
            "8 13 Epoch: 45 | ANN: trainLoss: 0.3917 | trainAcc: 83.6806% (482/576)\n",
            "9 13 Epoch: 45 | ANN: trainLoss: 0.3888 | trainAcc: 83.7500% (536/640)\n",
            "10 13 Epoch: 45 | ANN: trainLoss: 0.3919 | trainAcc: 84.0909% (592/704)\n",
            "11 13 Epoch: 45 | ANN: trainLoss: 0.3864 | trainAcc: 84.6354% (650/768)\n",
            "12 13 Epoch: 45 | ANN: trainLoss: 0.3634 | trainAcc: 84.7150% (654/772)\n",
            "0 4 Epoch: 45 | ANN: testLoss: 0.6593 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 45 | ANN: testLoss: 0.6032 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 45 | ANN: testLoss: 0.6199 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 45 | ANN: testLoss: 0.4666 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 46 | ANN: trainLoss: 0.3546 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 46 | ANN: trainLoss: 0.3388 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 46 | ANN: trainLoss: 0.3453 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 46 | ANN: trainLoss: 0.3727 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 46 | ANN: trainLoss: 0.3511 | trainAcc: 85.3125% (273/320)\n",
            "5 13 Epoch: 46 | ANN: trainLoss: 0.3453 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 46 | ANN: trainLoss: 0.3357 | trainAcc: 85.9375% (385/448)\n",
            "7 13 Epoch: 46 | ANN: trainLoss: 0.3285 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 46 | ANN: trainLoss: 0.3421 | trainAcc: 85.5903% (493/576)\n",
            "9 13 Epoch: 46 | ANN: trainLoss: 0.3479 | trainAcc: 85.0000% (544/640)\n",
            "10 13 Epoch: 46 | ANN: trainLoss: 0.3438 | trainAcc: 85.3693% (601/704)\n",
            "11 13 Epoch: 46 | ANN: trainLoss: 0.3404 | trainAcc: 85.2865% (655/768)\n",
            "12 13 Epoch: 46 | ANN: trainLoss: 0.3539 | trainAcc: 85.1036% (657/772)\n",
            "0 4 Epoch: 46 | ANN: testLoss: 0.6622 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 46 | ANN: testLoss: 0.6191 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 46 | ANN: testLoss: 0.6143 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 46 | ANN: testLoss: 0.7804 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 47 | ANN: trainLoss: 0.3865 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 47 | ANN: trainLoss: 0.3658 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 47 | ANN: trainLoss: 0.3342 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 47 | ANN: trainLoss: 0.3403 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 47 | ANN: trainLoss: 0.3596 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 47 | ANN: trainLoss: 0.3422 | trainAcc: 86.4583% (332/384)\n",
            "6 13 Epoch: 47 | ANN: trainLoss: 0.3414 | trainAcc: 85.4911% (383/448)\n",
            "7 13 Epoch: 47 | ANN: trainLoss: 0.3600 | trainAcc: 84.5703% (433/512)\n",
            "8 13 Epoch: 47 | ANN: trainLoss: 0.3723 | trainAcc: 84.8958% (489/576)\n",
            "9 13 Epoch: 47 | ANN: trainLoss: 0.3647 | trainAcc: 85.4688% (547/640)\n",
            "10 13 Epoch: 47 | ANN: trainLoss: 0.3635 | trainAcc: 85.6534% (603/704)\n",
            "11 13 Epoch: 47 | ANN: trainLoss: 0.3546 | trainAcc: 86.0677% (661/768)\n",
            "12 13 Epoch: 47 | ANN: trainLoss: 0.3491 | trainAcc: 86.0104% (664/772)\n",
            "0 4 Epoch: 47 | ANN: testLoss: 0.6602 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 47 | ANN: testLoss: 0.6113 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 47 | ANN: testLoss: 0.5925 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 47 | ANN: testLoss: 0.5941 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 48 | ANN: trainLoss: 0.3071 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 48 | ANN: trainLoss: 0.3059 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 48 | ANN: trainLoss: 0.3178 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 48 | ANN: trainLoss: 0.2922 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 48 | ANN: trainLoss: 0.3142 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 48 | ANN: trainLoss: 0.3120 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 48 | ANN: trainLoss: 0.3211 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 48 | ANN: trainLoss: 0.3401 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 48 | ANN: trainLoss: 0.3550 | trainAcc: 86.1111% (496/576)\n",
            "9 13 Epoch: 48 | ANN: trainLoss: 0.3628 | trainAcc: 85.7812% (549/640)\n",
            "10 13 Epoch: 48 | ANN: trainLoss: 0.3507 | trainAcc: 86.3636% (608/704)\n",
            "11 13 Epoch: 48 | ANN: trainLoss: 0.3471 | trainAcc: 86.3281% (663/768)\n",
            "12 13 Epoch: 48 | ANN: trainLoss: 0.3240 | trainAcc: 86.3990% (667/772)\n",
            "0 4 Epoch: 48 | ANN: testLoss: 0.7431 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 48 | ANN: testLoss: 0.6577 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 48 | ANN: testLoss: 0.6134 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 48 | ANN: testLoss: 0.4879 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 49 | ANN: trainLoss: 0.4373 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 49 | ANN: trainLoss: 0.3800 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 49 | ANN: trainLoss: 0.3450 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 49 | ANN: trainLoss: 0.3683 | trainAcc: 84.3750% (216/256)\n",
            "4 13 Epoch: 49 | ANN: trainLoss: 0.3567 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 49 | ANN: trainLoss: 0.3680 | trainAcc: 84.1146% (323/384)\n",
            "6 13 Epoch: 49 | ANN: trainLoss: 0.3686 | trainAcc: 83.9286% (376/448)\n",
            "7 13 Epoch: 49 | ANN: trainLoss: 0.3666 | trainAcc: 84.5703% (433/512)\n",
            "8 13 Epoch: 49 | ANN: trainLoss: 0.3515 | trainAcc: 85.0694% (490/576)\n",
            "9 13 Epoch: 49 | ANN: trainLoss: 0.3586 | trainAcc: 85.0000% (544/640)\n",
            "10 13 Epoch: 49 | ANN: trainLoss: 0.3537 | trainAcc: 85.0852% (599/704)\n",
            "11 13 Epoch: 49 | ANN: trainLoss: 0.3471 | trainAcc: 85.5469% (657/768)\n",
            "12 13 Epoch: 49 | ANN: trainLoss: 0.3367 | trainAcc: 85.4922% (660/772)\n",
            "0 4 Epoch: 49 | ANN: testLoss: 0.6573 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 49 | ANN: testLoss: 0.6883 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 49 | ANN: testLoss: 0.6451 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 49 | ANN: testLoss: 0.8908 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 50 | ANN: trainLoss: 0.4339 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 50 | ANN: trainLoss: 0.3550 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 50 | ANN: trainLoss: 0.3497 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 50 | ANN: trainLoss: 0.3156 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 50 | ANN: trainLoss: 0.3157 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 50 | ANN: trainLoss: 0.3423 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 50 | ANN: trainLoss: 0.3492 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 50 | ANN: trainLoss: 0.3506 | trainAcc: 86.3281% (442/512)\n",
            "8 13 Epoch: 50 | ANN: trainLoss: 0.3451 | trainAcc: 86.6319% (499/576)\n",
            "9 13 Epoch: 50 | ANN: trainLoss: 0.3330 | trainAcc: 87.1875% (558/640)\n",
            "10 13 Epoch: 50 | ANN: trainLoss: 0.3348 | trainAcc: 87.2159% (614/704)\n",
            "11 13 Epoch: 50 | ANN: trainLoss: 0.3367 | trainAcc: 87.2396% (670/768)\n",
            "12 13 Epoch: 50 | ANN: trainLoss: 0.3248 | trainAcc: 87.3057% (674/772)\n",
            "0 4 Epoch: 50 | ANN: testLoss: 0.6323 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 50 | ANN: testLoss: 0.6549 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 50 | ANN: testLoss: 0.6724 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 50 | ANN: testLoss: 1.1328 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 51 | ANN: trainLoss: 0.3064 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 51 | ANN: trainLoss: 0.2717 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 51 | ANN: trainLoss: 0.2876 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 51 | ANN: trainLoss: 0.3082 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 51 | ANN: trainLoss: 0.3113 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 51 | ANN: trainLoss: 0.3027 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 51 | ANN: trainLoss: 0.3028 | trainAcc: 89.7321% (402/448)\n",
            "7 13 Epoch: 51 | ANN: trainLoss: 0.3140 | trainAcc: 89.0625% (456/512)\n",
            "8 13 Epoch: 51 | ANN: trainLoss: 0.3013 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 51 | ANN: trainLoss: 0.3073 | trainAcc: 89.0625% (570/640)\n",
            "10 13 Epoch: 51 | ANN: trainLoss: 0.3064 | trainAcc: 88.9205% (626/704)\n",
            "11 13 Epoch: 51 | ANN: trainLoss: 0.3040 | trainAcc: 88.8021% (682/768)\n",
            "12 13 Epoch: 51 | ANN: trainLoss: 0.2808 | trainAcc: 88.8601% (686/772)\n",
            "0 4 Epoch: 51 | ANN: testLoss: 0.7979 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 51 | ANN: testLoss: 0.7038 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 51 | ANN: testLoss: 0.6794 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 51 | ANN: testLoss: 0.5133 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 52 | ANN: trainLoss: 0.3720 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 52 | ANN: trainLoss: 0.3748 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 52 | ANN: trainLoss: 0.3142 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 52 | ANN: trainLoss: 0.3143 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 52 | ANN: trainLoss: 0.2954 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 52 | ANN: trainLoss: 0.2915 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 52 | ANN: trainLoss: 0.2880 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 52 | ANN: trainLoss: 0.2765 | trainAcc: 89.2578% (457/512)\n",
            "8 13 Epoch: 52 | ANN: trainLoss: 0.2785 | trainAcc: 89.2361% (514/576)\n",
            "9 13 Epoch: 52 | ANN: trainLoss: 0.2666 | trainAcc: 89.8438% (575/640)\n",
            "10 13 Epoch: 52 | ANN: trainLoss: 0.2701 | trainAcc: 89.7727% (632/704)\n",
            "11 13 Epoch: 52 | ANN: trainLoss: 0.2774 | trainAcc: 89.3229% (686/768)\n",
            "12 13 Epoch: 52 | ANN: trainLoss: 0.3379 | trainAcc: 88.9896% (687/772)\n",
            "0 4 Epoch: 52 | ANN: testLoss: 0.6532 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 52 | ANN: testLoss: 0.6684 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 52 | ANN: testLoss: 0.6788 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 52 | ANN: testLoss: 0.5188 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 53 | ANN: trainLoss: 0.2178 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 53 | ANN: trainLoss: 0.2573 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 53 | ANN: trainLoss: 0.3215 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 53 | ANN: trainLoss: 0.3252 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 53 | ANN: trainLoss: 0.3288 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 53 | ANN: trainLoss: 0.3450 | trainAcc: 85.6771% (329/384)\n",
            "6 13 Epoch: 53 | ANN: trainLoss: 0.3484 | trainAcc: 85.4911% (383/448)\n",
            "7 13 Epoch: 53 | ANN: trainLoss: 0.3430 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 53 | ANN: trainLoss: 0.3309 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 53 | ANN: trainLoss: 0.3376 | trainAcc: 85.9375% (550/640)\n",
            "10 13 Epoch: 53 | ANN: trainLoss: 0.3395 | trainAcc: 85.9375% (605/704)\n",
            "11 13 Epoch: 53 | ANN: trainLoss: 0.3405 | trainAcc: 86.0677% (661/768)\n",
            "12 13 Epoch: 53 | ANN: trainLoss: 0.3579 | trainAcc: 86.0104% (664/772)\n",
            "0 4 Epoch: 53 | ANN: testLoss: 0.6757 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 53 | ANN: testLoss: 0.7050 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 53 | ANN: testLoss: 0.6967 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 53 | ANN: testLoss: 0.5225 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 54 | ANN: trainLoss: 0.2595 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 54 | ANN: trainLoss: 0.3446 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 54 | ANN: trainLoss: 0.3723 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 54 | ANN: trainLoss: 0.3782 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 54 | ANN: trainLoss: 0.3774 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 54 | ANN: trainLoss: 0.3815 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 54 | ANN: trainLoss: 0.3573 | trainAcc: 86.6071% (388/448)\n",
            "7 13 Epoch: 54 | ANN: trainLoss: 0.3527 | trainAcc: 87.3047% (447/512)\n",
            "8 13 Epoch: 54 | ANN: trainLoss: 0.3642 | trainAcc: 86.6319% (499/576)\n",
            "9 13 Epoch: 54 | ANN: trainLoss: 0.3642 | trainAcc: 85.7812% (549/640)\n",
            "10 13 Epoch: 54 | ANN: trainLoss: 0.3525 | trainAcc: 86.2216% (607/704)\n",
            "11 13 Epoch: 54 | ANN: trainLoss: 0.3476 | trainAcc: 86.0677% (661/768)\n",
            "12 13 Epoch: 54 | ANN: trainLoss: 0.3753 | trainAcc: 85.8808% (663/772)\n",
            "0 4 Epoch: 54 | ANN: testLoss: 0.6426 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 54 | ANN: testLoss: 0.6186 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 54 | ANN: testLoss: 0.6558 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 54 | ANN: testLoss: 0.9701 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 55 | ANN: trainLoss: 0.3625 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 55 | ANN: trainLoss: 0.3373 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 55 | ANN: trainLoss: 0.3946 | trainAcc: 84.3750% (162/192)\n",
            "3 13 Epoch: 55 | ANN: trainLoss: 0.3609 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 55 | ANN: trainLoss: 0.3727 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 55 | ANN: trainLoss: 0.3801 | trainAcc: 85.4167% (328/384)\n",
            "6 13 Epoch: 55 | ANN: trainLoss: 0.3668 | trainAcc: 86.6071% (388/448)\n",
            "7 13 Epoch: 55 | ANN: trainLoss: 0.3599 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 55 | ANN: trainLoss: 0.3533 | trainAcc: 87.6736% (505/576)\n",
            "9 13 Epoch: 55 | ANN: trainLoss: 0.3561 | trainAcc: 87.0312% (557/640)\n",
            "10 13 Epoch: 55 | ANN: trainLoss: 0.3641 | trainAcc: 85.9375% (605/704)\n",
            "11 13 Epoch: 55 | ANN: trainLoss: 0.3629 | trainAcc: 85.9375% (660/768)\n",
            "12 13 Epoch: 55 | ANN: trainLoss: 0.3403 | trainAcc: 86.0104% (664/772)\n",
            "0 4 Epoch: 55 | ANN: testLoss: 0.6427 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 55 | ANN: testLoss: 0.7074 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 55 | ANN: testLoss: 0.6771 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 55 | ANN: testLoss: 0.7168 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 56 | ANN: trainLoss: 0.3983 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 56 | ANN: trainLoss: 0.3729 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 56 | ANN: trainLoss: 0.3399 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 56 | ANN: trainLoss: 0.3305 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 56 | ANN: trainLoss: 0.3249 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 56 | ANN: trainLoss: 0.3218 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 56 | ANN: trainLoss: 0.3303 | trainAcc: 86.3839% (387/448)\n",
            "7 13 Epoch: 56 | ANN: trainLoss: 0.3290 | trainAcc: 86.3281% (442/512)\n",
            "8 13 Epoch: 56 | ANN: trainLoss: 0.3263 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 56 | ANN: trainLoss: 0.3178 | trainAcc: 86.5625% (554/640)\n",
            "10 13 Epoch: 56 | ANN: trainLoss: 0.3222 | trainAcc: 86.6477% (610/704)\n",
            "11 13 Epoch: 56 | ANN: trainLoss: 0.3111 | trainAcc: 87.2396% (670/768)\n",
            "12 13 Epoch: 56 | ANN: trainLoss: 0.3012 | trainAcc: 87.3057% (674/772)\n",
            "0 4 Epoch: 56 | ANN: testLoss: 0.6016 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 56 | ANN: testLoss: 0.7049 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 56 | ANN: testLoss: 0.6953 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 56 | ANN: testLoss: 0.5215 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 57 | ANN: trainLoss: 0.2752 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 57 | ANN: trainLoss: 0.3011 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 57 | ANN: trainLoss: 0.3206 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 57 | ANN: trainLoss: 0.3255 | trainAcc: 85.5469% (219/256)\n",
            "4 13 Epoch: 57 | ANN: trainLoss: 0.3254 | trainAcc: 85.6250% (274/320)\n",
            "5 13 Epoch: 57 | ANN: trainLoss: 0.3159 | trainAcc: 86.7188% (333/384)\n",
            "6 13 Epoch: 57 | ANN: trainLoss: 0.3104 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 57 | ANN: trainLoss: 0.3062 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 57 | ANN: trainLoss: 0.3088 | trainAcc: 87.6736% (505/576)\n",
            "9 13 Epoch: 57 | ANN: trainLoss: 0.3047 | trainAcc: 87.8125% (562/640)\n",
            "10 13 Epoch: 57 | ANN: trainLoss: 0.3088 | trainAcc: 87.5000% (616/704)\n",
            "11 13 Epoch: 57 | ANN: trainLoss: 0.3125 | trainAcc: 87.5000% (672/768)\n",
            "12 13 Epoch: 57 | ANN: trainLoss: 0.3406 | trainAcc: 87.3057% (674/772)\n",
            "0 4 Epoch: 57 | ANN: testLoss: 0.4066 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 57 | ANN: testLoss: 0.6473 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 57 | ANN: testLoss: 0.6516 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 57 | ANN: testLoss: 0.5382 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 58 | ANN: trainLoss: 0.2557 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 58 | ANN: trainLoss: 0.3116 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 58 | ANN: trainLoss: 0.2820 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 58 | ANN: trainLoss: 0.2785 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 58 | ANN: trainLoss: 0.2988 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 58 | ANN: trainLoss: 0.2901 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 58 | ANN: trainLoss: 0.2961 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 58 | ANN: trainLoss: 0.2996 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 58 | ANN: trainLoss: 0.3031 | trainAcc: 87.3264% (503/576)\n",
            "9 13 Epoch: 58 | ANN: trainLoss: 0.3010 | trainAcc: 87.5000% (560/640)\n",
            "10 13 Epoch: 58 | ANN: trainLoss: 0.3190 | trainAcc: 87.3580% (615/704)\n",
            "11 13 Epoch: 58 | ANN: trainLoss: 0.3234 | trainAcc: 87.3698% (671/768)\n",
            "12 13 Epoch: 58 | ANN: trainLoss: 0.3196 | trainAcc: 87.4352% (675/772)\n",
            "0 4 Epoch: 58 | ANN: testLoss: 0.7253 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 58 | ANN: testLoss: 0.7172 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 58 | ANN: testLoss: 0.6649 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 58 | ANN: testLoss: 0.4990 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 59 | ANN: trainLoss: 0.2714 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 59 | ANN: trainLoss: 0.2778 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 59 | ANN: trainLoss: 0.3244 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 59 | ANN: trainLoss: 0.3172 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 59 | ANN: trainLoss: 0.3031 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 59 | ANN: trainLoss: 0.2968 | trainAcc: 88.5417% (340/384)\n",
            "6 13 Epoch: 59 | ANN: trainLoss: 0.3062 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 59 | ANN: trainLoss: 0.3146 | trainAcc: 87.6953% (449/512)\n",
            "8 13 Epoch: 59 | ANN: trainLoss: 0.3022 | trainAcc: 88.3681% (509/576)\n",
            "9 13 Epoch: 59 | ANN: trainLoss: 0.2962 | trainAcc: 88.7500% (568/640)\n",
            "10 13 Epoch: 59 | ANN: trainLoss: 0.2838 | trainAcc: 89.0625% (627/704)\n",
            "11 13 Epoch: 59 | ANN: trainLoss: 0.2897 | trainAcc: 88.9323% (683/768)\n",
            "12 13 Epoch: 59 | ANN: trainLoss: 0.3148 | trainAcc: 88.8601% (686/772)\n",
            "0 4 Epoch: 59 | ANN: testLoss: 0.7585 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 59 | ANN: testLoss: 0.6680 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 59 | ANN: testLoss: 0.7060 | testAcc: 63.5417% (122/192)\n",
            "3 4 Epoch: 59 | ANN: testLoss: 1.0351 | testAcc: 63.2124% (122/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 60 | ANN: trainLoss: 0.2418 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 60 | ANN: trainLoss: 0.2978 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 60 | ANN: trainLoss: 0.2889 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 60 | ANN: trainLoss: 0.2807 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 60 | ANN: trainLoss: 0.2933 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 60 | ANN: trainLoss: 0.2956 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 60 | ANN: trainLoss: 0.2833 | trainAcc: 89.0625% (399/448)\n",
            "7 13 Epoch: 60 | ANN: trainLoss: 0.2900 | trainAcc: 89.0625% (456/512)\n",
            "8 13 Epoch: 60 | ANN: trainLoss: 0.2973 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 60 | ANN: trainLoss: 0.2844 | trainAcc: 89.2188% (571/640)\n",
            "10 13 Epoch: 60 | ANN: trainLoss: 0.2925 | trainAcc: 89.0625% (627/704)\n",
            "11 13 Epoch: 60 | ANN: trainLoss: 0.2875 | trainAcc: 89.5833% (688/768)\n",
            "12 13 Epoch: 60 | ANN: trainLoss: 0.2740 | trainAcc: 89.6373% (692/772)\n",
            "0 4 Epoch: 60 | ANN: testLoss: 0.8003 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 60 | ANN: testLoss: 0.6752 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 60 | ANN: testLoss: 0.6502 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 60 | ANN: testLoss: 1.2356 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 61 | ANN: trainLoss: 0.4112 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 61 | ANN: trainLoss: 0.4064 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 61 | ANN: trainLoss: 0.3303 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 61 | ANN: trainLoss: 0.3303 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 61 | ANN: trainLoss: 0.3087 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 61 | ANN: trainLoss: 0.3187 | trainAcc: 89.5833% (344/384)\n",
            "6 13 Epoch: 61 | ANN: trainLoss: 0.3186 | trainAcc: 89.5089% (401/448)\n",
            "7 13 Epoch: 61 | ANN: trainLoss: 0.3083 | trainAcc: 90.0391% (461/512)\n",
            "8 13 Epoch: 61 | ANN: trainLoss: 0.3136 | trainAcc: 89.4097% (515/576)\n",
            "9 13 Epoch: 61 | ANN: trainLoss: 0.3178 | trainAcc: 89.2188% (571/640)\n",
            "10 13 Epoch: 61 | ANN: trainLoss: 0.3175 | trainAcc: 88.7784% (625/704)\n",
            "11 13 Epoch: 61 | ANN: trainLoss: 0.3171 | trainAcc: 88.6719% (681/768)\n",
            "12 13 Epoch: 61 | ANN: trainLoss: 0.3282 | trainAcc: 88.6010% (684/772)\n",
            "0 4 Epoch: 61 | ANN: testLoss: 0.6973 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 61 | ANN: testLoss: 0.6526 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 61 | ANN: testLoss: 0.6499 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 61 | ANN: testLoss: 0.4877 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 62 | ANN: trainLoss: 0.2667 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 62 | ANN: trainLoss: 0.2597 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 62 | ANN: trainLoss: 0.2427 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 62 | ANN: trainLoss: 0.2567 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 62 | ANN: trainLoss: 0.2650 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 62 | ANN: trainLoss: 0.2875 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 62 | ANN: trainLoss: 0.2785 | trainAcc: 89.5089% (401/448)\n",
            "7 13 Epoch: 62 | ANN: trainLoss: 0.2788 | trainAcc: 89.8438% (460/512)\n",
            "8 13 Epoch: 62 | ANN: trainLoss: 0.2707 | trainAcc: 90.2778% (520/576)\n",
            "9 13 Epoch: 62 | ANN: trainLoss: 0.2816 | trainAcc: 89.5312% (573/640)\n",
            "10 13 Epoch: 62 | ANN: trainLoss: 0.2858 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 62 | ANN: trainLoss: 0.2879 | trainAcc: 89.3229% (686/768)\n",
            "12 13 Epoch: 62 | ANN: trainLoss: 0.2696 | trainAcc: 89.3782% (690/772)\n",
            "0 4 Epoch: 62 | ANN: testLoss: 0.6597 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 62 | ANN: testLoss: 0.6467 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 62 | ANN: testLoss: 0.6856 | testAcc: 64.5833% (124/192)\n",
            "3 4 Epoch: 62 | ANN: testLoss: 0.5976 | testAcc: 64.7668% (125/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 63 | ANN: trainLoss: 0.2417 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 63 | ANN: trainLoss: 0.2804 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 63 | ANN: trainLoss: 0.2573 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 63 | ANN: trainLoss: 0.2514 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 63 | ANN: trainLoss: 0.2666 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 63 | ANN: trainLoss: 0.2703 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 63 | ANN: trainLoss: 0.2855 | trainAcc: 89.0625% (399/448)\n",
            "7 13 Epoch: 63 | ANN: trainLoss: 0.2858 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 63 | ANN: trainLoss: 0.2747 | trainAcc: 89.2361% (514/576)\n",
            "9 13 Epoch: 63 | ANN: trainLoss: 0.2847 | trainAcc: 88.4375% (566/640)\n",
            "10 13 Epoch: 63 | ANN: trainLoss: 0.2807 | trainAcc: 88.6364% (624/704)\n",
            "11 13 Epoch: 63 | ANN: trainLoss: 0.2805 | trainAcc: 88.6719% (681/768)\n",
            "12 13 Epoch: 63 | ANN: trainLoss: 0.3143 | trainAcc: 88.4715% (683/772)\n",
            "0 4 Epoch: 63 | ANN: testLoss: 0.6219 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 63 | ANN: testLoss: 0.6901 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 63 | ANN: testLoss: 0.6485 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 63 | ANN: testLoss: 0.6386 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 64 | ANN: trainLoss: 0.1690 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 64 | ANN: trainLoss: 0.2052 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 64 | ANN: trainLoss: 0.2359 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 64 | ANN: trainLoss: 0.2847 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 64 | ANN: trainLoss: 0.2778 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 64 | ANN: trainLoss: 0.2821 | trainAcc: 88.5417% (340/384)\n",
            "6 13 Epoch: 64 | ANN: trainLoss: 0.2991 | trainAcc: 87.9464% (394/448)\n",
            "7 13 Epoch: 64 | ANN: trainLoss: 0.2818 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 64 | ANN: trainLoss: 0.2709 | trainAcc: 89.5833% (516/576)\n",
            "9 13 Epoch: 64 | ANN: trainLoss: 0.2771 | trainAcc: 89.6875% (574/640)\n",
            "10 13 Epoch: 64 | ANN: trainLoss: 0.2749 | trainAcc: 89.4886% (630/704)\n",
            "11 13 Epoch: 64 | ANN: trainLoss: 0.2719 | trainAcc: 89.1927% (685/768)\n",
            "12 13 Epoch: 64 | ANN: trainLoss: 0.3172 | trainAcc: 88.9896% (687/772)\n",
            "0 4 Epoch: 64 | ANN: testLoss: 0.6072 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 64 | ANN: testLoss: 0.6527 | testAcc: 66.4062% (85/128)\n",
            "2 4 Epoch: 64 | ANN: testLoss: 0.6445 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 64 | ANN: testLoss: 1.1410 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 65 | ANN: trainLoss: 0.3333 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 65 | ANN: trainLoss: 0.2636 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 65 | ANN: trainLoss: 0.2624 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 65 | ANN: trainLoss: 0.2682 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 65 | ANN: trainLoss: 0.2590 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 65 | ANN: trainLoss: 0.2680 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 65 | ANN: trainLoss: 0.2622 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 65 | ANN: trainLoss: 0.2615 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 65 | ANN: trainLoss: 0.2608 | trainAcc: 89.0625% (513/576)\n",
            "9 13 Epoch: 65 | ANN: trainLoss: 0.2634 | trainAcc: 88.5938% (567/640)\n",
            "10 13 Epoch: 65 | ANN: trainLoss: 0.2699 | trainAcc: 88.2102% (621/704)\n",
            "11 13 Epoch: 65 | ANN: trainLoss: 0.2681 | trainAcc: 88.4115% (679/768)\n",
            "12 13 Epoch: 65 | ANN: trainLoss: 0.3529 | trainAcc: 88.3420% (682/772)\n",
            "0 4 Epoch: 65 | ANN: testLoss: 0.7765 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 65 | ANN: testLoss: 0.6410 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 65 | ANN: testLoss: 0.6373 | testAcc: 65.6250% (126/192)\n",
            "3 4 Epoch: 65 | ANN: testLoss: 0.5535 | testAcc: 65.8031% (127/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 66 | ANN: trainLoss: 0.3124 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 66 | ANN: trainLoss: 0.2792 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 66 | ANN: trainLoss: 0.2880 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 66 | ANN: trainLoss: 0.2761 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 66 | ANN: trainLoss: 0.2725 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 66 | ANN: trainLoss: 0.2558 | trainAcc: 90.8854% (349/384)\n",
            "6 13 Epoch: 66 | ANN: trainLoss: 0.2489 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 66 | ANN: trainLoss: 0.2462 | trainAcc: 90.8203% (465/512)\n",
            "8 13 Epoch: 66 | ANN: trainLoss: 0.2551 | trainAcc: 90.7986% (523/576)\n",
            "9 13 Epoch: 66 | ANN: trainLoss: 0.2593 | trainAcc: 90.4688% (579/640)\n",
            "10 13 Epoch: 66 | ANN: trainLoss: 0.2578 | trainAcc: 90.9091% (640/704)\n",
            "11 13 Epoch: 66 | ANN: trainLoss: 0.2550 | trainAcc: 90.8854% (698/768)\n",
            "12 13 Epoch: 66 | ANN: trainLoss: 0.3554 | trainAcc: 90.8031% (701/772)\n",
            "0 4 Epoch: 66 | ANN: testLoss: 0.6416 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 66 | ANN: testLoss: 0.6422 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 66 | ANN: testLoss: 0.6193 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 66 | ANN: testLoss: 0.6513 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 67 | ANN: trainLoss: 0.2051 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 67 | ANN: trainLoss: 0.2625 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 67 | ANN: trainLoss: 0.2770 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 67 | ANN: trainLoss: 0.2520 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 67 | ANN: trainLoss: 0.2526 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 67 | ANN: trainLoss: 0.2716 | trainAcc: 91.4062% (351/384)\n",
            "6 13 Epoch: 67 | ANN: trainLoss: 0.2772 | trainAcc: 90.4018% (405/448)\n",
            "7 13 Epoch: 67 | ANN: trainLoss: 0.2677 | trainAcc: 90.4297% (463/512)\n",
            "8 13 Epoch: 67 | ANN: trainLoss: 0.2710 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 67 | ANN: trainLoss: 0.2685 | trainAcc: 89.8438% (575/640)\n",
            "10 13 Epoch: 67 | ANN: trainLoss: 0.2757 | trainAcc: 89.2045% (628/704)\n",
            "11 13 Epoch: 67 | ANN: trainLoss: 0.2716 | trainAcc: 89.0625% (684/768)\n",
            "12 13 Epoch: 67 | ANN: trainLoss: 0.2522 | trainAcc: 89.1192% (688/772)\n",
            "0 4 Epoch: 67 | ANN: testLoss: 0.6910 | testAcc: 59.3750% (38/64)\n",
            "1 4 Epoch: 67 | ANN: testLoss: 0.6258 | testAcc: 63.2812% (81/128)\n",
            "2 4 Epoch: 67 | ANN: testLoss: 0.6433 | testAcc: 63.5417% (122/192)\n",
            "3 4 Epoch: 67 | ANN: testLoss: 0.8105 | testAcc: 63.2124% (122/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 68 | ANN: trainLoss: 0.3458 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 68 | ANN: trainLoss: 0.3042 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 68 | ANN: trainLoss: 0.2977 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 68 | ANN: trainLoss: 0.2862 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 68 | ANN: trainLoss: 0.2933 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 68 | ANN: trainLoss: 0.2962 | trainAcc: 89.5833% (344/384)\n",
            "6 13 Epoch: 68 | ANN: trainLoss: 0.2985 | trainAcc: 89.5089% (401/448)\n",
            "7 13 Epoch: 68 | ANN: trainLoss: 0.2857 | trainAcc: 90.2344% (462/512)\n",
            "8 13 Epoch: 68 | ANN: trainLoss: 0.2836 | trainAcc: 90.2778% (520/576)\n",
            "9 13 Epoch: 68 | ANN: trainLoss: 0.2899 | trainAcc: 90.0000% (576/640)\n",
            "10 13 Epoch: 68 | ANN: trainLoss: 0.2926 | trainAcc: 89.9148% (633/704)\n",
            "11 13 Epoch: 68 | ANN: trainLoss: 0.2923 | trainAcc: 89.9740% (691/768)\n",
            "12 13 Epoch: 68 | ANN: trainLoss: 0.3214 | trainAcc: 89.7668% (693/772)\n",
            "0 4 Epoch: 68 | ANN: testLoss: 0.6247 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 68 | ANN: testLoss: 0.6256 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 68 | ANN: testLoss: 0.6466 | testAcc: 63.0208% (121/192)\n",
            "3 4 Epoch: 68 | ANN: testLoss: 0.8032 | testAcc: 62.6943% (121/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 69 | ANN: trainLoss: 0.2316 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 69 | ANN: trainLoss: 0.2412 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 69 | ANN: trainLoss: 0.2617 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 69 | ANN: trainLoss: 0.2557 | trainAcc: 90.2344% (231/256)\n",
            "4 13 Epoch: 69 | ANN: trainLoss: 0.2561 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 69 | ANN: trainLoss: 0.2512 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 69 | ANN: trainLoss: 0.2529 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 69 | ANN: trainLoss: 0.2752 | trainAcc: 90.2344% (462/512)\n",
            "8 13 Epoch: 69 | ANN: trainLoss: 0.2758 | trainAcc: 89.9306% (518/576)\n",
            "9 13 Epoch: 69 | ANN: trainLoss: 0.2689 | trainAcc: 90.0000% (576/640)\n",
            "10 13 Epoch: 69 | ANN: trainLoss: 0.2613 | trainAcc: 90.0568% (634/704)\n",
            "11 13 Epoch: 69 | ANN: trainLoss: 0.2685 | trainAcc: 89.9740% (691/768)\n",
            "12 13 Epoch: 69 | ANN: trainLoss: 0.2592 | trainAcc: 90.0259% (695/772)\n",
            "0 4 Epoch: 69 | ANN: testLoss: 0.6493 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 69 | ANN: testLoss: 0.6388 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 69 | ANN: testLoss: 0.6283 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 69 | ANN: testLoss: 0.5205 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 70 | ANN: trainLoss: 0.2712 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 70 | ANN: trainLoss: 0.2544 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 70 | ANN: trainLoss: 0.2433 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 70 | ANN: trainLoss: 0.2399 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 70 | ANN: trainLoss: 0.2775 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 70 | ANN: trainLoss: 0.2784 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 70 | ANN: trainLoss: 0.2850 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 70 | ANN: trainLoss: 0.2837 | trainAcc: 89.4531% (458/512)\n",
            "8 13 Epoch: 70 | ANN: trainLoss: 0.2847 | trainAcc: 89.2361% (514/576)\n",
            "9 13 Epoch: 70 | ANN: trainLoss: 0.2808 | trainAcc: 89.2188% (571/640)\n",
            "10 13 Epoch: 70 | ANN: trainLoss: 0.2803 | trainAcc: 89.2045% (628/704)\n",
            "11 13 Epoch: 70 | ANN: trainLoss: 0.2816 | trainAcc: 88.8021% (682/768)\n",
            "12 13 Epoch: 70 | ANN: trainLoss: 0.2994 | trainAcc: 88.7306% (685/772)\n",
            "0 4 Epoch: 70 | ANN: testLoss: 0.5578 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 70 | ANN: testLoss: 0.6119 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 70 | ANN: testLoss: 0.6348 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 70 | ANN: testLoss: 0.8884 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 71 | ANN: trainLoss: 0.2280 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 71 | ANN: trainLoss: 0.2415 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 71 | ANN: trainLoss: 0.2592 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 71 | ANN: trainLoss: 0.2800 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 71 | ANN: trainLoss: 0.2795 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 71 | ANN: trainLoss: 0.2859 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 71 | ANN: trainLoss: 0.2685 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 71 | ANN: trainLoss: 0.2606 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 71 | ANN: trainLoss: 0.2515 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 71 | ANN: trainLoss: 0.2577 | trainAcc: 89.3750% (572/640)\n",
            "10 13 Epoch: 71 | ANN: trainLoss: 0.2585 | trainAcc: 89.7727% (632/704)\n",
            "11 13 Epoch: 71 | ANN: trainLoss: 0.2619 | trainAcc: 89.3229% (686/768)\n",
            "12 13 Epoch: 71 | ANN: trainLoss: 0.2709 | trainAcc: 89.2487% (689/772)\n",
            "0 4 Epoch: 71 | ANN: testLoss: 0.7881 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 71 | ANN: testLoss: 0.6773 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 71 | ANN: testLoss: 0.6426 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 71 | ANN: testLoss: 0.6086 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 72 | ANN: trainLoss: 0.3734 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 72 | ANN: trainLoss: 0.3191 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 72 | ANN: trainLoss: 0.2640 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 72 | ANN: trainLoss: 0.2490 | trainAcc: 90.2344% (231/256)\n",
            "4 13 Epoch: 72 | ANN: trainLoss: 0.2383 | trainAcc: 90.0000% (288/320)\n",
            "5 13 Epoch: 72 | ANN: trainLoss: 0.2610 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 72 | ANN: trainLoss: 0.2535 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 72 | ANN: trainLoss: 0.2557 | trainAcc: 90.2344% (462/512)\n",
            "8 13 Epoch: 72 | ANN: trainLoss: 0.2604 | trainAcc: 90.4514% (521/576)\n",
            "9 13 Epoch: 72 | ANN: trainLoss: 0.2758 | trainAcc: 89.8438% (575/640)\n",
            "10 13 Epoch: 72 | ANN: trainLoss: 0.2759 | trainAcc: 90.0568% (634/704)\n",
            "11 13 Epoch: 72 | ANN: trainLoss: 0.2778 | trainAcc: 89.8438% (690/768)\n",
            "12 13 Epoch: 72 | ANN: trainLoss: 0.2770 | trainAcc: 89.8964% (694/772)\n",
            "0 4 Epoch: 72 | ANN: testLoss: 0.6205 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 72 | ANN: testLoss: 0.6233 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 72 | ANN: testLoss: 0.6514 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 72 | ANN: testLoss: 0.5442 | testAcc: 68.3938% (132/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 73 | ANN: trainLoss: 0.1959 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 73 | ANN: trainLoss: 0.2342 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 73 | ANN: trainLoss: 0.2308 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 73 | ANN: trainLoss: 0.2262 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 73 | ANN: trainLoss: 0.2325 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 73 | ANN: trainLoss: 0.2393 | trainAcc: 90.6250% (348/384)\n",
            "6 13 Epoch: 73 | ANN: trainLoss: 0.2454 | trainAcc: 90.6250% (406/448)\n",
            "7 13 Epoch: 73 | ANN: trainLoss: 0.2425 | trainAcc: 90.8203% (465/512)\n",
            "8 13 Epoch: 73 | ANN: trainLoss: 0.2397 | trainAcc: 90.9722% (524/576)\n",
            "9 13 Epoch: 73 | ANN: trainLoss: 0.2424 | trainAcc: 90.6250% (580/640)\n",
            "10 13 Epoch: 73 | ANN: trainLoss: 0.2411 | trainAcc: 91.1932% (642/704)\n",
            "11 13 Epoch: 73 | ANN: trainLoss: 0.2461 | trainAcc: 91.2760% (701/768)\n",
            "12 13 Epoch: 73 | ANN: trainLoss: 0.2542 | trainAcc: 91.3212% (705/772)\n",
            "0 4 Epoch: 73 | ANN: testLoss: 0.5921 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 73 | ANN: testLoss: 0.6031 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 73 | ANN: testLoss: 0.5974 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 73 | ANN: testLoss: 1.0451 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 74 | ANN: trainLoss: 0.2195 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 74 | ANN: trainLoss: 0.2357 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 74 | ANN: trainLoss: 0.2528 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 74 | ANN: trainLoss: 0.2376 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 74 | ANN: trainLoss: 0.2424 | trainAcc: 90.0000% (288/320)\n",
            "5 13 Epoch: 74 | ANN: trainLoss: 0.2515 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 74 | ANN: trainLoss: 0.2356 | trainAcc: 90.6250% (406/448)\n",
            "7 13 Epoch: 74 | ANN: trainLoss: 0.2329 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 74 | ANN: trainLoss: 0.2331 | trainAcc: 90.4514% (521/576)\n",
            "9 13 Epoch: 74 | ANN: trainLoss: 0.2367 | trainAcc: 90.4688% (579/640)\n",
            "10 13 Epoch: 74 | ANN: trainLoss: 0.2577 | trainAcc: 89.6307% (631/704)\n",
            "11 13 Epoch: 74 | ANN: trainLoss: 0.2556 | trainAcc: 89.7135% (689/768)\n",
            "12 13 Epoch: 74 | ANN: trainLoss: 0.2464 | trainAcc: 89.7668% (693/772)\n",
            "0 4 Epoch: 74 | ANN: testLoss: 0.6919 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 74 | ANN: testLoss: 0.6458 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 74 | ANN: testLoss: 0.6220 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 74 | ANN: testLoss: 0.8478 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 75 | ANN: trainLoss: 0.2962 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 75 | ANN: trainLoss: 0.2452 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 75 | ANN: trainLoss: 0.2809 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 75 | ANN: trainLoss: 0.2599 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 75 | ANN: trainLoss: 0.2561 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 75 | ANN: trainLoss: 0.2513 | trainAcc: 89.5833% (344/384)\n",
            "6 13 Epoch: 75 | ANN: trainLoss: 0.2388 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 75 | ANN: trainLoss: 0.2337 | trainAcc: 90.4297% (463/512)\n",
            "8 13 Epoch: 75 | ANN: trainLoss: 0.2372 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 75 | ANN: trainLoss: 0.2393 | trainAcc: 90.4688% (579/640)\n",
            "10 13 Epoch: 75 | ANN: trainLoss: 0.2396 | trainAcc: 90.6250% (638/704)\n",
            "11 13 Epoch: 75 | ANN: trainLoss: 0.2378 | trainAcc: 90.6250% (696/768)\n",
            "12 13 Epoch: 75 | ANN: trainLoss: 0.2432 | trainAcc: 90.5440% (699/772)\n",
            "0 4 Epoch: 75 | ANN: testLoss: 0.7768 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 75 | ANN: testLoss: 0.6841 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 75 | ANN: testLoss: 0.6377 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 75 | ANN: testLoss: 0.5376 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 76 | ANN: trainLoss: 0.3065 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 76 | ANN: trainLoss: 0.2686 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 76 | ANN: trainLoss: 0.2632 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 76 | ANN: trainLoss: 0.2702 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 76 | ANN: trainLoss: 0.2526 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 76 | ANN: trainLoss: 0.2420 | trainAcc: 90.3646% (347/384)\n",
            "6 13 Epoch: 76 | ANN: trainLoss: 0.2572 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 76 | ANN: trainLoss: 0.2415 | trainAcc: 90.0391% (461/512)\n",
            "8 13 Epoch: 76 | ANN: trainLoss: 0.2388 | trainAcc: 90.4514% (521/576)\n",
            "9 13 Epoch: 76 | ANN: trainLoss: 0.2457 | trainAcc: 90.1562% (577/640)\n",
            "10 13 Epoch: 76 | ANN: trainLoss: 0.2368 | trainAcc: 90.7670% (639/704)\n",
            "11 13 Epoch: 76 | ANN: trainLoss: 0.2340 | trainAcc: 90.6250% (696/768)\n",
            "12 13 Epoch: 76 | ANN: trainLoss: 0.2370 | trainAcc: 90.6736% (700/772)\n",
            "0 4 Epoch: 76 | ANN: testLoss: 0.5952 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 76 | ANN: testLoss: 0.6674 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 76 | ANN: testLoss: 0.6371 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 76 | ANN: testLoss: 0.8405 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 77 | ANN: trainLoss: 0.3683 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 77 | ANN: trainLoss: 0.2509 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 77 | ANN: trainLoss: 0.2437 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 77 | ANN: trainLoss: 0.2529 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 77 | ANN: trainLoss: 0.2302 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 77 | ANN: trainLoss: 0.2458 | trainAcc: 91.4062% (351/384)\n",
            "6 13 Epoch: 77 | ANN: trainLoss: 0.2374 | trainAcc: 91.7411% (411/448)\n",
            "7 13 Epoch: 77 | ANN: trainLoss: 0.2477 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 77 | ANN: trainLoss: 0.2488 | trainAcc: 90.2778% (520/576)\n",
            "9 13 Epoch: 77 | ANN: trainLoss: 0.2813 | trainAcc: 89.2188% (571/640)\n",
            "10 13 Epoch: 77 | ANN: trainLoss: 0.2790 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 77 | ANN: trainLoss: 0.2706 | trainAcc: 89.8438% (690/768)\n",
            "12 13 Epoch: 77 | ANN: trainLoss: 0.2540 | trainAcc: 89.8964% (694/772)\n",
            "0 4 Epoch: 77 | ANN: testLoss: 0.6123 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 77 | ANN: testLoss: 0.7102 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 77 | ANN: testLoss: 0.6545 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 77 | ANN: testLoss: 0.5653 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 78 | ANN: trainLoss: 0.1788 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 78 | ANN: trainLoss: 0.1734 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 78 | ANN: trainLoss: 0.1607 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 78 | ANN: trainLoss: 0.1724 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 78 | ANN: trainLoss: 0.1771 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 78 | ANN: trainLoss: 0.1735 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 78 | ANN: trainLoss: 0.1743 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 78 | ANN: trainLoss: 0.1858 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 78 | ANN: trainLoss: 0.1803 | trainAcc: 92.5347% (533/576)\n",
            "9 13 Epoch: 78 | ANN: trainLoss: 0.1897 | trainAcc: 92.0312% (589/640)\n",
            "10 13 Epoch: 78 | ANN: trainLoss: 0.1963 | trainAcc: 91.9034% (647/704)\n",
            "11 13 Epoch: 78 | ANN: trainLoss: 0.2073 | trainAcc: 91.4062% (702/768)\n",
            "12 13 Epoch: 78 | ANN: trainLoss: 0.1948 | trainAcc: 91.4508% (706/772)\n",
            "0 4 Epoch: 78 | ANN: testLoss: 0.5885 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 78 | ANN: testLoss: 0.6503 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 78 | ANN: testLoss: 0.6473 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 78 | ANN: testLoss: 0.5593 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 79 | ANN: trainLoss: 0.3053 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 79 | ANN: trainLoss: 0.2935 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 79 | ANN: trainLoss: 0.2618 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 79 | ANN: trainLoss: 0.2637 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 79 | ANN: trainLoss: 0.2577 | trainAcc: 90.0000% (288/320)\n",
            "5 13 Epoch: 79 | ANN: trainLoss: 0.2663 | trainAcc: 90.3646% (347/384)\n",
            "6 13 Epoch: 79 | ANN: trainLoss: 0.2596 | trainAcc: 90.6250% (406/448)\n",
            "7 13 Epoch: 79 | ANN: trainLoss: 0.2805 | trainAcc: 90.0391% (461/512)\n",
            "8 13 Epoch: 79 | ANN: trainLoss: 0.2816 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 79 | ANN: trainLoss: 0.2740 | trainAcc: 90.0000% (576/640)\n",
            "10 13 Epoch: 79 | ANN: trainLoss: 0.2667 | trainAcc: 90.1989% (635/704)\n",
            "11 13 Epoch: 79 | ANN: trainLoss: 0.2554 | trainAcc: 90.4948% (695/768)\n",
            "12 13 Epoch: 79 | ANN: trainLoss: 0.3616 | trainAcc: 90.2850% (697/772)\n",
            "0 4 Epoch: 79 | ANN: testLoss: 0.5338 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 79 | ANN: testLoss: 0.5492 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 79 | ANN: testLoss: 0.6357 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 79 | ANN: testLoss: 0.8815 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 80 | ANN: trainLoss: 0.3987 | trainAcc: 82.8125% (53/64)\n",
            "1 13 Epoch: 80 | ANN: trainLoss: 0.3291 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 80 | ANN: trainLoss: 0.2795 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 80 | ANN: trainLoss: 0.2497 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 80 | ANN: trainLoss: 0.2396 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 80 | ANN: trainLoss: 0.2360 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 80 | ANN: trainLoss: 0.2304 | trainAcc: 90.4018% (405/448)\n",
            "7 13 Epoch: 80 | ANN: trainLoss: 0.2354 | trainAcc: 90.2344% (462/512)\n",
            "8 13 Epoch: 80 | ANN: trainLoss: 0.2332 | trainAcc: 90.6250% (522/576)\n",
            "9 13 Epoch: 80 | ANN: trainLoss: 0.2318 | trainAcc: 90.6250% (580/640)\n",
            "10 13 Epoch: 80 | ANN: trainLoss: 0.2265 | trainAcc: 90.7670% (639/704)\n",
            "11 13 Epoch: 80 | ANN: trainLoss: 0.2247 | trainAcc: 90.7552% (697/768)\n",
            "12 13 Epoch: 80 | ANN: trainLoss: 0.2157 | trainAcc: 90.8031% (701/772)\n",
            "0 4 Epoch: 80 | ANN: testLoss: 0.6575 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 80 | ANN: testLoss: 0.6474 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 80 | ANN: testLoss: 0.6425 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 80 | ANN: testLoss: 0.4881 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 81 | ANN: trainLoss: 0.1802 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 81 | ANN: trainLoss: 0.1979 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 81 | ANN: trainLoss: 0.1824 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 81 | ANN: trainLoss: 0.2113 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 81 | ANN: trainLoss: 0.2086 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 81 | ANN: trainLoss: 0.1987 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 81 | ANN: trainLoss: 0.2126 | trainAcc: 90.8482% (407/448)\n",
            "7 13 Epoch: 81 | ANN: trainLoss: 0.2187 | trainAcc: 90.8203% (465/512)\n",
            "8 13 Epoch: 81 | ANN: trainLoss: 0.2210 | trainAcc: 90.9722% (524/576)\n",
            "9 13 Epoch: 81 | ANN: trainLoss: 0.2164 | trainAcc: 91.0938% (583/640)\n",
            "10 13 Epoch: 81 | ANN: trainLoss: 0.2201 | trainAcc: 91.0511% (641/704)\n",
            "11 13 Epoch: 81 | ANN: trainLoss: 0.2195 | trainAcc: 91.1458% (700/768)\n",
            "12 13 Epoch: 81 | ANN: trainLoss: 0.2773 | trainAcc: 91.0622% (703/772)\n",
            "0 4 Epoch: 81 | ANN: testLoss: 0.6801 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 81 | ANN: testLoss: 0.5858 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 81 | ANN: testLoss: 0.6543 | testAcc: 67.1875% (129/192)\n",
            "3 4 Epoch: 81 | ANN: testLoss: 0.4908 | testAcc: 67.3575% (130/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 82 | ANN: trainLoss: 0.2146 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 82 | ANN: trainLoss: 0.2476 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 82 | ANN: trainLoss: 0.2688 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 82 | ANN: trainLoss: 0.2461 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 82 | ANN: trainLoss: 0.2463 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 82 | ANN: trainLoss: 0.2365 | trainAcc: 90.8854% (349/384)\n",
            "6 13 Epoch: 82 | ANN: trainLoss: 0.2323 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 82 | ANN: trainLoss: 0.2202 | trainAcc: 91.6016% (469/512)\n",
            "8 13 Epoch: 82 | ANN: trainLoss: 0.2303 | trainAcc: 91.1458% (525/576)\n",
            "9 13 Epoch: 82 | ANN: trainLoss: 0.2319 | trainAcc: 90.7812% (581/640)\n",
            "10 13 Epoch: 82 | ANN: trainLoss: 0.2382 | trainAcc: 90.1989% (635/704)\n",
            "11 13 Epoch: 82 | ANN: trainLoss: 0.2358 | trainAcc: 90.3646% (694/768)\n",
            "12 13 Epoch: 82 | ANN: trainLoss: 0.2198 | trainAcc: 90.4145% (698/772)\n",
            "0 4 Epoch: 82 | ANN: testLoss: 0.7390 | testAcc: 56.2500% (36/64)\n",
            "1 4 Epoch: 82 | ANN: testLoss: 0.6854 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 82 | ANN: testLoss: 0.6412 | testAcc: 67.7083% (130/192)\n",
            "3 4 Epoch: 82 | ANN: testLoss: 0.6174 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 83 | ANN: trainLoss: 0.1804 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 83 | ANN: trainLoss: 0.2339 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 83 | ANN: trainLoss: 0.2495 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 83 | ANN: trainLoss: 0.2597 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 83 | ANN: trainLoss: 0.2612 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 83 | ANN: trainLoss: 0.2479 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 83 | ANN: trainLoss: 0.2352 | trainAcc: 91.7411% (411/448)\n",
            "7 13 Epoch: 83 | ANN: trainLoss: 0.2189 | trainAcc: 92.5781% (474/512)\n",
            "8 13 Epoch: 83 | ANN: trainLoss: 0.2138 | trainAcc: 92.8819% (535/576)\n",
            "9 13 Epoch: 83 | ANN: trainLoss: 0.2082 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 83 | ANN: trainLoss: 0.2066 | trainAcc: 93.1818% (656/704)\n",
            "11 13 Epoch: 83 | ANN: trainLoss: 0.2070 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 83 | ANN: trainLoss: 0.2006 | trainAcc: 92.8756% (717/772)\n",
            "0 4 Epoch: 83 | ANN: testLoss: 0.6287 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 83 | ANN: testLoss: 0.5675 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 83 | ANN: testLoss: 0.6377 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 83 | ANN: testLoss: 0.4789 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 84 | ANN: trainLoss: 0.2504 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 84 | ANN: trainLoss: 0.2329 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 84 | ANN: trainLoss: 0.2508 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 84 | ANN: trainLoss: 0.2270 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 84 | ANN: trainLoss: 0.2239 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 84 | ANN: trainLoss: 0.2170 | trainAcc: 91.4062% (351/384)\n",
            "6 13 Epoch: 84 | ANN: trainLoss: 0.2161 | trainAcc: 91.2946% (409/448)\n",
            "7 13 Epoch: 84 | ANN: trainLoss: 0.2148 | trainAcc: 91.7969% (470/512)\n",
            "8 13 Epoch: 84 | ANN: trainLoss: 0.2164 | trainAcc: 91.8403% (529/576)\n",
            "9 13 Epoch: 84 | ANN: trainLoss: 0.2200 | trainAcc: 91.5625% (586/640)\n",
            "10 13 Epoch: 84 | ANN: trainLoss: 0.2217 | trainAcc: 91.4773% (644/704)\n",
            "11 13 Epoch: 84 | ANN: trainLoss: 0.2215 | trainAcc: 91.5365% (703/768)\n",
            "12 13 Epoch: 84 | ANN: trainLoss: 0.2169 | trainAcc: 91.5803% (707/772)\n",
            "0 4 Epoch: 84 | ANN: testLoss: 0.6336 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 84 | ANN: testLoss: 0.6047 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 84 | ANN: testLoss: 0.6261 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 84 | ANN: testLoss: 0.5676 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 85 | ANN: trainLoss: 0.2650 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 85 | ANN: trainLoss: 0.2436 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 85 | ANN: trainLoss: 0.2642 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 85 | ANN: trainLoss: 0.2474 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 85 | ANN: trainLoss: 0.2503 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 85 | ANN: trainLoss: 0.2479 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 85 | ANN: trainLoss: 0.2508 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 85 | ANN: trainLoss: 0.2455 | trainAcc: 91.0156% (466/512)\n",
            "8 13 Epoch: 85 | ANN: trainLoss: 0.2465 | trainAcc: 90.9722% (524/576)\n",
            "9 13 Epoch: 85 | ANN: trainLoss: 0.2375 | trainAcc: 91.2500% (584/640)\n",
            "10 13 Epoch: 85 | ANN: trainLoss: 0.2348 | trainAcc: 91.6193% (645/704)\n",
            "11 13 Epoch: 85 | ANN: trainLoss: 0.2453 | trainAcc: 90.8854% (698/768)\n",
            "12 13 Epoch: 85 | ANN: trainLoss: 0.2279 | trainAcc: 90.9326% (702/772)\n",
            "0 4 Epoch: 85 | ANN: testLoss: 0.6570 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 85 | ANN: testLoss: 0.6011 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 85 | ANN: testLoss: 0.6220 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 85 | ANN: testLoss: 0.6387 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 86 | ANN: trainLoss: 0.1954 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 86 | ANN: trainLoss: 0.1809 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 86 | ANN: trainLoss: 0.2123 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 86 | ANN: trainLoss: 0.2062 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 86 | ANN: trainLoss: 0.2071 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 86 | ANN: trainLoss: 0.2041 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 86 | ANN: trainLoss: 0.2061 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 86 | ANN: trainLoss: 0.1951 | trainAcc: 92.5781% (474/512)\n",
            "8 13 Epoch: 86 | ANN: trainLoss: 0.1891 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 86 | ANN: trainLoss: 0.1846 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 86 | ANN: trainLoss: 0.1841 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 86 | ANN: trainLoss: 0.1800 | trainAcc: 93.0990% (715/768)\n",
            "12 13 Epoch: 86 | ANN: trainLoss: 0.1967 | trainAcc: 93.1347% (719/772)\n",
            "0 4 Epoch: 86 | ANN: testLoss: 0.5888 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 86 | ANN: testLoss: 0.6167 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 86 | ANN: testLoss: 0.6169 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 86 | ANN: testLoss: 0.4811 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 87 | ANN: trainLoss: 0.2438 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 87 | ANN: trainLoss: 0.2029 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 87 | ANN: trainLoss: 0.1873 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 87 | ANN: trainLoss: 0.1896 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 87 | ANN: trainLoss: 0.2122 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 87 | ANN: trainLoss: 0.2079 | trainAcc: 91.4062% (351/384)\n",
            "6 13 Epoch: 87 | ANN: trainLoss: 0.2021 | trainAcc: 92.1875% (413/448)\n",
            "7 13 Epoch: 87 | ANN: trainLoss: 0.2080 | trainAcc: 91.4062% (468/512)\n",
            "8 13 Epoch: 87 | ANN: trainLoss: 0.2086 | trainAcc: 91.1458% (525/576)\n",
            "9 13 Epoch: 87 | ANN: trainLoss: 0.2040 | trainAcc: 91.5625% (586/640)\n",
            "10 13 Epoch: 87 | ANN: trainLoss: 0.2219 | trainAcc: 90.9091% (640/704)\n",
            "11 13 Epoch: 87 | ANN: trainLoss: 0.2163 | trainAcc: 91.2760% (701/768)\n",
            "12 13 Epoch: 87 | ANN: trainLoss: 0.2141 | trainAcc: 91.3212% (705/772)\n",
            "0 4 Epoch: 87 | ANN: testLoss: 0.5472 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 87 | ANN: testLoss: 0.5657 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 87 | ANN: testLoss: 0.6124 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 87 | ANN: testLoss: 0.9506 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 88 | ANN: trainLoss: 0.3004 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 88 | ANN: trainLoss: 0.2254 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 88 | ANN: trainLoss: 0.2342 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 88 | ANN: trainLoss: 0.2328 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 88 | ANN: trainLoss: 0.2340 | trainAcc: 90.0000% (288/320)\n",
            "5 13 Epoch: 88 | ANN: trainLoss: 0.2340 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 88 | ANN: trainLoss: 0.2321 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 88 | ANN: trainLoss: 0.2216 | trainAcc: 90.4297% (463/512)\n",
            "8 13 Epoch: 88 | ANN: trainLoss: 0.2212 | trainAcc: 90.4514% (521/576)\n",
            "9 13 Epoch: 88 | ANN: trainLoss: 0.2161 | trainAcc: 90.7812% (581/640)\n",
            "10 13 Epoch: 88 | ANN: trainLoss: 0.2141 | trainAcc: 91.3352% (643/704)\n",
            "11 13 Epoch: 88 | ANN: trainLoss: 0.2148 | trainAcc: 91.5365% (703/768)\n",
            "12 13 Epoch: 88 | ANN: trainLoss: 0.2596 | trainAcc: 91.3212% (705/772)\n",
            "0 4 Epoch: 88 | ANN: testLoss: 0.7150 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 88 | ANN: testLoss: 0.6461 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 88 | ANN: testLoss: 0.6209 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 88 | ANN: testLoss: 0.6603 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 89 | ANN: trainLoss: 0.2736 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 89 | ANN: trainLoss: 0.2187 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 89 | ANN: trainLoss: 0.2169 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 89 | ANN: trainLoss: 0.2180 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 89 | ANN: trainLoss: 0.2208 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 89 | ANN: trainLoss: 0.2120 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 89 | ANN: trainLoss: 0.2076 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 89 | ANN: trainLoss: 0.2090 | trainAcc: 91.6016% (469/512)\n",
            "8 13 Epoch: 89 | ANN: trainLoss: 0.2030 | trainAcc: 91.8403% (529/576)\n",
            "9 13 Epoch: 89 | ANN: trainLoss: 0.2004 | trainAcc: 92.1875% (590/640)\n",
            "10 13 Epoch: 89 | ANN: trainLoss: 0.1980 | trainAcc: 92.1875% (649/704)\n",
            "11 13 Epoch: 89 | ANN: trainLoss: 0.1997 | trainAcc: 92.5781% (711/768)\n",
            "12 13 Epoch: 89 | ANN: trainLoss: 0.1949 | trainAcc: 92.6166% (715/772)\n",
            "0 4 Epoch: 89 | ANN: testLoss: 0.5873 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 89 | ANN: testLoss: 0.6373 | testAcc: 67.1875% (86/128)\n",
            "2 4 Epoch: 89 | ANN: testLoss: 0.6288 | testAcc: 66.6667% (128/192)\n",
            "3 4 Epoch: 89 | ANN: testLoss: 0.4716 | testAcc: 66.8394% (129/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 90 | ANN: trainLoss: 0.1513 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 90 | ANN: trainLoss: 0.1857 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 90 | ANN: trainLoss: 0.1772 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 90 | ANN: trainLoss: 0.1683 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 90 | ANN: trainLoss: 0.1795 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 90 | ANN: trainLoss: 0.1789 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 90 | ANN: trainLoss: 0.1783 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 90 | ANN: trainLoss: 0.1780 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 90 | ANN: trainLoss: 0.1734 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 90 | ANN: trainLoss: 0.1810 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 90 | ANN: trainLoss: 0.1869 | trainAcc: 93.1818% (656/704)\n",
            "11 13 Epoch: 90 | ANN: trainLoss: 0.1917 | trainAcc: 92.9688% (714/768)\n",
            "12 13 Epoch: 90 | ANN: trainLoss: 0.2999 | trainAcc: 92.8756% (717/772)\n",
            "0 4 Epoch: 90 | ANN: testLoss: 0.5377 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 90 | ANN: testLoss: 0.5377 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 90 | ANN: testLoss: 0.6113 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 90 | ANN: testLoss: 0.7498 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 91 | ANN: trainLoss: 0.2661 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 91 | ANN: trainLoss: 0.2648 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 91 | ANN: trainLoss: 0.2420 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 91 | ANN: trainLoss: 0.2567 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 91 | ANN: trainLoss: 0.2409 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 91 | ANN: trainLoss: 0.2257 | trainAcc: 90.6250% (348/384)\n",
            "6 13 Epoch: 91 | ANN: trainLoss: 0.2117 | trainAcc: 91.5179% (410/448)\n",
            "7 13 Epoch: 91 | ANN: trainLoss: 0.2146 | trainAcc: 91.4062% (468/512)\n",
            "8 13 Epoch: 91 | ANN: trainLoss: 0.2179 | trainAcc: 90.9722% (524/576)\n",
            "9 13 Epoch: 91 | ANN: trainLoss: 0.2208 | trainAcc: 90.6250% (580/640)\n",
            "10 13 Epoch: 91 | ANN: trainLoss: 0.2180 | trainAcc: 90.9091% (640/704)\n",
            "11 13 Epoch: 91 | ANN: trainLoss: 0.2180 | trainAcc: 90.8854% (698/768)\n",
            "12 13 Epoch: 91 | ANN: trainLoss: 0.2415 | trainAcc: 90.8031% (701/772)\n",
            "0 4 Epoch: 91 | ANN: testLoss: 0.6421 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 91 | ANN: testLoss: 0.5914 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 91 | ANN: testLoss: 0.6283 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 91 | ANN: testLoss: 0.7017 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 92 | ANN: trainLoss: 0.1382 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 92 | ANN: trainLoss: 0.1559 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 92 | ANN: trainLoss: 0.1636 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 92 | ANN: trainLoss: 0.1732 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 92 | ANN: trainLoss: 0.1658 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 92 | ANN: trainLoss: 0.1734 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 92 | ANN: trainLoss: 0.1722 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 92 | ANN: trainLoss: 0.1711 | trainAcc: 95.3125% (488/512)\n",
            "8 13 Epoch: 92 | ANN: trainLoss: 0.1851 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 92 | ANN: trainLoss: 0.1949 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 92 | ANN: trainLoss: 0.1991 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 92 | ANN: trainLoss: 0.2050 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 92 | ANN: trainLoss: 0.2899 | trainAcc: 93.6528% (723/772)\n",
            "0 4 Epoch: 92 | ANN: testLoss: 0.6798 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 92 | ANN: testLoss: 0.7158 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 92 | ANN: testLoss: 0.6081 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 92 | ANN: testLoss: 0.9974 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 93 | ANN: trainLoss: 0.3621 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 93 | ANN: trainLoss: 0.2795 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 93 | ANN: trainLoss: 0.2501 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 93 | ANN: trainLoss: 0.2988 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 93 | ANN: trainLoss: 0.2801 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 93 | ANN: trainLoss: 0.2745 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 93 | ANN: trainLoss: 0.2550 | trainAcc: 90.8482% (407/448)\n",
            "7 13 Epoch: 93 | ANN: trainLoss: 0.2535 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 93 | ANN: trainLoss: 0.2456 | trainAcc: 90.7986% (523/576)\n",
            "9 13 Epoch: 93 | ANN: trainLoss: 0.2435 | trainAcc: 90.9375% (582/640)\n",
            "10 13 Epoch: 93 | ANN: trainLoss: 0.2348 | trainAcc: 91.3352% (643/704)\n",
            "11 13 Epoch: 93 | ANN: trainLoss: 0.2308 | trainAcc: 91.5365% (703/768)\n",
            "12 13 Epoch: 93 | ANN: trainLoss: 0.2320 | trainAcc: 91.4508% (706/772)\n",
            "0 4 Epoch: 93 | ANN: testLoss: 0.7083 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 93 | ANN: testLoss: 0.5657 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 93 | ANN: testLoss: 0.6164 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 93 | ANN: testLoss: 0.5162 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 94 | ANN: trainLoss: 0.3058 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 94 | ANN: trainLoss: 0.2243 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 94 | ANN: trainLoss: 0.2237 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 94 | ANN: trainLoss: 0.2069 | trainAcc: 90.2344% (231/256)\n",
            "4 13 Epoch: 94 | ANN: trainLoss: 0.2105 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 94 | ANN: trainLoss: 0.2131 | trainAcc: 90.6250% (348/384)\n",
            "6 13 Epoch: 94 | ANN: trainLoss: 0.2112 | trainAcc: 90.8482% (407/448)\n",
            "7 13 Epoch: 94 | ANN: trainLoss: 0.2229 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 94 | ANN: trainLoss: 0.2196 | trainAcc: 90.7986% (523/576)\n",
            "9 13 Epoch: 94 | ANN: trainLoss: 0.2194 | trainAcc: 90.6250% (580/640)\n",
            "10 13 Epoch: 94 | ANN: trainLoss: 0.2139 | trainAcc: 91.1932% (642/704)\n",
            "11 13 Epoch: 94 | ANN: trainLoss: 0.2096 | trainAcc: 91.4062% (702/768)\n",
            "12 13 Epoch: 94 | ANN: trainLoss: 0.2085 | trainAcc: 91.3212% (705/772)\n",
            "0 4 Epoch: 94 | ANN: testLoss: 0.6155 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 94 | ANN: testLoss: 0.5661 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 94 | ANN: testLoss: 0.6221 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 94 | ANN: testLoss: 0.4730 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 95 | ANN: trainLoss: 0.1905 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 95 | ANN: trainLoss: 0.1834 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 95 | ANN: trainLoss: 0.1876 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 95 | ANN: trainLoss: 0.1762 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 95 | ANN: trainLoss: 0.1814 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 95 | ANN: trainLoss: 0.1879 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 95 | ANN: trainLoss: 0.1862 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 95 | ANN: trainLoss: 0.1910 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 95 | ANN: trainLoss: 0.1963 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 95 | ANN: trainLoss: 0.1929 | trainAcc: 92.8125% (594/640)\n",
            "10 13 Epoch: 95 | ANN: trainLoss: 0.1984 | trainAcc: 92.3295% (650/704)\n",
            "11 13 Epoch: 95 | ANN: trainLoss: 0.1932 | trainAcc: 92.5781% (711/768)\n",
            "12 13 Epoch: 95 | ANN: trainLoss: 0.1924 | trainAcc: 92.6166% (715/772)\n",
            "0 4 Epoch: 95 | ANN: testLoss: 0.7263 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 95 | ANN: testLoss: 0.6431 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 95 | ANN: testLoss: 0.6288 | testAcc: 68.2292% (131/192)\n",
            "3 4 Epoch: 95 | ANN: testLoss: 0.6521 | testAcc: 67.8756% (131/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 96 | ANN: trainLoss: 0.1798 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 96 | ANN: trainLoss: 0.1987 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 96 | ANN: trainLoss: 0.1821 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 96 | ANN: trainLoss: 0.2135 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 96 | ANN: trainLoss: 0.2133 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 96 | ANN: trainLoss: 0.2164 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 96 | ANN: trainLoss: 0.2126 | trainAcc: 92.1875% (413/448)\n",
            "7 13 Epoch: 96 | ANN: trainLoss: 0.2068 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 96 | ANN: trainLoss: 0.2132 | trainAcc: 92.1875% (531/576)\n",
            "9 13 Epoch: 96 | ANN: trainLoss: 0.2162 | trainAcc: 92.1875% (590/640)\n",
            "10 13 Epoch: 96 | ANN: trainLoss: 0.2114 | trainAcc: 92.3295% (650/704)\n",
            "11 13 Epoch: 96 | ANN: trainLoss: 0.2222 | trainAcc: 92.1875% (708/768)\n",
            "12 13 Epoch: 96 | ANN: trainLoss: 0.2077 | trainAcc: 92.2280% (712/772)\n",
            "0 4 Epoch: 96 | ANN: testLoss: 0.6745 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 96 | ANN: testLoss: 0.5720 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 96 | ANN: testLoss: 0.6132 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 96 | ANN: testLoss: 0.9683 | testAcc: 69.9482% (135/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 97 | ANN: trainLoss: 0.1165 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 97 | ANN: trainLoss: 0.1894 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 97 | ANN: trainLoss: 0.1698 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 97 | ANN: trainLoss: 0.1828 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 97 | ANN: trainLoss: 0.1955 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 97 | ANN: trainLoss: 0.1880 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 97 | ANN: trainLoss: 0.1883 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 97 | ANN: trainLoss: 0.1898 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 97 | ANN: trainLoss: 0.1980 | trainAcc: 92.8819% (535/576)\n",
            "9 13 Epoch: 97 | ANN: trainLoss: 0.2008 | trainAcc: 92.8125% (594/640)\n",
            "10 13 Epoch: 97 | ANN: trainLoss: 0.1996 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 97 | ANN: trainLoss: 0.1969 | trainAcc: 93.0990% (715/768)\n",
            "12 13 Epoch: 97 | ANN: trainLoss: 0.1827 | trainAcc: 93.1347% (719/772)\n",
            "0 4 Epoch: 97 | ANN: testLoss: 0.7334 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 97 | ANN: testLoss: 0.6789 | testAcc: 65.6250% (84/128)\n",
            "2 4 Epoch: 97 | ANN: testLoss: 0.6217 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 97 | ANN: testLoss: 0.6148 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 98 | ANN: trainLoss: 0.2428 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 98 | ANN: trainLoss: 0.2314 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 98 | ANN: trainLoss: 0.1994 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 98 | ANN: trainLoss: 0.1953 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 98 | ANN: trainLoss: 0.2014 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 98 | ANN: trainLoss: 0.2037 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 98 | ANN: trainLoss: 0.2092 | trainAcc: 92.1875% (413/448)\n",
            "7 13 Epoch: 98 | ANN: trainLoss: 0.2068 | trainAcc: 91.9922% (471/512)\n",
            "8 13 Epoch: 98 | ANN: trainLoss: 0.2042 | trainAcc: 92.0139% (530/576)\n",
            "9 13 Epoch: 98 | ANN: trainLoss: 0.2011 | trainAcc: 91.5625% (586/640)\n",
            "10 13 Epoch: 98 | ANN: trainLoss: 0.1977 | trainAcc: 91.7614% (646/704)\n",
            "11 13 Epoch: 98 | ANN: trainLoss: 0.1932 | trainAcc: 92.0573% (707/768)\n",
            "12 13 Epoch: 98 | ANN: trainLoss: 0.1799 | trainAcc: 92.0984% (711/772)\n",
            "0 4 Epoch: 98 | ANN: testLoss: 0.7612 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 98 | ANN: testLoss: 0.6919 | testAcc: 64.0625% (82/128)\n",
            "2 4 Epoch: 98 | ANN: testLoss: 0.6216 | testAcc: 69.2708% (133/192)\n",
            "3 4 Epoch: 98 | ANN: testLoss: 0.5202 | testAcc: 69.4301% (134/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 99 | ANN: trainLoss: 0.1725 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 99 | ANN: trainLoss: 0.1732 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 99 | ANN: trainLoss: 0.1768 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 99 | ANN: trainLoss: 0.1908 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 99 | ANN: trainLoss: 0.1892 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 99 | ANN: trainLoss: 0.1875 | trainAcc: 92.4479% (355/384)\n",
            "6 13 Epoch: 99 | ANN: trainLoss: 0.2000 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 99 | ANN: trainLoss: 0.2040 | trainAcc: 91.2109% (467/512)\n",
            "8 13 Epoch: 99 | ANN: trainLoss: 0.1961 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 99 | ANN: trainLoss: 0.2038 | trainAcc: 91.5625% (586/640)\n",
            "10 13 Epoch: 99 | ANN: trainLoss: 0.2097 | trainAcc: 91.6193% (645/704)\n",
            "11 13 Epoch: 99 | ANN: trainLoss: 0.2068 | trainAcc: 91.7969% (705/768)\n",
            "12 13 Epoch: 99 | ANN: trainLoss: 0.2227 | trainAcc: 91.7098% (708/772)\n",
            "0 4 Epoch: 99 | ANN: testLoss: 0.7360 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 99 | ANN: testLoss: 0.7059 | testAcc: 64.8438% (83/128)\n",
            "2 4 Epoch: 99 | ANN: testLoss: 0.6150 | testAcc: 69.7917% (134/192)\n",
            "3 4 Epoch: 99 | ANN: testLoss: 0.4632 | testAcc: 69.9482% (135/193)\n",
            "---------------------------------------------\n",
            "Converting using MaxNorm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 438.59it/s]\n",
            "100%|██████████| 13/13 [00:00<00:00, 435.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ANN accuracy: Test: 69.9500%\n",
            "SNN accuracy: max_norm: 70.4663%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "7o4VT4r0r-xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "outputId": "1592726d-8af9-414f-e9a8-36236e5a5d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions for all models...\n",
            "Evaluating LENet CNN...\n",
            "Evaluating LENet SNN...\n",
            "Evaluating LENet_FCL CNN...\n",
            "Evaluating LENet_FCL SNN...\n",
            "All predictions generated.\n",
            "LENet CNN|LENet SNN|LENet_FCL CNN|LENet_FCL SNN|"
          ]
        }
      ],
      "source": [
        "# @title Evaluate models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib # For colormaps access\n",
        "\n",
        "# Ensure all necessary variables from previous cells are available:\n",
        "# cnn_model_lenet, snn_model_lenet, cnn_model_lenet_fcl, snn_model_lenet_fcl\n",
        "# test_data, test_label (from the last train_test_split)\n",
        "# BATCH_SIZE, TIME_STEPS, device\n",
        "# data_loader function\n",
        "\n",
        "# 0. Prepare data loader for evaluation (using test_data from the last split)\n",
        "eval_test_loader = data_loader(test_data, test_label, batch=BATCH_SIZE, shuffle=False, drop=False)\n",
        "\n",
        "true_labels_list = []\n",
        "for _, targets_batch in eval_test_loader:\n",
        "    true_labels_list.extend(targets_batch.cpu().numpy())\n",
        "true_labels_np = np.array(true_labels_list)\n",
        "\n",
        "# --- 1. Get predictions for all models ---\n",
        "print(\"Generating predictions for all models...\")\n",
        "all_model_predictions = {}\n",
        "model_objects = {\n",
        "    \"LENet CNN\": cnn_model_lenet,\n",
        "    \"LENet SNN\": snn_model_lenet,\n",
        "    \"LENet_FCL CNN\": cnn_model_lenet_fcl,\n",
        "    \"LENet_FCL SNN\": snn_model_lenet_fcl\n",
        "}\n",
        "\n",
        "for model_name, model_obj in model_objects.items():\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    model_obj.eval().to(device)\n",
        "    current_preds = []\n",
        "    is_snn = \"SNN\" in model_name\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in eval_test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            if is_snn:\n",
        "                for m_module in model_obj.modules():\n",
        "                    if hasattr(m_module, 'reset'):\n",
        "                        m_module.reset()\n",
        "                accumulated_outputs = None\n",
        "                for t in range(TIME_STEPS):\n",
        "                    outputs_t = model_obj(inputs)\n",
        "                    if accumulated_outputs is None:\n",
        "                        accumulated_outputs = outputs_t.clone()\n",
        "                    else:\n",
        "                        accumulated_outputs += outputs_t\n",
        "                _, predicted = accumulated_outputs.max(1)\n",
        "            else: # ANN\n",
        "                outputs = model_obj(inputs)\n",
        "                _, predicted = outputs.max(1)\n",
        "            current_preds.extend(predicted.cpu().numpy())\n",
        "    all_model_predictions[model_name] = current_preds\n",
        "print(\"All predictions generated.\")\n",
        "\n",
        "# --- 2. Calculate Accuracies for all models ---\n",
        "accuracy_results = {}\n",
        "class_names = ['Rest', 'Elbow', 'Hand'] # Corresponds to labels 0, 1, 2\n",
        "\n",
        "for model_name, predictions in all_model_predictions.items():\n",
        "    overall_acc = accuracy_score(true_labels_np, predictions)\n",
        "    accuracy_results[model_name] = {\"Overall\": overall_acc}\n",
        "    for class_idx, class_name_key in enumerate(class_names):\n",
        "        class_indices = np.where(true_labels_np == class_idx)[0]\n",
        "        if len(class_indices) > 0:\n",
        "            class_true = true_labels_np[class_indices]\n",
        "            class_pred = np.array(predictions)[class_indices]\n",
        "            class_acc = accuracy_score(class_true, class_pred)\n",
        "            accuracy_results[model_name][class_name_key] = class_acc\n",
        "        else:\n",
        "            accuracy_results[model_name][class_name_key] = np.nan\n",
        "for model_name in all_model_predictions:\n",
        "  print(model_name, end=\"|\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title visualize results (Custom confusion matrix, green good, red bad)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib # For colormaps access\n",
        "\n",
        "# Ensure all necessary variables from previous cells are available:\n",
        "# cnn_model_lenet, snn_model_lenet, cnn_model_lenet_fcl, snn_model_lenet_fcl\n",
        "# test_data, test_label (from the last train_test_split)\n",
        "# EPOCHS, DROP_OUT, TEST_SIZE, BATCH_SIZE, TIME_STEPS, device (from training blocks)\n",
        "# channel_count, num_classes, data_length (from data loading cell)\n",
        "# all_data, all_label (from data loading cell, for full dataset stats)\n",
        "# all_model_predictions, accuracy_results, true_labels_np (from previous evaluation steps)\n",
        "# data_loader function (defined in a previous cell)\n",
        "\n",
        "# --- 0. Report Header: Hyperparameters and Data Specifications (Table Format) ---\n",
        "print(\"--- Experiment Configuration & Data Specifications ---\")\n",
        "\n",
        "# Collect all parameters\n",
        "config_params = {\n",
        "    \"Dataset File\": \"sub-011_eeg.mat\",\n",
        "    \"EEG Channels\": channel_count if 'channel_count' in locals() else \"N/A\",\n",
        "    \"Data Shape (Full Dataset)\": (\n",
        "        f\"Samples: {all_data.shape[0]}, Channels: {all_data.shape[1]}, Length: {all_data.shape[2]}\"\n",
        "        if 'all_data' in locals() and hasattr(all_data, 'shape') and len(all_data.shape) == 3\n",
        "        else \"N/A\"),\n",
        "    \"Class Distribution (Full Dataset)\": (\n",
        "        f\"Rest: {np.sum(all_label == 0)}, \"\n",
        "        f\"Elbow: {np.sum(all_label == 1)}, \"\n",
        "        f\"Hand: {np.sum(all_label == 2)}\"\n",
        "    ) if 'all_label' in locals() and isinstance(all_label, np.ndarray) and all_label.ndim >=1 else \"N/A\",\n",
        "    \"Bandpass Filter (Reported)\": \"2-45Hz\", #Change in case other filtering technique was used\n",
        "    \"Sampling Ratio (Reported)\": \"90Hz\", #Change in case other sampling was used\n",
        "    \"TRAINING EPOCHS\": EPOCHS if 'EPOCHS' in locals() else \"N/A\",\n",
        "    \"Dropout\": DROP_OUT if 'DROP_OUT' in locals() else \"N/A\",\n",
        "    \"Test Split\": TEST_SIZE if 'TEST_SIZE' in locals() else \"N/A\",\n",
        "    \"Batch Size (Evaluation)\": BATCH_SIZE if 'BATCH_SIZE' in locals() else \"N/A\",\n",
        "    \"SNN Time Steps\": TIME_STEPS if 'TIME_STEPS' in locals() else \"N/A\"\n",
        "}\n",
        "\n",
        "# Determine column widths\n",
        "max_key_len = max(len(key) for key in config_params.keys())\n",
        "# Ensure values are strings for len() calculation\n",
        "str_config_values = [str(val) for val in config_params.values()]\n",
        "max_val_len = max(len(val) for val in str_config_values)\n",
        "\n",
        "key_col_width = max_key_len + 2 # Add some padding\n",
        "val_col_width = max_val_len + 2\n",
        "\n",
        "# Print table header\n",
        "print(\"+\" + \"-\" * (key_col_width) + \"+\" + \"-\" * (val_col_width) + \"+\")\n",
        "print(f\"| {'Parameter':<{key_col_width-1}} | {'Value':<{val_col_width-1}} |\")\n",
        "print(\"+\" + \"=\" * (key_col_width) + \"+\" + \"=\" * (val_col_width) + \"+\")\n",
        "\n",
        "# Print table rows\n",
        "for idx, (key, value) in enumerate(config_params.items()):\n",
        "    print(f\"| {key:<{key_col_width-1}} | {str(value):<{val_col_width-1}} |\")\n",
        "\n",
        "print(\"+\" + \"-\" * (key_col_width) + \"+\" + \"-\" * (val_col_width) + \"+\")\n",
        "print(\"\\n\") # Add a bit of space before the main output\n",
        "\n",
        "# --- 1. Plot Confusion Matrices and Print Descriptions ---\n",
        "# This section assumes all_model_predictions, true_labels_np, accuracy_results are pre-computed\n",
        "# from previous cells if this cell is run in isolation.\n",
        "print(\"\\n--- Confusion Matrices and Descriptions ---\")\n",
        "class_names_display = ['Rest', 'Elbow', 'Hand'] # For display purposes in CM\n",
        "figure_counter = 1\n",
        "\n",
        "try:\n",
        "    cmap_greens = matplotlib.colormaps['Greens']\n",
        "    cmap_reds = matplotlib.colormaps['Reds']\n",
        "except AttributeError: # Older matplotlib\n",
        "    cmap_greens = plt.cm.get_cmap('Greens')\n",
        "    cmap_reds = plt.cm.get_cmap('Reds')\n",
        "\n",
        "# Thresholds for color logic\n",
        "threshold_diagonal_good = 0.5  # Above this is green on diagonal\n",
        "threshold_off_diagonal_bad = 0.2 # Above this is red on off-diagonal (significant misclassification)\n",
        "\n",
        "# Check if necessary variables for plotting exist\n",
        "if 'all_model_predictions' in locals() and 'true_labels_np' in locals() and 'accuracy_results' in locals():\n",
        "    for model_name, predictions in all_model_predictions.items():\n",
        "        cm = confusion_matrix(true_labels_np, predictions, labels=range(len(class_names_display)))\n",
        "\n",
        "        # --- Custom Color Logic V2 ---\n",
        "        row_sums = cm.sum(axis=1, keepdims=True)\n",
        "        cm_normalized_row = np.zeros_like(cm, dtype=float)\n",
        "        for r_idx in range(cm.shape[0]):\n",
        "            if row_sums[r_idx, 0] > 0:\n",
        "                cm_normalized_row[r_idx, :] = cm[r_idx, :] / row_sums[r_idx, 0]\n",
        "\n",
        "        num_classes_cm = cm.shape[0]\n",
        "        color_matrix_rgb = np.zeros((num_classes_cm, num_classes_cm, 3))\n",
        "        colormap_input_values = np.zeros((num_classes_cm, num_classes_cm))\n",
        "\n",
        "\n",
        "        for i in range(num_classes_cm):\n",
        "            for j in range(num_classes_cm):\n",
        "                norm_value = cm_normalized_row[i, j]\n",
        "                color_val_for_cmap = 0.0\n",
        "\n",
        "                if row_sums[i, 0] == 0:\n",
        "                    color_matrix_rgb[i, j, :] = [0.95, 0.95, 0.95]\n",
        "                elif i == j:\n",
        "                    if norm_value > threshold_diagonal_good:\n",
        "                        color_val_for_cmap = norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_greens(color_val_for_cmap)[:3]\n",
        "                    else:\n",
        "                        color_val_for_cmap = 1.0 - norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_reds(color_val_for_cmap)[:3]\n",
        "                else:\n",
        "                    if norm_value > threshold_off_diagonal_bad:\n",
        "                        color_val_for_cmap = norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_reds(color_val_for_cmap)[:3]\n",
        "                    else:\n",
        "                        color_val_for_cmap = 1.0 - norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_greens(color_val_for_cmap)[:3]\n",
        "                colormap_input_values[i,j] = color_val_for_cmap\n",
        "        # --- End Custom Color Logic V2 ---\n",
        "\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names_display)\n",
        "        fig, ax = plt.subplots(figsize=(8, 7))\n",
        "        disp.plot(ax=ax, include_values=True, cmap='Greys', colorbar=False, values_format='d')\n",
        "\n",
        "        if ax.images:\n",
        "            ax.images[0].remove()\n",
        "        ax.imshow(color_matrix_rgb)\n",
        "\n",
        "        if disp.text_ is not None:\n",
        "            for i in range(num_classes_cm):\n",
        "                for j in range(num_classes_cm):\n",
        "                    if disp.text_[i, j] is not None:\n",
        "                        text_color = \"white\" if colormap_input_values[i,j] > 0.5 else \"black\"\n",
        "                        if row_sums[i,0] == 0:\n",
        "                            text_color = \"black\"\n",
        "                        disp.text_[i, j].set_color(text_color)\n",
        "\n",
        "        ax.set_title(f'Confusion Matrix - {model_name}\\n(Custom Good/Bad Row-Normalized Colors)')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"\\nFigure {figure_counter}: Confusion Matrix for {model_name}.\")\n",
        "        print(f\"This matrix visualizes classification performance with custom colors based on row-normalized values\")\n",
        "\n",
        "\n",
        "        # Ensure class_names is defined for accessing accuracy_results\n",
        "        class_names_for_acc = ['Rest', 'Elbow', 'Hand'] # Define or ensure it's passed from global scope\n",
        "\n",
        "        overall_acc_val = accuracy_results[model_name]['Overall'] * 100\n",
        "        print(f\"Overall Accuracy: {overall_acc_val:.2f}%.\")\n",
        "        for class_idx, class_name_label in enumerate(class_names_display):\n",
        "            # Use a consistent key for accuracy_results, assuming class_names_for_acc matches keys\n",
        "            current_class_key = class_names_for_acc[class_idx]\n",
        "            if current_class_key in accuracy_results[model_name]:\n",
        "                class_acc_val = accuracy_results[model_name][current_class_key]\n",
        "                if not np.isnan(class_acc_val):\n",
        "                    print(f\"Accuracy for {class_name_label}: {class_acc_val*100:.2f}%.\")\n",
        "                else:\n",
        "                    print(f\"Accuracy for {class_name_label}: N/A (value is NaN).\")\n",
        "            else:\n",
        "                print(f\"Accuracy for {class_name_label}: N/A (key not found).\")\n",
        "        print(\"-\" * 70)\n",
        "        figure_counter += 1\n",
        "else:\n",
        "    print(\"Required variables (all_model_predictions, true_labels_np, or accuracy_results) not found for plotting confusion matrices.\")\n",
        "\n",
        "\n",
        "# --- 2. Accuracy Table ---\n",
        "print(\"\\n\\n--- Model Performance Summary Table ---\")\n",
        "# Ensure class_names is defined for table header\n",
        "class_names_for_table = ['Rest', 'Elbow', 'Hand'] # Define or ensure it's passed\n",
        "\n",
        "if 'accuracy_results' in locals():\n",
        "    header = f\"| {'Model':<17} | {'Overall Acc.':<15} | {class_names_for_table[0]+' Acc.':<12} | {class_names_for_table[1]+' Acc.':<12} | {class_names_for_table[2]+' Acc.':<12} |\"\n",
        "    separator = \"|-------------------|-----------------|--------------|--------------|--------------|\"\n",
        "    print(header)\n",
        "    print(separator)\n",
        "    for model_name_key in accuracy_results:\n",
        "        overall_str = f\"{accuracy_results[model_name_key]['Overall']*100:.2f}%\"\n",
        "\n",
        "        rest_acc_val = accuracy_results[model_name_key].get(class_names_for_table[0], np.nan)\n",
        "        rest_str = f\"{rest_acc_val*100:.2f}%\" if not np.isnan(rest_acc_val) else \"N/A\"\n",
        "\n",
        "        elbow_acc_val = accuracy_results[model_name_key].get(class_names_for_table[1], np.nan)\n",
        "        elbow_str = f\"{elbow_acc_val*100:.2f}%\" if not np.isnan(elbow_acc_val) else \"N/A\"\n",
        "\n",
        "        hand_acc_val = accuracy_results[model_name_key].get(class_names_for_table[2], np.nan)\n",
        "        hand_str = f\"{hand_acc_val*100:.2f}%\" if not np.isnan(hand_acc_val) else \"N/A\"\n",
        "\n",
        "        row = f\"| {model_name_key:<17} | {overall_str:<15} | {rest_str:<12} | {elbow_str:<12} | {hand_str:<12} |\"\n",
        "        print(row)\n",
        "else:\n",
        "    print(\"Accuracy results not available for summary table.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "collapsed": true,
        "id": "sBup_2nVClAB",
        "outputId": "ebc94b8c-8c24-4d73-c5fc-1ab15f81766e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Experiment Configuration & Data Specifications ---\n",
            "+-----------------------------------+-----------------------------------------+\n",
            "| Parameter                          | Value                                    |\n",
            "+===================================+=========================================+\n",
            "| Dataset File                       | sub-011_eeg.mat                          |\n",
            "| EEG Channels                       | 62                                       |\n",
            "| Data Shape (Full Dataset)          | Samples: 965, Channels: 62, Length: 360  |\n",
            "| Class Distribution (Full Dataset)  | Rest: 365, Elbow: 300, Hand: 300         |\n",
            "| Bandpass Filter (Reported)         | 2-45Hz                                   |\n",
            "| Sampling Ratio (Reported)          | 90Hz                                     |\n",
            "| TRAINING EPOCHS                    | 100                                      |\n",
            "| Dropout                            | 0.25                                     |\n",
            "| Test Split                         | 0.2                                      |\n",
            "| Batch Size (Evaluation)            | 64                                       |\n",
            "| SNN Time Steps                     | 100                                      |\n",
            "+-----------------------------------+-----------------------------------------+\n",
            "\n",
            "\n",
            "\n",
            "--- Confusion Matrices and Descriptions ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 1: Confusion Matrix for LENet CNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 69.43%.\n",
            "Accuracy for Rest: 96.00%.\n",
            "Accuracy for Elbow: 39.06%.\n",
            "Accuracy for Hand: 68.52%.\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 2: Confusion Matrix for LENet SNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 70.47%.\n",
            "Accuracy for Rest: 97.33%.\n",
            "Accuracy for Elbow: 42.19%.\n",
            "Accuracy for Hand: 66.67%.\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 3: Confusion Matrix for LENet_FCL CNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 69.95%.\n",
            "Accuracy for Rest: 96.00%.\n",
            "Accuracy for Elbow: 46.88%.\n",
            "Accuracy for Hand: 61.11%.\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 4: Confusion Matrix for LENet_FCL SNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 70.47%.\n",
            "Accuracy for Rest: 97.33%.\n",
            "Accuracy for Elbow: 48.44%.\n",
            "Accuracy for Hand: 59.26%.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "--- Model Performance Summary Table ---\n",
            "| Model             | Overall Acc.    | Rest Acc.    | Elbow Acc.   | Hand Acc.    |\n",
            "|-------------------|-----------------|--------------|--------------|--------------|\n",
            "| LENet CNN         | 69.43%          | 96.00%       | 39.06%       | 68.52%       |\n",
            "| LENet SNN         | 70.47%          | 97.33%       | 42.19%       | 66.67%       |\n",
            "| LENet_FCL CNN     | 69.95%          | 96.00%       | 46.88%       | 61.11%       |\n",
            "| LENet_FCL SNN     | 70.47%          | 97.33%       | 48.44%       | 59.26%       |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd # For a nicer table output\n",
        "\n",
        "# --- Energy Calculation Constants (from the paper) ---\n",
        "E_MAC_PJ = 4.6  # pJ per MAC operation\n",
        "E_AC_PJ = 0.9   # pJ per AC operation (for SNN spike processing)\n",
        "\n",
        "def get_macs_and_energy(\n",
        "    model, sample_input, model_name=\"Model\", T_snn=100, avg_firing_rate=0.02\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates MACs and estimates energy consumption for ANN and SNN versions.\n",
        "    \"\"\"\n",
        "    layer_flops_mac_list = []\n",
        "    layer_info_list = []  # For debugging or detailed info\n",
        "    hooks = []\n",
        "\n",
        "    def hook_fn_flops(module, input_tensor_tuple, output_tensor):\n",
        "        input_t = input_tensor_tuple[0] # input_tensor is a tuple\n",
        "        module_type_name = str(type(module)).split(\".\")[-1][:-2]\n",
        "        macs = 0\n",
        "\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            c_in = module.in_channels\n",
        "            c_out = module.out_channels\n",
        "            k_h, k_w = module.kernel_size\n",
        "            # Output tensor shape: (batch, c_out, h_out, w_out)\n",
        "            h_out, w_out = output_tensor.shape[2], output_tensor.shape[3]\n",
        "            groups = module.groups\n",
        "            # MACs for Conv2d: (C_in_per_group * K_h * K_w) * (C_out_per_group * H_out * W_out) * groups\n",
        "            # Simplified: C_in * K_h * K_w * C_out * H_out * W_out / groups\n",
        "            macs = (c_in // groups) * k_h * k_w * c_out * h_out * w_out\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            n_in = module.in_features\n",
        "            n_out = module.out_features\n",
        "            macs = n_in * n_out\n",
        "\n",
        "        if macs > 0:\n",
        "            layer_flops_mac_list.append(macs)\n",
        "            layer_info_list.append(\n",
        "                {\n",
        "                    \"type\": module_type_name,\n",
        "                    \"macs\": macs,\n",
        "                    \"in_shape\": tuple(input_t.shape),\n",
        "                    \"out_shape\": tuple(output_tensor.shape),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # Register hooks for Conv2d and Linear layers\n",
        "    for _, module_item in model.named_modules():\n",
        "        if isinstance(module_item, (nn.Conv2d, nn.Linear)):\n",
        "            hooks.append(module_item.register_forward_hook(hook_fn_flops))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Ensure sample_input is on the same device as the model\n",
        "        model_device = next(model.parameters()).device\n",
        "        model(sample_input.to(model_device))\n",
        "\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    # --- Calculate ANN Energy ---\n",
        "    total_macs_all_layers = sum(layer_flops_mac_list)\n",
        "    energy_ann_pj = E_MAC_PJ * total_macs_all_layers\n",
        "\n",
        "    # --- Calculate SNN Energy ---\n",
        "    if not layer_flops_mac_list:\n",
        "        energy_snn_pj = 0\n",
        "        macs_layer1_for_snn = 0\n",
        "    else:\n",
        "        macs_layer1_for_snn = layer_flops_mac_list[0]\n",
        "        energy_snn_term1_pj = E_MAC_PJ * macs_layer1_for_snn\n",
        "\n",
        "        sops_sum_layers_2_L = 0\n",
        "        for i in range(1, len(layer_flops_mac_list)):\n",
        "            macs_layer_i = layer_flops_mac_list[i]\n",
        "            sops_layer_i = avg_firing_rate * T_snn * macs_layer_i\n",
        "            sops_sum_layers_2_L += sops_layer_i\n",
        "\n",
        "        energy_snn_term2_pj = E_AC_PJ * sops_sum_layers_2_L\n",
        "        energy_snn_pj = energy_snn_term1_pj + energy_snn_term2_pj\n",
        "\n",
        "    # For debugging: print detailed FLOPs\n",
        "    # print(f\"\\n--- MACs for {model_name} ---\")\n",
        "    # for i, info in enumerate(layer_info_list):\n",
        "    #     print(f\"  Layer {i} ({info['type']}): {info['macs']:,} MACs. In: {info['in_shape']}, Out: {info['out_shape']}\")\n",
        "    # print(f\"Total MACs: {total_macs_all_layers:,}\")\n",
        "    # print(f\"MACs Layer 1 (for SNN calc): {macs_layer1_for_snn:,}\")\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"Model Name\": model_name,\n",
        "        \"Total MACs\": f\"{total_macs_all_layers:,}\",\n",
        "        \"ANN Energy (pJ)\": f\"{energy_ann_pj:,.2f}\",\n",
        "        \"SNN Energy (pJ)\": f\"{energy_snn_pj:,.2f}\",\n",
        "        \"fr (SNN)\": avg_firing_rate,\n",
        "        \"T (SNN)\": T_snn,\n",
        "    }\n",
        "\n",
        "# --- Prepare models and input for energy calculation ---\n",
        "# Assuming 'channel_count' and 'data_length' are available from your data loading\n",
        "# And 'device' is also defined ('cuda' or 'cpu')\n",
        "\n",
        "# Use the existing trained models if available, or initialize new ones\n",
        "# For FLOPs calculation, weights don't matter, only architecture and input size.\n",
        "\n",
        "# Ensure channel_count and data_length are defined (from your data loading cell)\n",
        "# Example:\n",
        "# file = scio.loadmat('/content/sub-011_eeg.mat')\n",
        "# all_data = file['all_data']\n",
        "# channel_count = all_data.shape[1] # Should be 60 for sub-011\n",
        "# data_length = all_data.shape[2]   # Should be 1000\n",
        "\n",
        "if 'channel_count' not in locals() or 'data_length' not in locals():\n",
        "    print(\"Please ensure 'channel_count' and 'data_length' are defined from your dataset.\")\n",
        "    # Fallback values if not defined, adjust as necessary\n",
        "    channel_count = 60 # Example from sub-011\n",
        "    data_length = 1000 # Example\n",
        "    print(f\"Using fallback: channel_count={channel_count}, data_length={data_length}\")\n",
        "\n",
        "\n",
        "sample_input_tensor = torch.randn(1, 1, channel_count, data_length)\n",
        "\n",
        "# Initialize models (architectures are needed for FLOPs count)\n",
        "# Using the same DROP_OUT as in your training cells for consistency, though it doesn't affect FLOPs\n",
        "DROP_OUT_ENERGY_CALC = 0.25 # from your LENet training\n",
        "lenet_model_for_energy = LENet(classes_num=3, channel_count=channel_count, drop_out=DROP_OUT_ENERGY_CALC).to(device)\n",
        "lenet_model_for_energy.apply(initialize_weights) # Ensure all layers are built\n",
        "\n",
        "DROP_OUT_FCL_ENERGY_CALC = 0.2 # from your LENet_FCL training\n",
        "lenet_fcl_model_for_energy = LENet_FCL(classes_num=3, channel_count=channel_count, drop_out=DROP_OUT_FCL_ENERGY_CALC).to(device)\n",
        "# For LENet_FCL, the FC layer is created on the first forward pass.\n",
        "# So, we need to do a dummy forward pass if it hasn't been trained/used.\n",
        "lenet_fcl_model_for_energy.eval()\n",
        "with torch.no_grad():\n",
        "    lenet_fcl_model_for_energy(sample_input_tensor.to(device))\n",
        "lenet_fcl_model_for_energy.apply(initialize_weights)\n",
        "\n",
        "\n",
        "# --- Perform Energy Calculations ---\n",
        "T_SNN_VALUE = 100 # Time steps from your SNN conversion\n",
        "AVG_FIRING_RATE = 0.02 # Assumed average firing rate (2%)\n",
        "\n",
        "energy_results = []\n",
        "\n",
        "# 1. LENet (CCB version)\n",
        "results_lenet = get_macs_and_energy(\n",
        "    lenet_model_for_energy,\n",
        "    sample_input_tensor,\n",
        "    model_name=\"LENet (CCB)\",\n",
        "    T_snn=T_SNN_VALUE,\n",
        "    avg_firing_rate=AVG_FIRING_RATE,\n",
        ")\n",
        "energy_results.append(results_lenet)\n",
        "\n",
        "# 2. LENet_FCL (Fully Connected Layer version)\n",
        "results_lenet_fcl = get_macs_and_energy(\n",
        "    lenet_fcl_model_for_energy,\n",
        "    sample_input_tensor,\n",
        "    model_name=\"LENet_FCL\",\n",
        "    T_snn=T_SNN_VALUE,\n",
        "    avg_firing_rate=AVG_FIRING_RATE,\n",
        ")\n",
        "energy_results.append(results_lenet_fcl)\n",
        "\n",
        "# --- Display Results in a Table ---\n",
        "print(\"\\n--- Estimated Energy Consumption ---\")\n",
        "print(f\"Constants: E_MAC = {E_MAC_PJ} pJ, E_AC = {E_AC_PJ} pJ\")\n",
        "print(f\"SNN Parameters: T = {T_SNN_VALUE}, Assumed Avg Firing Rate (fr) = {AVG_FIRING_RATE*100:.1f}%\")\n",
        "print(\"Note: SNN Energy is calculated based on the paper's formula.\")\n",
        "print(\"      FLOPs(1) [MAC-based energy] + Subsequent Layers [AC-based, fr-scaled energy]\")\n",
        "\n",
        "# Using pandas for a slightly nicer table, otherwise simple print\n",
        "try:\n",
        "    df_energy = pd.DataFrame(energy_results)\n",
        "    print(df_energy.to_string(index=False))\n",
        "except ImportError:\n",
        "    print(\"\\nPandas not found, printing basic table:\")\n",
        "    header = list(energy_results[0].keys())\n",
        "    print(\"\\t\".join(header))\n",
        "    for res in energy_results:\n",
        "        print(\"\\t\".join(map(str, res.values())))\n",
        "\n",
        "print(\"\\nNote: The actual energy can vary based on hardware, specific firing patterns,\")\n",
        "print(\"and how 'FLOPs(1)' (the first layer's energy contribution to SNN) is precisely defined.\")\n",
        "print(\"The values here are estimations based on the provided formulas and assumptions.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "JXy2ibddkwob",
        "outputId": "8f2590c6-7e15-408d-b524-1e94d02ad3b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please ensure 'channel_count' and 'data_length' are defined from your dataset.\n",
            "Using fallback: channel_count=60, data_length=1000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'LENet' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-17c11f66f1a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# Using the same DROP_OUT as in your training cells for consistency, though it doesn't affect FLOPs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mDROP_OUT_ENERGY_CALC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m \u001b[0;31m# from your LENet training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mlenet_model_for_energy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLENet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannel_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDROP_OUT_ENERGY_CALC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0mlenet_model_for_energy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitialize_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Ensure all layers are built\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LENet' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}