{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlonResearch/SNN-for-MI-EEG/blob/main/MI3_SNNforMIeeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xy9K5bJ9aB7D",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as da\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import scipy.io as scio\n",
        "!pip install spikingjelly -q\n",
        "from spikingjelly.activation_based import ann2snn\n",
        "#!apt install -y graphviz graphviz-dev -q\n",
        "#!pip install nnviz -q\n",
        "#from nnviz import drawing\n",
        "#import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3lAEV1BHaBp9",
        "outputId": "785ba198-a533-4806-ba8c-1f4ce8151665"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wed May  7 12:51:23 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0             28W /   70W |    1072MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "CYNshZR5ZhDy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Data loader and other functions\n",
        "#Defining functions\n",
        "\n",
        "def data_loader(data, label, batch=64, shuffle=True, drop=False):\n",
        "    \"\"\"\n",
        "    Preprocess the data to fit model.\n",
        "    Feed data into data_loader.\n",
        "    input:\n",
        "        data (float): samples*length*ch (samples*ch*length).\n",
        "        label (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        batch (int): batch size\n",
        "        shuffle (bool): shuffle data before input into decoder\n",
        "        drop (bool): drop the last samples if True\n",
        "    output:\n",
        "        data loader\n",
        "    \"\"\"\n",
        "    label = torch.LongTensor(label.flatten()).to(device)\n",
        "    if data.shape[1] >= data.shape[2]:\n",
        "        data = torch.tensor(data.swapaxes(1, 2))\n",
        "    data = torch.unsqueeze(data, dim=1).type('torch.FloatTensor').to(device)\n",
        "    data = da.TensorDataset(data, label)\n",
        "    loader = da.DataLoader(dataset=data, batch_size=batch, shuffle=shuffle, drop_last=drop)\n",
        "    return loader\n",
        "\n",
        "\n",
        "def val_snn(Dec, test_loader, T=None):\n",
        "    Dec.eval().to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if T is not None:\n",
        "        corrects = np.zeros(T)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            if T is None:\n",
        "                outputs = Dec(inputs)\n",
        "                correct += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            else:\n",
        "                for m in Dec.modules():\n",
        "                    if hasattr(m, 'reset'):\n",
        "                        m.reset()\n",
        "                for t in range(T):\n",
        "                    if t == 0:\n",
        "                        outputs = Dec(inputs)\n",
        "                    else:\n",
        "                        outputs += Dec(inputs)\n",
        "                    corrects[t] += (outputs.argmax(dim=1) == targets.to(device)).float().sum().item()\n",
        "            total += targets.shape[0]\n",
        "    return correct / total if T is None else corrects / total\n",
        "\n",
        "\n",
        "def anntosnn(cnn_model, train_x, train_y, test_x, test_y, batch=64, T=None):\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    print('---------------------------------------------')\n",
        "    print('Converting using MaxNorm')\n",
        "    model_converter = ann2snn.Converter(mode='max', dataloader=train_loader)\n",
        "    snn_model = model_converter(cnn_model)\n",
        "    mode_max_accs = val_snn(snn_model, test_loader, T=T)\n",
        "\n",
        "    return mode_max_accs\n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def train_ann(cnn_model, train_x, train_y, test_x, test_y, ep=500, batch=64):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        train_x, test_x (float): samples*length*ch (samples*ch*length).\n",
        "        train_y, test_y (int): samples, ie.: [0, 1, 1, 0, ..., 2].\n",
        "        ep (int): total train and test epoch\n",
        "        batch (int): batch size\n",
        "    output:\n",
        "        train acc, test acc, weight_file\n",
        "    \"\"\"\n",
        "    # Define training configuration\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.01)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=ep)\n",
        "\n",
        "    # Define data loader\n",
        "    train_loader = data_loader(train_x, train_y, batch=batch)\n",
        "    test_loader = data_loader(test_x, test_y, batch=batch)\n",
        "\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "    for epoch in range(ep):\n",
        "        # Train ANN\n",
        "        cnn_model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        print('\\n')\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = cnn_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            print(batch_idx, len(train_loader), 'Epoch: %d | ANN: trainLoss: %.4f | trainAcc: %.4f%% (%d/%d)'\n",
        "                  % (epoch, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        train_acc.append(round(correct / total, 4))\n",
        "\n",
        "        # Test ANN\n",
        "        cnn_model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "                outputs = cnn_model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "                print(batch_idx, len(test_loader), 'Epoch: %d | ANN: testLoss: %.4f | testAcc: %.4f%% (%d/%d)'\n",
        "                      % (epoch, val_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "        test_acc.append(round(correct / total, 4))\n",
        "\n",
        "    train_acc = np.asarray(train_acc[-1])\n",
        "    test_acc = np.asarray(test_acc[-1])\n",
        "    return train_acc, test_acc,cnn_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "EC8xjyPDaM6G"
      },
      "outputs": [],
      "source": [
        "# @title Model definitions\n",
        "# Model 2a\n",
        "\n",
        "class LENet(nn.Module):\n",
        "    \"\"\"\n",
        "        LENet Model\n",
        "    input:\n",
        "         data shape as: batch_size*1*channel*length (64*1*22*1000) BCI IV-2a\n",
        "         batch_size：64\n",
        "         channel：22\n",
        "         length：1000\n",
        "    output:\n",
        "        classes_num\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classes_num=3, channel_count=22, drop_out = 0.5):\n",
        "        super(LENet, self).__init__()\n",
        "        self.drop_out = drop_out\n",
        "\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,64) #\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,32) #\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            # Temporal Convolution block kernel_size (1,16) #\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            # Temporal Convolution block fusion kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            # Spatial Convolution block kernel_size (channel,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(channel_count, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            # Feature Fusion Convolution block kernel_size (1,16) and (1,1) #\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),  #\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.CCB = nn.Sequential(\n",
        "            # Classification Convolution block kernel_size (1,1) #\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=classes_num,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "        x = self.CCB(x)\n",
        "        return x\n",
        "\n",
        "class LENet_FCL(nn.Module):\n",
        "    def __init__(self, classes_num=3, channel_count=60, drop_out=0.5):\n",
        "        super(LENet_FCL, self).__init__()\n",
        "        self.drop_out = drop_out\n",
        "\n",
        "        # Keep all the convolutional layers the same\n",
        "        self.block_TCB_1 = nn.Sequential(\n",
        "            nn.ZeroPad2d((32, 31, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 64),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_2 = nn.Sequential(\n",
        "            nn.ZeroPad2d((16, 15, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 32),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "        self.block_TCB_3 = nn.Sequential(\n",
        "            nn.ZeroPad2d((8, 7, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=8,\n",
        "                kernel_size=(1, 16),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(8)\n",
        "        )\n",
        "\n",
        "        self.TCB_fusion = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=24,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(24)\n",
        "        )\n",
        "\n",
        "        self.SCB = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=24,\n",
        "                out_channels=16,\n",
        "                kernel_size=(channel_count, 1),\n",
        "                groups=8,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 4)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        self.FFCB = nn.Sequential(\n",
        "            nn.ZeroPad2d((7, 8, 0, 0)),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 16),\n",
        "                groups=16,\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=16,\n",
        "                kernel_size=(1, 1),\n",
        "                bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d((1, 8)),\n",
        "            nn.Dropout(self.drop_out)\n",
        "        )\n",
        "\n",
        "        # We'll determine the size of the FC layer in the forward pass\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = None\n",
        "        self.classes_num = classes_num\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block_TCB_1(x)\n",
        "        x2 = self.block_TCB_2(x)\n",
        "        x3 = self.block_TCB_3(x)\n",
        "        x4 = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.TCB_fusion(x4)\n",
        "        x = self.SCB(x)\n",
        "        x = self.FFCB(x)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Create the FC layer on first forward pass if it doesn't exist\n",
        "        if self.fc is None:\n",
        "            in_features = x.shape[1]\n",
        "            self.fc = nn.Linear(in_features, self.classes_num).to(x.device)\n",
        "            # Initialize weights for the new layer\n",
        "            nn.init.kaiming_normal_(self.fc.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            if self.fc.bias is not None:\n",
        "                nn.init.constant_(self.fc.bias, 0)\n",
        "\n",
        "        # Apply the FC layer\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "T0hmMue2aMqA",
        "outputId": "67985fb5-4c44-4b83-cbb2-c71072825f2d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channel count: 62\n",
            "Data shape: (965, 62, 360)\n",
            "Label shape: (965, 1)\n",
            "Class distribution: Rest: 365, Elbow: 300, Hand: 300\n",
            "Balanced class distribution: Rest: 365, Elbow: 300, Hand: 300\n",
            "Balanced data shape: (965, 62, 360)\n",
            "Balanced label shape: (965, 1)\n"
          ]
        }
      ],
      "source": [
        "# @title Loading the data\n",
        "\"\"\"\n",
        "Loading the data\n",
        "\"\"\"\n",
        "\n",
        "# Getting real samples\n",
        "#Locally load the dataset\n",
        "#file = scio.loadmat('Datasets\\BCICIV_2a_gdf\\Derivatives\\A01T.mat')\n",
        "\n",
        "# Google Colab load the dataset\n",
        "file = scio.loadmat('/content/sub-011_eeg90hz_7-35.mat')\n",
        "\n",
        "\n",
        "all_data = file['all_data']\n",
        "all_label = file['all_label']\n",
        "REDUCE_REST = 1\n",
        "\n",
        "# Print data information\n",
        "channel_count = all_data.shape[1]\n",
        "num_classes = len(np.unique(all_label.flatten()))\n",
        "data_length = all_data.shape[2] # Assuming data is samples*ch*length\n",
        "print(f\"Channel count: {channel_count}\")\n",
        "print(f\"Data shape: {all_data.shape}\")\n",
        "print(f\"Label shape: {all_label.shape}\")\n",
        "print(f\"Class distribution: Rest: {np.sum(all_label == 0)}, Elbow: {np.sum(all_label == 1)}, Hand: {np.sum(all_label == 2)}\")\n",
        "\n",
        "# Address class imbalance for 'Rest' class (label 0)\n",
        "label_0_indices = np.where(all_label.flatten() == 0)[0]\n",
        "label_other_indices = np.where(all_label.flatten() != 0)[0]\n",
        "\n",
        "# Randomly select \"REDUCE_REST\"% of label 0 indices\n",
        "num_label_0_to_keep = int(len(label_0_indices) * REDUCE_REST)\n",
        "#np.random.seed(42) # for reproducibility\n",
        "selected_label_0_indices = np.random.choice(label_0_indices, size=num_label_0_to_keep, replace=False)\n",
        "\n",
        "# Combine selected label 0 indices with all other labels\n",
        "balanced_indices = np.concatenate((selected_label_0_indices, label_other_indices))\n",
        "np.random.shuffle(balanced_indices) # Shuffle the combined dataset\n",
        "\n",
        "all_data_balanced = all_data[balanced_indices]\n",
        "all_label_balanced = all_label[balanced_indices]\n",
        "\n",
        "print(f\"Balanced class distribution: Rest: {np.sum(all_label_balanced == 0)}, Elbow: {np.sum(all_label_balanced == 1)}, Hand: {np.sum(all_label_balanced == 2)}\")\n",
        "print(f\"Balanced data shape: {all_data_balanced.shape}\")\n",
        "print(f\"Balanced label shape: {all_label_balanced.shape}\")\n",
        "\n",
        "datasetX = torch.tensor(all_data_balanced, dtype=torch.float32)\n",
        "datasetY = torch.tensor(all_label_balanced, dtype=torch.int64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LENet to SNN Conversion Framework execution\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 64\n",
        "TIME_STEPS = 100  # T for SNN\n",
        "TEST_SIZE = 0.2\n",
        "DROP_OUT = 0.35\n",
        "\n",
        "\n",
        "# Split the data\n",
        "print(f\"{100 - (TEST_SIZE * 100)}% of the dataset is used for training and {TEST_SIZE * 100}% is used for testing.\")\n",
        "train_data, test_data, train_label, test_label = train_test_split(datasetX, datasetY, test_size=TEST_SIZE, shuffle=True,\n",
        "                                                                  random_state=0)\n",
        "\n",
        "# Initialize model\n",
        "cnn_model_lenet_ccb = LENet(classes_num=3, channel_count=channel_count, drop_out = DROP_OUT).to(device)\n",
        "cnn_model_lenet_ccb.apply(initialize_weights)\n",
        "\n",
        "# Train CNN model\n",
        "train_acc, test_acc,  cnn_model_lenet_ccb = train_ann(cnn_model_lenet_ccb, train_data, train_label, test_data, test_label,\n",
        "                                              ep=EPOCHS, batch=BATCH_SIZE)\n",
        "max_norm_acc = anntosnn( cnn_model_lenet_ccb, train_data, train_label, test_data, test_label,\n",
        "                        batch=BATCH_SIZE, T=TIME_STEPS)\n",
        "snn_model_lenet = ann2snn.Converter(mode='max', dataloader=data_loader(train_data, train_label, batch=BATCH_SIZE))( cnn_model_lenet_ccb)\n",
        "\n",
        "print('\\n')\n",
        "print('ANN accuracy: Test: %.4f%%' % (test_acc * 100))\n",
        "print('SNN accuracy: max_norm: %.4f%%' % (max_norm_acc[-1] * 100))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JiMa-ylsC2bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfbb46a6-de46-4cc7-ee45-859f0f02467a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "3 4 Epoch: 236 | ANN: testLoss: 1.3692 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 237 | ANN: trainLoss: 0.1519 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 237 | ANN: trainLoss: 0.1046 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 237 | ANN: trainLoss: 0.1006 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 237 | ANN: trainLoss: 0.0939 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 237 | ANN: trainLoss: 0.1039 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 237 | ANN: trainLoss: 0.1041 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 237 | ANN: trainLoss: 0.1067 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 237 | ANN: trainLoss: 0.1024 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 237 | ANN: trainLoss: 0.1056 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 237 | ANN: trainLoss: 0.1056 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 237 | ANN: trainLoss: 0.1013 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 237 | ANN: trainLoss: 0.1022 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 237 | ANN: trainLoss: 0.0966 | trainAcc: 96.2435% (743/772)\n",
            "0 4 Epoch: 237 | ANN: testLoss: 0.9582 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 237 | ANN: testLoss: 0.8242 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 237 | ANN: testLoss: 0.8790 | testAcc: 66.1458% (127/192)\n",
            "3 4 Epoch: 237 | ANN: testLoss: 0.7051 | testAcc: 66.3212% (128/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 238 | ANN: trainLoss: 0.1359 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 238 | ANN: trainLoss: 0.1488 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 238 | ANN: trainLoss: 0.1410 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 238 | ANN: trainLoss: 0.1350 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 238 | ANN: trainLoss: 0.1240 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 238 | ANN: trainLoss: 0.1201 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 238 | ANN: trainLoss: 0.1131 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 238 | ANN: trainLoss: 0.1076 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 238 | ANN: trainLoss: 0.1176 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 238 | ANN: trainLoss: 0.1188 | trainAcc: 95.6250% (612/640)\n",
            "10 13 Epoch: 238 | ANN: trainLoss: 0.1144 | trainAcc: 95.8807% (675/704)\n",
            "11 13 Epoch: 238 | ANN: trainLoss: 0.1185 | trainAcc: 95.7031% (735/768)\n",
            "12 13 Epoch: 238 | ANN: trainLoss: 0.1840 | trainAcc: 95.3368% (736/772)\n",
            "0 4 Epoch: 238 | ANN: testLoss: 0.6112 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 238 | ANN: testLoss: 0.6454 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 238 | ANN: testLoss: 0.7289 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 238 | ANN: testLoss: 0.5467 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 239 | ANN: trainLoss: 0.1027 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 239 | ANN: trainLoss: 0.0795 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 239 | ANN: trainLoss: 0.0949 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 239 | ANN: trainLoss: 0.0939 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 239 | ANN: trainLoss: 0.0999 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 239 | ANN: trainLoss: 0.1124 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 239 | ANN: trainLoss: 0.1087 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 239 | ANN: trainLoss: 0.1062 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 239 | ANN: trainLoss: 0.1099 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 239 | ANN: trainLoss: 0.1180 | trainAcc: 95.6250% (612/640)\n",
            "10 13 Epoch: 239 | ANN: trainLoss: 0.1164 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 239 | ANN: trainLoss: 0.1117 | trainAcc: 95.9635% (737/768)\n",
            "12 13 Epoch: 239 | ANN: trainLoss: 0.1116 | trainAcc: 95.9845% (741/772)\n",
            "0 4 Epoch: 239 | ANN: testLoss: 0.6041 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 239 | ANN: testLoss: 0.8131 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 239 | ANN: testLoss: 0.7414 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 239 | ANN: testLoss: 0.5677 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 240 | ANN: trainLoss: 0.1129 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 240 | ANN: trainLoss: 0.1116 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 240 | ANN: trainLoss: 0.1246 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 240 | ANN: trainLoss: 0.1039 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 240 | ANN: trainLoss: 0.0915 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 240 | ANN: trainLoss: 0.1004 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 240 | ANN: trainLoss: 0.1073 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 240 | ANN: trainLoss: 0.1078 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 240 | ANN: trainLoss: 0.1020 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 240 | ANN: trainLoss: 0.1083 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 240 | ANN: trainLoss: 0.1103 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 240 | ANN: trainLoss: 0.1071 | trainAcc: 96.6146% (742/768)\n",
            "12 13 Epoch: 240 | ANN: trainLoss: 0.1070 | trainAcc: 96.6321% (746/772)\n",
            "0 4 Epoch: 240 | ANN: testLoss: 0.8554 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 240 | ANN: testLoss: 0.7034 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 240 | ANN: testLoss: 0.7126 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 240 | ANN: testLoss: 1.2230 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 241 | ANN: trainLoss: 0.1566 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 241 | ANN: trainLoss: 0.1055 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 241 | ANN: trainLoss: 0.0929 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 241 | ANN: trainLoss: 0.1048 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 241 | ANN: trainLoss: 0.0974 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 241 | ANN: trainLoss: 0.1087 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 241 | ANN: trainLoss: 0.1161 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 241 | ANN: trainLoss: 0.1091 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 241 | ANN: trainLoss: 0.1031 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 241 | ANN: trainLoss: 0.1077 | trainAcc: 95.6250% (612/640)\n",
            "10 13 Epoch: 241 | ANN: trainLoss: 0.1038 | trainAcc: 95.8807% (675/704)\n",
            "11 13 Epoch: 241 | ANN: trainLoss: 0.0995 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 241 | ANN: trainLoss: 0.1005 | trainAcc: 96.1140% (742/772)\n",
            "0 4 Epoch: 241 | ANN: testLoss: 0.5069 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 241 | ANN: testLoss: 0.6139 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 241 | ANN: testLoss: 0.6778 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 241 | ANN: testLoss: 0.5729 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 242 | ANN: trainLoss: 0.0461 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 242 | ANN: trainLoss: 0.0824 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 242 | ANN: trainLoss: 0.1043 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 242 | ANN: trainLoss: 0.0933 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 242 | ANN: trainLoss: 0.1015 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 242 | ANN: trainLoss: 0.0997 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 242 | ANN: trainLoss: 0.1031 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 242 | ANN: trainLoss: 0.1004 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 242 | ANN: trainLoss: 0.0970 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 242 | ANN: trainLoss: 0.0904 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 242 | ANN: trainLoss: 0.0889 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 242 | ANN: trainLoss: 0.0932 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 242 | ANN: trainLoss: 0.1452 | trainAcc: 96.7617% (747/772)\n",
            "0 4 Epoch: 242 | ANN: testLoss: 0.6456 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 242 | ANN: testLoss: 0.6518 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 242 | ANN: testLoss: 0.6634 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 242 | ANN: testLoss: 0.4975 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 243 | ANN: trainLoss: 0.1583 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 243 | ANN: trainLoss: 0.1433 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 243 | ANN: trainLoss: 0.1175 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 243 | ANN: trainLoss: 0.1218 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 243 | ANN: trainLoss: 0.1247 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 243 | ANN: trainLoss: 0.1423 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 243 | ANN: trainLoss: 0.1424 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 243 | ANN: trainLoss: 0.1424 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 243 | ANN: trainLoss: 0.1366 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 243 | ANN: trainLoss: 0.1309 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 243 | ANN: trainLoss: 0.1284 | trainAcc: 95.4545% (672/704)\n",
            "11 13 Epoch: 243 | ANN: trainLoss: 0.1220 | trainAcc: 95.8333% (736/768)\n",
            "12 13 Epoch: 243 | ANN: trainLoss: 0.1162 | trainAcc: 95.8549% (740/772)\n",
            "0 4 Epoch: 243 | ANN: testLoss: 0.7002 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 243 | ANN: testLoss: 0.7310 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 243 | ANN: testLoss: 0.7692 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 243 | ANN: testLoss: 0.6029 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 244 | ANN: trainLoss: 0.1120 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 244 | ANN: trainLoss: 0.0940 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 244 | ANN: trainLoss: 0.0898 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 244 | ANN: trainLoss: 0.0869 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 244 | ANN: trainLoss: 0.0927 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 244 | ANN: trainLoss: 0.0898 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 244 | ANN: trainLoss: 0.0924 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 244 | ANN: trainLoss: 0.0892 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 244 | ANN: trainLoss: 0.0900 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 244 | ANN: trainLoss: 0.0907 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 244 | ANN: trainLoss: 0.0892 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 244 | ANN: trainLoss: 0.0878 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 244 | ANN: trainLoss: 0.0822 | trainAcc: 96.7617% (747/772)\n",
            "0 4 Epoch: 244 | ANN: testLoss: 0.7207 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 244 | ANN: testLoss: 0.7417 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 244 | ANN: testLoss: 0.7206 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 244 | ANN: testLoss: 0.5405 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 245 | ANN: trainLoss: 0.1300 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 245 | ANN: trainLoss: 0.1378 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 245 | ANN: trainLoss: 0.1133 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 245 | ANN: trainLoss: 0.0945 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 245 | ANN: trainLoss: 0.0833 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 245 | ANN: trainLoss: 0.0776 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 245 | ANN: trainLoss: 0.0743 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 245 | ANN: trainLoss: 0.0809 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 245 | ANN: trainLoss: 0.0826 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 245 | ANN: trainLoss: 0.0886 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 245 | ANN: trainLoss: 0.0872 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 245 | ANN: trainLoss: 0.0888 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 245 | ANN: trainLoss: 0.0917 | trainAcc: 96.2435% (743/772)\n",
            "0 4 Epoch: 245 | ANN: testLoss: 0.5054 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 245 | ANN: testLoss: 0.6794 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 245 | ANN: testLoss: 0.6947 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 245 | ANN: testLoss: 0.5214 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 246 | ANN: trainLoss: 0.0715 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 246 | ANN: trainLoss: 0.1063 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 246 | ANN: trainLoss: 0.1234 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 246 | ANN: trainLoss: 0.1163 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 246 | ANN: trainLoss: 0.1111 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 246 | ANN: trainLoss: 0.1040 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 246 | ANN: trainLoss: 0.0943 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 246 | ANN: trainLoss: 0.0952 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 246 | ANN: trainLoss: 0.1025 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 246 | ANN: trainLoss: 0.0968 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 246 | ANN: trainLoss: 0.0902 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 246 | ANN: trainLoss: 0.0887 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 246 | ANN: trainLoss: 0.0847 | trainAcc: 97.1503% (750/772)\n",
            "0 4 Epoch: 246 | ANN: testLoss: 0.6343 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 246 | ANN: testLoss: 0.6313 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 246 | ANN: testLoss: 0.6946 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 246 | ANN: testLoss: 0.5343 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 247 | ANN: trainLoss: 0.0830 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 247 | ANN: trainLoss: 0.1557 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 247 | ANN: trainLoss: 0.1201 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 247 | ANN: trainLoss: 0.1159 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 247 | ANN: trainLoss: 0.1064 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 247 | ANN: trainLoss: 0.1001 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 247 | ANN: trainLoss: 0.0942 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 247 | ANN: trainLoss: 0.0987 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 247 | ANN: trainLoss: 0.0905 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 247 | ANN: trainLoss: 0.0877 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 247 | ANN: trainLoss: 0.0851 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 247 | ANN: trainLoss: 0.0813 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 247 | ANN: trainLoss: 0.1782 | trainAcc: 97.1503% (750/772)\n",
            "0 4 Epoch: 247 | ANN: testLoss: 1.0725 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 247 | ANN: testLoss: 0.7912 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 247 | ANN: testLoss: 0.6987 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 247 | ANN: testLoss: 0.5244 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 248 | ANN: trainLoss: 0.0399 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 248 | ANN: trainLoss: 0.0636 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 248 | ANN: trainLoss: 0.0698 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 248 | ANN: trainLoss: 0.0997 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 248 | ANN: trainLoss: 0.0943 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 248 | ANN: trainLoss: 0.0914 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 248 | ANN: trainLoss: 0.0862 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 248 | ANN: trainLoss: 0.0885 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 248 | ANN: trainLoss: 0.0906 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 248 | ANN: trainLoss: 0.0950 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 248 | ANN: trainLoss: 0.0921 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 248 | ANN: trainLoss: 0.1041 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 248 | ANN: trainLoss: 0.1021 | trainAcc: 96.8912% (748/772)\n",
            "0 4 Epoch: 248 | ANN: testLoss: 0.7753 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 248 | ANN: testLoss: 0.6831 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 248 | ANN: testLoss: 0.7804 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 248 | ANN: testLoss: 1.4146 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 249 | ANN: trainLoss: 0.0657 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 249 | ANN: trainLoss: 0.1153 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 249 | ANN: trainLoss: 0.1077 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 249 | ANN: trainLoss: 0.1030 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 249 | ANN: trainLoss: 0.1037 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 249 | ANN: trainLoss: 0.1037 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 249 | ANN: trainLoss: 0.0974 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 249 | ANN: trainLoss: 0.1027 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 249 | ANN: trainLoss: 0.0962 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 249 | ANN: trainLoss: 0.0930 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 249 | ANN: trainLoss: 0.0919 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 249 | ANN: trainLoss: 0.0883 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 249 | ANN: trainLoss: 0.0828 | trainAcc: 96.8912% (748/772)\n",
            "0 4 Epoch: 249 | ANN: testLoss: 0.4222 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 249 | ANN: testLoss: 0.5863 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 249 | ANN: testLoss: 0.6611 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 249 | ANN: testLoss: 0.4962 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 250 | ANN: trainLoss: 0.0799 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 250 | ANN: trainLoss: 0.0743 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 250 | ANN: trainLoss: 0.0723 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 250 | ANN: trainLoss: 0.0649 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 250 | ANN: trainLoss: 0.0685 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 250 | ANN: trainLoss: 0.0652 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 250 | ANN: trainLoss: 0.0747 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 250 | ANN: trainLoss: 0.0850 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 250 | ANN: trainLoss: 0.0805 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 250 | ANN: trainLoss: 0.0889 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 250 | ANN: trainLoss: 0.0865 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 250 | ANN: trainLoss: 0.0819 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 250 | ANN: trainLoss: 0.0764 | trainAcc: 97.4093% (752/772)\n",
            "0 4 Epoch: 250 | ANN: testLoss: 0.6396 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 250 | ANN: testLoss: 0.6051 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 250 | ANN: testLoss: 0.6702 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 250 | ANN: testLoss: 0.5110 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 251 | ANN: trainLoss: 0.0581 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 251 | ANN: trainLoss: 0.0768 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 251 | ANN: trainLoss: 0.0814 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 251 | ANN: trainLoss: 0.0823 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 251 | ANN: trainLoss: 0.0846 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 251 | ANN: trainLoss: 0.0838 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 251 | ANN: trainLoss: 0.0896 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 251 | ANN: trainLoss: 0.1006 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 251 | ANN: trainLoss: 0.0926 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 251 | ANN: trainLoss: 0.0902 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 251 | ANN: trainLoss: 0.0925 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 251 | ANN: trainLoss: 0.0910 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 251 | ANN: trainLoss: 0.3581 | trainAcc: 96.1140% (742/772)\n",
            "0 4 Epoch: 251 | ANN: testLoss: 0.6369 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 251 | ANN: testLoss: 0.6159 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 251 | ANN: testLoss: 0.6452 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 251 | ANN: testLoss: 0.7936 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 252 | ANN: trainLoss: 0.0444 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 252 | ANN: trainLoss: 0.1037 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 252 | ANN: trainLoss: 0.0941 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 252 | ANN: trainLoss: 0.0949 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 252 | ANN: trainLoss: 0.0910 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 252 | ANN: trainLoss: 0.0952 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 252 | ANN: trainLoss: 0.0939 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 252 | ANN: trainLoss: 0.0927 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 252 | ANN: trainLoss: 0.0949 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 252 | ANN: trainLoss: 0.0947 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 252 | ANN: trainLoss: 0.0976 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 252 | ANN: trainLoss: 0.1009 | trainAcc: 96.6146% (742/768)\n",
            "12 13 Epoch: 252 | ANN: trainLoss: 0.1588 | trainAcc: 96.5026% (745/772)\n",
            "0 4 Epoch: 252 | ANN: testLoss: 0.8782 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 252 | ANN: testLoss: 0.7317 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 252 | ANN: testLoss: 0.6973 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 252 | ANN: testLoss: 0.5250 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 253 | ANN: trainLoss: 0.1306 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 253 | ANN: trainLoss: 0.1095 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 253 | ANN: trainLoss: 0.1437 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 253 | ANN: trainLoss: 0.1367 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 253 | ANN: trainLoss: 0.1130 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 253 | ANN: trainLoss: 0.1079 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 253 | ANN: trainLoss: 0.1060 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 253 | ANN: trainLoss: 0.1027 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 253 | ANN: trainLoss: 0.1016 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 253 | ANN: trainLoss: 0.1050 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 253 | ANN: trainLoss: 0.1045 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 253 | ANN: trainLoss: 0.1049 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 253 | ANN: trainLoss: 0.1208 | trainAcc: 96.2435% (743/772)\n",
            "0 4 Epoch: 253 | ANN: testLoss: 0.5666 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 253 | ANN: testLoss: 0.7840 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 253 | ANN: testLoss: 0.7390 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 253 | ANN: testLoss: 0.5618 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 254 | ANN: trainLoss: 0.1030 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 254 | ANN: trainLoss: 0.0966 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 254 | ANN: trainLoss: 0.0819 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 254 | ANN: trainLoss: 0.0840 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 254 | ANN: trainLoss: 0.0809 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 254 | ANN: trainLoss: 0.0909 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 254 | ANN: trainLoss: 0.0852 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 254 | ANN: trainLoss: 0.0806 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 254 | ANN: trainLoss: 0.0932 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 254 | ANN: trainLoss: 0.0997 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 254 | ANN: trainLoss: 0.0975 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 254 | ANN: trainLoss: 0.1008 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 254 | ANN: trainLoss: 0.1104 | trainAcc: 96.7617% (747/772)\n",
            "0 4 Epoch: 254 | ANN: testLoss: 0.6330 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 254 | ANN: testLoss: 0.6172 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 254 | ANN: testLoss: 0.7060 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 254 | ANN: testLoss: 1.2941 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 255 | ANN: trainLoss: 0.1004 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 255 | ANN: trainLoss: 0.0743 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 255 | ANN: trainLoss: 0.0814 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 255 | ANN: trainLoss: 0.1004 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 255 | ANN: trainLoss: 0.0969 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 255 | ANN: trainLoss: 0.1003 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 255 | ANN: trainLoss: 0.0915 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 255 | ANN: trainLoss: 0.0881 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 255 | ANN: trainLoss: 0.0840 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 255 | ANN: trainLoss: 0.0848 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 255 | ANN: trainLoss: 0.0840 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 255 | ANN: trainLoss: 0.0914 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 255 | ANN: trainLoss: 0.1178 | trainAcc: 97.0207% (749/772)\n",
            "0 4 Epoch: 255 | ANN: testLoss: 0.8702 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 255 | ANN: testLoss: 0.7295 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 255 | ANN: testLoss: 0.7376 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 255 | ANN: testLoss: 0.5533 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 256 | ANN: trainLoss: 0.1559 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 256 | ANN: trainLoss: 0.1526 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 256 | ANN: trainLoss: 0.1534 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 256 | ANN: trainLoss: 0.1456 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 256 | ANN: trainLoss: 0.1308 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 256 | ANN: trainLoss: 0.1279 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 256 | ANN: trainLoss: 0.1279 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 256 | ANN: trainLoss: 0.1202 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 256 | ANN: trainLoss: 0.1177 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 256 | ANN: trainLoss: 0.1154 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 256 | ANN: trainLoss: 0.1152 | trainAcc: 95.4545% (672/704)\n",
            "11 13 Epoch: 256 | ANN: trainLoss: 0.1113 | trainAcc: 95.8333% (736/768)\n",
            "12 13 Epoch: 256 | ANN: trainLoss: 0.1254 | trainAcc: 95.7254% (739/772)\n",
            "0 4 Epoch: 256 | ANN: testLoss: 0.5094 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 256 | ANN: testLoss: 0.6376 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 256 | ANN: testLoss: 0.7046 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 256 | ANN: testLoss: 0.5393 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 257 | ANN: trainLoss: 0.1100 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 257 | ANN: trainLoss: 0.0769 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 257 | ANN: trainLoss: 0.0938 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 257 | ANN: trainLoss: 0.0834 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 257 | ANN: trainLoss: 0.0833 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 257 | ANN: trainLoss: 0.0842 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 257 | ANN: trainLoss: 0.0823 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 257 | ANN: trainLoss: 0.0794 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 257 | ANN: trainLoss: 0.0811 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 257 | ANN: trainLoss: 0.0938 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 257 | ANN: trainLoss: 0.1015 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 257 | ANN: trainLoss: 0.0995 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 257 | ANN: trainLoss: 0.2217 | trainAcc: 97.4093% (752/772)\n",
            "0 4 Epoch: 257 | ANN: testLoss: 0.7806 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 257 | ANN: testLoss: 0.8061 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 257 | ANN: testLoss: 0.7435 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 257 | ANN: testLoss: 1.2736 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 258 | ANN: trainLoss: 0.0696 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 258 | ANN: trainLoss: 0.0752 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 258 | ANN: trainLoss: 0.0684 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 258 | ANN: trainLoss: 0.1236 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 258 | ANN: trainLoss: 0.1150 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 258 | ANN: trainLoss: 0.1087 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 258 | ANN: trainLoss: 0.1083 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 258 | ANN: trainLoss: 0.1143 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 258 | ANN: trainLoss: 0.1254 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 258 | ANN: trainLoss: 0.1215 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 258 | ANN: trainLoss: 0.1257 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 258 | ANN: trainLoss: 0.1286 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 258 | ANN: trainLoss: 0.1200 | trainAcc: 95.2073% (735/772)\n",
            "0 4 Epoch: 258 | ANN: testLoss: 0.7437 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 258 | ANN: testLoss: 0.8442 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 258 | ANN: testLoss: 0.8406 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 258 | ANN: testLoss: 1.1394 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 259 | ANN: trainLoss: 0.1578 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 259 | ANN: trainLoss: 0.1373 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 259 | ANN: trainLoss: 0.1178 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 259 | ANN: trainLoss: 0.1002 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 259 | ANN: trainLoss: 0.1265 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 259 | ANN: trainLoss: 0.1330 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 259 | ANN: trainLoss: 0.1388 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 259 | ANN: trainLoss: 0.1440 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 259 | ANN: trainLoss: 0.1357 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 259 | ANN: trainLoss: 0.1267 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 259 | ANN: trainLoss: 0.1292 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 259 | ANN: trainLoss: 0.1288 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 259 | ANN: trainLoss: 0.1200 | trainAcc: 94.5596% (730/772)\n",
            "0 4 Epoch: 259 | ANN: testLoss: 0.8544 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 259 | ANN: testLoss: 0.8200 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 259 | ANN: testLoss: 0.7143 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 259 | ANN: testLoss: 1.0089 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 260 | ANN: trainLoss: 0.1868 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 260 | ANN: trainLoss: 0.1638 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 260 | ANN: trainLoss: 0.1299 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 260 | ANN: trainLoss: 0.1215 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 260 | ANN: trainLoss: 0.1175 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 260 | ANN: trainLoss: 0.1080 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 260 | ANN: trainLoss: 0.0970 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 260 | ANN: trainLoss: 0.0897 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 260 | ANN: trainLoss: 0.0852 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 260 | ANN: trainLoss: 0.0820 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 260 | ANN: trainLoss: 0.0874 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 260 | ANN: trainLoss: 0.0829 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 260 | ANN: trainLoss: 0.1032 | trainAcc: 96.8912% (748/772)\n",
            "0 4 Epoch: 260 | ANN: testLoss: 0.6588 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 260 | ANN: testLoss: 0.6494 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 260 | ANN: testLoss: 0.6730 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 260 | ANN: testLoss: 0.7095 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 261 | ANN: trainLoss: 0.1054 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 261 | ANN: trainLoss: 0.0958 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 261 | ANN: trainLoss: 0.0804 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 261 | ANN: trainLoss: 0.1124 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 261 | ANN: trainLoss: 0.1095 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 261 | ANN: trainLoss: 0.0994 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 261 | ANN: trainLoss: 0.1016 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 261 | ANN: trainLoss: 0.0978 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 261 | ANN: trainLoss: 0.0967 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 261 | ANN: trainLoss: 0.0923 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 261 | ANN: trainLoss: 0.0902 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 261 | ANN: trainLoss: 0.1020 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 261 | ANN: trainLoss: 0.0957 | trainAcc: 97.1503% (750/772)\n",
            "0 4 Epoch: 261 | ANN: testLoss: 0.7607 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 261 | ANN: testLoss: 0.7513 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 261 | ANN: testLoss: 0.7198 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 261 | ANN: testLoss: 0.5460 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 262 | ANN: trainLoss: 0.0666 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 262 | ANN: trainLoss: 0.0945 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 262 | ANN: trainLoss: 0.1170 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 262 | ANN: trainLoss: 0.0975 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 262 | ANN: trainLoss: 0.0984 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 262 | ANN: trainLoss: 0.1022 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 262 | ANN: trainLoss: 0.0925 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 262 | ANN: trainLoss: 0.0973 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 262 | ANN: trainLoss: 0.0928 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 262 | ANN: trainLoss: 0.0868 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 262 | ANN: trainLoss: 0.0856 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 262 | ANN: trainLoss: 0.0816 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 262 | ANN: trainLoss: 0.0887 | trainAcc: 97.9275% (756/772)\n",
            "0 4 Epoch: 262 | ANN: testLoss: 0.5827 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 262 | ANN: testLoss: 0.6840 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 262 | ANN: testLoss: 0.7046 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 262 | ANN: testLoss: 0.6156 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 263 | ANN: trainLoss: 0.0780 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 263 | ANN: trainLoss: 0.0789 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 263 | ANN: trainLoss: 0.1393 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 263 | ANN: trainLoss: 0.1177 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 263 | ANN: trainLoss: 0.1066 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 263 | ANN: trainLoss: 0.1078 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 263 | ANN: trainLoss: 0.1054 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 263 | ANN: trainLoss: 0.1074 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 263 | ANN: trainLoss: 0.1077 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 263 | ANN: trainLoss: 0.1031 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 263 | ANN: trainLoss: 0.0986 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 263 | ANN: trainLoss: 0.0980 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 263 | ANN: trainLoss: 0.1608 | trainAcc: 96.7617% (747/772)\n",
            "0 4 Epoch: 263 | ANN: testLoss: 0.4712 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 263 | ANN: testLoss: 0.6151 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 263 | ANN: testLoss: 0.6969 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 263 | ANN: testLoss: 0.5575 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 264 | ANN: trainLoss: 0.0887 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 264 | ANN: trainLoss: 0.0820 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 264 | ANN: trainLoss: 0.0822 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 264 | ANN: trainLoss: 0.0855 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 264 | ANN: trainLoss: 0.0862 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 264 | ANN: trainLoss: 0.0788 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 264 | ANN: trainLoss: 0.0748 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 264 | ANN: trainLoss: 0.0715 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 264 | ANN: trainLoss: 0.0669 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 264 | ANN: trainLoss: 0.0689 | trainAcc: 98.1250% (628/640)\n",
            "10 13 Epoch: 264 | ANN: trainLoss: 0.0686 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 264 | ANN: trainLoss: 0.0676 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 264 | ANN: trainLoss: 0.0756 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 264 | ANN: testLoss: 0.9756 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 264 | ANN: testLoss: 0.8161 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 264 | ANN: testLoss: 0.7438 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 264 | ANN: testLoss: 0.5579 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 265 | ANN: trainLoss: 0.1214 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 265 | ANN: trainLoss: 0.2187 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 265 | ANN: trainLoss: 0.1670 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 265 | ANN: trainLoss: 0.1450 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 265 | ANN: trainLoss: 0.1227 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 265 | ANN: trainLoss: 0.1146 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 265 | ANN: trainLoss: 0.1063 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 265 | ANN: trainLoss: 0.1013 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 265 | ANN: trainLoss: 0.1014 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 265 | ANN: trainLoss: 0.0976 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 265 | ANN: trainLoss: 0.0989 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 265 | ANN: trainLoss: 0.0969 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 265 | ANN: trainLoss: 0.1183 | trainAcc: 97.2798% (751/772)\n",
            "0 4 Epoch: 265 | ANN: testLoss: 0.7822 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 265 | ANN: testLoss: 0.8223 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 265 | ANN: testLoss: 0.7832 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 265 | ANN: testLoss: 0.5875 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 266 | ANN: trainLoss: 0.0597 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 266 | ANN: trainLoss: 0.0672 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 266 | ANN: trainLoss: 0.0662 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 266 | ANN: trainLoss: 0.0830 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 266 | ANN: trainLoss: 0.0816 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 266 | ANN: trainLoss: 0.0868 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 266 | ANN: trainLoss: 0.0829 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 266 | ANN: trainLoss: 0.1002 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 266 | ANN: trainLoss: 0.1013 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 266 | ANN: trainLoss: 0.1002 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 266 | ANN: trainLoss: 0.1012 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 266 | ANN: trainLoss: 0.1047 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 266 | ANN: trainLoss: 0.1258 | trainAcc: 96.1140% (742/772)\n",
            "0 4 Epoch: 266 | ANN: testLoss: 0.6720 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 266 | ANN: testLoss: 0.7802 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 266 | ANN: testLoss: 0.7103 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 266 | ANN: testLoss: 0.5355 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 267 | ANN: trainLoss: 0.0676 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 267 | ANN: trainLoss: 0.0740 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 267 | ANN: trainLoss: 0.0633 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 267 | ANN: trainLoss: 0.0605 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 267 | ANN: trainLoss: 0.0805 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 267 | ANN: trainLoss: 0.0905 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 267 | ANN: trainLoss: 0.0829 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 267 | ANN: trainLoss: 0.0844 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 267 | ANN: trainLoss: 0.0838 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 267 | ANN: trainLoss: 0.0863 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 267 | ANN: trainLoss: 0.0839 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 267 | ANN: trainLoss: 0.0845 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 267 | ANN: trainLoss: 0.0904 | trainAcc: 96.8912% (748/772)\n",
            "0 4 Epoch: 267 | ANN: testLoss: 0.7567 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 267 | ANN: testLoss: 0.6712 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 267 | ANN: testLoss: 0.7317 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 267 | ANN: testLoss: 0.5498 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 268 | ANN: trainLoss: 0.1300 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 268 | ANN: trainLoss: 0.1162 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 268 | ANN: trainLoss: 0.1032 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 268 | ANN: trainLoss: 0.0971 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 268 | ANN: trainLoss: 0.0960 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 268 | ANN: trainLoss: 0.0954 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 268 | ANN: trainLoss: 0.1060 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 268 | ANN: trainLoss: 0.1049 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 268 | ANN: trainLoss: 0.1001 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 268 | ANN: trainLoss: 0.0979 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 268 | ANN: trainLoss: 0.0947 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 268 | ANN: trainLoss: 0.0964 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 268 | ANN: trainLoss: 0.1093 | trainAcc: 96.6321% (746/772)\n",
            "0 4 Epoch: 268 | ANN: testLoss: 0.7508 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 268 | ANN: testLoss: 0.7065 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 268 | ANN: testLoss: 0.7562 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 268 | ANN: testLoss: 0.5672 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 269 | ANN: trainLoss: 0.0754 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 269 | ANN: trainLoss: 0.0847 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 269 | ANN: trainLoss: 0.0775 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 269 | ANN: trainLoss: 0.0870 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 269 | ANN: trainLoss: 0.0746 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 269 | ANN: trainLoss: 0.0726 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 269 | ANN: trainLoss: 0.0883 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 269 | ANN: trainLoss: 0.0862 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 269 | ANN: trainLoss: 0.0931 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 269 | ANN: trainLoss: 0.0974 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 269 | ANN: trainLoss: 0.0982 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 269 | ANN: trainLoss: 0.0980 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 269 | ANN: trainLoss: 0.0936 | trainAcc: 96.2435% (743/772)\n",
            "0 4 Epoch: 269 | ANN: testLoss: 0.7311 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 269 | ANN: testLoss: 0.7512 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 269 | ANN: testLoss: 0.7158 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 269 | ANN: testLoss: 0.5380 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 270 | ANN: trainLoss: 0.0521 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 270 | ANN: trainLoss: 0.0668 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 270 | ANN: trainLoss: 0.0747 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 270 | ANN: trainLoss: 0.0758 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 270 | ANN: trainLoss: 0.0815 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 270 | ANN: trainLoss: 0.0794 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 270 | ANN: trainLoss: 0.0758 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 270 | ANN: trainLoss: 0.0699 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 270 | ANN: trainLoss: 0.0700 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 270 | ANN: trainLoss: 0.0766 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 270 | ANN: trainLoss: 0.0736 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 270 | ANN: trainLoss: 0.0739 | trainAcc: 97.5260% (749/768)\n",
            "12 13 Epoch: 270 | ANN: trainLoss: 0.0787 | trainAcc: 97.5389% (753/772)\n",
            "0 4 Epoch: 270 | ANN: testLoss: 0.6709 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 270 | ANN: testLoss: 0.6460 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 270 | ANN: testLoss: 0.7024 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 270 | ANN: testLoss: 0.7331 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 271 | ANN: trainLoss: 0.0986 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 271 | ANN: trainLoss: 0.0845 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 271 | ANN: trainLoss: 0.0951 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 271 | ANN: trainLoss: 0.0810 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 271 | ANN: trainLoss: 0.0761 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 271 | ANN: trainLoss: 0.0885 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 271 | ANN: trainLoss: 0.0871 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 271 | ANN: trainLoss: 0.0889 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 271 | ANN: trainLoss: 0.0880 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 271 | ANN: trainLoss: 0.0879 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 271 | ANN: trainLoss: 0.0848 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 271 | ANN: trainLoss: 0.0874 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 271 | ANN: trainLoss: 0.0809 | trainAcc: 96.8912% (748/772)\n",
            "0 4 Epoch: 271 | ANN: testLoss: 0.7768 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 271 | ANN: testLoss: 0.7483 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 271 | ANN: testLoss: 0.7237 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 271 | ANN: testLoss: 0.5456 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 272 | ANN: trainLoss: 0.1123 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 272 | ANN: trainLoss: 0.1124 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 272 | ANN: trainLoss: 0.0833 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 272 | ANN: trainLoss: 0.0910 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 272 | ANN: trainLoss: 0.0843 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 272 | ANN: trainLoss: 0.0792 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 272 | ANN: trainLoss: 0.0763 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 272 | ANN: trainLoss: 0.0727 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 272 | ANN: trainLoss: 0.0697 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 272 | ANN: trainLoss: 0.0685 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 272 | ANN: trainLoss: 0.0648 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 272 | ANN: trainLoss: 0.0674 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 272 | ANN: trainLoss: 0.0677 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 272 | ANN: testLoss: 0.6630 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 272 | ANN: testLoss: 0.7371 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 272 | ANN: testLoss: 0.7095 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 272 | ANN: testLoss: 0.5323 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 273 | ANN: trainLoss: 0.1801 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 273 | ANN: trainLoss: 0.1084 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 273 | ANN: trainLoss: 0.0913 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 273 | ANN: trainLoss: 0.0925 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 273 | ANN: trainLoss: 0.0841 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 273 | ANN: trainLoss: 0.0788 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 273 | ANN: trainLoss: 0.0763 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 273 | ANN: trainLoss: 0.0754 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 273 | ANN: trainLoss: 0.0838 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 273 | ANN: trainLoss: 0.0833 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 273 | ANN: trainLoss: 0.0795 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 273 | ANN: trainLoss: 0.0808 | trainAcc: 97.5260% (749/768)\n",
            "12 13 Epoch: 273 | ANN: trainLoss: 0.1312 | trainAcc: 97.2798% (751/772)\n",
            "0 4 Epoch: 273 | ANN: testLoss: 0.7722 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 273 | ANN: testLoss: 0.5984 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 273 | ANN: testLoss: 0.6904 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 273 | ANN: testLoss: 0.6206 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 274 | ANN: trainLoss: 0.0474 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 274 | ANN: trainLoss: 0.0469 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 274 | ANN: trainLoss: 0.0598 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 274 | ANN: trainLoss: 0.0636 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 274 | ANN: trainLoss: 0.0737 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 274 | ANN: trainLoss: 0.0776 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 274 | ANN: trainLoss: 0.0828 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 274 | ANN: trainLoss: 0.1021 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 274 | ANN: trainLoss: 0.1084 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 274 | ANN: trainLoss: 0.1098 | trainAcc: 95.4688% (611/640)\n",
            "10 13 Epoch: 274 | ANN: trainLoss: 0.1093 | trainAcc: 95.3125% (671/704)\n",
            "11 13 Epoch: 274 | ANN: trainLoss: 0.1037 | trainAcc: 95.7031% (735/768)\n",
            "12 13 Epoch: 274 | ANN: trainLoss: 0.1869 | trainAcc: 95.4663% (737/772)\n",
            "0 4 Epoch: 274 | ANN: testLoss: 0.7130 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 274 | ANN: testLoss: 0.9272 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 274 | ANN: testLoss: 0.9291 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 274 | ANN: testLoss: 0.6968 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 275 | ANN: trainLoss: 0.2644 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 275 | ANN: trainLoss: 0.1800 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 275 | ANN: trainLoss: 0.1600 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 275 | ANN: trainLoss: 0.1402 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 275 | ANN: trainLoss: 0.1470 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 275 | ANN: trainLoss: 0.1307 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 275 | ANN: trainLoss: 0.1193 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 275 | ANN: trainLoss: 0.1097 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 275 | ANN: trainLoss: 0.1144 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 275 | ANN: trainLoss: 0.1159 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 275 | ANN: trainLoss: 0.1217 | trainAcc: 95.8807% (675/704)\n",
            "11 13 Epoch: 275 | ANN: trainLoss: 0.1264 | trainAcc: 95.7031% (735/768)\n",
            "12 13 Epoch: 275 | ANN: trainLoss: 0.2223 | trainAcc: 95.5959% (738/772)\n",
            "0 4 Epoch: 275 | ANN: testLoss: 0.8138 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 275 | ANN: testLoss: 0.6827 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 275 | ANN: testLoss: 0.7766 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 275 | ANN: testLoss: 0.5824 | testAcc: 70.9845% (137/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 276 | ANN: trainLoss: 0.1570 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 276 | ANN: trainLoss: 0.1195 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 276 | ANN: trainLoss: 0.1233 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 276 | ANN: trainLoss: 0.1186 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 276 | ANN: trainLoss: 0.1050 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 276 | ANN: trainLoss: 0.1042 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 276 | ANN: trainLoss: 0.1122 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 276 | ANN: trainLoss: 0.1181 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 276 | ANN: trainLoss: 0.1228 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 276 | ANN: trainLoss: 0.1298 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 276 | ANN: trainLoss: 0.1207 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 276 | ANN: trainLoss: 0.1182 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 276 | ANN: trainLoss: 0.1100 | trainAcc: 96.3731% (744/772)\n",
            "0 4 Epoch: 276 | ANN: testLoss: 0.6413 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 276 | ANN: testLoss: 0.7441 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 276 | ANN: testLoss: 0.7582 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 276 | ANN: testLoss: 0.6434 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 277 | ANN: trainLoss: 0.0579 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 277 | ANN: trainLoss: 0.0602 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 277 | ANN: trainLoss: 0.0580 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 277 | ANN: trainLoss: 0.0645 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 277 | ANN: trainLoss: 0.0693 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 277 | ANN: trainLoss: 0.0756 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 277 | ANN: trainLoss: 0.0741 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 277 | ANN: trainLoss: 0.0874 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 277 | ANN: trainLoss: 0.0939 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 277 | ANN: trainLoss: 0.0909 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 277 | ANN: trainLoss: 0.0952 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 277 | ANN: trainLoss: 0.0923 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 277 | ANN: trainLoss: 0.0890 | trainAcc: 97.2798% (751/772)\n",
            "0 4 Epoch: 277 | ANN: testLoss: 0.7917 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 277 | ANN: testLoss: 0.6971 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 277 | ANN: testLoss: 0.7267 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 277 | ANN: testLoss: 0.5577 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 278 | ANN: trainLoss: 0.1025 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 278 | ANN: trainLoss: 0.1063 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 278 | ANN: trainLoss: 0.1073 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 278 | ANN: trainLoss: 0.0973 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 278 | ANN: trainLoss: 0.0857 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 278 | ANN: trainLoss: 0.0890 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 278 | ANN: trainLoss: 0.0827 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 278 | ANN: trainLoss: 0.0780 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 278 | ANN: trainLoss: 0.0745 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 278 | ANN: trainLoss: 0.0744 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 278 | ANN: trainLoss: 0.0761 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 278 | ANN: trainLoss: 0.0749 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 278 | ANN: trainLoss: 0.0718 | trainAcc: 97.9275% (756/772)\n",
            "0 4 Epoch: 278 | ANN: testLoss: 0.4582 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 278 | ANN: testLoss: 0.6051 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 278 | ANN: testLoss: 0.6928 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 278 | ANN: testLoss: 0.5199 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 279 | ANN: trainLoss: 0.0657 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 279 | ANN: trainLoss: 0.0786 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 279 | ANN: trainLoss: 0.0742 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 279 | ANN: trainLoss: 0.0618 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 279 | ANN: trainLoss: 0.0607 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 279 | ANN: trainLoss: 0.0699 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 279 | ANN: trainLoss: 0.0769 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 279 | ANN: trainLoss: 0.0788 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 279 | ANN: trainLoss: 0.0814 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 279 | ANN: trainLoss: 0.0764 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 279 | ANN: trainLoss: 0.0719 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 279 | ANN: trainLoss: 0.0749 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 279 | ANN: trainLoss: 0.1127 | trainAcc: 97.5389% (753/772)\n",
            "0 4 Epoch: 279 | ANN: testLoss: 0.9687 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 279 | ANN: testLoss: 0.7666 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 279 | ANN: testLoss: 0.7170 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 279 | ANN: testLoss: 0.5396 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 280 | ANN: trainLoss: 0.1312 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 280 | ANN: trainLoss: 0.0889 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 280 | ANN: trainLoss: 0.0801 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 280 | ANN: trainLoss: 0.0812 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 280 | ANN: trainLoss: 0.0707 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 280 | ANN: trainLoss: 0.0760 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 280 | ANN: trainLoss: 0.0769 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 280 | ANN: trainLoss: 0.0798 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 280 | ANN: trainLoss: 0.0781 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 280 | ANN: trainLoss: 0.0844 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 280 | ANN: trainLoss: 0.0808 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 280 | ANN: trainLoss: 0.0788 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 280 | ANN: trainLoss: 0.0784 | trainAcc: 97.2798% (751/772)\n",
            "0 4 Epoch: 280 | ANN: testLoss: 0.7365 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 280 | ANN: testLoss: 0.6981 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 280 | ANN: testLoss: 0.7000 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 280 | ANN: testLoss: 0.5250 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 281 | ANN: trainLoss: 0.0753 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 281 | ANN: trainLoss: 0.0705 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 281 | ANN: trainLoss: 0.0734 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 281 | ANN: trainLoss: 0.0691 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 281 | ANN: trainLoss: 0.0737 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 281 | ANN: trainLoss: 0.0961 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 281 | ANN: trainLoss: 0.0957 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 281 | ANN: trainLoss: 0.0938 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 281 | ANN: trainLoss: 0.0929 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 281 | ANN: trainLoss: 0.0944 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 281 | ANN: trainLoss: 0.0957 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 281 | ANN: trainLoss: 0.0917 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 281 | ANN: trainLoss: 0.0885 | trainAcc: 96.5026% (745/772)\n",
            "0 4 Epoch: 281 | ANN: testLoss: 0.6134 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 281 | ANN: testLoss: 0.5604 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 281 | ANN: testLoss: 0.7102 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 281 | ANN: testLoss: 0.5371 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 282 | ANN: trainLoss: 0.1460 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 282 | ANN: trainLoss: 0.1024 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 282 | ANN: trainLoss: 0.1088 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 282 | ANN: trainLoss: 0.1025 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 282 | ANN: trainLoss: 0.0901 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 282 | ANN: trainLoss: 0.0802 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 282 | ANN: trainLoss: 0.0760 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 282 | ANN: trainLoss: 0.0862 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 282 | ANN: trainLoss: 0.0847 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 282 | ANN: trainLoss: 0.0861 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 282 | ANN: trainLoss: 0.0840 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 282 | ANN: trainLoss: 0.0840 | trainAcc: 97.5260% (749/768)\n",
            "12 13 Epoch: 282 | ANN: trainLoss: 0.0979 | trainAcc: 97.4093% (752/772)\n",
            "0 4 Epoch: 282 | ANN: testLoss: 0.5800 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 282 | ANN: testLoss: 0.7233 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 282 | ANN: testLoss: 0.7275 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 282 | ANN: testLoss: 0.5558 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 283 | ANN: trainLoss: 0.0519 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 283 | ANN: trainLoss: 0.0626 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 283 | ANN: trainLoss: 0.0622 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 283 | ANN: trainLoss: 0.0750 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 283 | ANN: trainLoss: 0.0770 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 283 | ANN: trainLoss: 0.0783 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 283 | ANN: trainLoss: 0.0716 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 283 | ANN: trainLoss: 0.0811 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 283 | ANN: trainLoss: 0.0780 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 283 | ANN: trainLoss: 0.0731 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 283 | ANN: trainLoss: 0.0739 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 283 | ANN: trainLoss: 0.0759 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 283 | ANN: trainLoss: 0.0728 | trainAcc: 97.1503% (750/772)\n",
            "0 4 Epoch: 283 | ANN: testLoss: 0.7810 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 283 | ANN: testLoss: 0.8757 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 283 | ANN: testLoss: 0.7838 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 283 | ANN: testLoss: 0.5885 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 284 | ANN: trainLoss: 0.1162 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 284 | ANN: trainLoss: 0.1001 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 284 | ANN: trainLoss: 0.0930 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 284 | ANN: trainLoss: 0.0899 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 284 | ANN: trainLoss: 0.0758 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 284 | ANN: trainLoss: 0.0712 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 284 | ANN: trainLoss: 0.0760 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 284 | ANN: trainLoss: 0.0743 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 284 | ANN: trainLoss: 0.0792 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 284 | ANN: trainLoss: 0.0765 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 284 | ANN: trainLoss: 0.0728 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 284 | ANN: trainLoss: 0.0685 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 284 | ANN: trainLoss: 0.0932 | trainAcc: 97.6684% (754/772)\n",
            "0 4 Epoch: 284 | ANN: testLoss: 0.6110 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 284 | ANN: testLoss: 0.7635 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 284 | ANN: testLoss: 0.7633 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 284 | ANN: testLoss: 0.5725 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 285 | ANN: trainLoss: 0.0843 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 285 | ANN: trainLoss: 0.0633 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 285 | ANN: trainLoss: 0.0549 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 285 | ANN: trainLoss: 0.0507 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 285 | ANN: trainLoss: 0.0550 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 285 | ANN: trainLoss: 0.0605 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 285 | ANN: trainLoss: 0.0670 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 285 | ANN: trainLoss: 0.0663 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 285 | ANN: trainLoss: 0.0612 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 285 | ANN: trainLoss: 0.0613 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 285 | ANN: trainLoss: 0.0624 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 285 | ANN: trainLoss: 0.0634 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 285 | ANN: trainLoss: 0.0814 | trainAcc: 97.9275% (756/772)\n",
            "0 4 Epoch: 285 | ANN: testLoss: 0.6485 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 285 | ANN: testLoss: 0.7528 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 285 | ANN: testLoss: 0.7822 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 285 | ANN: testLoss: 0.5869 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 286 | ANN: trainLoss: 0.0562 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 286 | ANN: trainLoss: 0.0588 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 286 | ANN: trainLoss: 0.0627 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 286 | ANN: trainLoss: 0.0820 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 286 | ANN: trainLoss: 0.0776 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 286 | ANN: trainLoss: 0.0764 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 286 | ANN: trainLoss: 0.0740 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 286 | ANN: trainLoss: 0.0806 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 286 | ANN: trainLoss: 0.0922 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 286 | ANN: trainLoss: 0.0951 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 286 | ANN: trainLoss: 0.0964 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 286 | ANN: trainLoss: 0.0995 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 286 | ANN: trainLoss: 0.0950 | trainAcc: 96.3731% (744/772)\n",
            "0 4 Epoch: 286 | ANN: testLoss: 1.1820 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 286 | ANN: testLoss: 0.8971 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 286 | ANN: testLoss: 0.7575 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 286 | ANN: testLoss: 1.4871 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 287 | ANN: trainLoss: 0.0973 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 287 | ANN: trainLoss: 0.1254 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 287 | ANN: trainLoss: 0.0923 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 287 | ANN: trainLoss: 0.0944 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 287 | ANN: trainLoss: 0.0850 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 287 | ANN: trainLoss: 0.0865 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 287 | ANN: trainLoss: 0.0842 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 287 | ANN: trainLoss: 0.0790 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 287 | ANN: trainLoss: 0.0771 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 287 | ANN: trainLoss: 0.0772 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 287 | ANN: trainLoss: 0.0760 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 287 | ANN: trainLoss: 0.0710 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 287 | ANN: trainLoss: 0.1050 | trainAcc: 97.5389% (753/772)\n",
            "0 4 Epoch: 287 | ANN: testLoss: 0.9625 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 287 | ANN: testLoss: 0.8149 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 287 | ANN: testLoss: 0.7377 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 287 | ANN: testLoss: 1.2149 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 288 | ANN: trainLoss: 0.0328 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 288 | ANN: trainLoss: 0.0353 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 288 | ANN: trainLoss: 0.0317 | trainAcc: 100.0000% (192/192)\n",
            "3 13 Epoch: 288 | ANN: trainLoss: 0.0354 | trainAcc: 99.6094% (255/256)\n",
            "4 13 Epoch: 288 | ANN: trainLoss: 0.0660 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 288 | ANN: trainLoss: 0.0626 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 288 | ANN: trainLoss: 0.0585 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 288 | ANN: trainLoss: 0.0610 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 288 | ANN: trainLoss: 0.0640 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 288 | ANN: trainLoss: 0.0666 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 288 | ANN: trainLoss: 0.0651 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 288 | ANN: trainLoss: 0.0666 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 288 | ANN: trainLoss: 0.0687 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 288 | ANN: testLoss: 0.6645 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 288 | ANN: testLoss: 0.7916 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 288 | ANN: testLoss: 0.7521 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 288 | ANN: testLoss: 0.5659 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 289 | ANN: trainLoss: 0.0563 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 289 | ANN: trainLoss: 0.0618 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 289 | ANN: trainLoss: 0.0833 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 289 | ANN: trainLoss: 0.0880 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 289 | ANN: trainLoss: 0.0768 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 289 | ANN: trainLoss: 0.0697 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 289 | ANN: trainLoss: 0.0750 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 289 | ANN: trainLoss: 0.0884 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 289 | ANN: trainLoss: 0.0858 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 289 | ANN: trainLoss: 0.0875 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 289 | ANN: trainLoss: 0.0858 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 289 | ANN: trainLoss: 0.0896 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 289 | ANN: trainLoss: 0.1692 | trainAcc: 97.0207% (749/772)\n",
            "0 4 Epoch: 289 | ANN: testLoss: 0.7088 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 289 | ANN: testLoss: 0.6587 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 289 | ANN: testLoss: 0.7629 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 289 | ANN: testLoss: 0.6296 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 290 | ANN: trainLoss: 0.0272 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 290 | ANN: trainLoss: 0.0737 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 290 | ANN: trainLoss: 0.0717 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 290 | ANN: trainLoss: 0.0802 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 290 | ANN: trainLoss: 0.0800 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 290 | ANN: trainLoss: 0.0727 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 290 | ANN: trainLoss: 0.0671 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 290 | ANN: trainLoss: 0.0640 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 290 | ANN: trainLoss: 0.0703 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 290 | ANN: trainLoss: 0.0686 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 290 | ANN: trainLoss: 0.0785 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 290 | ANN: trainLoss: 0.0764 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 290 | ANN: trainLoss: 0.1619 | trainAcc: 97.0207% (749/772)\n",
            "0 4 Epoch: 290 | ANN: testLoss: 0.6318 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 290 | ANN: testLoss: 0.7063 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 290 | ANN: testLoss: 0.7538 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 290 | ANN: testLoss: 0.5654 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 291 | ANN: trainLoss: 0.0603 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 291 | ANN: trainLoss: 0.0654 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 291 | ANN: trainLoss: 0.0726 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 291 | ANN: trainLoss: 0.0901 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 291 | ANN: trainLoss: 0.0837 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 291 | ANN: trainLoss: 0.0824 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 291 | ANN: trainLoss: 0.0919 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 291 | ANN: trainLoss: 0.0913 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 291 | ANN: trainLoss: 0.0932 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 291 | ANN: trainLoss: 0.0881 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 291 | ANN: trainLoss: 0.0843 | trainAcc: 97.8693% (689/704)\n",
            "11 13 Epoch: 291 | ANN: trainLoss: 0.0800 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 291 | ANN: trainLoss: 0.0777 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 291 | ANN: testLoss: 0.8802 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 291 | ANN: testLoss: 0.8065 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 291 | ANN: testLoss: 0.7338 | testAcc: 70.3125% (135/192)\n",
            "3 4 Epoch: 291 | ANN: testLoss: 0.5605 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 292 | ANN: trainLoss: 0.1169 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 292 | ANN: trainLoss: 0.0973 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 292 | ANN: trainLoss: 0.1037 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 292 | ANN: trainLoss: 0.1067 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 292 | ANN: trainLoss: 0.1055 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 292 | ANN: trainLoss: 0.1062 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 292 | ANN: trainLoss: 0.1149 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 292 | ANN: trainLoss: 0.1081 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 292 | ANN: trainLoss: 0.1021 | trainAcc: 96.0069% (553/576)\n",
            "9 13 Epoch: 292 | ANN: trainLoss: 0.1111 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 292 | ANN: trainLoss: 0.1059 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 292 | ANN: trainLoss: 0.1024 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 292 | ANN: trainLoss: 0.0951 | trainAcc: 96.5026% (745/772)\n",
            "0 4 Epoch: 292 | ANN: testLoss: 0.5905 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 292 | ANN: testLoss: 0.7168 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 292 | ANN: testLoss: 0.7445 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 292 | ANN: testLoss: 0.7175 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 293 | ANN: trainLoss: 0.0636 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 293 | ANN: trainLoss: 0.0639 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 293 | ANN: trainLoss: 0.0674 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 293 | ANN: trainLoss: 0.0611 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 293 | ANN: trainLoss: 0.0597 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 293 | ANN: trainLoss: 0.0556 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 293 | ANN: trainLoss: 0.0825 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 293 | ANN: trainLoss: 0.0768 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 293 | ANN: trainLoss: 0.0772 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 293 | ANN: trainLoss: 0.0760 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 293 | ANN: trainLoss: 0.0750 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 293 | ANN: trainLoss: 0.0717 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 293 | ANN: trainLoss: 0.0700 | trainAcc: 97.4093% (752/772)\n",
            "0 4 Epoch: 293 | ANN: testLoss: 0.8170 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 293 | ANN: testLoss: 0.7055 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 293 | ANN: testLoss: 0.7445 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 293 | ANN: testLoss: 0.5584 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 294 | ANN: trainLoss: 0.0572 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 294 | ANN: trainLoss: 0.0680 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 294 | ANN: trainLoss: 0.0663 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 294 | ANN: trainLoss: 0.0706 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 294 | ANN: trainLoss: 0.0828 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 294 | ANN: trainLoss: 0.0832 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 294 | ANN: trainLoss: 0.0879 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 294 | ANN: trainLoss: 0.0827 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 294 | ANN: trainLoss: 0.0814 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 294 | ANN: trainLoss: 0.0832 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 294 | ANN: trainLoss: 0.0890 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 294 | ANN: trainLoss: 0.0847 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 294 | ANN: trainLoss: 0.0962 | trainAcc: 96.3731% (744/772)\n",
            "0 4 Epoch: 294 | ANN: testLoss: 1.0087 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 294 | ANN: testLoss: 0.7390 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 294 | ANN: testLoss: 0.7537 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 294 | ANN: testLoss: 1.1320 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 295 | ANN: trainLoss: 0.0313 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 295 | ANN: trainLoss: 0.0396 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 295 | ANN: trainLoss: 0.0354 | trainAcc: 100.0000% (192/192)\n",
            "3 13 Epoch: 295 | ANN: trainLoss: 0.0328 | trainAcc: 100.0000% (256/256)\n",
            "4 13 Epoch: 295 | ANN: trainLoss: 0.0343 | trainAcc: 99.6875% (319/320)\n",
            "5 13 Epoch: 295 | ANN: trainLoss: 0.0452 | trainAcc: 99.4792% (382/384)\n",
            "6 13 Epoch: 295 | ANN: trainLoss: 0.0476 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 295 | ANN: trainLoss: 0.0443 | trainAcc: 99.4141% (509/512)\n",
            "8 13 Epoch: 295 | ANN: trainLoss: 0.0413 | trainAcc: 99.4792% (573/576)\n",
            "9 13 Epoch: 295 | ANN: trainLoss: 0.0492 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 295 | ANN: trainLoss: 0.0501 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 295 | ANN: trainLoss: 0.0499 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 295 | ANN: trainLoss: 0.0910 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 295 | ANN: testLoss: 0.4915 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 295 | ANN: testLoss: 0.7668 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 295 | ANN: testLoss: 0.7845 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 295 | ANN: testLoss: 0.6249 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 296 | ANN: trainLoss: 0.0247 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 296 | ANN: trainLoss: 0.0346 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 296 | ANN: trainLoss: 0.0492 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 296 | ANN: trainLoss: 0.0546 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 296 | ANN: trainLoss: 0.0535 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 296 | ANN: trainLoss: 0.0538 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 296 | ANN: trainLoss: 0.0693 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 296 | ANN: trainLoss: 0.0741 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 296 | ANN: trainLoss: 0.0733 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 296 | ANN: trainLoss: 0.0745 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 296 | ANN: trainLoss: 0.0773 | trainAcc: 96.8750% (682/704)\n",
            "11 13 Epoch: 296 | ANN: trainLoss: 0.0777 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 296 | ANN: trainLoss: 0.1576 | trainAcc: 96.5026% (745/772)\n",
            "0 4 Epoch: 296 | ANN: testLoss: 0.4421 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 296 | ANN: testLoss: 0.8340 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 296 | ANN: testLoss: 0.7592 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 296 | ANN: testLoss: 0.5694 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 297 | ANN: trainLoss: 0.0928 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 297 | ANN: trainLoss: 0.0824 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 297 | ANN: trainLoss: 0.0923 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 297 | ANN: trainLoss: 0.0790 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 297 | ANN: trainLoss: 0.0775 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 297 | ANN: trainLoss: 0.0805 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 297 | ANN: trainLoss: 0.0827 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 297 | ANN: trainLoss: 0.1025 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 297 | ANN: trainLoss: 0.0978 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 297 | ANN: trainLoss: 0.1093 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 297 | ANN: trainLoss: 0.1123 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 297 | ANN: trainLoss: 0.1101 | trainAcc: 95.8333% (736/768)\n",
            "12 13 Epoch: 297 | ANN: trainLoss: 0.2425 | trainAcc: 95.5959% (738/772)\n",
            "0 4 Epoch: 297 | ANN: testLoss: 0.6854 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 297 | ANN: testLoss: 0.8258 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 297 | ANN: testLoss: 0.7630 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 297 | ANN: testLoss: 0.5725 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 298 | ANN: trainLoss: 0.1520 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 298 | ANN: trainLoss: 0.0844 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 298 | ANN: trainLoss: 0.0887 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 298 | ANN: trainLoss: 0.0813 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 298 | ANN: trainLoss: 0.0746 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 298 | ANN: trainLoss: 0.0812 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 298 | ANN: trainLoss: 0.0827 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 298 | ANN: trainLoss: 0.0936 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 298 | ANN: trainLoss: 0.0927 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 298 | ANN: trainLoss: 0.1058 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 298 | ANN: trainLoss: 0.0995 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 298 | ANN: trainLoss: 0.1006 | trainAcc: 96.6146% (742/768)\n",
            "12 13 Epoch: 298 | ANN: trainLoss: 0.1728 | trainAcc: 96.5026% (745/772)\n",
            "0 4 Epoch: 298 | ANN: testLoss: 0.5498 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 298 | ANN: testLoss: 0.6845 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 298 | ANN: testLoss: 0.7835 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 298 | ANN: testLoss: 0.5891 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 299 | ANN: trainLoss: 0.0677 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 299 | ANN: trainLoss: 0.0625 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 299 | ANN: trainLoss: 0.0707 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 299 | ANN: trainLoss: 0.0668 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 299 | ANN: trainLoss: 0.0672 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 299 | ANN: trainLoss: 0.0817 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 299 | ANN: trainLoss: 0.0859 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 299 | ANN: trainLoss: 0.0826 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 299 | ANN: trainLoss: 0.0808 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 299 | ANN: trainLoss: 0.0784 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 299 | ANN: trainLoss: 0.0786 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 299 | ANN: trainLoss: 0.0804 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 299 | ANN: trainLoss: 0.2561 | trainAcc: 97.1503% (750/772)\n",
            "0 4 Epoch: 299 | ANN: testLoss: 0.8545 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 299 | ANN: testLoss: 0.8278 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 299 | ANN: testLoss: 0.7732 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 299 | ANN: testLoss: 1.1828 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 300 | ANN: trainLoss: 0.1295 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 300 | ANN: trainLoss: 0.1265 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 300 | ANN: trainLoss: 0.1080 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 300 | ANN: trainLoss: 0.1085 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 300 | ANN: trainLoss: 0.1011 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 300 | ANN: trainLoss: 0.0932 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 300 | ANN: trainLoss: 0.1087 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 300 | ANN: trainLoss: 0.1050 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 300 | ANN: trainLoss: 0.1149 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 300 | ANN: trainLoss: 0.1190 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 300 | ANN: trainLoss: 0.1111 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 300 | ANN: trainLoss: 0.1144 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 300 | ANN: trainLoss: 0.1583 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 300 | ANN: testLoss: 0.6758 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 300 | ANN: testLoss: 0.7778 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 300 | ANN: testLoss: 0.8909 | testAcc: 68.7500% (132/192)\n",
            "3 4 Epoch: 300 | ANN: testLoss: 0.6696 | testAcc: 68.9119% (133/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 301 | ANN: trainLoss: 0.1660 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 301 | ANN: trainLoss: 0.1287 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 301 | ANN: trainLoss: 0.1334 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 301 | ANN: trainLoss: 0.1325 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 301 | ANN: trainLoss: 0.1204 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 301 | ANN: trainLoss: 0.1326 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 301 | ANN: trainLoss: 0.1234 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 301 | ANN: trainLoss: 0.1449 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 301 | ANN: trainLoss: 0.1405 | trainAcc: 94.0972% (542/576)\n",
            "9 13 Epoch: 301 | ANN: trainLoss: 0.1534 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 301 | ANN: trainLoss: 0.1460 | trainAcc: 93.6080% (659/704)\n",
            "11 13 Epoch: 301 | ANN: trainLoss: 0.1427 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 301 | ANN: trainLoss: 0.2366 | trainAcc: 93.6528% (723/772)\n",
            "0 4 Epoch: 301 | ANN: testLoss: 0.6307 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 301 | ANN: testLoss: 0.8355 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 301 | ANN: testLoss: 0.8438 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 301 | ANN: testLoss: 0.6683 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 302 | ANN: trainLoss: 0.1270 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 302 | ANN: trainLoss: 0.1002 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 302 | ANN: trainLoss: 0.1336 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 302 | ANN: trainLoss: 0.1301 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 302 | ANN: trainLoss: 0.1581 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 302 | ANN: trainLoss: 0.1414 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 302 | ANN: trainLoss: 0.1378 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 302 | ANN: trainLoss: 0.1259 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 302 | ANN: trainLoss: 0.1417 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 302 | ANN: trainLoss: 0.1450 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 302 | ANN: trainLoss: 0.1412 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 302 | ANN: trainLoss: 0.1443 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 302 | ANN: trainLoss: 0.1382 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 302 | ANN: testLoss: 0.9834 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 302 | ANN: testLoss: 0.6921 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 302 | ANN: testLoss: 0.7274 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 302 | ANN: testLoss: 0.5455 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 303 | ANN: trainLoss: 0.0479 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 303 | ANN: trainLoss: 0.0838 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 303 | ANN: trainLoss: 0.0943 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 303 | ANN: trainLoss: 0.1135 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 303 | ANN: trainLoss: 0.1293 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 303 | ANN: trainLoss: 0.1175 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 303 | ANN: trainLoss: 0.1271 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 303 | ANN: trainLoss: 0.1200 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 303 | ANN: trainLoss: 0.1200 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 303 | ANN: trainLoss: 0.1202 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 303 | ANN: trainLoss: 0.1209 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 303 | ANN: trainLoss: 0.1138 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 303 | ANN: trainLoss: 0.1066 | trainAcc: 95.2073% (735/772)\n",
            "0 4 Epoch: 303 | ANN: testLoss: 0.5735 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 303 | ANN: testLoss: 0.7283 | testAcc: 68.7500% (88/128)\n",
            "2 4 Epoch: 303 | ANN: testLoss: 0.6593 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 303 | ANN: testLoss: 0.5136 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 304 | ANN: trainLoss: 0.0508 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 304 | ANN: trainLoss: 0.0595 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 304 | ANN: trainLoss: 0.0943 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 304 | ANN: trainLoss: 0.1000 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 304 | ANN: trainLoss: 0.0902 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 304 | ANN: trainLoss: 0.0934 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 304 | ANN: trainLoss: 0.0939 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 304 | ANN: trainLoss: 0.0953 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 304 | ANN: trainLoss: 0.0957 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 304 | ANN: trainLoss: 0.0966 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 304 | ANN: trainLoss: 0.0992 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 304 | ANN: trainLoss: 0.0955 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 304 | ANN: trainLoss: 0.1325 | trainAcc: 96.3731% (744/772)\n",
            "0 4 Epoch: 304 | ANN: testLoss: 0.9350 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 304 | ANN: testLoss: 0.7288 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 304 | ANN: testLoss: 0.6907 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 304 | ANN: testLoss: 0.5182 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 305 | ANN: trainLoss: 0.0636 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 305 | ANN: trainLoss: 0.0786 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 305 | ANN: trainLoss: 0.0645 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 305 | ANN: trainLoss: 0.0611 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 305 | ANN: trainLoss: 0.0587 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 305 | ANN: trainLoss: 0.0609 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 305 | ANN: trainLoss: 0.0858 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 305 | ANN: trainLoss: 0.0875 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 305 | ANN: trainLoss: 0.0861 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 305 | ANN: trainLoss: 0.0868 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 305 | ANN: trainLoss: 0.0872 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 305 | ANN: trainLoss: 0.0836 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 305 | ANN: trainLoss: 0.1364 | trainAcc: 97.5389% (753/772)\n",
            "0 4 Epoch: 305 | ANN: testLoss: 0.8653 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 305 | ANN: testLoss: 0.7322 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 305 | ANN: testLoss: 0.6726 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 305 | ANN: testLoss: 1.5586 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 306 | ANN: trainLoss: 0.1467 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 306 | ANN: trainLoss: 0.1506 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 306 | ANN: trainLoss: 0.1260 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 306 | ANN: trainLoss: 0.1193 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 306 | ANN: trainLoss: 0.1069 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 306 | ANN: trainLoss: 0.1143 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 306 | ANN: trainLoss: 0.1086 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 306 | ANN: trainLoss: 0.1090 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 306 | ANN: trainLoss: 0.1046 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 306 | ANN: trainLoss: 0.1015 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 306 | ANN: trainLoss: 0.0972 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 306 | ANN: trainLoss: 0.0978 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 306 | ANN: trainLoss: 0.1303 | trainAcc: 95.9845% (741/772)\n",
            "0 4 Epoch: 306 | ANN: testLoss: 0.6360 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 306 | ANN: testLoss: 0.7625 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 306 | ANN: testLoss: 0.7017 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 306 | ANN: testLoss: 0.5327 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 307 | ANN: trainLoss: 0.0506 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 307 | ANN: trainLoss: 0.0784 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 307 | ANN: trainLoss: 0.0671 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 307 | ANN: trainLoss: 0.0856 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 307 | ANN: trainLoss: 0.0774 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 307 | ANN: trainLoss: 0.0907 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 307 | ANN: trainLoss: 0.0912 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 307 | ANN: trainLoss: 0.0921 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 307 | ANN: trainLoss: 0.0867 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 307 | ANN: trainLoss: 0.0848 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 307 | ANN: trainLoss: 0.0841 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 307 | ANN: trainLoss: 0.0877 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 307 | ANN: trainLoss: 0.1097 | trainAcc: 97.0207% (749/772)\n",
            "0 4 Epoch: 307 | ANN: testLoss: 0.7063 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 307 | ANN: testLoss: 0.7227 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 307 | ANN: testLoss: 0.7269 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 307 | ANN: testLoss: 1.2282 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 308 | ANN: trainLoss: 0.1521 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 308 | ANN: trainLoss: 0.0976 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 308 | ANN: trainLoss: 0.0937 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 308 | ANN: trainLoss: 0.0980 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 308 | ANN: trainLoss: 0.0929 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 308 | ANN: trainLoss: 0.0910 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 308 | ANN: trainLoss: 0.0964 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 308 | ANN: trainLoss: 0.0960 | trainAcc: 95.8984% (491/512)\n",
            "8 13 Epoch: 308 | ANN: trainLoss: 0.0992 | trainAcc: 95.6597% (551/576)\n",
            "9 13 Epoch: 308 | ANN: trainLoss: 0.0953 | trainAcc: 96.0938% (615/640)\n",
            "10 13 Epoch: 308 | ANN: trainLoss: 0.1006 | trainAcc: 95.8807% (675/704)\n",
            "11 13 Epoch: 308 | ANN: trainLoss: 0.0964 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 308 | ANN: trainLoss: 0.1033 | trainAcc: 96.1140% (742/772)\n",
            "0 4 Epoch: 308 | ANN: testLoss: 0.7564 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 308 | ANN: testLoss: 0.7131 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 308 | ANN: testLoss: 0.6717 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 308 | ANN: testLoss: 2.1395 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 309 | ANN: trainLoss: 0.0641 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 309 | ANN: trainLoss: 0.0577 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 309 | ANN: trainLoss: 0.0487 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 309 | ANN: trainLoss: 0.0472 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 309 | ANN: trainLoss: 0.0544 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 309 | ANN: trainLoss: 0.0857 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 309 | ANN: trainLoss: 0.0810 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 309 | ANN: trainLoss: 0.0772 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 309 | ANN: trainLoss: 0.0808 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 309 | ANN: trainLoss: 0.0769 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 309 | ANN: trainLoss: 0.0758 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 309 | ANN: trainLoss: 0.0774 | trainAcc: 97.5260% (749/768)\n",
            "12 13 Epoch: 309 | ANN: trainLoss: 0.0723 | trainAcc: 97.5389% (753/772)\n",
            "0 4 Epoch: 309 | ANN: testLoss: 0.4933 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 309 | ANN: testLoss: 0.7077 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 309 | ANN: testLoss: 0.7174 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 309 | ANN: testLoss: 0.5489 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 310 | ANN: trainLoss: 0.1111 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 310 | ANN: trainLoss: 0.0850 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 310 | ANN: trainLoss: 0.0888 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 310 | ANN: trainLoss: 0.0800 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 310 | ANN: trainLoss: 0.0748 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 310 | ANN: trainLoss: 0.0771 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 310 | ANN: trainLoss: 0.0716 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 310 | ANN: trainLoss: 0.0809 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 310 | ANN: trainLoss: 0.0790 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 310 | ANN: trainLoss: 0.0862 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 310 | ANN: trainLoss: 0.0855 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 310 | ANN: trainLoss: 0.0841 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 310 | ANN: trainLoss: 0.0845 | trainAcc: 96.7617% (747/772)\n",
            "0 4 Epoch: 310 | ANN: testLoss: 0.6788 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 310 | ANN: testLoss: 0.7117 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 310 | ANN: testLoss: 0.6994 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 310 | ANN: testLoss: 1.2486 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 311 | ANN: trainLoss: 0.0698 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 311 | ANN: trainLoss: 0.0522 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 311 | ANN: trainLoss: 0.0554 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 311 | ANN: trainLoss: 0.0575 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 311 | ANN: trainLoss: 0.0557 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 311 | ANN: trainLoss: 0.0531 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 311 | ANN: trainLoss: 0.0570 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 311 | ANN: trainLoss: 0.0622 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 311 | ANN: trainLoss: 0.0585 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 311 | ANN: trainLoss: 0.0587 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 311 | ANN: trainLoss: 0.0597 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 311 | ANN: trainLoss: 0.0579 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 311 | ANN: trainLoss: 0.0541 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 311 | ANN: testLoss: 0.6597 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 311 | ANN: testLoss: 0.6373 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 311 | ANN: testLoss: 0.7056 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 311 | ANN: testLoss: 0.7988 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 312 | ANN: trainLoss: 0.0188 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 312 | ANN: trainLoss: 0.0281 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 312 | ANN: trainLoss: 0.0324 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 312 | ANN: trainLoss: 0.0357 | trainAcc: 99.6094% (255/256)\n",
            "4 13 Epoch: 312 | ANN: trainLoss: 0.0381 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 312 | ANN: trainLoss: 0.0468 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 312 | ANN: trainLoss: 0.0459 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 312 | ANN: trainLoss: 0.0424 | trainAcc: 99.4141% (509/512)\n",
            "8 13 Epoch: 312 | ANN: trainLoss: 0.0437 | trainAcc: 99.3056% (572/576)\n",
            "9 13 Epoch: 312 | ANN: trainLoss: 0.0552 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 312 | ANN: trainLoss: 0.0551 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 312 | ANN: trainLoss: 0.0610 | trainAcc: 98.3073% (755/768)\n",
            "12 13 Epoch: 312 | ANN: trainLoss: 0.0567 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 312 | ANN: testLoss: 0.6089 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 312 | ANN: testLoss: 0.6645 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 312 | ANN: testLoss: 0.6780 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 312 | ANN: testLoss: 2.3012 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 313 | ANN: trainLoss: 0.0379 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 313 | ANN: trainLoss: 0.0703 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 313 | ANN: trainLoss: 0.0822 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 313 | ANN: trainLoss: 0.0736 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 313 | ANN: trainLoss: 0.0655 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 313 | ANN: trainLoss: 0.0579 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 313 | ANN: trainLoss: 0.0614 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 313 | ANN: trainLoss: 0.0603 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 313 | ANN: trainLoss: 0.0563 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 313 | ANN: trainLoss: 0.0618 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 313 | ANN: trainLoss: 0.0635 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 313 | ANN: trainLoss: 0.0692 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 313 | ANN: trainLoss: 0.0664 | trainAcc: 97.2798% (751/772)\n",
            "0 4 Epoch: 313 | ANN: testLoss: 0.6004 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 313 | ANN: testLoss: 0.7345 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 313 | ANN: testLoss: 0.6759 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 313 | ANN: testLoss: 0.5764 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 314 | ANN: trainLoss: 0.0781 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 314 | ANN: trainLoss: 0.0777 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 314 | ANN: trainLoss: 0.0630 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 314 | ANN: trainLoss: 0.0512 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 314 | ANN: trainLoss: 0.0695 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 314 | ANN: trainLoss: 0.0748 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 314 | ANN: trainLoss: 0.0688 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 314 | ANN: trainLoss: 0.0677 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 314 | ANN: trainLoss: 0.0674 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 314 | ANN: trainLoss: 0.0676 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 314 | ANN: trainLoss: 0.0663 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 314 | ANN: trainLoss: 0.0649 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 314 | ANN: trainLoss: 0.1001 | trainAcc: 97.2798% (751/772)\n",
            "0 4 Epoch: 314 | ANN: testLoss: 0.5543 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 314 | ANN: testLoss: 0.7064 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 314 | ANN: testLoss: 0.7025 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 314 | ANN: testLoss: 0.5269 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 315 | ANN: trainLoss: 0.0869 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 315 | ANN: trainLoss: 0.0734 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 315 | ANN: trainLoss: 0.0577 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 315 | ANN: trainLoss: 0.0611 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 315 | ANN: trainLoss: 0.0644 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 315 | ANN: trainLoss: 0.0678 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 315 | ANN: trainLoss: 0.0623 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 315 | ANN: trainLoss: 0.0628 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 315 | ANN: trainLoss: 0.0627 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 315 | ANN: trainLoss: 0.0643 | trainAcc: 98.1250% (628/640)\n",
            "10 13 Epoch: 315 | ANN: trainLoss: 0.0670 | trainAcc: 97.8693% (689/704)\n",
            "11 13 Epoch: 315 | ANN: trainLoss: 0.0698 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 315 | ANN: trainLoss: 0.0903 | trainAcc: 97.5389% (753/772)\n",
            "0 4 Epoch: 315 | ANN: testLoss: 0.5005 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 315 | ANN: testLoss: 0.8135 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 315 | ANN: testLoss: 0.7478 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 315 | ANN: testLoss: 0.6961 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 316 | ANN: trainLoss: 0.0823 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 316 | ANN: trainLoss: 0.0571 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 316 | ANN: trainLoss: 0.0490 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 316 | ANN: trainLoss: 0.0460 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 316 | ANN: trainLoss: 0.0557 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 316 | ANN: trainLoss: 0.0732 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 316 | ANN: trainLoss: 0.0848 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 316 | ANN: trainLoss: 0.0880 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 316 | ANN: trainLoss: 0.0891 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 316 | ANN: trainLoss: 0.0840 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 316 | ANN: trainLoss: 0.0839 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 316 | ANN: trainLoss: 0.0815 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 316 | ANN: trainLoss: 0.0833 | trainAcc: 97.1503% (750/772)\n",
            "0 4 Epoch: 316 | ANN: testLoss: 0.7193 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 316 | ANN: testLoss: 0.7478 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 316 | ANN: testLoss: 0.6747 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 316 | ANN: testLoss: 0.5088 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 317 | ANN: trainLoss: 0.0911 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 317 | ANN: trainLoss: 0.0587 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 317 | ANN: trainLoss: 0.0745 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 317 | ANN: trainLoss: 0.0829 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 317 | ANN: trainLoss: 0.0769 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 317 | ANN: trainLoss: 0.0710 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 317 | ANN: trainLoss: 0.0773 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 317 | ANN: trainLoss: 0.0749 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 317 | ANN: trainLoss: 0.0751 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 317 | ANN: trainLoss: 0.0717 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 317 | ANN: trainLoss: 0.0746 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 317 | ANN: trainLoss: 0.0724 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 317 | ANN: trainLoss: 0.0672 | trainAcc: 97.4093% (752/772)\n",
            "0 4 Epoch: 317 | ANN: testLoss: 0.5245 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 317 | ANN: testLoss: 0.5899 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 317 | ANN: testLoss: 0.6874 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 317 | ANN: testLoss: 0.8847 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 318 | ANN: trainLoss: 0.0486 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 318 | ANN: trainLoss: 0.0384 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 318 | ANN: trainLoss: 0.0638 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 318 | ANN: trainLoss: 0.0621 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 318 | ANN: trainLoss: 0.0688 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 318 | ANN: trainLoss: 0.0695 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 318 | ANN: trainLoss: 0.0745 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 318 | ANN: trainLoss: 0.0704 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 318 | ANN: trainLoss: 0.0715 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 318 | ANN: trainLoss: 0.0684 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 318 | ANN: trainLoss: 0.0657 | trainAcc: 97.8693% (689/704)\n",
            "11 13 Epoch: 318 | ANN: trainLoss: 0.0650 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 318 | ANN: trainLoss: 0.1047 | trainAcc: 97.5389% (753/772)\n",
            "0 4 Epoch: 318 | ANN: testLoss: 0.9389 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 318 | ANN: testLoss: 0.7620 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 318 | ANN: testLoss: 0.7017 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 318 | ANN: testLoss: 2.1598 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 319 | ANN: trainLoss: 0.0302 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 319 | ANN: trainLoss: 0.0957 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 319 | ANN: trainLoss: 0.0852 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 319 | ANN: trainLoss: 0.0780 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 319 | ANN: trainLoss: 0.0698 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 319 | ANN: trainLoss: 0.0789 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 319 | ANN: trainLoss: 0.0761 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 319 | ANN: trainLoss: 0.0744 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 319 | ANN: trainLoss: 0.0761 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 319 | ANN: trainLoss: 0.0722 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 319 | ANN: trainLoss: 0.0697 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 319 | ANN: trainLoss: 0.0771 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 319 | ANN: trainLoss: 0.0728 | trainAcc: 97.4093% (752/772)\n",
            "0 4 Epoch: 319 | ANN: testLoss: 1.0273 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 319 | ANN: testLoss: 0.7693 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 319 | ANN: testLoss: 0.7309 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 319 | ANN: testLoss: 0.9322 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 320 | ANN: trainLoss: 0.0311 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 320 | ANN: trainLoss: 0.0563 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 320 | ANN: trainLoss: 0.0556 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 320 | ANN: trainLoss: 0.0607 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 320 | ANN: trainLoss: 0.0649 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 320 | ANN: trainLoss: 0.0610 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 320 | ANN: trainLoss: 0.0608 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 320 | ANN: trainLoss: 0.0560 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 320 | ANN: trainLoss: 0.0603 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 320 | ANN: trainLoss: 0.0589 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 320 | ANN: trainLoss: 0.0670 | trainAcc: 98.2955% (692/704)\n",
            "11 13 Epoch: 320 | ANN: trainLoss: 0.0661 | trainAcc: 98.3073% (755/768)\n",
            "12 13 Epoch: 320 | ANN: trainLoss: 0.0888 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 320 | ANN: testLoss: 0.6561 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 320 | ANN: testLoss: 0.6747 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 320 | ANN: testLoss: 0.7018 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 320 | ANN: testLoss: 0.5268 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 321 | ANN: trainLoss: 0.1038 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 321 | ANN: trainLoss: 0.1241 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 321 | ANN: trainLoss: 0.1034 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 321 | ANN: trainLoss: 0.0932 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 321 | ANN: trainLoss: 0.0855 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 321 | ANN: trainLoss: 0.0800 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 321 | ANN: trainLoss: 0.0751 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 321 | ANN: trainLoss: 0.0817 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 321 | ANN: trainLoss: 0.0773 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 321 | ANN: trainLoss: 0.0766 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 321 | ANN: trainLoss: 0.0720 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 321 | ANN: trainLoss: 0.0732 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 321 | ANN: trainLoss: 0.1331 | trainAcc: 96.8912% (748/772)\n",
            "0 4 Epoch: 321 | ANN: testLoss: 0.6378 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 321 | ANN: testLoss: 0.9146 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 321 | ANN: testLoss: 0.7786 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 321 | ANN: testLoss: 0.5840 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 322 | ANN: trainLoss: 0.0678 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 322 | ANN: trainLoss: 0.0649 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 322 | ANN: trainLoss: 0.1026 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 322 | ANN: trainLoss: 0.1287 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 322 | ANN: trainLoss: 0.1113 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 322 | ANN: trainLoss: 0.1304 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 322 | ANN: trainLoss: 0.1335 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 322 | ANN: trainLoss: 0.1230 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 322 | ANN: trainLoss: 0.1230 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 322 | ANN: trainLoss: 0.1239 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 322 | ANN: trainLoss: 0.1196 | trainAcc: 95.3125% (671/704)\n",
            "11 13 Epoch: 322 | ANN: trainLoss: 0.1147 | trainAcc: 95.5729% (734/768)\n",
            "12 13 Epoch: 322 | ANN: trainLoss: 0.1374 | trainAcc: 95.4663% (737/772)\n",
            "0 4 Epoch: 322 | ANN: testLoss: 0.4292 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 322 | ANN: testLoss: 0.6603 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 322 | ANN: testLoss: 0.7395 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 322 | ANN: testLoss: 2.4231 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 323 | ANN: trainLoss: 0.1142 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 323 | ANN: trainLoss: 0.0896 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 323 | ANN: trainLoss: 0.0702 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 323 | ANN: trainLoss: 0.0833 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 323 | ANN: trainLoss: 0.1075 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 323 | ANN: trainLoss: 0.1082 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 323 | ANN: trainLoss: 0.1070 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 323 | ANN: trainLoss: 0.1008 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 323 | ANN: trainLoss: 0.0947 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 323 | ANN: trainLoss: 0.0955 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 323 | ANN: trainLoss: 0.0991 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 323 | ANN: trainLoss: 0.1030 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 323 | ANN: trainLoss: 0.3852 | trainAcc: 95.7254% (739/772)\n",
            "0 4 Epoch: 323 | ANN: testLoss: 1.0830 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 323 | ANN: testLoss: 0.8948 | testAcc: 67.9688% (87/128)\n",
            "2 4 Epoch: 323 | ANN: testLoss: 0.8292 | testAcc: 70.8333% (136/192)\n",
            "3 4 Epoch: 323 | ANN: testLoss: 1.1628 | testAcc: 70.4663% (136/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 324 | ANN: trainLoss: 0.1274 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 324 | ANN: trainLoss: 0.1237 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 324 | ANN: trainLoss: 0.1009 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 324 | ANN: trainLoss: 0.0843 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 324 | ANN: trainLoss: 0.0762 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 324 | ANN: trainLoss: 0.0752 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 324 | ANN: trainLoss: 0.0681 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 324 | ANN: trainLoss: 0.0703 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 324 | ANN: trainLoss: 0.0722 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 324 | ANN: trainLoss: 0.0689 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 324 | ANN: trainLoss: 0.0722 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 324 | ANN: trainLoss: 0.0701 | trainAcc: 97.5260% (749/768)\n",
            "12 13 Epoch: 324 | ANN: trainLoss: 0.0725 | trainAcc: 97.5389% (753/772)\n",
            "0 4 Epoch: 324 | ANN: testLoss: 0.7636 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 324 | ANN: testLoss: 0.7825 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 324 | ANN: testLoss: 0.7747 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 324 | ANN: testLoss: 0.5862 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 325 | ANN: trainLoss: 0.0466 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 325 | ANN: trainLoss: 0.0512 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 325 | ANN: trainLoss: 0.0602 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 325 | ANN: trainLoss: 0.0746 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 325 | ANN: trainLoss: 0.0693 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 325 | ANN: trainLoss: 0.0734 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 325 | ANN: trainLoss: 0.0790 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 325 | ANN: trainLoss: 0.0776 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 325 | ANN: trainLoss: 0.0782 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 325 | ANN: trainLoss: 0.0824 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 325 | ANN: trainLoss: 0.0838 | trainAcc: 96.8750% (682/704)\n",
            "11 13 Epoch: 325 | ANN: trainLoss: 0.0873 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 325 | ANN: trainLoss: 0.0825 | trainAcc: 96.7617% (747/772)\n",
            "0 4 Epoch: 325 | ANN: testLoss: 0.4950 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 325 | ANN: testLoss: 0.8444 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 325 | ANN: testLoss: 0.7716 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 325 | ANN: testLoss: 0.6732 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 326 | ANN: trainLoss: 0.1432 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 326 | ANN: trainLoss: 0.1577 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 326 | ANN: trainLoss: 0.1332 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 326 | ANN: trainLoss: 0.1205 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 326 | ANN: trainLoss: 0.1055 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 326 | ANN: trainLoss: 0.0984 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 326 | ANN: trainLoss: 0.0909 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 326 | ANN: trainLoss: 0.0874 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 326 | ANN: trainLoss: 0.1026 | trainAcc: 95.6597% (551/576)\n",
            "9 13 Epoch: 326 | ANN: trainLoss: 0.1002 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 326 | ANN: trainLoss: 0.0951 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 326 | ANN: trainLoss: 0.0906 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 326 | ANN: trainLoss: 0.0866 | trainAcc: 96.5026% (745/772)\n",
            "0 4 Epoch: 326 | ANN: testLoss: 1.0156 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 326 | ANN: testLoss: 0.8262 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 326 | ANN: testLoss: 0.7928 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 326 | ANN: testLoss: 0.5946 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 327 | ANN: trainLoss: 0.0551 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 327 | ANN: trainLoss: 0.0728 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 327 | ANN: trainLoss: 0.0688 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 327 | ANN: trainLoss: 0.0612 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 327 | ANN: trainLoss: 0.0563 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 327 | ANN: trainLoss: 0.0523 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 327 | ANN: trainLoss: 0.0588 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 327 | ANN: trainLoss: 0.0548 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 327 | ANN: trainLoss: 0.0527 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 327 | ANN: trainLoss: 0.0586 | trainAcc: 98.1250% (628/640)\n",
            "10 13 Epoch: 327 | ANN: trainLoss: 0.0578 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 327 | ANN: trainLoss: 0.0571 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 327 | ANN: trainLoss: 0.0800 | trainAcc: 97.7979% (755/772)\n",
            "0 4 Epoch: 327 | ANN: testLoss: 0.7593 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 327 | ANN: testLoss: 0.7642 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 327 | ANN: testLoss: 0.7957 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 327 | ANN: testLoss: 0.5968 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 328 | ANN: trainLoss: 0.0799 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 328 | ANN: trainLoss: 0.0848 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 328 | ANN: trainLoss: 0.0720 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 328 | ANN: trainLoss: 0.0714 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 328 | ANN: trainLoss: 0.0610 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 328 | ANN: trainLoss: 0.0659 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 328 | ANN: trainLoss: 0.0698 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 328 | ANN: trainLoss: 0.0649 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 328 | ANN: trainLoss: 0.0760 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 328 | ANN: trainLoss: 0.0726 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 328 | ANN: trainLoss: 0.0688 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 328 | ANN: trainLoss: 0.0763 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 328 | ANN: trainLoss: 0.0852 | trainAcc: 97.2798% (751/772)\n",
            "0 4 Epoch: 328 | ANN: testLoss: 0.9798 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 328 | ANN: testLoss: 0.7362 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 328 | ANN: testLoss: 0.7763 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 328 | ANN: testLoss: 0.5863 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 329 | ANN: trainLoss: 0.0261 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 329 | ANN: trainLoss: 0.0522 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 329 | ANN: trainLoss: 0.0564 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 329 | ANN: trainLoss: 0.0518 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 329 | ANN: trainLoss: 0.0497 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 329 | ANN: trainLoss: 0.0477 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 329 | ANN: trainLoss: 0.0457 | trainAcc: 99.1071% (444/448)\n",
            "7 13 Epoch: 329 | ANN: trainLoss: 0.0451 | trainAcc: 99.2188% (508/512)\n",
            "8 13 Epoch: 329 | ANN: trainLoss: 0.0483 | trainAcc: 99.1319% (571/576)\n",
            "9 13 Epoch: 329 | ANN: trainLoss: 0.0486 | trainAcc: 99.0625% (634/640)\n",
            "10 13 Epoch: 329 | ANN: trainLoss: 0.0469 | trainAcc: 99.1477% (698/704)\n",
            "11 13 Epoch: 329 | ANN: trainLoss: 0.0452 | trainAcc: 99.2188% (762/768)\n",
            "12 13 Epoch: 329 | ANN: trainLoss: 0.0431 | trainAcc: 99.2228% (766/772)\n",
            "0 4 Epoch: 329 | ANN: testLoss: 0.7510 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 329 | ANN: testLoss: 0.8346 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 329 | ANN: testLoss: 0.7440 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 329 | ANN: testLoss: 1.3145 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 330 | ANN: trainLoss: 0.0797 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 330 | ANN: trainLoss: 0.0865 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 330 | ANN: trainLoss: 0.0738 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 330 | ANN: trainLoss: 0.0685 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 330 | ANN: trainLoss: 0.0684 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 330 | ANN: trainLoss: 0.0697 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 330 | ANN: trainLoss: 0.0720 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 330 | ANN: trainLoss: 0.0688 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 330 | ANN: trainLoss: 0.0666 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 330 | ANN: trainLoss: 0.0688 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 330 | ANN: trainLoss: 0.0728 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 330 | ANN: trainLoss: 0.0689 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 330 | ANN: trainLoss: 0.0769 | trainAcc: 97.4093% (752/772)\n",
            "0 4 Epoch: 330 | ANN: testLoss: 1.1818 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 330 | ANN: testLoss: 0.8713 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 330 | ANN: testLoss: 0.7369 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 330 | ANN: testLoss: 0.5532 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 331 | ANN: trainLoss: 0.0708 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 331 | ANN: trainLoss: 0.0626 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 331 | ANN: trainLoss: 0.0563 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 331 | ANN: trainLoss: 0.0615 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 331 | ANN: trainLoss: 0.0557 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 331 | ANN: trainLoss: 0.0625 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 331 | ANN: trainLoss: 0.0627 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 331 | ANN: trainLoss: 0.0685 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 331 | ANN: trainLoss: 0.0664 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 331 | ANN: trainLoss: 0.0658 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 331 | ANN: trainLoss: 0.0668 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 331 | ANN: trainLoss: 0.0713 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 331 | ANN: trainLoss: 0.0725 | trainAcc: 97.0207% (749/772)\n",
            "0 4 Epoch: 331 | ANN: testLoss: 0.6766 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 331 | ANN: testLoss: 0.7278 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 331 | ANN: testLoss: 0.7236 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 331 | ANN: testLoss: 0.5460 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 332 | ANN: trainLoss: 0.0465 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 332 | ANN: trainLoss: 0.0404 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 332 | ANN: trainLoss: 0.0489 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 332 | ANN: trainLoss: 0.0579 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 332 | ANN: trainLoss: 0.0642 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 332 | ANN: trainLoss: 0.0554 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 332 | ANN: trainLoss: 0.0563 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 332 | ANN: trainLoss: 0.0606 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 332 | ANN: trainLoss: 0.0573 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 332 | ANN: trainLoss: 0.0616 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 332 | ANN: trainLoss: 0.0586 | trainAcc: 98.2955% (692/704)\n",
            "11 13 Epoch: 332 | ANN: trainLoss: 0.0551 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 332 | ANN: trainLoss: 0.1199 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 332 | ANN: testLoss: 0.7033 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 332 | ANN: testLoss: 0.7752 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 332 | ANN: testLoss: 0.7082 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 332 | ANN: testLoss: 2.5679 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 333 | ANN: trainLoss: 0.1028 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 333 | ANN: trainLoss: 0.0793 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 333 | ANN: trainLoss: 0.0742 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 333 | ANN: trainLoss: 0.0801 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 333 | ANN: trainLoss: 0.0833 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 333 | ANN: trainLoss: 0.1331 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 333 | ANN: trainLoss: 0.1178 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 333 | ANN: trainLoss: 0.1164 | trainAcc: 96.2891% (493/512)\n",
            "8 13 Epoch: 333 | ANN: trainLoss: 0.1106 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 333 | ANN: trainLoss: 0.1043 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 333 | ANN: trainLoss: 0.1008 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 333 | ANN: trainLoss: 0.0959 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 333 | ANN: trainLoss: 0.0892 | trainAcc: 97.0207% (749/772)\n",
            "0 4 Epoch: 333 | ANN: testLoss: 0.8728 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 333 | ANN: testLoss: 0.7322 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 333 | ANN: testLoss: 0.8002 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 333 | ANN: testLoss: 0.6009 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 334 | ANN: trainLoss: 0.0775 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 334 | ANN: trainLoss: 0.0567 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 334 | ANN: trainLoss: 0.0490 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 334 | ANN: trainLoss: 0.0395 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 334 | ANN: trainLoss: 0.0576 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 334 | ANN: trainLoss: 0.0680 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 334 | ANN: trainLoss: 0.0658 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 334 | ANN: trainLoss: 0.0637 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 334 | ANN: trainLoss: 0.0615 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 334 | ANN: trainLoss: 0.0641 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 334 | ANN: trainLoss: 0.0618 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 334 | ANN: trainLoss: 0.0611 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 334 | ANN: trainLoss: 0.1491 | trainAcc: 97.6684% (754/772)\n",
            "0 4 Epoch: 334 | ANN: testLoss: 0.9203 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 334 | ANN: testLoss: 0.8596 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 334 | ANN: testLoss: 0.8188 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 334 | ANN: testLoss: 0.6147 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 335 | ANN: trainLoss: 0.0339 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 335 | ANN: trainLoss: 0.0962 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 335 | ANN: trainLoss: 0.0837 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 335 | ANN: trainLoss: 0.0668 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 335 | ANN: trainLoss: 0.0596 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 335 | ANN: trainLoss: 0.0585 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 335 | ANN: trainLoss: 0.0726 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 335 | ANN: trainLoss: 0.0727 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 335 | ANN: trainLoss: 0.0745 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 335 | ANN: trainLoss: 0.0896 | trainAcc: 97.0312% (621/640)\n",
            "10 13 Epoch: 335 | ANN: trainLoss: 0.0921 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 335 | ANN: trainLoss: 0.0918 | trainAcc: 96.6146% (742/768)\n",
            "12 13 Epoch: 335 | ANN: trainLoss: 0.1730 | trainAcc: 96.5026% (745/772)\n",
            "0 4 Epoch: 335 | ANN: testLoss: 1.0661 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 335 | ANN: testLoss: 0.9904 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 335 | ANN: testLoss: 0.8483 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 335 | ANN: testLoss: 0.6379 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 336 | ANN: trainLoss: 0.0108 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 336 | ANN: trainLoss: 0.0305 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 336 | ANN: trainLoss: 0.0929 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 336 | ANN: trainLoss: 0.0911 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 336 | ANN: trainLoss: 0.0912 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 336 | ANN: trainLoss: 0.0905 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 336 | ANN: trainLoss: 0.0859 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 336 | ANN: trainLoss: 0.0886 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 336 | ANN: trainLoss: 0.0910 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 336 | ANN: trainLoss: 0.0838 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 336 | ANN: trainLoss: 0.0860 | trainAcc: 96.8750% (682/704)\n",
            "11 13 Epoch: 336 | ANN: trainLoss: 0.0869 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 336 | ANN: trainLoss: 0.2138 | trainAcc: 96.7617% (747/772)\n",
            "0 4 Epoch: 336 | ANN: testLoss: 0.9694 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 336 | ANN: testLoss: 1.0053 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 336 | ANN: testLoss: 0.9632 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 336 | ANN: testLoss: 0.8993 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 337 | ANN: trainLoss: 0.1061 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 337 | ANN: trainLoss: 0.1019 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 337 | ANN: trainLoss: 0.0948 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 337 | ANN: trainLoss: 0.1157 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 337 | ANN: trainLoss: 0.1178 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 337 | ANN: trainLoss: 0.1236 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 337 | ANN: trainLoss: 0.1388 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 337 | ANN: trainLoss: 0.1295 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 337 | ANN: trainLoss: 0.1231 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 337 | ANN: trainLoss: 0.1233 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 337 | ANN: trainLoss: 0.1195 | trainAcc: 95.1705% (670/704)\n",
            "11 13 Epoch: 337 | ANN: trainLoss: 0.1134 | trainAcc: 95.4427% (733/768)\n",
            "12 13 Epoch: 337 | ANN: trainLoss: 0.1826 | trainAcc: 95.2073% (735/772)\n",
            "0 4 Epoch: 337 | ANN: testLoss: 0.8519 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 337 | ANN: testLoss: 0.8391 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 337 | ANN: testLoss: 0.7741 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 337 | ANN: testLoss: 0.9926 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 338 | ANN: trainLoss: 0.0489 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 338 | ANN: trainLoss: 0.0693 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 338 | ANN: trainLoss: 0.0888 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 338 | ANN: trainLoss: 0.0821 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 338 | ANN: trainLoss: 0.0968 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 338 | ANN: trainLoss: 0.0941 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 338 | ANN: trainLoss: 0.0884 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 338 | ANN: trainLoss: 0.0837 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 338 | ANN: trainLoss: 0.0878 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 338 | ANN: trainLoss: 0.0911 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 338 | ANN: trainLoss: 0.1043 | trainAcc: 96.4489% (679/704)\n",
            "11 13 Epoch: 338 | ANN: trainLoss: 0.1074 | trainAcc: 96.3542% (740/768)\n",
            "12 13 Epoch: 338 | ANN: trainLoss: 0.1394 | trainAcc: 96.2435% (743/772)\n",
            "0 4 Epoch: 338 | ANN: testLoss: 0.5568 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 338 | ANN: testLoss: 0.7623 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 338 | ANN: testLoss: 0.7655 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 338 | ANN: testLoss: 0.5743 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 339 | ANN: trainLoss: 0.1095 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 339 | ANN: trainLoss: 0.0799 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 339 | ANN: trainLoss: 0.1010 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 339 | ANN: trainLoss: 0.0874 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 339 | ANN: trainLoss: 0.0827 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 339 | ANN: trainLoss: 0.0730 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 339 | ANN: trainLoss: 0.0718 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 339 | ANN: trainLoss: 0.0874 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 339 | ANN: trainLoss: 0.0867 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 339 | ANN: trainLoss: 0.0831 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 339 | ANN: trainLoss: 0.0857 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 339 | ANN: trainLoss: 0.0837 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 339 | ANN: trainLoss: 0.2424 | trainAcc: 96.8912% (748/772)\n",
            "0 4 Epoch: 339 | ANN: testLoss: 1.1375 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 339 | ANN: testLoss: 0.8508 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 339 | ANN: testLoss: 0.8893 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 339 | ANN: testLoss: 0.6670 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 340 | ANN: trainLoss: 0.0397 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 340 | ANN: trainLoss: 0.0367 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 340 | ANN: trainLoss: 0.0340 | trainAcc: 100.0000% (192/192)\n",
            "3 13 Epoch: 340 | ANN: trainLoss: 0.0394 | trainAcc: 99.6094% (255/256)\n",
            "4 13 Epoch: 340 | ANN: trainLoss: 0.0455 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 340 | ANN: trainLoss: 0.0436 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 340 | ANN: trainLoss: 0.0496 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 340 | ANN: trainLoss: 0.0485 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 340 | ANN: trainLoss: 0.0515 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 340 | ANN: trainLoss: 0.0489 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 340 | ANN: trainLoss: 0.0554 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 340 | ANN: trainLoss: 0.0563 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 340 | ANN: trainLoss: 0.2762 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 340 | ANN: testLoss: 0.8226 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 340 | ANN: testLoss: 0.6885 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 340 | ANN: testLoss: 0.8128 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 340 | ANN: testLoss: 0.6096 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 341 | ANN: trainLoss: 0.1121 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 341 | ANN: trainLoss: 0.0750 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 341 | ANN: trainLoss: 0.0767 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 341 | ANN: trainLoss: 0.0693 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 341 | ANN: trainLoss: 0.0637 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 341 | ANN: trainLoss: 0.0577 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 341 | ANN: trainLoss: 0.0673 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 341 | ANN: trainLoss: 0.0704 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 341 | ANN: trainLoss: 0.0889 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 341 | ANN: trainLoss: 0.0871 | trainAcc: 96.5625% (618/640)\n",
            "10 13 Epoch: 341 | ANN: trainLoss: 0.0940 | trainAcc: 96.3068% (678/704)\n",
            "11 13 Epoch: 341 | ANN: trainLoss: 0.0905 | trainAcc: 96.4844% (741/768)\n",
            "12 13 Epoch: 341 | ANN: trainLoss: 0.1180 | trainAcc: 96.3731% (744/772)\n",
            "0 4 Epoch: 341 | ANN: testLoss: 1.0352 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 341 | ANN: testLoss: 0.9655 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 341 | ANN: testLoss: 0.9588 | testAcc: 71.3542% (137/192)\n",
            "3 4 Epoch: 341 | ANN: testLoss: 0.7226 | testAcc: 71.5026% (138/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 342 | ANN: trainLoss: 0.2023 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 342 | ANN: trainLoss: 0.1150 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 342 | ANN: trainLoss: 0.1294 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 342 | ANN: trainLoss: 0.1086 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 342 | ANN: trainLoss: 0.0958 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 342 | ANN: trainLoss: 0.0921 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 342 | ANN: trainLoss: 0.0953 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 342 | ANN: trainLoss: 0.0961 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 342 | ANN: trainLoss: 0.0911 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 342 | ANN: trainLoss: 0.0963 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 342 | ANN: trainLoss: 0.0993 | trainAcc: 96.7330% (681/704)\n",
            "11 13 Epoch: 342 | ANN: trainLoss: 0.0979 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 342 | ANN: trainLoss: 0.1924 | trainAcc: 96.6321% (746/772)\n",
            "0 4 Epoch: 342 | ANN: testLoss: 0.9593 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 342 | ANN: testLoss: 0.8827 | testAcc: 69.5312% (89/128)\n",
            "2 4 Epoch: 342 | ANN: testLoss: 0.8494 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 342 | ANN: testLoss: 0.6416 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 343 | ANN: trainLoss: 0.0847 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 343 | ANN: trainLoss: 0.0821 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 343 | ANN: trainLoss: 0.0925 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 343 | ANN: trainLoss: 0.0860 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 343 | ANN: trainLoss: 0.0830 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 343 | ANN: trainLoss: 0.0831 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 343 | ANN: trainLoss: 0.0857 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 343 | ANN: trainLoss: 0.0812 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 343 | ANN: trainLoss: 0.0810 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 343 | ANN: trainLoss: 0.0792 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 343 | ANN: trainLoss: 0.0776 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 343 | ANN: trainLoss: 0.0795 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 343 | ANN: trainLoss: 0.1117 | trainAcc: 96.8912% (748/772)\n",
            "0 4 Epoch: 343 | ANN: testLoss: 1.0418 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 343 | ANN: testLoss: 0.8852 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 343 | ANN: testLoss: 0.8192 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 343 | ANN: testLoss: 0.6144 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 344 | ANN: trainLoss: 0.1671 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 344 | ANN: trainLoss: 0.1544 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 344 | ANN: trainLoss: 0.1349 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 344 | ANN: trainLoss: 0.1242 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 344 | ANN: trainLoss: 0.1238 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 344 | ANN: trainLoss: 0.1166 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 344 | ANN: trainLoss: 0.1183 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 344 | ANN: trainLoss: 0.1112 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 344 | ANN: trainLoss: 0.1027 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 344 | ANN: trainLoss: 0.0996 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 344 | ANN: trainLoss: 0.0923 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 344 | ANN: trainLoss: 0.0899 | trainAcc: 96.6146% (742/768)\n",
            "12 13 Epoch: 344 | ANN: trainLoss: 0.0841 | trainAcc: 96.6321% (746/772)\n",
            "0 4 Epoch: 344 | ANN: testLoss: 0.7459 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 344 | ANN: testLoss: 0.7937 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 344 | ANN: testLoss: 0.8735 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 344 | ANN: testLoss: 0.6553 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 345 | ANN: trainLoss: 0.1739 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 345 | ANN: trainLoss: 0.1133 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 345 | ANN: trainLoss: 0.0976 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 345 | ANN: trainLoss: 0.0824 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 345 | ANN: trainLoss: 0.0804 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 345 | ANN: trainLoss: 0.0800 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 345 | ANN: trainLoss: 0.0818 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 345 | ANN: trainLoss: 0.0786 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 345 | ANN: trainLoss: 0.0807 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 345 | ANN: trainLoss: 0.0762 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 345 | ANN: trainLoss: 0.0802 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 345 | ANN: trainLoss: 0.0776 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 345 | ANN: trainLoss: 0.0750 | trainAcc: 96.7617% (747/772)\n",
            "0 4 Epoch: 345 | ANN: testLoss: 1.0821 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 345 | ANN: testLoss: 0.8841 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 345 | ANN: testLoss: 0.8309 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 345 | ANN: testLoss: 0.6233 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 346 | ANN: trainLoss: 0.1518 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 346 | ANN: trainLoss: 0.1314 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 346 | ANN: trainLoss: 0.1004 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 346 | ANN: trainLoss: 0.0885 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 346 | ANN: trainLoss: 0.0758 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 346 | ANN: trainLoss: 0.0741 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 346 | ANN: trainLoss: 0.0764 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 346 | ANN: trainLoss: 0.0766 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 346 | ANN: trainLoss: 0.0776 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 346 | ANN: trainLoss: 0.0769 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 346 | ANN: trainLoss: 0.0748 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 346 | ANN: trainLoss: 0.0864 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 346 | ANN: trainLoss: 0.3418 | trainAcc: 96.6321% (746/772)\n",
            "0 4 Epoch: 346 | ANN: testLoss: 0.8331 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 346 | ANN: testLoss: 0.8705 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 346 | ANN: testLoss: 0.8082 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 346 | ANN: testLoss: 0.6086 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 347 | ANN: trainLoss: 0.1047 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 347 | ANN: trainLoss: 0.0655 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 347 | ANN: trainLoss: 0.0831 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 347 | ANN: trainLoss: 0.0746 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 347 | ANN: trainLoss: 0.0703 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 347 | ANN: trainLoss: 0.0645 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 347 | ANN: trainLoss: 0.0733 | trainAcc: 96.8750% (434/448)\n",
            "7 13 Epoch: 347 | ANN: trainLoss: 0.0683 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 347 | ANN: trainLoss: 0.0704 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 347 | ANN: trainLoss: 0.0677 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 347 | ANN: trainLoss: 0.0644 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 347 | ANN: trainLoss: 0.0698 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 347 | ANN: trainLoss: 0.0651 | trainAcc: 97.4093% (752/772)\n",
            "0 4 Epoch: 347 | ANN: testLoss: 0.6862 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 347 | ANN: testLoss: 0.8117 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 347 | ANN: testLoss: 0.7523 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 347 | ANN: testLoss: 0.7504 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 348 | ANN: trainLoss: 0.0616 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 348 | ANN: trainLoss: 0.0623 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 348 | ANN: trainLoss: 0.0765 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 348 | ANN: trainLoss: 0.0854 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 348 | ANN: trainLoss: 0.0772 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 348 | ANN: trainLoss: 0.0716 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 348 | ANN: trainLoss: 0.0669 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 348 | ANN: trainLoss: 0.0731 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 348 | ANN: trainLoss: 0.0770 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 348 | ANN: trainLoss: 0.0781 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 348 | ANN: trainLoss: 0.0811 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 348 | ANN: trainLoss: 0.0766 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 348 | ANN: trainLoss: 0.1083 | trainAcc: 97.1503% (750/772)\n",
            "0 4 Epoch: 348 | ANN: testLoss: 0.6015 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 348 | ANN: testLoss: 0.7687 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 348 | ANN: testLoss: 0.7716 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 348 | ANN: testLoss: 1.7530 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 349 | ANN: trainLoss: 0.0813 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 349 | ANN: trainLoss: 0.1068 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 349 | ANN: trainLoss: 0.0856 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 349 | ANN: trainLoss: 0.0754 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 349 | ANN: trainLoss: 0.0688 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 349 | ANN: trainLoss: 0.0637 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 349 | ANN: trainLoss: 0.0716 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 349 | ANN: trainLoss: 0.0717 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 349 | ANN: trainLoss: 0.0718 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 349 | ANN: trainLoss: 0.0695 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 349 | ANN: trainLoss: 0.0851 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 349 | ANN: trainLoss: 0.0873 | trainAcc: 96.7448% (743/768)\n",
            "12 13 Epoch: 349 | ANN: trainLoss: 0.1009 | trainAcc: 96.6321% (746/772)\n",
            "0 4 Epoch: 349 | ANN: testLoss: 1.0602 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 349 | ANN: testLoss: 0.7038 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 349 | ANN: testLoss: 0.7676 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 349 | ANN: testLoss: 0.5766 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 350 | ANN: trainLoss: 0.1218 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 350 | ANN: trainLoss: 0.0904 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 350 | ANN: trainLoss: 0.0900 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 350 | ANN: trainLoss: 0.0943 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 350 | ANN: trainLoss: 0.0936 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 350 | ANN: trainLoss: 0.0963 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 350 | ANN: trainLoss: 0.0927 | trainAcc: 95.7589% (429/448)\n",
            "7 13 Epoch: 350 | ANN: trainLoss: 0.0938 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 350 | ANN: trainLoss: 0.0883 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 350 | ANN: trainLoss: 0.0892 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 350 | ANN: trainLoss: 0.0832 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 350 | ANN: trainLoss: 0.0788 | trainAcc: 96.8750% (744/768)\n",
            "12 13 Epoch: 350 | ANN: trainLoss: 0.3033 | trainAcc: 96.5026% (745/772)\n",
            "0 4 Epoch: 350 | ANN: testLoss: 0.7678 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 350 | ANN: testLoss: 0.7520 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 350 | ANN: testLoss: 0.7539 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 350 | ANN: testLoss: 0.6653 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 351 | ANN: trainLoss: 0.0502 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 351 | ANN: trainLoss: 0.0431 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 351 | ANN: trainLoss: 0.0670 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 351 | ANN: trainLoss: 0.0676 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 351 | ANN: trainLoss: 0.0900 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 351 | ANN: trainLoss: 0.0842 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 351 | ANN: trainLoss: 0.0832 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 351 | ANN: trainLoss: 0.0946 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 351 | ANN: trainLoss: 0.0960 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 351 | ANN: trainLoss: 0.0944 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 351 | ANN: trainLoss: 0.0943 | trainAcc: 96.5909% (680/704)\n",
            "11 13 Epoch: 351 | ANN: trainLoss: 0.1032 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 351 | ANN: trainLoss: 0.1019 | trainAcc: 96.2435% (743/772)\n",
            "0 4 Epoch: 351 | ANN: testLoss: 1.0242 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 351 | ANN: testLoss: 0.9586 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 351 | ANN: testLoss: 0.8563 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 351 | ANN: testLoss: 0.6647 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 352 | ANN: trainLoss: 0.0391 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 352 | ANN: trainLoss: 0.0435 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 352 | ANN: trainLoss: 0.0437 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 352 | ANN: trainLoss: 0.0416 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 352 | ANN: trainLoss: 0.0435 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 352 | ANN: trainLoss: 0.0478 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 352 | ANN: trainLoss: 0.0473 | trainAcc: 99.1071% (444/448)\n",
            "7 13 Epoch: 352 | ANN: trainLoss: 0.0457 | trainAcc: 99.2188% (508/512)\n",
            "8 13 Epoch: 352 | ANN: trainLoss: 0.0577 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 352 | ANN: trainLoss: 0.0561 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 352 | ANN: trainLoss: 0.0571 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 352 | ANN: trainLoss: 0.0584 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 352 | ANN: trainLoss: 0.0739 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 352 | ANN: testLoss: 0.5574 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 352 | ANN: testLoss: 0.6884 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 352 | ANN: testLoss: 0.8105 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 352 | ANN: testLoss: 0.6081 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 353 | ANN: trainLoss: 0.0664 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 353 | ANN: trainLoss: 0.0651 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 353 | ANN: trainLoss: 0.0798 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 353 | ANN: trainLoss: 0.0755 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 353 | ANN: trainLoss: 0.0686 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 353 | ANN: trainLoss: 0.0644 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 353 | ANN: trainLoss: 0.0616 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 353 | ANN: trainLoss: 0.0597 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 353 | ANN: trainLoss: 0.0561 | trainAcc: 98.9583% (570/576)\n",
            "9 13 Epoch: 353 | ANN: trainLoss: 0.0560 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 353 | ANN: trainLoss: 0.0592 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 353 | ANN: trainLoss: 0.0611 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 353 | ANN: trainLoss: 0.2826 | trainAcc: 97.9275% (756/772)\n",
            "0 4 Epoch: 353 | ANN: testLoss: 0.9404 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 353 | ANN: testLoss: 0.8959 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 353 | ANN: testLoss: 0.7993 | testAcc: 72.3958% (139/192)\n",
            "3 4 Epoch: 353 | ANN: testLoss: 0.5999 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 354 | ANN: trainLoss: 0.0605 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 354 | ANN: trainLoss: 0.0466 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 354 | ANN: trainLoss: 0.0514 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 354 | ANN: trainLoss: 0.0661 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 354 | ANN: trainLoss: 0.0750 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 354 | ANN: trainLoss: 0.0749 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 354 | ANN: trainLoss: 0.0716 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 354 | ANN: trainLoss: 0.0705 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 354 | ANN: trainLoss: 0.0701 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 354 | ANN: trainLoss: 0.0708 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 354 | ANN: trainLoss: 0.0700 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 354 | ANN: trainLoss: 0.0698 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 354 | ANN: trainLoss: 0.0655 | trainAcc: 97.4093% (752/772)\n",
            "0 4 Epoch: 354 | ANN: testLoss: 0.3676 | testAcc: 85.9375% (55/64)\n",
            "1 4 Epoch: 354 | ANN: testLoss: 0.7206 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 354 | ANN: testLoss: 0.7597 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 354 | ANN: testLoss: 0.5698 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 355 | ANN: trainLoss: 0.1150 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 355 | ANN: trainLoss: 0.0694 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 355 | ANN: trainLoss: 0.0600 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 355 | ANN: trainLoss: 0.1065 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 355 | ANN: trainLoss: 0.1015 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 355 | ANN: trainLoss: 0.1059 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 355 | ANN: trainLoss: 0.0965 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 355 | ANN: trainLoss: 0.0890 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 355 | ANN: trainLoss: 0.0933 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 355 | ANN: trainLoss: 0.0887 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 355 | ANN: trainLoss: 0.0848 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 355 | ANN: trainLoss: 0.0833 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 355 | ANN: trainLoss: 0.0820 | trainAcc: 97.2798% (751/772)\n",
            "0 4 Epoch: 355 | ANN: testLoss: 0.7004 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 355 | ANN: testLoss: 0.6531 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 355 | ANN: testLoss: 0.7381 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 355 | ANN: testLoss: 0.5604 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 356 | ANN: trainLoss: 0.0470 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 356 | ANN: trainLoss: 0.0388 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 356 | ANN: trainLoss: 0.0479 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 356 | ANN: trainLoss: 0.0464 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 356 | ANN: trainLoss: 0.0471 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 356 | ANN: trainLoss: 0.0534 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 356 | ANN: trainLoss: 0.0523 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 356 | ANN: trainLoss: 0.0543 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 356 | ANN: trainLoss: 0.0522 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 356 | ANN: trainLoss: 0.0491 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 356 | ANN: trainLoss: 0.0480 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 356 | ANN: trainLoss: 0.0489 | trainAcc: 98.3073% (755/768)\n",
            "12 13 Epoch: 356 | ANN: trainLoss: 0.0777 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 356 | ANN: testLoss: 0.8669 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 356 | ANN: testLoss: 0.7878 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 356 | ANN: testLoss: 0.6935 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 356 | ANN: testLoss: 1.3220 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 357 | ANN: trainLoss: 0.0683 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 357 | ANN: trainLoss: 0.0776 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 357 | ANN: trainLoss: 0.0774 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 357 | ANN: trainLoss: 0.0711 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 357 | ANN: trainLoss: 0.0634 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 357 | ANN: trainLoss: 0.0585 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 357 | ANN: trainLoss: 0.0550 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 357 | ANN: trainLoss: 0.0515 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 357 | ANN: trainLoss: 0.0568 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 357 | ANN: trainLoss: 0.0600 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 357 | ANN: trainLoss: 0.0590 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 357 | ANN: trainLoss: 0.0580 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 357 | ANN: trainLoss: 0.0628 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 357 | ANN: testLoss: 0.7600 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 357 | ANN: testLoss: 0.7250 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 357 | ANN: testLoss: 0.7612 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 357 | ANN: testLoss: 0.5733 | testAcc: 73.0570% (141/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 358 | ANN: trainLoss: 0.0584 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 358 | ANN: trainLoss: 0.0573 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 358 | ANN: trainLoss: 0.0510 | trainAcc: 100.0000% (192/192)\n",
            "3 13 Epoch: 358 | ANN: trainLoss: 0.0547 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 358 | ANN: trainLoss: 0.0528 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 358 | ANN: trainLoss: 0.0655 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 358 | ANN: trainLoss: 0.0826 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 358 | ANN: trainLoss: 0.0778 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 358 | ANN: trainLoss: 0.0816 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 358 | ANN: trainLoss: 0.0835 | trainAcc: 96.8750% (620/640)\n",
            "10 13 Epoch: 358 | ANN: trainLoss: 0.0824 | trainAcc: 96.8750% (682/704)\n",
            "11 13 Epoch: 358 | ANN: trainLoss: 0.0778 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 358 | ANN: trainLoss: 0.0862 | trainAcc: 97.0207% (749/772)\n",
            "0 4 Epoch: 358 | ANN: testLoss: 0.9991 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 358 | ANN: testLoss: 0.8323 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 358 | ANN: testLoss: 0.7739 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 358 | ANN: testLoss: 0.5898 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 359 | ANN: trainLoss: 0.0612 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 359 | ANN: trainLoss: 0.0485 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 359 | ANN: trainLoss: 0.0425 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 359 | ANN: trainLoss: 0.0550 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 359 | ANN: trainLoss: 0.0498 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 359 | ANN: trainLoss: 0.0449 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 359 | ANN: trainLoss: 0.0475 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 359 | ANN: trainLoss: 0.0472 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 359 | ANN: trainLoss: 0.0512 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 359 | ANN: trainLoss: 0.0564 | trainAcc: 98.1250% (628/640)\n",
            "10 13 Epoch: 359 | ANN: trainLoss: 0.0538 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 359 | ANN: trainLoss: 0.0532 | trainAcc: 98.3073% (755/768)\n",
            "12 13 Epoch: 359 | ANN: trainLoss: 0.0657 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 359 | ANN: testLoss: 0.6846 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 359 | ANN: testLoss: 0.7042 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 359 | ANN: testLoss: 0.7468 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 359 | ANN: testLoss: 1.1717 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 360 | ANN: trainLoss: 0.0437 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 360 | ANN: trainLoss: 0.0439 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 360 | ANN: trainLoss: 0.0589 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 360 | ANN: trainLoss: 0.0622 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 360 | ANN: trainLoss: 0.0674 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 360 | ANN: trainLoss: 0.0616 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 360 | ANN: trainLoss: 0.0587 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 360 | ANN: trainLoss: 0.0627 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 360 | ANN: trainLoss: 0.0627 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 360 | ANN: trainLoss: 0.0636 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 360 | ANN: trainLoss: 0.0634 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 360 | ANN: trainLoss: 0.0646 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 360 | ANN: trainLoss: 0.1564 | trainAcc: 97.7979% (755/772)\n",
            "0 4 Epoch: 360 | ANN: testLoss: 0.8916 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 360 | ANN: testLoss: 0.8326 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 360 | ANN: testLoss: 0.7637 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 360 | ANN: testLoss: 0.5728 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 361 | ANN: trainLoss: 0.0404 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 361 | ANN: trainLoss: 0.0649 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 361 | ANN: trainLoss: 0.0939 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 361 | ANN: trainLoss: 0.0857 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 361 | ANN: trainLoss: 0.0734 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 361 | ANN: trainLoss: 0.0655 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 361 | ANN: trainLoss: 0.0630 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 361 | ANN: trainLoss: 0.0615 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 361 | ANN: trainLoss: 0.0595 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 361 | ANN: trainLoss: 0.0553 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 361 | ANN: trainLoss: 0.0516 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 361 | ANN: trainLoss: 0.0573 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 361 | ANN: trainLoss: 0.0916 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 361 | ANN: testLoss: 0.8772 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 361 | ANN: testLoss: 0.8116 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 361 | ANN: testLoss: 0.7584 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 361 | ANN: testLoss: 0.5781 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 362 | ANN: trainLoss: 0.0729 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 362 | ANN: trainLoss: 0.0740 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 362 | ANN: trainLoss: 0.0667 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 362 | ANN: trainLoss: 0.0626 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 362 | ANN: trainLoss: 0.0612 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 362 | ANN: trainLoss: 0.0580 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 362 | ANN: trainLoss: 0.0603 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 362 | ANN: trainLoss: 0.0593 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 362 | ANN: trainLoss: 0.0593 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 362 | ANN: trainLoss: 0.0599 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 362 | ANN: trainLoss: 0.0565 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 362 | ANN: trainLoss: 0.0581 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 362 | ANN: trainLoss: 0.0684 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 362 | ANN: testLoss: 0.6151 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 362 | ANN: testLoss: 0.7656 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 362 | ANN: testLoss: 0.7683 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 362 | ANN: testLoss: 0.5784 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 363 | ANN: trainLoss: 0.0829 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 363 | ANN: trainLoss: 0.0671 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 363 | ANN: trainLoss: 0.0586 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 363 | ANN: trainLoss: 0.0663 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 363 | ANN: trainLoss: 0.0745 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 363 | ANN: trainLoss: 0.0741 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 363 | ANN: trainLoss: 0.0721 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 363 | ANN: trainLoss: 0.0683 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 363 | ANN: trainLoss: 0.0656 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 363 | ANN: trainLoss: 0.0637 | trainAcc: 98.1250% (628/640)\n",
            "10 13 Epoch: 363 | ANN: trainLoss: 0.0620 | trainAcc: 98.2955% (692/704)\n",
            "11 13 Epoch: 363 | ANN: trainLoss: 0.0609 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 363 | ANN: trainLoss: 0.0778 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 363 | ANN: testLoss: 0.5921 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 363 | ANN: testLoss: 0.7642 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 363 | ANN: testLoss: 0.7584 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 363 | ANN: testLoss: 0.5768 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 364 | ANN: trainLoss: 0.0940 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 364 | ANN: trainLoss: 0.0785 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 364 | ANN: trainLoss: 0.1191 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 364 | ANN: trainLoss: 0.1058 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 364 | ANN: trainLoss: 0.0931 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 364 | ANN: trainLoss: 0.0860 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 364 | ANN: trainLoss: 0.0820 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 364 | ANN: trainLoss: 0.0770 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 364 | ANN: trainLoss: 0.0783 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 364 | ANN: trainLoss: 0.0796 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 364 | ANN: trainLoss: 0.0780 | trainAcc: 97.8693% (689/704)\n",
            "11 13 Epoch: 364 | ANN: trainLoss: 0.0732 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 364 | ANN: trainLoss: 0.0988 | trainAcc: 97.9275% (756/772)\n",
            "0 4 Epoch: 364 | ANN: testLoss: 0.6456 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 364 | ANN: testLoss: 0.7730 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 364 | ANN: testLoss: 0.7539 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 364 | ANN: testLoss: 0.6695 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 365 | ANN: trainLoss: 0.0570 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 365 | ANN: trainLoss: 0.0796 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 365 | ANN: trainLoss: 0.0889 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 365 | ANN: trainLoss: 0.0718 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 365 | ANN: trainLoss: 0.0835 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 365 | ANN: trainLoss: 0.0726 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 365 | ANN: trainLoss: 0.0807 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 365 | ANN: trainLoss: 0.0816 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 365 | ANN: trainLoss: 0.0808 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 365 | ANN: trainLoss: 0.0765 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 365 | ANN: trainLoss: 0.0723 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 365 | ANN: trainLoss: 0.0703 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 365 | ANN: trainLoss: 0.0961 | trainAcc: 97.0207% (749/772)\n",
            "0 4 Epoch: 365 | ANN: testLoss: 0.4709 | testAcc: 87.5000% (56/64)\n",
            "1 4 Epoch: 365 | ANN: testLoss: 0.6344 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 365 | ANN: testLoss: 0.7410 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 365 | ANN: testLoss: 0.5558 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 366 | ANN: trainLoss: 0.0584 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 366 | ANN: trainLoss: 0.0639 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 366 | ANN: trainLoss: 0.0646 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 366 | ANN: trainLoss: 0.0610 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 366 | ANN: trainLoss: 0.0538 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 366 | ANN: trainLoss: 0.0617 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 366 | ANN: trainLoss: 0.0618 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 366 | ANN: trainLoss: 0.0606 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 366 | ANN: trainLoss: 0.0583 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 366 | ANN: trainLoss: 0.0586 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 366 | ANN: trainLoss: 0.0580 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 366 | ANN: trainLoss: 0.0549 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 366 | ANN: trainLoss: 0.0529 | trainAcc: 97.7979% (755/772)\n",
            "0 4 Epoch: 366 | ANN: testLoss: 0.7275 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 366 | ANN: testLoss: 0.8324 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 366 | ANN: testLoss: 0.7551 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 366 | ANN: testLoss: 0.5678 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 367 | ANN: trainLoss: 0.0556 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 367 | ANN: trainLoss: 0.0809 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 367 | ANN: trainLoss: 0.0798 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 367 | ANN: trainLoss: 0.0710 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 367 | ANN: trainLoss: 0.0629 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 367 | ANN: trainLoss: 0.0707 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 367 | ANN: trainLoss: 0.0762 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 367 | ANN: trainLoss: 0.0723 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 367 | ANN: trainLoss: 0.0667 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 367 | ANN: trainLoss: 0.0629 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 367 | ANN: trainLoss: 0.0599 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 367 | ANN: trainLoss: 0.0590 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 367 | ANN: trainLoss: 0.0564 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 367 | ANN: testLoss: 0.5917 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 367 | ANN: testLoss: 0.6036 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 367 | ANN: testLoss: 0.7485 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 367 | ANN: testLoss: 0.5614 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 368 | ANN: trainLoss: 0.0708 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 368 | ANN: trainLoss: 0.0820 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 368 | ANN: trainLoss: 0.0884 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 368 | ANN: trainLoss: 0.0700 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 368 | ANN: trainLoss: 0.0601 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 368 | ANN: trainLoss: 0.0599 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 368 | ANN: trainLoss: 0.0636 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 368 | ANN: trainLoss: 0.0725 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 368 | ANN: trainLoss: 0.0671 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 368 | ANN: trainLoss: 0.0736 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 368 | ANN: trainLoss: 0.0784 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 368 | ANN: trainLoss: 0.0747 | trainAcc: 97.5260% (749/768)\n",
            "12 13 Epoch: 368 | ANN: trainLoss: 0.0696 | trainAcc: 97.5389% (753/772)\n",
            "0 4 Epoch: 368 | ANN: testLoss: 0.5287 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 368 | ANN: testLoss: 0.6251 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 368 | ANN: testLoss: 0.7303 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 368 | ANN: testLoss: 0.8485 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 369 | ANN: trainLoss: 0.0833 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 369 | ANN: trainLoss: 0.0624 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 369 | ANN: trainLoss: 0.0486 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 369 | ANN: trainLoss: 0.0590 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 369 | ANN: trainLoss: 0.0538 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 369 | ANN: trainLoss: 0.0508 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 369 | ANN: trainLoss: 0.0457 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 369 | ANN: trainLoss: 0.0427 | trainAcc: 99.4141% (509/512)\n",
            "8 13 Epoch: 369 | ANN: trainLoss: 0.0430 | trainAcc: 99.4792% (573/576)\n",
            "9 13 Epoch: 369 | ANN: trainLoss: 0.0430 | trainAcc: 99.5312% (637/640)\n",
            "10 13 Epoch: 369 | ANN: trainLoss: 0.0427 | trainAcc: 99.5739% (701/704)\n",
            "11 13 Epoch: 369 | ANN: trainLoss: 0.0437 | trainAcc: 99.4792% (764/768)\n",
            "12 13 Epoch: 369 | ANN: trainLoss: 0.0432 | trainAcc: 99.4819% (768/772)\n",
            "0 4 Epoch: 369 | ANN: testLoss: 0.9957 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 369 | ANN: testLoss: 0.7262 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 369 | ANN: testLoss: 0.7143 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 369 | ANN: testLoss: 0.5357 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 370 | ANN: trainLoss: 0.0578 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 370 | ANN: trainLoss: 0.0921 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 370 | ANN: trainLoss: 0.0975 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 370 | ANN: trainLoss: 0.0915 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 370 | ANN: trainLoss: 0.0944 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 370 | ANN: trainLoss: 0.0869 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 370 | ANN: trainLoss: 0.0803 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 370 | ANN: trainLoss: 0.0796 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 370 | ANN: trainLoss: 0.0760 | trainAcc: 97.3958% (561/576)\n",
            "9 13 Epoch: 370 | ANN: trainLoss: 0.0740 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 370 | ANN: trainLoss: 0.0715 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 370 | ANN: trainLoss: 0.0686 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 370 | ANN: trainLoss: 0.0861 | trainAcc: 97.6684% (754/772)\n",
            "0 4 Epoch: 370 | ANN: testLoss: 0.8139 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 370 | ANN: testLoss: 0.7740 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 370 | ANN: testLoss: 0.7635 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 370 | ANN: testLoss: 0.5728 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 371 | ANN: trainLoss: 0.0613 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 371 | ANN: trainLoss: 0.0521 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 371 | ANN: trainLoss: 0.0579 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 371 | ANN: trainLoss: 0.0548 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 371 | ANN: trainLoss: 0.0548 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 371 | ANN: trainLoss: 0.0625 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 371 | ANN: trainLoss: 0.0641 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 371 | ANN: trainLoss: 0.0601 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 371 | ANN: trainLoss: 0.0581 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 371 | ANN: trainLoss: 0.0545 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 371 | ANN: trainLoss: 0.0598 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 371 | ANN: trainLoss: 0.0567 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 371 | ANN: trainLoss: 0.0526 | trainAcc: 97.7979% (755/772)\n",
            "0 4 Epoch: 371 | ANN: testLoss: 0.5978 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 371 | ANN: testLoss: 0.8100 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 371 | ANN: testLoss: 0.7410 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 371 | ANN: testLoss: 0.5980 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 372 | ANN: trainLoss: 0.0435 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 372 | ANN: trainLoss: 0.0554 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 372 | ANN: trainLoss: 0.0569 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 372 | ANN: trainLoss: 0.0493 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 372 | ANN: trainLoss: 0.0576 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 372 | ANN: trainLoss: 0.0583 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 372 | ANN: trainLoss: 0.0665 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 372 | ANN: trainLoss: 0.0611 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 372 | ANN: trainLoss: 0.0610 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 372 | ANN: trainLoss: 0.0621 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 372 | ANN: trainLoss: 0.0608 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 372 | ANN: trainLoss: 0.0581 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 372 | ANN: trainLoss: 0.1204 | trainAcc: 97.9275% (756/772)\n",
            "0 4 Epoch: 372 | ANN: testLoss: 0.7563 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 372 | ANN: testLoss: 0.8947 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 372 | ANN: testLoss: 0.7650 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 372 | ANN: testLoss: 0.6032 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 373 | ANN: trainLoss: 0.0285 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 373 | ANN: trainLoss: 0.0289 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 373 | ANN: trainLoss: 0.0362 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 373 | ANN: trainLoss: 0.0395 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 373 | ANN: trainLoss: 0.0363 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 373 | ANN: trainLoss: 0.0402 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 373 | ANN: trainLoss: 0.0476 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 373 | ANN: trainLoss: 0.0559 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 373 | ANN: trainLoss: 0.0528 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 373 | ANN: trainLoss: 0.0570 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 373 | ANN: trainLoss: 0.0585 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 373 | ANN: trainLoss: 0.0581 | trainAcc: 98.3073% (755/768)\n",
            "12 13 Epoch: 373 | ANN: trainLoss: 0.0555 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 373 | ANN: testLoss: 0.7301 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 373 | ANN: testLoss: 0.7365 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 373 | ANN: testLoss: 0.7180 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 373 | ANN: testLoss: 0.8954 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 374 | ANN: trainLoss: 0.0257 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 374 | ANN: trainLoss: 0.0314 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 374 | ANN: trainLoss: 0.0377 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 374 | ANN: trainLoss: 0.0400 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 374 | ANN: trainLoss: 0.0422 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 374 | ANN: trainLoss: 0.0496 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 374 | ANN: trainLoss: 0.0514 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 374 | ANN: trainLoss: 0.0562 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 374 | ANN: trainLoss: 0.0556 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 374 | ANN: trainLoss: 0.0549 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 374 | ANN: trainLoss: 0.0566 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 374 | ANN: trainLoss: 0.0565 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 374 | ANN: trainLoss: 0.1419 | trainAcc: 97.9275% (756/772)\n",
            "0 4 Epoch: 374 | ANN: testLoss: 0.8762 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 374 | ANN: testLoss: 0.7078 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 374 | ANN: testLoss: 0.7356 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 374 | ANN: testLoss: 0.5814 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 375 | ANN: trainLoss: 0.0285 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 375 | ANN: trainLoss: 0.0428 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 375 | ANN: trainLoss: 0.0469 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 375 | ANN: trainLoss: 0.0500 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 375 | ANN: trainLoss: 0.0481 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 375 | ANN: trainLoss: 0.0472 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 375 | ANN: trainLoss: 0.0502 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 375 | ANN: trainLoss: 0.0514 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 375 | ANN: trainLoss: 0.0538 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 375 | ANN: trainLoss: 0.0511 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 375 | ANN: trainLoss: 0.0530 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 375 | ANN: trainLoss: 0.0508 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 375 | ANN: trainLoss: 0.0484 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 375 | ANN: testLoss: 0.6526 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 375 | ANN: testLoss: 0.9211 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 375 | ANN: testLoss: 0.7808 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 375 | ANN: testLoss: 0.7480 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 376 | ANN: trainLoss: 0.0298 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 376 | ANN: trainLoss: 0.0584 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 376 | ANN: trainLoss: 0.0462 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 376 | ANN: trainLoss: 0.0556 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 376 | ANN: trainLoss: 0.0531 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 376 | ANN: trainLoss: 0.0574 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 376 | ANN: trainLoss: 0.0546 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 376 | ANN: trainLoss: 0.0530 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 376 | ANN: trainLoss: 0.0531 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 376 | ANN: trainLoss: 0.0589 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 376 | ANN: trainLoss: 0.0552 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 376 | ANN: trainLoss: 0.0539 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 376 | ANN: trainLoss: 0.0547 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 376 | ANN: testLoss: 0.8132 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 376 | ANN: testLoss: 0.8955 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 376 | ANN: testLoss: 0.7600 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 376 | ANN: testLoss: 0.5700 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 377 | ANN: trainLoss: 0.0638 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 377 | ANN: trainLoss: 0.0961 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 377 | ANN: trainLoss: 0.0722 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 377 | ANN: trainLoss: 0.0831 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 377 | ANN: trainLoss: 0.0750 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 377 | ANN: trainLoss: 0.0676 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 377 | ANN: trainLoss: 0.0699 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 377 | ANN: trainLoss: 0.0648 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 377 | ANN: trainLoss: 0.0641 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 377 | ANN: trainLoss: 0.0677 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 377 | ANN: trainLoss: 0.0683 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 377 | ANN: trainLoss: 0.0664 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 377 | ANN: trainLoss: 0.0629 | trainAcc: 97.2798% (751/772)\n",
            "0 4 Epoch: 377 | ANN: testLoss: 0.7266 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 377 | ANN: testLoss: 0.6638 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 377 | ANN: testLoss: 0.7257 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 377 | ANN: testLoss: 1.5126 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 378 | ANN: trainLoss: 0.0572 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 378 | ANN: trainLoss: 0.0448 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 378 | ANN: trainLoss: 0.0390 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 378 | ANN: trainLoss: 0.0448 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 378 | ANN: trainLoss: 0.0423 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 378 | ANN: trainLoss: 0.0533 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 378 | ANN: trainLoss: 0.0470 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 378 | ANN: trainLoss: 0.0486 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 378 | ANN: trainLoss: 0.0481 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 378 | ANN: trainLoss: 0.0449 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 378 | ANN: trainLoss: 0.0435 | trainAcc: 98.8636% (696/704)\n",
            "11 13 Epoch: 378 | ANN: trainLoss: 0.0415 | trainAcc: 98.9583% (760/768)\n",
            "12 13 Epoch: 378 | ANN: trainLoss: 0.2952 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 378 | ANN: testLoss: 0.5869 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 378 | ANN: testLoss: 0.7458 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 378 | ANN: testLoss: 0.7342 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 378 | ANN: testLoss: 1.1697 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 379 | ANN: trainLoss: 0.0361 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 379 | ANN: trainLoss: 0.0359 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 379 | ANN: trainLoss: 0.0437 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 379 | ANN: trainLoss: 0.0362 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 379 | ANN: trainLoss: 0.0338 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 379 | ANN: trainLoss: 0.0346 | trainAcc: 99.4792% (382/384)\n",
            "6 13 Epoch: 379 | ANN: trainLoss: 0.0385 | trainAcc: 99.1071% (444/448)\n",
            "7 13 Epoch: 379 | ANN: trainLoss: 0.0406 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 379 | ANN: trainLoss: 0.0440 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 379 | ANN: trainLoss: 0.0424 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 379 | ANN: trainLoss: 0.0421 | trainAcc: 99.0057% (697/704)\n",
            "11 13 Epoch: 379 | ANN: trainLoss: 0.0443 | trainAcc: 98.9583% (760/768)\n",
            "12 13 Epoch: 379 | ANN: trainLoss: 0.0428 | trainAcc: 98.9637% (764/772)\n",
            "0 4 Epoch: 379 | ANN: testLoss: 1.0529 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 379 | ANN: testLoss: 0.8420 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 379 | ANN: testLoss: 0.7715 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 379 | ANN: testLoss: 0.5787 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 380 | ANN: trainLoss: 0.0271 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 380 | ANN: trainLoss: 0.0392 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 380 | ANN: trainLoss: 0.0354 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 380 | ANN: trainLoss: 0.0395 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 380 | ANN: trainLoss: 0.0483 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 380 | ANN: trainLoss: 0.0470 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 380 | ANN: trainLoss: 0.0462 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 380 | ANN: trainLoss: 0.0447 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 380 | ANN: trainLoss: 0.0456 | trainAcc: 98.9583% (570/576)\n",
            "9 13 Epoch: 380 | ANN: trainLoss: 0.0486 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 380 | ANN: trainLoss: 0.0510 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 380 | ANN: trainLoss: 0.0534 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 380 | ANN: trainLoss: 0.3069 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 380 | ANN: testLoss: 0.7858 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 380 | ANN: testLoss: 0.7116 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 380 | ANN: testLoss: 0.7493 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 380 | ANN: testLoss: 1.7114 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 381 | ANN: trainLoss: 0.0375 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 381 | ANN: trainLoss: 0.0686 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 381 | ANN: trainLoss: 0.0555 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 381 | ANN: trainLoss: 0.0624 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 381 | ANN: trainLoss: 0.0616 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 381 | ANN: trainLoss: 0.0564 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 381 | ANN: trainLoss: 0.0559 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 381 | ANN: trainLoss: 0.0535 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 381 | ANN: trainLoss: 0.0545 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 381 | ANN: trainLoss: 0.0513 | trainAcc: 98.1250% (628/640)\n",
            "10 13 Epoch: 381 | ANN: trainLoss: 0.0558 | trainAcc: 97.8693% (689/704)\n",
            "11 13 Epoch: 381 | ANN: trainLoss: 0.0611 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 381 | ANN: trainLoss: 0.0565 | trainAcc: 97.6684% (754/772)\n",
            "0 4 Epoch: 381 | ANN: testLoss: 0.7165 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 381 | ANN: testLoss: 0.7293 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 381 | ANN: testLoss: 0.7448 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 381 | ANN: testLoss: 0.6098 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 382 | ANN: trainLoss: 0.0633 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 382 | ANN: trainLoss: 0.0659 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 382 | ANN: trainLoss: 0.0487 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 382 | ANN: trainLoss: 0.0484 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 382 | ANN: trainLoss: 0.1121 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 382 | ANN: trainLoss: 0.1047 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 382 | ANN: trainLoss: 0.1143 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 382 | ANN: trainLoss: 0.1079 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 382 | ANN: trainLoss: 0.1026 | trainAcc: 97.0486% (559/576)\n",
            "9 13 Epoch: 382 | ANN: trainLoss: 0.0955 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 382 | ANN: trainLoss: 0.0883 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 382 | ANN: trainLoss: 0.0852 | trainAcc: 97.5260% (749/768)\n",
            "12 13 Epoch: 382 | ANN: trainLoss: 0.0821 | trainAcc: 97.5389% (753/772)\n",
            "0 4 Epoch: 382 | ANN: testLoss: 0.7052 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 382 | ANN: testLoss: 0.8182 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 382 | ANN: testLoss: 0.7513 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 382 | ANN: testLoss: 0.5635 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 383 | ANN: trainLoss: 0.1312 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 383 | ANN: trainLoss: 0.0825 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 383 | ANN: trainLoss: 0.0837 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 383 | ANN: trainLoss: 0.0725 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 383 | ANN: trainLoss: 0.0632 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 383 | ANN: trainLoss: 0.0642 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 383 | ANN: trainLoss: 0.0679 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 383 | ANN: trainLoss: 0.0720 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 383 | ANN: trainLoss: 0.0693 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 383 | ANN: trainLoss: 0.0672 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 383 | ANN: trainLoss: 0.0649 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 383 | ANN: trainLoss: 0.0612 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 383 | ANN: trainLoss: 0.1172 | trainAcc: 97.9275% (756/772)\n",
            "0 4 Epoch: 383 | ANN: testLoss: 0.8367 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 383 | ANN: testLoss: 0.8479 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 383 | ANN: testLoss: 0.7422 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 383 | ANN: testLoss: 0.5879 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 384 | ANN: trainLoss: 0.0953 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 384 | ANN: trainLoss: 0.0897 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 384 | ANN: trainLoss: 0.0702 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 384 | ANN: trainLoss: 0.0615 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 384 | ANN: trainLoss: 0.0586 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 384 | ANN: trainLoss: 0.0590 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 384 | ANN: trainLoss: 0.0551 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 384 | ANN: trainLoss: 0.0541 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 384 | ANN: trainLoss: 0.0618 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 384 | ANN: trainLoss: 0.0587 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 384 | ANN: trainLoss: 0.0571 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 384 | ANN: trainLoss: 0.0548 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 384 | ANN: trainLoss: 0.0536 | trainAcc: 97.9275% (756/772)\n",
            "0 4 Epoch: 384 | ANN: testLoss: 0.3828 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 384 | ANN: testLoss: 0.6787 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 384 | ANN: testLoss: 0.7230 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 384 | ANN: testLoss: 1.6179 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 385 | ANN: trainLoss: 0.0252 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 385 | ANN: trainLoss: 0.0573 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 385 | ANN: trainLoss: 0.0568 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 385 | ANN: trainLoss: 0.0764 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 385 | ANN: trainLoss: 0.0654 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 385 | ANN: trainLoss: 0.0639 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 385 | ANN: trainLoss: 0.0696 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 385 | ANN: trainLoss: 0.0665 | trainAcc: 97.0703% (497/512)\n",
            "8 13 Epoch: 385 | ANN: trainLoss: 0.0671 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 385 | ANN: trainLoss: 0.0685 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 385 | ANN: trainLoss: 0.0713 | trainAcc: 97.1591% (684/704)\n",
            "11 13 Epoch: 385 | ANN: trainLoss: 0.0731 | trainAcc: 97.0052% (745/768)\n",
            "12 13 Epoch: 385 | ANN: trainLoss: 0.1690 | trainAcc: 96.7617% (747/772)\n",
            "0 4 Epoch: 385 | ANN: testLoss: 0.9436 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 385 | ANN: testLoss: 0.7732 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 385 | ANN: testLoss: 0.7661 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 385 | ANN: testLoss: 2.0639 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 386 | ANN: trainLoss: 0.0659 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 386 | ANN: trainLoss: 0.0846 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 386 | ANN: trainLoss: 0.0637 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 386 | ANN: trainLoss: 0.0693 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 386 | ANN: trainLoss: 0.0668 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 386 | ANN: trainLoss: 0.0668 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 386 | ANN: trainLoss: 0.0655 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 386 | ANN: trainLoss: 0.0741 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 386 | ANN: trainLoss: 0.0724 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 386 | ANN: trainLoss: 0.0685 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 386 | ANN: trainLoss: 0.0661 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 386 | ANN: trainLoss: 0.0624 | trainAcc: 97.5260% (749/768)\n",
            "12 13 Epoch: 386 | ANN: trainLoss: 0.0595 | trainAcc: 97.5389% (753/772)\n",
            "0 4 Epoch: 386 | ANN: testLoss: 0.9626 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 386 | ANN: testLoss: 0.9146 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 386 | ANN: testLoss: 0.8004 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 386 | ANN: testLoss: 1.1257 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 387 | ANN: trainLoss: 0.0482 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 387 | ANN: trainLoss: 0.0915 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 387 | ANN: trainLoss: 0.1085 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 387 | ANN: trainLoss: 0.0980 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 387 | ANN: trainLoss: 0.0837 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 387 | ANN: trainLoss: 0.0770 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 387 | ANN: trainLoss: 0.0707 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 387 | ANN: trainLoss: 0.0659 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 387 | ANN: trainLoss: 0.0657 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 387 | ANN: trainLoss: 0.0631 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 387 | ANN: trainLoss: 0.0642 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 387 | ANN: trainLoss: 0.0610 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 387 | ANN: trainLoss: 0.1645 | trainAcc: 97.9275% (756/772)\n",
            "0 4 Epoch: 387 | ANN: testLoss: 0.7354 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 387 | ANN: testLoss: 0.7014 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 387 | ANN: testLoss: 0.7909 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 387 | ANN: testLoss: 0.6110 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 388 | ANN: trainLoss: 0.0199 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 388 | ANN: trainLoss: 0.0202 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 388 | ANN: trainLoss: 0.0641 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 388 | ANN: trainLoss: 0.0561 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 388 | ANN: trainLoss: 0.0560 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 388 | ANN: trainLoss: 0.0685 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 388 | ANN: trainLoss: 0.0778 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 388 | ANN: trainLoss: 0.0756 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 388 | ANN: trainLoss: 0.0745 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 388 | ANN: trainLoss: 0.0734 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 388 | ANN: trainLoss: 0.0711 | trainAcc: 97.8693% (689/704)\n",
            "11 13 Epoch: 388 | ANN: trainLoss: 0.0706 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 388 | ANN: trainLoss: 0.0938 | trainAcc: 97.6684% (754/772)\n",
            "0 4 Epoch: 388 | ANN: testLoss: 0.9481 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 388 | ANN: testLoss: 0.7728 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 388 | ANN: testLoss: 0.7631 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 388 | ANN: testLoss: 0.9430 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 389 | ANN: trainLoss: 0.0322 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 389 | ANN: trainLoss: 0.0585 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 389 | ANN: trainLoss: 0.0744 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 389 | ANN: trainLoss: 0.0748 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 389 | ANN: trainLoss: 0.0738 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 389 | ANN: trainLoss: 0.0696 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 389 | ANN: trainLoss: 0.0692 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 389 | ANN: trainLoss: 0.0660 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 389 | ANN: trainLoss: 0.0671 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 389 | ANN: trainLoss: 0.0622 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 389 | ANN: trainLoss: 0.0594 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 389 | ANN: trainLoss: 0.0571 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 389 | ANN: trainLoss: 0.0774 | trainAcc: 97.7979% (755/772)\n",
            "0 4 Epoch: 389 | ANN: testLoss: 0.9178 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 389 | ANN: testLoss: 0.8168 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 389 | ANN: testLoss: 0.7573 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 389 | ANN: testLoss: 0.5681 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 390 | ANN: trainLoss: 0.0407 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 390 | ANN: trainLoss: 0.1069 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 390 | ANN: trainLoss: 0.0794 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 390 | ANN: trainLoss: 0.0799 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 390 | ANN: trainLoss: 0.0704 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 390 | ANN: trainLoss: 0.0642 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 390 | ANN: trainLoss: 0.0598 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 390 | ANN: trainLoss: 0.0547 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 390 | ANN: trainLoss: 0.0525 | trainAcc: 98.9583% (570/576)\n",
            "9 13 Epoch: 390 | ANN: trainLoss: 0.0574 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 390 | ANN: trainLoss: 0.0564 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 390 | ANN: trainLoss: 0.0587 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 390 | ANN: trainLoss: 0.0549 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 390 | ANN: testLoss: 0.8032 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 390 | ANN: testLoss: 0.6726 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 390 | ANN: testLoss: 0.7029 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 390 | ANN: testLoss: 1.3988 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 391 | ANN: trainLoss: 0.0731 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 391 | ANN: trainLoss: 0.0456 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 391 | ANN: trainLoss: 0.0465 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 391 | ANN: trainLoss: 0.0503 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 391 | ANN: trainLoss: 0.0463 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 391 | ANN: trainLoss: 0.0412 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 391 | ANN: trainLoss: 0.0437 | trainAcc: 99.1071% (444/448)\n",
            "7 13 Epoch: 391 | ANN: trainLoss: 0.0420 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 391 | ANN: trainLoss: 0.0460 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 391 | ANN: trainLoss: 0.0433 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 391 | ANN: trainLoss: 0.0419 | trainAcc: 98.8636% (696/704)\n",
            "11 13 Epoch: 391 | ANN: trainLoss: 0.0436 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 391 | ANN: trainLoss: 0.0872 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 391 | ANN: testLoss: 0.6538 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 391 | ANN: testLoss: 0.7032 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 391 | ANN: testLoss: 0.7111 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 391 | ANN: testLoss: 1.0742 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 392 | ANN: trainLoss: 0.0352 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 392 | ANN: trainLoss: 0.0592 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 392 | ANN: trainLoss: 0.0664 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 392 | ANN: trainLoss: 0.0739 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 392 | ANN: trainLoss: 0.0767 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 392 | ANN: trainLoss: 0.0753 | trainAcc: 96.8750% (372/384)\n",
            "6 13 Epoch: 392 | ANN: trainLoss: 0.0692 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 392 | ANN: trainLoss: 0.0669 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 392 | ANN: trainLoss: 0.0612 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 392 | ANN: trainLoss: 0.0599 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 392 | ANN: trainLoss: 0.0573 | trainAcc: 97.8693% (689/704)\n",
            "11 13 Epoch: 392 | ANN: trainLoss: 0.0552 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 392 | ANN: trainLoss: 0.0510 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 392 | ANN: testLoss: 0.9135 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 392 | ANN: testLoss: 0.7564 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 392 | ANN: testLoss: 0.7408 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 392 | ANN: testLoss: 0.5809 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 393 | ANN: trainLoss: 0.0990 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 393 | ANN: trainLoss: 0.0762 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 393 | ANN: trainLoss: 0.0630 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 393 | ANN: trainLoss: 0.0586 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 393 | ANN: trainLoss: 0.0562 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 393 | ANN: trainLoss: 0.0497 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 393 | ANN: trainLoss: 0.0504 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 393 | ANN: trainLoss: 0.0536 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 393 | ANN: trainLoss: 0.0573 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 393 | ANN: trainLoss: 0.0583 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 393 | ANN: trainLoss: 0.0583 | trainAcc: 98.2955% (692/704)\n",
            "11 13 Epoch: 393 | ANN: trainLoss: 0.0588 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 393 | ANN: trainLoss: 0.0549 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 393 | ANN: testLoss: 0.7123 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 393 | ANN: testLoss: 0.6744 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 393 | ANN: testLoss: 0.7332 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 393 | ANN: testLoss: 0.5896 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 394 | ANN: trainLoss: 0.0688 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 394 | ANN: trainLoss: 0.0647 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 394 | ANN: trainLoss: 0.0748 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 394 | ANN: trainLoss: 0.0640 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 394 | ANN: trainLoss: 0.0584 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 394 | ANN: trainLoss: 0.0516 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 394 | ANN: trainLoss: 0.0485 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 394 | ANN: trainLoss: 0.0459 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 394 | ANN: trainLoss: 0.0451 | trainAcc: 99.1319% (571/576)\n",
            "9 13 Epoch: 394 | ANN: trainLoss: 0.0435 | trainAcc: 99.2188% (635/640)\n",
            "10 13 Epoch: 394 | ANN: trainLoss: 0.0425 | trainAcc: 99.2898% (699/704)\n",
            "11 13 Epoch: 394 | ANN: trainLoss: 0.0463 | trainAcc: 99.2188% (762/768)\n",
            "12 13 Epoch: 394 | ANN: trainLoss: 0.0955 | trainAcc: 98.9637% (764/772)\n",
            "0 4 Epoch: 394 | ANN: testLoss: 0.7932 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 394 | ANN: testLoss: 0.8157 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 394 | ANN: testLoss: 0.7431 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 394 | ANN: testLoss: 0.5597 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 395 | ANN: trainLoss: 0.0891 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 395 | ANN: trainLoss: 0.1116 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 395 | ANN: trainLoss: 0.0876 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 395 | ANN: trainLoss: 0.0762 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 395 | ANN: trainLoss: 0.0711 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 395 | ANN: trainLoss: 0.0693 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 395 | ANN: trainLoss: 0.0665 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 395 | ANN: trainLoss: 0.0636 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 395 | ANN: trainLoss: 0.0672 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 395 | ANN: trainLoss: 0.0644 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 395 | ANN: trainLoss: 0.0603 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 395 | ANN: trainLoss: 0.0628 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 395 | ANN: trainLoss: 0.0584 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 395 | ANN: testLoss: 0.8293 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 395 | ANN: testLoss: 0.6699 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 395 | ANN: testLoss: 0.7502 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 395 | ANN: testLoss: 0.8652 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 396 | ANN: trainLoss: 0.0467 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 396 | ANN: trainLoss: 0.0434 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 396 | ANN: trainLoss: 0.0394 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 396 | ANN: trainLoss: 0.0385 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 396 | ANN: trainLoss: 0.0421 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 396 | ANN: trainLoss: 0.0435 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 396 | ANN: trainLoss: 0.0440 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 396 | ANN: trainLoss: 0.0484 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 396 | ANN: trainLoss: 0.0488 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 396 | ANN: trainLoss: 0.0458 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 396 | ANN: trainLoss: 0.0479 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 396 | ANN: trainLoss: 0.0581 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 396 | ANN: trainLoss: 0.0590 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 396 | ANN: testLoss: 0.8788 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 396 | ANN: testLoss: 0.8002 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 396 | ANN: testLoss: 0.7541 | testAcc: 73.4375% (141/192)\n",
            "3 4 Epoch: 396 | ANN: testLoss: 0.5818 | testAcc: 73.5751% (142/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 397 | ANN: trainLoss: 0.0429 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 397 | ANN: trainLoss: 0.0596 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 397 | ANN: trainLoss: 0.0606 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 397 | ANN: trainLoss: 0.0692 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 397 | ANN: trainLoss: 0.0646 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 397 | ANN: trainLoss: 0.0718 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 397 | ANN: trainLoss: 0.0739 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 397 | ANN: trainLoss: 0.0775 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 397 | ANN: trainLoss: 0.0732 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 397 | ANN: trainLoss: 0.0705 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 397 | ANN: trainLoss: 0.0693 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 397 | ANN: trainLoss: 0.0655 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 397 | ANN: trainLoss: 0.0612 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 397 | ANN: testLoss: 0.7580 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 397 | ANN: testLoss: 0.7387 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 397 | ANN: testLoss: 0.7555 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 397 | ANN: testLoss: 0.5667 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 398 | ANN: trainLoss: 0.0607 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 398 | ANN: trainLoss: 0.0442 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 398 | ANN: trainLoss: 0.0534 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 398 | ANN: trainLoss: 0.0460 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 398 | ANN: trainLoss: 0.0407 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 398 | ANN: trainLoss: 0.0429 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 398 | ANN: trainLoss: 0.0458 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 398 | ANN: trainLoss: 0.0471 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 398 | ANN: trainLoss: 0.0432 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 398 | ANN: trainLoss: 0.0416 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 398 | ANN: trainLoss: 0.0417 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 398 | ANN: trainLoss: 0.0479 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 398 | ANN: trainLoss: 0.0862 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 398 | ANN: testLoss: 0.7405 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 398 | ANN: testLoss: 0.8267 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 398 | ANN: testLoss: 0.7636 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 398 | ANN: testLoss: 0.5739 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 399 | ANN: trainLoss: 0.0697 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 399 | ANN: trainLoss: 0.0675 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 399 | ANN: trainLoss: 0.0711 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 399 | ANN: trainLoss: 0.0854 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 399 | ANN: trainLoss: 0.0735 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 399 | ANN: trainLoss: 0.0708 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 399 | ANN: trainLoss: 0.0701 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 399 | ANN: trainLoss: 0.0714 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 399 | ANN: trainLoss: 0.0682 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 399 | ANN: trainLoss: 0.0642 | trainAcc: 98.1250% (628/640)\n",
            "10 13 Epoch: 399 | ANN: trainLoss: 0.0628 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 399 | ANN: trainLoss: 0.0620 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 399 | ANN: trainLoss: 0.0617 | trainAcc: 97.9275% (756/772)\n",
            "0 4 Epoch: 399 | ANN: testLoss: 1.0533 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 399 | ANN: testLoss: 0.7776 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 399 | ANN: testLoss: 0.7655 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 399 | ANN: testLoss: 0.5995 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 400 | ANN: trainLoss: 0.0734 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 400 | ANN: trainLoss: 0.0445 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 400 | ANN: trainLoss: 0.0444 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 400 | ANN: trainLoss: 0.0422 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 400 | ANN: trainLoss: 0.0431 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 400 | ANN: trainLoss: 0.0480 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 400 | ANN: trainLoss: 0.0549 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 400 | ANN: trainLoss: 0.0578 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 400 | ANN: trainLoss: 0.0545 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 400 | ANN: trainLoss: 0.0576 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 400 | ANN: trainLoss: 0.0548 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 400 | ANN: trainLoss: 0.0547 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 400 | ANN: trainLoss: 0.1050 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 400 | ANN: testLoss: 0.7419 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 400 | ANN: testLoss: 0.6987 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 400 | ANN: testLoss: 0.7623 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 400 | ANN: testLoss: 0.5791 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 401 | ANN: trainLoss: 0.0418 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 401 | ANN: trainLoss: 0.0436 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 401 | ANN: trainLoss: 0.0412 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 401 | ANN: trainLoss: 0.0438 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 401 | ANN: trainLoss: 0.0426 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 401 | ANN: trainLoss: 0.0416 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 401 | ANN: trainLoss: 0.0414 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 401 | ANN: trainLoss: 0.0418 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 401 | ANN: trainLoss: 0.0415 | trainAcc: 98.9583% (570/576)\n",
            "9 13 Epoch: 401 | ANN: trainLoss: 0.0411 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 401 | ANN: trainLoss: 0.0434 | trainAcc: 98.8636% (696/704)\n",
            "11 13 Epoch: 401 | ANN: trainLoss: 0.0416 | trainAcc: 98.9583% (760/768)\n",
            "12 13 Epoch: 401 | ANN: trainLoss: 0.0684 | trainAcc: 98.8342% (763/772)\n",
            "0 4 Epoch: 401 | ANN: testLoss: 0.7251 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 401 | ANN: testLoss: 0.7802 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 401 | ANN: testLoss: 0.7714 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 401 | ANN: testLoss: 0.5786 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 402 | ANN: trainLoss: 0.1093 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 402 | ANN: trainLoss: 0.0633 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 402 | ANN: trainLoss: 0.0585 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 402 | ANN: trainLoss: 0.0579 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 402 | ANN: trainLoss: 0.0707 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 402 | ANN: trainLoss: 0.0607 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 402 | ANN: trainLoss: 0.0738 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 402 | ANN: trainLoss: 0.0671 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 402 | ANN: trainLoss: 0.0630 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 402 | ANN: trainLoss: 0.0619 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 402 | ANN: trainLoss: 0.0646 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 402 | ANN: trainLoss: 0.0706 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 402 | ANN: trainLoss: 0.0944 | trainAcc: 97.7979% (755/772)\n",
            "0 4 Epoch: 402 | ANN: testLoss: 0.6999 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 402 | ANN: testLoss: 0.6428 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 402 | ANN: testLoss: 0.7486 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 402 | ANN: testLoss: 1.1365 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 403 | ANN: trainLoss: 0.0904 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 403 | ANN: trainLoss: 0.0788 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 403 | ANN: trainLoss: 0.0706 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 403 | ANN: trainLoss: 0.0629 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 403 | ANN: trainLoss: 0.0571 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 403 | ANN: trainLoss: 0.0576 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 403 | ANN: trainLoss: 0.0535 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 403 | ANN: trainLoss: 0.0526 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 403 | ANN: trainLoss: 0.0580 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 403 | ANN: trainLoss: 0.0572 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 403 | ANN: trainLoss: 0.0555 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 403 | ANN: trainLoss: 0.0617 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 403 | ANN: trainLoss: 0.0580 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 403 | ANN: testLoss: 0.6196 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 403 | ANN: testLoss: 0.7626 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 403 | ANN: testLoss: 0.7091 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 403 | ANN: testLoss: 0.5319 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 404 | ANN: trainLoss: 0.0386 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 404 | ANN: trainLoss: 0.1064 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 404 | ANN: trainLoss: 0.0763 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 404 | ANN: trainLoss: 0.0658 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 404 | ANN: trainLoss: 0.0564 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 404 | ANN: trainLoss: 0.0673 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 404 | ANN: trainLoss: 0.0644 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 404 | ANN: trainLoss: 0.0628 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 404 | ANN: trainLoss: 0.0618 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 404 | ANN: trainLoss: 0.0618 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 404 | ANN: trainLoss: 0.0695 | trainAcc: 97.3011% (685/704)\n",
            "11 13 Epoch: 404 | ANN: trainLoss: 0.0677 | trainAcc: 97.3958% (748/768)\n",
            "12 13 Epoch: 404 | ANN: trainLoss: 0.0676 | trainAcc: 97.4093% (752/772)\n",
            "0 4 Epoch: 404 | ANN: testLoss: 0.7768 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 404 | ANN: testLoss: 0.8188 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 404 | ANN: testLoss: 0.7136 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 404 | ANN: testLoss: 0.5385 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 405 | ANN: trainLoss: 0.0452 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 405 | ANN: trainLoss: 0.0310 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 405 | ANN: trainLoss: 0.0496 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 405 | ANN: trainLoss: 0.0459 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 405 | ANN: trainLoss: 0.0465 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 405 | ANN: trainLoss: 0.0447 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 405 | ANN: trainLoss: 0.0404 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 405 | ANN: trainLoss: 0.0444 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 405 | ANN: trainLoss: 0.0443 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 405 | ANN: trainLoss: 0.0439 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 405 | ANN: trainLoss: 0.0423 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 405 | ANN: trainLoss: 0.0437 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 405 | ANN: trainLoss: 0.0466 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 405 | ANN: testLoss: 0.9067 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 405 | ANN: testLoss: 0.7753 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 405 | ANN: testLoss: 0.7369 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 405 | ANN: testLoss: 0.5527 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 406 | ANN: trainLoss: 0.0248 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 406 | ANN: trainLoss: 0.0307 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 406 | ANN: trainLoss: 0.0285 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 406 | ANN: trainLoss: 0.0322 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 406 | ANN: trainLoss: 0.0468 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 406 | ANN: trainLoss: 0.0530 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 406 | ANN: trainLoss: 0.0490 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 406 | ANN: trainLoss: 0.0454 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 406 | ANN: trainLoss: 0.0420 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 406 | ANN: trainLoss: 0.0451 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 406 | ANN: trainLoss: 0.0427 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 406 | ANN: trainLoss: 0.0430 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 406 | ANN: trainLoss: 0.0872 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 406 | ANN: testLoss: 0.5283 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 406 | ANN: testLoss: 0.6486 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 406 | ANN: testLoss: 0.7442 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 406 | ANN: testLoss: 0.5588 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 407 | ANN: trainLoss: 0.0652 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 407 | ANN: trainLoss: 0.0730 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 407 | ANN: trainLoss: 0.0669 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 407 | ANN: trainLoss: 0.0636 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 407 | ANN: trainLoss: 0.0703 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 407 | ANN: trainLoss: 0.0688 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 407 | ANN: trainLoss: 0.0654 | trainAcc: 97.0982% (435/448)\n",
            "7 13 Epoch: 407 | ANN: trainLoss: 0.0620 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 407 | ANN: trainLoss: 0.0583 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 407 | ANN: trainLoss: 0.0617 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 407 | ANN: trainLoss: 0.0599 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 407 | ANN: trainLoss: 0.0605 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 407 | ANN: trainLoss: 0.0699 | trainAcc: 97.6684% (754/772)\n",
            "0 4 Epoch: 407 | ANN: testLoss: 0.5422 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 407 | ANN: testLoss: 0.7415 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 407 | ANN: testLoss: 0.7531 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 407 | ANN: testLoss: 0.5697 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 408 | ANN: trainLoss: 0.0347 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 408 | ANN: trainLoss: 0.0315 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 408 | ANN: trainLoss: 0.0347 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 408 | ANN: trainLoss: 0.0341 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 408 | ANN: trainLoss: 0.0406 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 408 | ANN: trainLoss: 0.0581 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 408 | ANN: trainLoss: 0.0543 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 408 | ANN: trainLoss: 0.0525 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 408 | ANN: trainLoss: 0.0515 | trainAcc: 98.9583% (570/576)\n",
            "9 13 Epoch: 408 | ANN: trainLoss: 0.0565 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 408 | ANN: trainLoss: 0.0555 | trainAcc: 98.8636% (696/704)\n",
            "11 13 Epoch: 408 | ANN: trainLoss: 0.0543 | trainAcc: 98.8281% (759/768)\n",
            "12 13 Epoch: 408 | ANN: trainLoss: 0.0576 | trainAcc: 98.8342% (763/772)\n",
            "0 4 Epoch: 408 | ANN: testLoss: 0.7724 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 408 | ANN: testLoss: 0.8351 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 408 | ANN: testLoss: 0.7220 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 408 | ANN: testLoss: 0.5416 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 409 | ANN: trainLoss: 0.0518 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 409 | ANN: trainLoss: 0.0435 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 409 | ANN: trainLoss: 0.0523 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 409 | ANN: trainLoss: 0.0499 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 409 | ANN: trainLoss: 0.0450 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 409 | ANN: trainLoss: 0.0454 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 409 | ANN: trainLoss: 0.0434 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 409 | ANN: trainLoss: 0.0412 | trainAcc: 99.4141% (509/512)\n",
            "8 13 Epoch: 409 | ANN: trainLoss: 0.0429 | trainAcc: 99.3056% (572/576)\n",
            "9 13 Epoch: 409 | ANN: trainLoss: 0.0446 | trainAcc: 99.0625% (634/640)\n",
            "10 13 Epoch: 409 | ANN: trainLoss: 0.0430 | trainAcc: 99.0057% (697/704)\n",
            "11 13 Epoch: 409 | ANN: trainLoss: 0.0429 | trainAcc: 98.9583% (760/768)\n",
            "12 13 Epoch: 409 | ANN: trainLoss: 0.0436 | trainAcc: 98.9637% (764/772)\n",
            "0 4 Epoch: 409 | ANN: testLoss: 0.6786 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 409 | ANN: testLoss: 0.7573 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 409 | ANN: testLoss: 0.6994 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 409 | ANN: testLoss: 1.0248 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 410 | ANN: trainLoss: 0.0742 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 410 | ANN: trainLoss: 0.0534 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 410 | ANN: trainLoss: 0.0463 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 410 | ANN: trainLoss: 0.0634 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 410 | ANN: trainLoss: 0.0597 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 410 | ANN: trainLoss: 0.0602 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 410 | ANN: trainLoss: 0.0546 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 410 | ANN: trainLoss: 0.0514 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 410 | ANN: trainLoss: 0.0498 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 410 | ANN: trainLoss: 0.0486 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 410 | ANN: trainLoss: 0.0482 | trainAcc: 98.8636% (696/704)\n",
            "11 13 Epoch: 410 | ANN: trainLoss: 0.0476 | trainAcc: 98.8281% (759/768)\n",
            "12 13 Epoch: 410 | ANN: trainLoss: 0.0827 | trainAcc: 98.7047% (762/772)\n",
            "0 4 Epoch: 410 | ANN: testLoss: 0.6633 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 410 | ANN: testLoss: 0.6575 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 410 | ANN: testLoss: 0.6996 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 410 | ANN: testLoss: 1.3008 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 411 | ANN: trainLoss: 0.0155 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 411 | ANN: trainLoss: 0.0198 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 411 | ANN: trainLoss: 0.0274 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 411 | ANN: trainLoss: 0.0327 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 411 | ANN: trainLoss: 0.0311 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 411 | ANN: trainLoss: 0.0346 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 411 | ANN: trainLoss: 0.0394 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 411 | ANN: trainLoss: 0.0446 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 411 | ANN: trainLoss: 0.0456 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 411 | ANN: trainLoss: 0.0449 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 411 | ANN: trainLoss: 0.0511 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 411 | ANN: trainLoss: 0.0497 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 411 | ANN: trainLoss: 0.0527 | trainAcc: 98.7047% (762/772)\n",
            "0 4 Epoch: 411 | ANN: testLoss: 0.7964 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 411 | ANN: testLoss: 0.8085 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 411 | ANN: testLoss: 0.7148 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 411 | ANN: testLoss: 0.5370 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 412 | ANN: trainLoss: 0.0487 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 412 | ANN: trainLoss: 0.0584 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 412 | ANN: trainLoss: 0.0471 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 412 | ANN: trainLoss: 0.0565 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 412 | ANN: trainLoss: 0.0527 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 412 | ANN: trainLoss: 0.0502 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 412 | ANN: trainLoss: 0.0540 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 412 | ANN: trainLoss: 0.0514 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 412 | ANN: trainLoss: 0.0558 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 412 | ANN: trainLoss: 0.0566 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 412 | ANN: trainLoss: 0.0548 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 412 | ANN: trainLoss: 0.0537 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 412 | ANN: trainLoss: 0.0508 | trainAcc: 97.6684% (754/772)\n",
            "0 4 Epoch: 412 | ANN: testLoss: 0.9075 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 412 | ANN: testLoss: 0.7559 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 412 | ANN: testLoss: 0.7217 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 412 | ANN: testLoss: 0.5427 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 413 | ANN: trainLoss: 0.0312 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 413 | ANN: trainLoss: 0.0521 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 413 | ANN: trainLoss: 0.0610 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 413 | ANN: trainLoss: 0.0613 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 413 | ANN: trainLoss: 0.0600 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 413 | ANN: trainLoss: 0.0525 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 413 | ANN: trainLoss: 0.0499 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 413 | ANN: trainLoss: 0.0461 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 413 | ANN: trainLoss: 0.0495 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 413 | ANN: trainLoss: 0.0512 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 413 | ANN: trainLoss: 0.0493 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 413 | ANN: trainLoss: 0.0485 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 413 | ANN: trainLoss: 0.0628 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 413 | ANN: testLoss: 0.9861 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 413 | ANN: testLoss: 0.8602 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 413 | ANN: testLoss: 0.7583 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 413 | ANN: testLoss: 0.5735 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 414 | ANN: trainLoss: 0.0387 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 414 | ANN: trainLoss: 0.0375 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 414 | ANN: trainLoss: 0.0381 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 414 | ANN: trainLoss: 0.0422 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 414 | ANN: trainLoss: 0.0423 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 414 | ANN: trainLoss: 0.0402 | trainAcc: 99.4792% (382/384)\n",
            "6 13 Epoch: 414 | ANN: trainLoss: 0.0396 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 414 | ANN: trainLoss: 0.0432 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 414 | ANN: trainLoss: 0.0470 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 414 | ANN: trainLoss: 0.0442 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 414 | ANN: trainLoss: 0.0423 | trainAcc: 99.0057% (697/704)\n",
            "11 13 Epoch: 414 | ANN: trainLoss: 0.0400 | trainAcc: 99.0885% (761/768)\n",
            "12 13 Epoch: 414 | ANN: trainLoss: 0.0476 | trainAcc: 99.0933% (765/772)\n",
            "0 4 Epoch: 414 | ANN: testLoss: 0.4562 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 414 | ANN: testLoss: 0.6064 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 414 | ANN: testLoss: 0.7178 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 414 | ANN: testLoss: 2.3296 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 415 | ANN: trainLoss: 0.0256 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 415 | ANN: trainLoss: 0.0587 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 415 | ANN: trainLoss: 0.0593 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 415 | ANN: trainLoss: 0.0546 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 415 | ANN: trainLoss: 0.0516 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 415 | ANN: trainLoss: 0.0544 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 415 | ANN: trainLoss: 0.0540 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 415 | ANN: trainLoss: 0.0509 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 415 | ANN: trainLoss: 0.0475 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 415 | ANN: trainLoss: 0.0451 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 415 | ANN: trainLoss: 0.0450 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 415 | ANN: trainLoss: 0.0435 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 415 | ANN: trainLoss: 0.2123 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 415 | ANN: testLoss: 0.7395 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 415 | ANN: testLoss: 0.9048 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 415 | ANN: testLoss: 0.7455 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 415 | ANN: testLoss: 1.0189 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 416 | ANN: trainLoss: 0.0480 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 416 | ANN: trainLoss: 0.0540 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 416 | ANN: trainLoss: 0.0479 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 416 | ANN: trainLoss: 0.0473 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 416 | ANN: trainLoss: 0.0539 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 416 | ANN: trainLoss: 0.0577 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 416 | ANN: trainLoss: 0.0558 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 416 | ANN: trainLoss: 0.0528 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 416 | ANN: trainLoss: 0.0505 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 416 | ANN: trainLoss: 0.0486 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 416 | ANN: trainLoss: 0.0489 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 416 | ANN: trainLoss: 0.0497 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 416 | ANN: trainLoss: 0.1303 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 416 | ANN: testLoss: 0.9974 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 416 | ANN: testLoss: 0.7154 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 416 | ANN: testLoss: 0.8048 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 416 | ANN: testLoss: 0.6036 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 417 | ANN: trainLoss: 0.0270 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 417 | ANN: trainLoss: 0.0252 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 417 | ANN: trainLoss: 0.0347 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 417 | ANN: trainLoss: 0.0479 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 417 | ANN: trainLoss: 0.0522 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 417 | ANN: trainLoss: 0.0503 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 417 | ANN: trainLoss: 0.0508 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 417 | ANN: trainLoss: 0.0463 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 417 | ANN: trainLoss: 0.0478 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 417 | ANN: trainLoss: 0.0525 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 417 | ANN: trainLoss: 0.0510 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 417 | ANN: trainLoss: 0.0488 | trainAcc: 98.8281% (759/768)\n",
            "12 13 Epoch: 417 | ANN: trainLoss: 0.1022 | trainAcc: 98.7047% (762/772)\n",
            "0 4 Epoch: 417 | ANN: testLoss: 0.6439 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 417 | ANN: testLoss: 0.7283 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 417 | ANN: testLoss: 0.7376 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 417 | ANN: testLoss: 0.5564 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 418 | ANN: trainLoss: 0.0402 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 418 | ANN: trainLoss: 0.0472 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 418 | ANN: trainLoss: 0.0739 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 418 | ANN: trainLoss: 0.0652 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 418 | ANN: trainLoss: 0.0685 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 418 | ANN: trainLoss: 0.0695 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 418 | ANN: trainLoss: 0.0720 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 418 | ANN: trainLoss: 0.0730 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 418 | ANN: trainLoss: 0.0686 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 418 | ANN: trainLoss: 0.0649 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 418 | ANN: trainLoss: 0.0654 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 418 | ANN: trainLoss: 0.0675 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 418 | ANN: trainLoss: 0.0698 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 418 | ANN: testLoss: 0.6160 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 418 | ANN: testLoss: 0.6821 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 418 | ANN: testLoss: 0.7251 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 418 | ANN: testLoss: 0.5439 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 419 | ANN: trainLoss: 0.0497 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 419 | ANN: trainLoss: 0.0721 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 419 | ANN: trainLoss: 0.0711 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 419 | ANN: trainLoss: 0.0791 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 419 | ANN: trainLoss: 0.0748 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 419 | ANN: trainLoss: 0.0740 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 419 | ANN: trainLoss: 0.0721 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 419 | ANN: trainLoss: 0.0662 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 419 | ANN: trainLoss: 0.0658 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 419 | ANN: trainLoss: 0.0611 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 419 | ANN: trainLoss: 0.0577 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 419 | ANN: trainLoss: 0.0600 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 419 | ANN: trainLoss: 0.0556 | trainAcc: 97.7979% (755/772)\n",
            "0 4 Epoch: 419 | ANN: testLoss: 0.5549 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 419 | ANN: testLoss: 0.6250 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 419 | ANN: testLoss: 0.7213 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 419 | ANN: testLoss: 0.5468 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 420 | ANN: trainLoss: 0.0238 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 420 | ANN: trainLoss: 0.0337 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 420 | ANN: trainLoss: 0.0380 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 420 | ANN: trainLoss: 0.0405 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 420 | ANN: trainLoss: 0.0396 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 420 | ANN: trainLoss: 0.0361 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 420 | ANN: trainLoss: 0.0350 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 420 | ANN: trainLoss: 0.0364 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 420 | ANN: trainLoss: 0.0385 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 420 | ANN: trainLoss: 0.0392 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 420 | ANN: trainLoss: 0.0379 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 420 | ANN: trainLoss: 0.0381 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 420 | ANN: trainLoss: 0.0905 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 420 | ANN: testLoss: 0.7877 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 420 | ANN: testLoss: 0.7022 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 420 | ANN: testLoss: 0.7252 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 420 | ANN: testLoss: 0.5439 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 421 | ANN: trainLoss: 0.0301 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 421 | ANN: trainLoss: 0.0403 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 421 | ANN: trainLoss: 0.0425 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 421 | ANN: trainLoss: 0.0365 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 421 | ANN: trainLoss: 0.0421 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 421 | ANN: trainLoss: 0.0461 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 421 | ANN: trainLoss: 0.0436 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 421 | ANN: trainLoss: 0.0476 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 421 | ANN: trainLoss: 0.0483 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 421 | ANN: trainLoss: 0.0457 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 421 | ANN: trainLoss: 0.0445 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 421 | ANN: trainLoss: 0.0428 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 421 | ANN: trainLoss: 0.0885 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 421 | ANN: testLoss: 1.0631 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 421 | ANN: testLoss: 0.8135 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 421 | ANN: testLoss: 0.7334 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 421 | ANN: testLoss: 0.5545 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 422 | ANN: trainLoss: 0.0440 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 422 | ANN: trainLoss: 0.0445 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 422 | ANN: trainLoss: 0.0443 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 422 | ANN: trainLoss: 0.0462 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 422 | ANN: trainLoss: 0.0422 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 422 | ANN: trainLoss: 0.0458 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 422 | ANN: trainLoss: 0.0512 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 422 | ANN: trainLoss: 0.0550 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 422 | ANN: trainLoss: 0.0551 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 422 | ANN: trainLoss: 0.0516 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 422 | ANN: trainLoss: 0.0524 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 422 | ANN: trainLoss: 0.0520 | trainAcc: 98.3073% (755/768)\n",
            "12 13 Epoch: 422 | ANN: trainLoss: 0.0489 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 422 | ANN: testLoss: 0.8901 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 422 | ANN: testLoss: 0.7252 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 422 | ANN: testLoss: 0.7409 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 422 | ANN: testLoss: 0.5557 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 423 | ANN: trainLoss: 0.1175 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 423 | ANN: trainLoss: 0.0792 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 423 | ANN: trainLoss: 0.0819 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 423 | ANN: trainLoss: 0.0761 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 423 | ANN: trainLoss: 0.0752 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 423 | ANN: trainLoss: 0.0720 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 423 | ANN: trainLoss: 0.0704 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 423 | ANN: trainLoss: 0.0653 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 423 | ANN: trainLoss: 0.0668 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 423 | ANN: trainLoss: 0.0661 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 423 | ANN: trainLoss: 0.0681 | trainAcc: 97.8693% (689/704)\n",
            "11 13 Epoch: 423 | ANN: trainLoss: 0.0651 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 423 | ANN: trainLoss: 0.1974 | trainAcc: 97.7979% (755/772)\n",
            "0 4 Epoch: 423 | ANN: testLoss: 0.7692 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 423 | ANN: testLoss: 0.7011 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 423 | ANN: testLoss: 0.7579 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 423 | ANN: testLoss: 0.5685 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 424 | ANN: trainLoss: 0.0355 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 424 | ANN: trainLoss: 0.0374 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 424 | ANN: trainLoss: 0.0370 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 424 | ANN: trainLoss: 0.0453 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 424 | ANN: trainLoss: 0.0390 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 424 | ANN: trainLoss: 0.0447 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 424 | ANN: trainLoss: 0.0505 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 424 | ANN: trainLoss: 0.0472 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 424 | ANN: trainLoss: 0.0524 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 424 | ANN: trainLoss: 0.0525 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 424 | ANN: trainLoss: 0.0517 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 424 | ANN: trainLoss: 0.0507 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 424 | ANN: trainLoss: 0.0501 | trainAcc: 97.6684% (754/772)\n",
            "0 4 Epoch: 424 | ANN: testLoss: 0.9217 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 424 | ANN: testLoss: 0.6938 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 424 | ANN: testLoss: 0.7366 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 424 | ANN: testLoss: 1.0356 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 425 | ANN: trainLoss: 0.0471 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 425 | ANN: trainLoss: 0.0482 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 425 | ANN: trainLoss: 0.0402 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 425 | ANN: trainLoss: 0.0562 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 425 | ANN: trainLoss: 0.0518 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 425 | ANN: trainLoss: 0.0502 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 425 | ANN: trainLoss: 0.0459 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 425 | ANN: trainLoss: 0.0467 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 425 | ANN: trainLoss: 0.0567 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 425 | ANN: trainLoss: 0.0552 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 425 | ANN: trainLoss: 0.0534 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 425 | ANN: trainLoss: 0.0544 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 425 | ANN: trainLoss: 0.1314 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 425 | ANN: testLoss: 0.5498 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 425 | ANN: testLoss: 0.6517 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 425 | ANN: testLoss: 0.7532 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 425 | ANN: testLoss: 0.5665 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 426 | ANN: trainLoss: 0.0262 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 426 | ANN: trainLoss: 0.0295 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 426 | ANN: trainLoss: 0.0365 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 426 | ANN: trainLoss: 0.0569 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 426 | ANN: trainLoss: 0.0504 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 426 | ANN: trainLoss: 0.0455 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 426 | ANN: trainLoss: 0.0489 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 426 | ANN: trainLoss: 0.0536 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 426 | ANN: trainLoss: 0.0525 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 426 | ANN: trainLoss: 0.0484 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 426 | ANN: trainLoss: 0.0481 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 426 | ANN: trainLoss: 0.0471 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 426 | ANN: trainLoss: 0.0438 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 426 | ANN: testLoss: 0.7704 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 426 | ANN: testLoss: 0.7959 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 426 | ANN: testLoss: 0.7273 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 426 | ANN: testLoss: 0.7383 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 427 | ANN: trainLoss: 0.0652 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 427 | ANN: trainLoss: 0.0394 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 427 | ANN: trainLoss: 0.0675 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 427 | ANN: trainLoss: 0.0634 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 427 | ANN: trainLoss: 0.0592 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 427 | ANN: trainLoss: 0.0539 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 427 | ANN: trainLoss: 0.0585 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 427 | ANN: trainLoss: 0.0575 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 427 | ANN: trainLoss: 0.0618 | trainAcc: 97.7431% (563/576)\n",
            "9 13 Epoch: 427 | ANN: trainLoss: 0.0593 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 427 | ANN: trainLoss: 0.0584 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 427 | ANN: trainLoss: 0.0559 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 427 | ANN: trainLoss: 0.2390 | trainAcc: 97.7979% (755/772)\n",
            "0 4 Epoch: 427 | ANN: testLoss: 0.7907 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 427 | ANN: testLoss: 0.6850 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 427 | ANN: testLoss: 0.7595 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 427 | ANN: testLoss: 0.5733 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 428 | ANN: trainLoss: 0.0500 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 428 | ANN: trainLoss: 0.0388 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 428 | ANN: trainLoss: 0.0464 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 428 | ANN: trainLoss: 0.0413 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 428 | ANN: trainLoss: 0.0447 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 428 | ANN: trainLoss: 0.0404 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 428 | ANN: trainLoss: 0.0382 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 428 | ANN: trainLoss: 0.0422 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 428 | ANN: trainLoss: 0.0443 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 428 | ANN: trainLoss: 0.0456 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 428 | ANN: trainLoss: 0.0439 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 428 | ANN: trainLoss: 0.0417 | trainAcc: 98.8281% (759/768)\n",
            "12 13 Epoch: 428 | ANN: trainLoss: 0.0387 | trainAcc: 98.8342% (763/772)\n",
            "0 4 Epoch: 428 | ANN: testLoss: 0.4895 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 428 | ANN: testLoss: 0.6189 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 428 | ANN: testLoss: 0.7539 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 428 | ANN: testLoss: 0.5666 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 429 | ANN: trainLoss: 0.0305 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 429 | ANN: trainLoss: 0.0267 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 429 | ANN: trainLoss: 0.0389 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 429 | ANN: trainLoss: 0.0424 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 429 | ANN: trainLoss: 0.0386 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 429 | ANN: trainLoss: 0.0364 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 429 | ANN: trainLoss: 0.0379 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 429 | ANN: trainLoss: 0.0372 | trainAcc: 99.2188% (508/512)\n",
            "8 13 Epoch: 429 | ANN: trainLoss: 0.0398 | trainAcc: 99.1319% (571/576)\n",
            "9 13 Epoch: 429 | ANN: trainLoss: 0.0427 | trainAcc: 99.0625% (634/640)\n",
            "10 13 Epoch: 429 | ANN: trainLoss: 0.0424 | trainAcc: 99.1477% (698/704)\n",
            "11 13 Epoch: 429 | ANN: trainLoss: 0.0454 | trainAcc: 98.9583% (760/768)\n",
            "12 13 Epoch: 429 | ANN: trainLoss: 0.0421 | trainAcc: 98.9637% (764/772)\n",
            "0 4 Epoch: 429 | ANN: testLoss: 0.7279 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 429 | ANN: testLoss: 0.7407 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 429 | ANN: testLoss: 0.7439 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 429 | ANN: testLoss: 0.5581 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 430 | ANN: trainLoss: 0.0386 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 430 | ANN: trainLoss: 0.0408 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 430 | ANN: trainLoss: 0.0342 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 430 | ANN: trainLoss: 0.0386 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 430 | ANN: trainLoss: 0.0340 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 430 | ANN: trainLoss: 0.0338 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 430 | ANN: trainLoss: 0.0379 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 430 | ANN: trainLoss: 0.0387 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 430 | ANN: trainLoss: 0.0385 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 430 | ANN: trainLoss: 0.0400 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 430 | ANN: trainLoss: 0.0386 | trainAcc: 99.0057% (697/704)\n",
            "11 13 Epoch: 430 | ANN: trainLoss: 0.0380 | trainAcc: 98.9583% (760/768)\n",
            "12 13 Epoch: 430 | ANN: trainLoss: 0.1581 | trainAcc: 98.8342% (763/772)\n",
            "0 4 Epoch: 430 | ANN: testLoss: 0.6411 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 430 | ANN: testLoss: 0.6552 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 430 | ANN: testLoss: 0.7418 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 430 | ANN: testLoss: 0.9024 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 431 | ANN: trainLoss: 0.0299 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 431 | ANN: trainLoss: 0.0728 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 431 | ANN: trainLoss: 0.0710 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 431 | ANN: trainLoss: 0.0636 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 431 | ANN: trainLoss: 0.0561 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 431 | ANN: trainLoss: 0.0593 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 431 | ANN: trainLoss: 0.0586 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 431 | ANN: trainLoss: 0.0551 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 431 | ANN: trainLoss: 0.0505 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 431 | ANN: trainLoss: 0.0475 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 431 | ANN: trainLoss: 0.0448 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 431 | ANN: trainLoss: 0.0443 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 431 | ANN: trainLoss: 0.0580 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 431 | ANN: testLoss: 0.9247 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 431 | ANN: testLoss: 0.7787 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 431 | ANN: testLoss: 0.7597 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 431 | ANN: testLoss: 0.5698 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 432 | ANN: trainLoss: 0.0420 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 432 | ANN: trainLoss: 0.0315 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 432 | ANN: trainLoss: 0.0333 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 432 | ANN: trainLoss: 0.0424 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 432 | ANN: trainLoss: 0.0451 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 432 | ANN: trainLoss: 0.0509 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 432 | ANN: trainLoss: 0.0550 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 432 | ANN: trainLoss: 0.0527 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 432 | ANN: trainLoss: 0.0603 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 432 | ANN: trainLoss: 0.0573 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 432 | ANN: trainLoss: 0.0591 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 432 | ANN: trainLoss: 0.0577 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 432 | ANN: trainLoss: 0.1690 | trainAcc: 97.7979% (755/772)\n",
            "0 4 Epoch: 432 | ANN: testLoss: 0.7214 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 432 | ANN: testLoss: 0.8321 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 432 | ANN: testLoss: 0.7786 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 432 | ANN: testLoss: 0.6048 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 433 | ANN: trainLoss: 0.0433 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 433 | ANN: trainLoss: 0.0566 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 433 | ANN: trainLoss: 0.0585 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 433 | ANN: trainLoss: 0.0536 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 433 | ANN: trainLoss: 0.0477 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 433 | ANN: trainLoss: 0.0451 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 433 | ANN: trainLoss: 0.0425 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 433 | ANN: trainLoss: 0.0468 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 433 | ANN: trainLoss: 0.0445 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 433 | ANN: trainLoss: 0.0454 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 433 | ANN: trainLoss: 0.0464 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 433 | ANN: trainLoss: 0.0458 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 433 | ANN: trainLoss: 0.0518 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 433 | ANN: testLoss: 0.8684 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 433 | ANN: testLoss: 0.8419 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 433 | ANN: testLoss: 0.7676 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 433 | ANN: testLoss: 0.5809 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 434 | ANN: trainLoss: 0.0543 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 434 | ANN: trainLoss: 0.0410 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 434 | ANN: trainLoss: 0.0594 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 434 | ANN: trainLoss: 0.0486 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 434 | ANN: trainLoss: 0.0467 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 434 | ANN: trainLoss: 0.0492 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 434 | ANN: trainLoss: 0.0482 | trainAcc: 99.1071% (444/448)\n",
            "7 13 Epoch: 434 | ANN: trainLoss: 0.0493 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 434 | ANN: trainLoss: 0.0473 | trainAcc: 98.9583% (570/576)\n",
            "9 13 Epoch: 434 | ANN: trainLoss: 0.0457 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 434 | ANN: trainLoss: 0.0450 | trainAcc: 98.8636% (696/704)\n",
            "11 13 Epoch: 434 | ANN: trainLoss: 0.0447 | trainAcc: 98.8281% (759/768)\n",
            "12 13 Epoch: 434 | ANN: trainLoss: 0.0602 | trainAcc: 98.8342% (763/772)\n",
            "0 4 Epoch: 434 | ANN: testLoss: 0.4473 | testAcc: 87.5000% (56/64)\n",
            "1 4 Epoch: 434 | ANN: testLoss: 0.5968 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 434 | ANN: testLoss: 0.7787 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 434 | ANN: testLoss: 0.5840 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 435 | ANN: trainLoss: 0.1008 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 435 | ANN: trainLoss: 0.0704 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 435 | ANN: trainLoss: 0.0539 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 435 | ANN: trainLoss: 0.0558 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 435 | ANN: trainLoss: 0.0606 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 435 | ANN: trainLoss: 0.0546 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 435 | ANN: trainLoss: 0.0522 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 435 | ANN: trainLoss: 0.0498 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 435 | ANN: trainLoss: 0.0483 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 435 | ANN: trainLoss: 0.0473 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 435 | ANN: trainLoss: 0.0473 | trainAcc: 98.8636% (696/704)\n",
            "11 13 Epoch: 435 | ANN: trainLoss: 0.0546 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 435 | ANN: trainLoss: 0.0507 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 435 | ANN: testLoss: 0.7250 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 435 | ANN: testLoss: 0.8365 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 435 | ANN: testLoss: 0.7428 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 435 | ANN: testLoss: 0.5834 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 436 | ANN: trainLoss: 0.0227 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 436 | ANN: trainLoss: 0.0349 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 436 | ANN: trainLoss: 0.0477 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 436 | ANN: trainLoss: 0.0494 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 436 | ANN: trainLoss: 0.0504 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 436 | ANN: trainLoss: 0.0490 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 436 | ANN: trainLoss: 0.0457 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 436 | ANN: trainLoss: 0.0458 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 436 | ANN: trainLoss: 0.0499 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 436 | ANN: trainLoss: 0.0492 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 436 | ANN: trainLoss: 0.0530 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 436 | ANN: trainLoss: 0.0506 | trainAcc: 98.3073% (755/768)\n",
            "12 13 Epoch: 436 | ANN: trainLoss: 0.0583 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 436 | ANN: testLoss: 0.7036 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 436 | ANN: testLoss: 0.7380 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 436 | ANN: testLoss: 0.7678 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 436 | ANN: testLoss: 0.5786 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 437 | ANN: trainLoss: 0.0682 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 437 | ANN: trainLoss: 0.0514 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 437 | ANN: trainLoss: 0.0504 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 437 | ANN: trainLoss: 0.0413 | trainAcc: 99.6094% (255/256)\n",
            "4 13 Epoch: 437 | ANN: trainLoss: 0.0381 | trainAcc: 99.6875% (319/320)\n",
            "5 13 Epoch: 437 | ANN: trainLoss: 0.0353 | trainAcc: 99.7396% (383/384)\n",
            "6 13 Epoch: 437 | ANN: trainLoss: 0.0416 | trainAcc: 99.5536% (446/448)\n",
            "7 13 Epoch: 437 | ANN: trainLoss: 0.0434 | trainAcc: 99.4141% (509/512)\n",
            "8 13 Epoch: 437 | ANN: trainLoss: 0.0430 | trainAcc: 99.3056% (572/576)\n",
            "9 13 Epoch: 437 | ANN: trainLoss: 0.0450 | trainAcc: 99.2188% (635/640)\n",
            "10 13 Epoch: 437 | ANN: trainLoss: 0.0440 | trainAcc: 99.1477% (698/704)\n",
            "11 13 Epoch: 437 | ANN: trainLoss: 0.0481 | trainAcc: 98.9583% (760/768)\n",
            "12 13 Epoch: 437 | ANN: trainLoss: 0.0444 | trainAcc: 98.9637% (764/772)\n",
            "0 4 Epoch: 437 | ANN: testLoss: 0.5745 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 437 | ANN: testLoss: 0.7953 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 437 | ANN: testLoss: 0.7833 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 437 | ANN: testLoss: 0.5994 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 438 | ANN: trainLoss: 0.0643 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 438 | ANN: trainLoss: 0.0421 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 438 | ANN: trainLoss: 0.0450 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 438 | ANN: trainLoss: 0.0481 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 438 | ANN: trainLoss: 0.0443 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 438 | ANN: trainLoss: 0.0403 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 438 | ANN: trainLoss: 0.0420 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 438 | ANN: trainLoss: 0.0402 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 438 | ANN: trainLoss: 0.0441 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 438 | ANN: trainLoss: 0.0444 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 438 | ANN: trainLoss: 0.0521 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 438 | ANN: trainLoss: 0.0547 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 438 | ANN: trainLoss: 0.1184 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 438 | ANN: testLoss: 0.7002 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 438 | ANN: testLoss: 0.6951 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 438 | ANN: testLoss: 0.7838 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 438 | ANN: testLoss: 0.5942 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 439 | ANN: trainLoss: 0.0239 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 439 | ANN: trainLoss: 0.0351 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 439 | ANN: trainLoss: 0.0330 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 439 | ANN: trainLoss: 0.0476 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 439 | ANN: trainLoss: 0.0470 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 439 | ANN: trainLoss: 0.0452 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 439 | ANN: trainLoss: 0.0450 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 439 | ANN: trainLoss: 0.0407 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 439 | ANN: trainLoss: 0.0480 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 439 | ANN: trainLoss: 0.0467 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 439 | ANN: trainLoss: 0.0441 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 439 | ANN: trainLoss: 0.0447 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 439 | ANN: trainLoss: 0.0417 | trainAcc: 98.7047% (762/772)\n",
            "0 4 Epoch: 439 | ANN: testLoss: 0.9574 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 439 | ANN: testLoss: 0.7948 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 439 | ANN: testLoss: 0.7444 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 439 | ANN: testLoss: 0.5886 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 440 | ANN: trainLoss: 0.1555 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 440 | ANN: trainLoss: 0.1334 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 440 | ANN: trainLoss: 0.0977 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 440 | ANN: trainLoss: 0.0832 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 440 | ANN: trainLoss: 0.0701 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 440 | ANN: trainLoss: 0.0630 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 440 | ANN: trainLoss: 0.0628 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 440 | ANN: trainLoss: 0.0591 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 440 | ANN: trainLoss: 0.0549 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 440 | ANN: trainLoss: 0.0539 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 440 | ANN: trainLoss: 0.0518 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 440 | ANN: trainLoss: 0.0516 | trainAcc: 98.8281% (759/768)\n",
            "12 13 Epoch: 440 | ANN: trainLoss: 0.1040 | trainAcc: 98.7047% (762/772)\n",
            "0 4 Epoch: 440 | ANN: testLoss: 0.9261 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 440 | ANN: testLoss: 0.8179 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 440 | ANN: testLoss: 0.7812 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 440 | ANN: testLoss: 0.5899 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 441 | ANN: trainLoss: 0.0309 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 441 | ANN: trainLoss: 0.0263 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 441 | ANN: trainLoss: 0.0282 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 441 | ANN: trainLoss: 0.0349 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 441 | ANN: trainLoss: 0.0371 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 441 | ANN: trainLoss: 0.0441 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 441 | ANN: trainLoss: 0.0425 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 441 | ANN: trainLoss: 0.0438 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 441 | ANN: trainLoss: 0.0410 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 441 | ANN: trainLoss: 0.0432 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 441 | ANN: trainLoss: 0.0432 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 441 | ANN: trainLoss: 0.0445 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 441 | ANN: trainLoss: 0.0513 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 441 | ANN: testLoss: 0.4205 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 441 | ANN: testLoss: 0.8006 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 441 | ANN: testLoss: 0.7498 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 441 | ANN: testLoss: 0.5685 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 442 | ANN: trainLoss: 0.0495 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 442 | ANN: trainLoss: 0.0423 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 442 | ANN: trainLoss: 0.0451 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 442 | ANN: trainLoss: 0.0470 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 442 | ANN: trainLoss: 0.0472 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 442 | ANN: trainLoss: 0.0558 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 442 | ANN: trainLoss: 0.0542 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 442 | ANN: trainLoss: 0.0547 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 442 | ANN: trainLoss: 0.0547 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 442 | ANN: trainLoss: 0.0599 | trainAcc: 98.1250% (628/640)\n",
            "10 13 Epoch: 442 | ANN: trainLoss: 0.0617 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 442 | ANN: trainLoss: 0.0615 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 442 | ANN: trainLoss: 0.0574 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 442 | ANN: testLoss: 1.0580 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 442 | ANN: testLoss: 0.9127 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 442 | ANN: testLoss: 0.7865 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 442 | ANN: testLoss: 0.5900 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 443 | ANN: trainLoss: 0.0702 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 443 | ANN: trainLoss: 0.0445 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 443 | ANN: trainLoss: 0.0353 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 443 | ANN: trainLoss: 0.0404 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 443 | ANN: trainLoss: 0.0397 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 443 | ANN: trainLoss: 0.0451 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 443 | ANN: trainLoss: 0.0469 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 443 | ANN: trainLoss: 0.0482 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 443 | ANN: trainLoss: 0.0449 | trainAcc: 98.9583% (570/576)\n",
            "9 13 Epoch: 443 | ANN: trainLoss: 0.0483 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 443 | ANN: trainLoss: 0.0485 | trainAcc: 98.8636% (696/704)\n",
            "11 13 Epoch: 443 | ANN: trainLoss: 0.0473 | trainAcc: 98.9583% (760/768)\n",
            "12 13 Epoch: 443 | ANN: trainLoss: 0.0510 | trainAcc: 98.9637% (764/772)\n",
            "0 4 Epoch: 443 | ANN: testLoss: 0.5697 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 443 | ANN: testLoss: 0.7060 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 443 | ANN: testLoss: 0.7504 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 443 | ANN: testLoss: 0.6320 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 444 | ANN: trainLoss: 0.0680 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 444 | ANN: trainLoss: 0.0700 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 444 | ANN: trainLoss: 0.0549 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 444 | ANN: trainLoss: 0.0479 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 444 | ANN: trainLoss: 0.0543 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 444 | ANN: trainLoss: 0.0595 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 444 | ANN: trainLoss: 0.0624 | trainAcc: 97.3214% (436/448)\n",
            "7 13 Epoch: 444 | ANN: trainLoss: 0.0617 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 444 | ANN: trainLoss: 0.0670 | trainAcc: 97.2222% (560/576)\n",
            "9 13 Epoch: 444 | ANN: trainLoss: 0.0657 | trainAcc: 97.3438% (623/640)\n",
            "10 13 Epoch: 444 | ANN: trainLoss: 0.0626 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 444 | ANN: trainLoss: 0.0599 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 444 | ANN: trainLoss: 0.0554 | trainAcc: 97.6684% (754/772)\n",
            "0 4 Epoch: 444 | ANN: testLoss: 0.4364 | testAcc: 87.5000% (56/64)\n",
            "1 4 Epoch: 444 | ANN: testLoss: 0.5720 | testAcc: 82.0312% (105/128)\n",
            "2 4 Epoch: 444 | ANN: testLoss: 0.7450 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 444 | ANN: testLoss: 0.5700 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 445 | ANN: trainLoss: 0.0583 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 445 | ANN: trainLoss: 0.0478 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 445 | ANN: trainLoss: 0.0468 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 445 | ANN: trainLoss: 0.0435 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 445 | ANN: trainLoss: 0.0564 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 445 | ANN: trainLoss: 0.0588 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 445 | ANN: trainLoss: 0.0550 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 445 | ANN: trainLoss: 0.0521 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 445 | ANN: trainLoss: 0.0506 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 445 | ANN: trainLoss: 0.0521 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 445 | ANN: trainLoss: 0.0492 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 445 | ANN: trainLoss: 0.0472 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 445 | ANN: trainLoss: 0.0468 | trainAcc: 98.7047% (762/772)\n",
            "0 4 Epoch: 445 | ANN: testLoss: 0.6079 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 445 | ANN: testLoss: 0.7628 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 445 | ANN: testLoss: 0.7593 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 445 | ANN: testLoss: 0.5697 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 446 | ANN: trainLoss: 0.0487 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 446 | ANN: trainLoss: 0.0854 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 446 | ANN: trainLoss: 0.0616 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 446 | ANN: trainLoss: 0.0559 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 446 | ANN: trainLoss: 0.0534 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 446 | ANN: trainLoss: 0.0501 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 446 | ANN: trainLoss: 0.0511 | trainAcc: 99.1071% (444/448)\n",
            "7 13 Epoch: 446 | ANN: trainLoss: 0.0484 | trainAcc: 99.2188% (508/512)\n",
            "8 13 Epoch: 446 | ANN: trainLoss: 0.0541 | trainAcc: 99.1319% (571/576)\n",
            "9 13 Epoch: 446 | ANN: trainLoss: 0.0514 | trainAcc: 99.2188% (635/640)\n",
            "10 13 Epoch: 446 | ANN: trainLoss: 0.0496 | trainAcc: 99.1477% (698/704)\n",
            "11 13 Epoch: 446 | ANN: trainLoss: 0.0499 | trainAcc: 99.0885% (761/768)\n",
            "12 13 Epoch: 446 | ANN: trainLoss: 0.0884 | trainAcc: 98.8342% (763/772)\n",
            "0 4 Epoch: 446 | ANN: testLoss: 0.6910 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 446 | ANN: testLoss: 0.8384 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 446 | ANN: testLoss: 0.7486 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 446 | ANN: testLoss: 1.7040 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 447 | ANN: trainLoss: 0.0288 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 447 | ANN: trainLoss: 0.0348 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 447 | ANN: trainLoss: 0.0388 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 447 | ANN: trainLoss: 0.0366 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 447 | ANN: trainLoss: 0.0426 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 447 | ANN: trainLoss: 0.0429 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 447 | ANN: trainLoss: 0.0424 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 447 | ANN: trainLoss: 0.0396 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 447 | ANN: trainLoss: 0.0389 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 447 | ANN: trainLoss: 0.0416 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 447 | ANN: trainLoss: 0.0425 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 447 | ANN: trainLoss: 0.0415 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 447 | ANN: trainLoss: 0.1038 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 447 | ANN: testLoss: 0.9016 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 447 | ANN: testLoss: 0.8141 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 447 | ANN: testLoss: 0.7210 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 447 | ANN: testLoss: 0.5430 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 448 | ANN: trainLoss: 0.0501 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 448 | ANN: trainLoss: 0.0443 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 448 | ANN: trainLoss: 0.0416 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 448 | ANN: trainLoss: 0.0500 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 448 | ANN: trainLoss: 0.0473 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 448 | ANN: trainLoss: 0.0492 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 448 | ANN: trainLoss: 0.0544 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 448 | ANN: trainLoss: 0.0550 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 448 | ANN: trainLoss: 0.0531 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 448 | ANN: trainLoss: 0.0518 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 448 | ANN: trainLoss: 0.0540 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 448 | ANN: trainLoss: 0.0523 | trainAcc: 98.8281% (759/768)\n",
            "12 13 Epoch: 448 | ANN: trainLoss: 0.0526 | trainAcc: 98.8342% (763/772)\n",
            "0 4 Epoch: 448 | ANN: testLoss: 0.7519 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 448 | ANN: testLoss: 0.7494 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 448 | ANN: testLoss: 0.7597 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 448 | ANN: testLoss: 0.5880 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 449 | ANN: trainLoss: 0.0538 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 449 | ANN: trainLoss: 0.0416 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 449 | ANN: trainLoss: 0.0468 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 449 | ANN: trainLoss: 0.0762 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 449 | ANN: trainLoss: 0.0695 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 449 | ANN: trainLoss: 0.0711 | trainAcc: 97.9167% (376/384)\n",
            "6 13 Epoch: 449 | ANN: trainLoss: 0.0703 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 449 | ANN: trainLoss: 0.0737 | trainAcc: 97.4609% (499/512)\n",
            "8 13 Epoch: 449 | ANN: trainLoss: 0.0704 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 449 | ANN: trainLoss: 0.0673 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 449 | ANN: trainLoss: 0.0680 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 449 | ANN: trainLoss: 0.0666 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 449 | ANN: trainLoss: 0.0858 | trainAcc: 97.6684% (754/772)\n",
            "0 4 Epoch: 449 | ANN: testLoss: 0.9818 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 449 | ANN: testLoss: 0.7167 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 449 | ANN: testLoss: 0.7679 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 449 | ANN: testLoss: 0.5803 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 450 | ANN: trainLoss: 0.0655 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 450 | ANN: trainLoss: 0.0619 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 450 | ANN: trainLoss: 0.0738 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 450 | ANN: trainLoss: 0.0757 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 450 | ANN: trainLoss: 0.0686 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 450 | ANN: trainLoss: 0.0640 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 450 | ANN: trainLoss: 0.0624 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 450 | ANN: trainLoss: 0.0570 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 450 | ANN: trainLoss: 0.0574 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 450 | ANN: trainLoss: 0.0535 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 450 | ANN: trainLoss: 0.0533 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 450 | ANN: trainLoss: 0.0517 | trainAcc: 98.8281% (759/768)\n",
            "12 13 Epoch: 450 | ANN: trainLoss: 0.0742 | trainAcc: 98.7047% (762/772)\n",
            "0 4 Epoch: 450 | ANN: testLoss: 0.7956 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 450 | ANN: testLoss: 0.8721 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 450 | ANN: testLoss: 0.7765 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 450 | ANN: testLoss: 0.5825 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 451 | ANN: trainLoss: 0.1526 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 451 | ANN: trainLoss: 0.0832 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 451 | ANN: trainLoss: 0.0662 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 451 | ANN: trainLoss: 0.0580 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 451 | ANN: trainLoss: 0.0517 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 451 | ANN: trainLoss: 0.0512 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 451 | ANN: trainLoss: 0.0498 | trainAcc: 99.1071% (444/448)\n",
            "7 13 Epoch: 451 | ANN: trainLoss: 0.0533 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 451 | ANN: trainLoss: 0.0498 | trainAcc: 98.9583% (570/576)\n",
            "9 13 Epoch: 451 | ANN: trainLoss: 0.0566 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 451 | ANN: trainLoss: 0.0550 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 451 | ANN: trainLoss: 0.0535 | trainAcc: 98.8281% (759/768)\n",
            "12 13 Epoch: 451 | ANN: trainLoss: 0.0497 | trainAcc: 98.8342% (763/772)\n",
            "0 4 Epoch: 451 | ANN: testLoss: 0.6572 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 451 | ANN: testLoss: 0.7545 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 451 | ANN: testLoss: 0.7754 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 451 | ANN: testLoss: 1.1296 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 452 | ANN: trainLoss: 0.0542 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 452 | ANN: trainLoss: 0.0530 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 452 | ANN: trainLoss: 0.0539 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 452 | ANN: trainLoss: 0.0603 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 452 | ANN: trainLoss: 0.0541 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 452 | ANN: trainLoss: 0.0512 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 452 | ANN: trainLoss: 0.0485 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 452 | ANN: trainLoss: 0.0520 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 452 | ANN: trainLoss: 0.0485 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 452 | ANN: trainLoss: 0.0470 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 452 | ANN: trainLoss: 0.0457 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 452 | ANN: trainLoss: 0.0435 | trainAcc: 98.8281% (759/768)\n",
            "12 13 Epoch: 452 | ANN: trainLoss: 0.0488 | trainAcc: 98.8342% (763/772)\n",
            "0 4 Epoch: 452 | ANN: testLoss: 0.8090 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 452 | ANN: testLoss: 0.7380 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 452 | ANN: testLoss: 0.7797 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 452 | ANN: testLoss: 0.5848 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 453 | ANN: trainLoss: 0.0739 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 453 | ANN: trainLoss: 0.0920 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 453 | ANN: trainLoss: 0.0813 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 453 | ANN: trainLoss: 0.0820 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 453 | ANN: trainLoss: 0.0790 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 453 | ANN: trainLoss: 0.0795 | trainAcc: 97.1354% (373/384)\n",
            "6 13 Epoch: 453 | ANN: trainLoss: 0.0733 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 453 | ANN: trainLoss: 0.0691 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 453 | ANN: trainLoss: 0.0698 | trainAcc: 97.5694% (562/576)\n",
            "9 13 Epoch: 453 | ANN: trainLoss: 0.0720 | trainAcc: 97.5000% (624/640)\n",
            "10 13 Epoch: 453 | ANN: trainLoss: 0.0678 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 453 | ANN: trainLoss: 0.0669 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 453 | ANN: trainLoss: 0.0756 | trainAcc: 97.6684% (754/772)\n",
            "0 4 Epoch: 453 | ANN: testLoss: 0.8313 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 453 | ANN: testLoss: 0.6216 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 453 | ANN: testLoss: 0.7681 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 453 | ANN: testLoss: 0.5763 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 454 | ANN: trainLoss: 0.0520 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 454 | ANN: trainLoss: 0.0630 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 454 | ANN: trainLoss: 0.0674 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 454 | ANN: trainLoss: 0.0546 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 454 | ANN: trainLoss: 0.0575 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 454 | ANN: trainLoss: 0.0519 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 454 | ANN: trainLoss: 0.0519 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 454 | ANN: trainLoss: 0.0491 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 454 | ANN: trainLoss: 0.0522 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 454 | ANN: trainLoss: 0.0517 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 454 | ANN: trainLoss: 0.0487 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 454 | ANN: trainLoss: 0.0489 | trainAcc: 98.3073% (755/768)\n",
            "12 13 Epoch: 454 | ANN: trainLoss: 0.0723 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 454 | ANN: testLoss: 0.6004 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 454 | ANN: testLoss: 0.6910 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 454 | ANN: testLoss: 0.7205 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 454 | ANN: testLoss: 0.5421 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 455 | ANN: trainLoss: 0.0165 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 455 | ANN: trainLoss: 0.0497 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 455 | ANN: trainLoss: 0.0411 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 455 | ANN: trainLoss: 0.0364 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 455 | ANN: trainLoss: 0.0333 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 455 | ANN: trainLoss: 0.0357 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 455 | ANN: trainLoss: 0.0322 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 455 | ANN: trainLoss: 0.0405 | trainAcc: 99.2188% (508/512)\n",
            "8 13 Epoch: 455 | ANN: trainLoss: 0.0421 | trainAcc: 99.1319% (571/576)\n",
            "9 13 Epoch: 455 | ANN: trainLoss: 0.0444 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 455 | ANN: trainLoss: 0.0434 | trainAcc: 99.0057% (697/704)\n",
            "11 13 Epoch: 455 | ANN: trainLoss: 0.0455 | trainAcc: 98.9583% (760/768)\n",
            "12 13 Epoch: 455 | ANN: trainLoss: 0.0990 | trainAcc: 98.8342% (763/772)\n",
            "0 4 Epoch: 455 | ANN: testLoss: 0.5014 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 455 | ANN: testLoss: 0.7699 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 455 | ANN: testLoss: 0.7354 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 455 | ANN: testLoss: 0.5523 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 456 | ANN: trainLoss: 0.0147 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 456 | ANN: trainLoss: 0.0304 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 456 | ANN: trainLoss: 0.0264 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 456 | ANN: trainLoss: 0.0389 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 456 | ANN: trainLoss: 0.0417 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 456 | ANN: trainLoss: 0.0422 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 456 | ANN: trainLoss: 0.0403 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 456 | ANN: trainLoss: 0.0377 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 456 | ANN: trainLoss: 0.0436 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 456 | ANN: trainLoss: 0.0424 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 456 | ANN: trainLoss: 0.0414 | trainAcc: 98.8636% (696/704)\n",
            "11 13 Epoch: 456 | ANN: trainLoss: 0.0405 | trainAcc: 98.8281% (759/768)\n",
            "12 13 Epoch: 456 | ANN: trainLoss: 0.0394 | trainAcc: 98.8342% (763/772)\n",
            "0 4 Epoch: 456 | ANN: testLoss: 0.6981 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 456 | ANN: testLoss: 0.6808 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 456 | ANN: testLoss: 0.7603 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 456 | ANN: testLoss: 0.5703 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 457 | ANN: trainLoss: 0.0319 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 457 | ANN: trainLoss: 0.0434 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 457 | ANN: trainLoss: 0.0506 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 457 | ANN: trainLoss: 0.0496 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 457 | ANN: trainLoss: 0.0467 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 457 | ANN: trainLoss: 0.0426 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 457 | ANN: trainLoss: 0.0504 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 457 | ANN: trainLoss: 0.0569 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 457 | ANN: trainLoss: 0.0541 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 457 | ANN: trainLoss: 0.0538 | trainAcc: 97.8125% (626/640)\n",
            "10 13 Epoch: 457 | ANN: trainLoss: 0.0555 | trainAcc: 97.7273% (688/704)\n",
            "11 13 Epoch: 457 | ANN: trainLoss: 0.0529 | trainAcc: 97.9167% (752/768)\n",
            "12 13 Epoch: 457 | ANN: trainLoss: 0.0777 | trainAcc: 97.7979% (755/772)\n",
            "0 4 Epoch: 457 | ANN: testLoss: 1.1416 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 457 | ANN: testLoss: 0.8520 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 457 | ANN: testLoss: 0.7488 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 457 | ANN: testLoss: 0.7776 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 458 | ANN: trainLoss: 0.0949 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 458 | ANN: trainLoss: 0.0658 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 458 | ANN: trainLoss: 0.0507 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 458 | ANN: trainLoss: 0.0500 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 458 | ANN: trainLoss: 0.0451 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 458 | ANN: trainLoss: 0.0462 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 458 | ANN: trainLoss: 0.0446 | trainAcc: 99.1071% (444/448)\n",
            "7 13 Epoch: 458 | ANN: trainLoss: 0.0456 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 458 | ANN: trainLoss: 0.0449 | trainAcc: 98.9583% (570/576)\n",
            "9 13 Epoch: 458 | ANN: trainLoss: 0.0422 | trainAcc: 99.0625% (634/640)\n",
            "10 13 Epoch: 458 | ANN: trainLoss: 0.0432 | trainAcc: 99.1477% (698/704)\n",
            "11 13 Epoch: 458 | ANN: trainLoss: 0.0417 | trainAcc: 99.2188% (762/768)\n",
            "12 13 Epoch: 458 | ANN: trainLoss: 0.0431 | trainAcc: 99.2228% (766/772)\n",
            "0 4 Epoch: 458 | ANN: testLoss: 0.6320 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 458 | ANN: testLoss: 0.8483 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 458 | ANN: testLoss: 0.7729 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 458 | ANN: testLoss: 0.5910 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 459 | ANN: trainLoss: 0.0450 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 459 | ANN: trainLoss: 0.0306 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 459 | ANN: trainLoss: 0.0295 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 459 | ANN: trainLoss: 0.0307 | trainAcc: 99.6094% (255/256)\n",
            "4 13 Epoch: 459 | ANN: trainLoss: 0.0280 | trainAcc: 99.6875% (319/320)\n",
            "5 13 Epoch: 459 | ANN: trainLoss: 0.0321 | trainAcc: 99.4792% (382/384)\n",
            "6 13 Epoch: 459 | ANN: trainLoss: 0.0331 | trainAcc: 99.5536% (446/448)\n",
            "7 13 Epoch: 459 | ANN: trainLoss: 0.0325 | trainAcc: 99.6094% (510/512)\n",
            "8 13 Epoch: 459 | ANN: trainLoss: 0.0429 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 459 | ANN: trainLoss: 0.0444 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 459 | ANN: trainLoss: 0.0437 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 459 | ANN: trainLoss: 0.0443 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 459 | ANN: trainLoss: 0.0731 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 459 | ANN: testLoss: 0.4620 | testAcc: 90.6250% (58/64)\n",
            "1 4 Epoch: 459 | ANN: testLoss: 0.7639 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 459 | ANN: testLoss: 0.7720 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 459 | ANN: testLoss: 0.5937 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 460 | ANN: trainLoss: 0.0177 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 460 | ANN: trainLoss: 0.0311 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 460 | ANN: trainLoss: 0.0407 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 460 | ANN: trainLoss: 0.0403 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 460 | ANN: trainLoss: 0.0400 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 460 | ANN: trainLoss: 0.0439 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 460 | ANN: trainLoss: 0.0441 | trainAcc: 99.1071% (444/448)\n",
            "7 13 Epoch: 460 | ANN: trainLoss: 0.0436 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 460 | ANN: trainLoss: 0.0423 | trainAcc: 98.9583% (570/576)\n",
            "9 13 Epoch: 460 | ANN: trainLoss: 0.0502 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 460 | ANN: trainLoss: 0.0483 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 460 | ANN: trainLoss: 0.0487 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 460 | ANN: trainLoss: 0.2991 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 460 | ANN: testLoss: 0.6927 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 460 | ANN: testLoss: 0.7242 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 460 | ANN: testLoss: 0.7661 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 460 | ANN: testLoss: 2.7493 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 461 | ANN: trainLoss: 0.0226 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 461 | ANN: trainLoss: 0.0224 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 461 | ANN: trainLoss: 0.0216 | trainAcc: 100.0000% (192/192)\n",
            "3 13 Epoch: 461 | ANN: trainLoss: 0.0237 | trainAcc: 99.6094% (255/256)\n",
            "4 13 Epoch: 461 | ANN: trainLoss: 0.0264 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 461 | ANN: trainLoss: 0.0258 | trainAcc: 99.4792% (382/384)\n",
            "6 13 Epoch: 461 | ANN: trainLoss: 0.0282 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 461 | ANN: trainLoss: 0.0283 | trainAcc: 99.4141% (509/512)\n",
            "8 13 Epoch: 461 | ANN: trainLoss: 0.0294 | trainAcc: 99.3056% (572/576)\n",
            "9 13 Epoch: 461 | ANN: trainLoss: 0.0278 | trainAcc: 99.3750% (636/640)\n",
            "10 13 Epoch: 461 | ANN: trainLoss: 0.0295 | trainAcc: 99.2898% (699/704)\n",
            "11 13 Epoch: 461 | ANN: trainLoss: 0.0297 | trainAcc: 99.3490% (763/768)\n",
            "12 13 Epoch: 461 | ANN: trainLoss: 0.0277 | trainAcc: 99.3523% (767/772)\n",
            "0 4 Epoch: 461 | ANN: testLoss: 0.9065 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 461 | ANN: testLoss: 0.6762 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 461 | ANN: testLoss: 0.7654 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 461 | ANN: testLoss: 0.5759 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 462 | ANN: trainLoss: 0.0436 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 462 | ANN: trainLoss: 0.0542 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 462 | ANN: trainLoss: 0.0480 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 462 | ANN: trainLoss: 0.0708 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 462 | ANN: trainLoss: 0.0620 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 462 | ANN: trainLoss: 0.0548 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 462 | ANN: trainLoss: 0.0597 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 462 | ANN: trainLoss: 0.0563 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 462 | ANN: trainLoss: 0.0577 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 462 | ANN: trainLoss: 0.0552 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 462 | ANN: trainLoss: 0.0588 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 462 | ANN: trainLoss: 0.0572 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 462 | ANN: trainLoss: 0.0710 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 462 | ANN: testLoss: 0.7173 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 462 | ANN: testLoss: 0.7183 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 462 | ANN: testLoss: 0.7757 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 462 | ANN: testLoss: 0.5923 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 463 | ANN: trainLoss: 0.0319 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 463 | ANN: trainLoss: 0.0354 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 463 | ANN: trainLoss: 0.0543 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 463 | ANN: trainLoss: 0.0708 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 463 | ANN: trainLoss: 0.0613 | trainAcc: 97.1875% (311/320)\n",
            "5 13 Epoch: 463 | ANN: trainLoss: 0.0586 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 463 | ANN: trainLoss: 0.0513 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 463 | ANN: trainLoss: 0.0531 | trainAcc: 97.8516% (501/512)\n",
            "8 13 Epoch: 463 | ANN: trainLoss: 0.0522 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 463 | ANN: trainLoss: 0.0519 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 463 | ANN: trainLoss: 0.0525 | trainAcc: 98.0114% (690/704)\n",
            "11 13 Epoch: 463 | ANN: trainLoss: 0.0590 | trainAcc: 97.7865% (751/768)\n",
            "12 13 Epoch: 463 | ANN: trainLoss: 0.0893 | trainAcc: 97.6684% (754/772)\n",
            "0 4 Epoch: 463 | ANN: testLoss: 0.9592 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 463 | ANN: testLoss: 0.8031 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 463 | ANN: testLoss: 0.7752 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 463 | ANN: testLoss: 1.3419 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 464 | ANN: trainLoss: 0.0333 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 464 | ANN: trainLoss: 0.0362 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 464 | ANN: trainLoss: 0.0398 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 464 | ANN: trainLoss: 0.0398 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 464 | ANN: trainLoss: 0.0463 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 464 | ANN: trainLoss: 0.0478 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 464 | ANN: trainLoss: 0.0483 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 464 | ANN: trainLoss: 0.0503 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 464 | ANN: trainLoss: 0.0559 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 464 | ANN: trainLoss: 0.0514 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 464 | ANN: trainLoss: 0.0507 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 464 | ANN: trainLoss: 0.0491 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 464 | ANN: trainLoss: 0.0507 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 464 | ANN: testLoss: 0.7629 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 464 | ANN: testLoss: 0.7483 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 464 | ANN: testLoss: 0.7604 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 464 | ANN: testLoss: 0.6961 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 465 | ANN: trainLoss: 0.0407 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 465 | ANN: trainLoss: 0.0400 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 465 | ANN: trainLoss: 0.0399 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 465 | ANN: trainLoss: 0.0336 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 465 | ANN: trainLoss: 0.0396 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 465 | ANN: trainLoss: 0.0384 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 465 | ANN: trainLoss: 0.0485 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 465 | ANN: trainLoss: 0.0479 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 465 | ANN: trainLoss: 0.0513 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 465 | ANN: trainLoss: 0.0530 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 465 | ANN: trainLoss: 0.0518 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 465 | ANN: trainLoss: 0.0524 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 465 | ANN: trainLoss: 0.1254 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 465 | ANN: testLoss: 0.6887 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 465 | ANN: testLoss: 0.7791 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 465 | ANN: testLoss: 0.7908 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 465 | ANN: testLoss: 0.5955 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 466 | ANN: trainLoss: 0.0460 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 466 | ANN: trainLoss: 0.0435 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 466 | ANN: trainLoss: 0.0369 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 466 | ANN: trainLoss: 0.0345 | trainAcc: 99.6094% (255/256)\n",
            "4 13 Epoch: 466 | ANN: trainLoss: 0.0509 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 466 | ANN: trainLoss: 0.0530 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 466 | ANN: trainLoss: 0.0511 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 466 | ANN: trainLoss: 0.0488 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 466 | ANN: trainLoss: 0.0552 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 466 | ANN: trainLoss: 0.0523 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 466 | ANN: trainLoss: 0.0515 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 466 | ANN: trainLoss: 0.0512 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 466 | ANN: trainLoss: 0.0474 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 466 | ANN: testLoss: 0.7209 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 466 | ANN: testLoss: 0.7253 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 466 | ANN: testLoss: 0.7520 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 466 | ANN: testLoss: 0.5816 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 467 | ANN: trainLoss: 0.0308 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 467 | ANN: trainLoss: 0.0311 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 467 | ANN: trainLoss: 0.0558 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 467 | ANN: trainLoss: 0.0501 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 467 | ANN: trainLoss: 0.0479 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 467 | ANN: trainLoss: 0.0477 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 467 | ANN: trainLoss: 0.0427 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 467 | ANN: trainLoss: 0.0392 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 467 | ANN: trainLoss: 0.0391 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 467 | ANN: trainLoss: 0.0399 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 467 | ANN: trainLoss: 0.0408 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 467 | ANN: trainLoss: 0.0447 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 467 | ANN: trainLoss: 0.0419 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 467 | ANN: testLoss: 0.7962 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 467 | ANN: testLoss: 0.7481 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 467 | ANN: testLoss: 0.7634 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 467 | ANN: testLoss: 0.6488 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 468 | ANN: trainLoss: 0.0243 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 468 | ANN: trainLoss: 0.0511 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 468 | ANN: trainLoss: 0.0876 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 468 | ANN: trainLoss: 0.0713 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 468 | ANN: trainLoss: 0.0632 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 468 | ANN: trainLoss: 0.0636 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 468 | ANN: trainLoss: 0.0590 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 468 | ANN: trainLoss: 0.0525 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 468 | ANN: trainLoss: 0.0526 | trainAcc: 98.9583% (570/576)\n",
            "9 13 Epoch: 468 | ANN: trainLoss: 0.0496 | trainAcc: 99.0625% (634/640)\n",
            "10 13 Epoch: 468 | ANN: trainLoss: 0.0485 | trainAcc: 99.0057% (697/704)\n",
            "11 13 Epoch: 468 | ANN: trainLoss: 0.0473 | trainAcc: 99.0885% (761/768)\n",
            "12 13 Epoch: 468 | ANN: trainLoss: 0.1556 | trainAcc: 98.8342% (763/772)\n",
            "0 4 Epoch: 468 | ANN: testLoss: 0.7340 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 468 | ANN: testLoss: 0.6961 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 468 | ANN: testLoss: 0.7704 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 468 | ANN: testLoss: 0.5781 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 469 | ANN: trainLoss: 0.0172 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 469 | ANN: trainLoss: 0.0340 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 469 | ANN: trainLoss: 0.0320 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 469 | ANN: trainLoss: 0.0323 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 469 | ANN: trainLoss: 0.0306 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 469 | ANN: trainLoss: 0.0383 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 469 | ANN: trainLoss: 0.0387 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 469 | ANN: trainLoss: 0.0420 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 469 | ANN: trainLoss: 0.0425 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 469 | ANN: trainLoss: 0.0485 | trainAcc: 98.1250% (628/640)\n",
            "10 13 Epoch: 469 | ANN: trainLoss: 0.0465 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 469 | ANN: trainLoss: 0.0459 | trainAcc: 98.3073% (755/768)\n",
            "12 13 Epoch: 469 | ANN: trainLoss: 0.0617 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 469 | ANN: testLoss: 0.6902 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 469 | ANN: testLoss: 0.6642 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 469 | ANN: testLoss: 0.7477 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 469 | ANN: testLoss: 0.5608 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 470 | ANN: trainLoss: 0.0642 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 470 | ANN: trainLoss: 0.0459 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 470 | ANN: trainLoss: 0.0486 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 470 | ANN: trainLoss: 0.0485 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 470 | ANN: trainLoss: 0.0453 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 470 | ANN: trainLoss: 0.0437 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 470 | ANN: trainLoss: 0.0496 | trainAcc: 98.6607% (442/448)\n",
            "7 13 Epoch: 470 | ANN: trainLoss: 0.0463 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 470 | ANN: trainLoss: 0.0444 | trainAcc: 98.9583% (570/576)\n",
            "9 13 Epoch: 470 | ANN: trainLoss: 0.0424 | trainAcc: 99.0625% (634/640)\n",
            "10 13 Epoch: 470 | ANN: trainLoss: 0.0416 | trainAcc: 99.0057% (697/704)\n",
            "11 13 Epoch: 470 | ANN: trainLoss: 0.0392 | trainAcc: 99.0885% (761/768)\n",
            "12 13 Epoch: 470 | ANN: trainLoss: 0.0560 | trainAcc: 98.9637% (764/772)\n",
            "0 4 Epoch: 470 | ANN: testLoss: 0.6996 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 470 | ANN: testLoss: 0.6997 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 470 | ANN: testLoss: 0.7489 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 470 | ANN: testLoss: 0.6457 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 471 | ANN: trainLoss: 0.0413 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 471 | ANN: trainLoss: 0.0517 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 471 | ANN: trainLoss: 0.0511 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 471 | ANN: trainLoss: 0.0476 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 471 | ANN: trainLoss: 0.0452 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 471 | ANN: trainLoss: 0.0388 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 471 | ANN: trainLoss: 0.0503 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 471 | ANN: trainLoss: 0.0498 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 471 | ANN: trainLoss: 0.0486 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 471 | ANN: trainLoss: 0.0467 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 471 | ANN: trainLoss: 0.0468 | trainAcc: 99.0057% (697/704)\n",
            "11 13 Epoch: 471 | ANN: trainLoss: 0.0458 | trainAcc: 99.0885% (761/768)\n",
            "12 13 Epoch: 471 | ANN: trainLoss: 0.1718 | trainAcc: 98.7047% (762/772)\n",
            "0 4 Epoch: 471 | ANN: testLoss: 0.7250 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 471 | ANN: testLoss: 0.6646 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 471 | ANN: testLoss: 0.7715 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 471 | ANN: testLoss: 0.7492 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 472 | ANN: trainLoss: 0.0289 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 472 | ANN: trainLoss: 0.0502 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 472 | ANN: trainLoss: 0.0483 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 472 | ANN: trainLoss: 0.0452 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 472 | ANN: trainLoss: 0.0403 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 472 | ANN: trainLoss: 0.0445 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 472 | ANN: trainLoss: 0.0436 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 472 | ANN: trainLoss: 0.0402 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 472 | ANN: trainLoss: 0.0478 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 472 | ANN: trainLoss: 0.0470 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 472 | ANN: trainLoss: 0.0460 | trainAcc: 98.7216% (695/704)\n",
            "11 13 Epoch: 472 | ANN: trainLoss: 0.0472 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 472 | ANN: trainLoss: 0.2292 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 472 | ANN: testLoss: 0.7101 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 472 | ANN: testLoss: 0.7467 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 472 | ANN: testLoss: 0.7862 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 472 | ANN: testLoss: 0.5912 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 473 | ANN: trainLoss: 0.0551 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 473 | ANN: trainLoss: 0.0722 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 473 | ANN: trainLoss: 0.0648 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 473 | ANN: trainLoss: 0.0579 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 473 | ANN: trainLoss: 0.0552 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 473 | ANN: trainLoss: 0.0550 | trainAcc: 97.6562% (375/384)\n",
            "6 13 Epoch: 473 | ANN: trainLoss: 0.0515 | trainAcc: 97.9911% (439/448)\n",
            "7 13 Epoch: 473 | ANN: trainLoss: 0.0534 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 473 | ANN: trainLoss: 0.0532 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 473 | ANN: trainLoss: 0.0532 | trainAcc: 97.9688% (627/640)\n",
            "10 13 Epoch: 473 | ANN: trainLoss: 0.0507 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 473 | ANN: trainLoss: 0.0495 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 473 | ANN: trainLoss: 0.0458 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 473 | ANN: testLoss: 0.7677 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 473 | ANN: testLoss: 0.8505 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 473 | ANN: testLoss: 0.7781 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 473 | ANN: testLoss: 0.5887 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 474 | ANN: trainLoss: 0.0273 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 474 | ANN: trainLoss: 0.0458 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 474 | ANN: trainLoss: 0.0483 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 474 | ANN: trainLoss: 0.0492 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 474 | ANN: trainLoss: 0.0468 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 474 | ANN: trainLoss: 0.0425 | trainAcc: 98.6979% (379/384)\n",
            "6 13 Epoch: 474 | ANN: trainLoss: 0.0400 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 474 | ANN: trainLoss: 0.0410 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 474 | ANN: trainLoss: 0.0409 | trainAcc: 98.9583% (570/576)\n",
            "9 13 Epoch: 474 | ANN: trainLoss: 0.0401 | trainAcc: 99.0625% (634/640)\n",
            "10 13 Epoch: 474 | ANN: trainLoss: 0.0437 | trainAcc: 98.8636% (696/704)\n",
            "11 13 Epoch: 474 | ANN: trainLoss: 0.0441 | trainAcc: 98.9583% (760/768)\n",
            "12 13 Epoch: 474 | ANN: trainLoss: 0.0465 | trainAcc: 98.9637% (764/772)\n",
            "0 4 Epoch: 474 | ANN: testLoss: 0.9945 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 474 | ANN: testLoss: 0.9466 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 474 | ANN: testLoss: 0.7648 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 474 | ANN: testLoss: 0.5757 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 475 | ANN: trainLoss: 0.0397 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 475 | ANN: trainLoss: 0.0363 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 475 | ANN: trainLoss: 0.0699 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 475 | ANN: trainLoss: 0.0631 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 475 | ANN: trainLoss: 0.0603 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 475 | ANN: trainLoss: 0.0596 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 475 | ANN: trainLoss: 0.0586 | trainAcc: 97.5446% (437/448)\n",
            "7 13 Epoch: 475 | ANN: trainLoss: 0.0606 | trainAcc: 97.2656% (498/512)\n",
            "8 13 Epoch: 475 | ANN: trainLoss: 0.0629 | trainAcc: 96.8750% (558/576)\n",
            "9 13 Epoch: 475 | ANN: trainLoss: 0.0603 | trainAcc: 97.1875% (622/640)\n",
            "10 13 Epoch: 475 | ANN: trainLoss: 0.0571 | trainAcc: 97.4432% (686/704)\n",
            "11 13 Epoch: 475 | ANN: trainLoss: 0.0594 | trainAcc: 97.2656% (747/768)\n",
            "12 13 Epoch: 475 | ANN: trainLoss: 0.0551 | trainAcc: 97.2798% (751/772)\n",
            "0 4 Epoch: 475 | ANN: testLoss: 0.6656 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 475 | ANN: testLoss: 0.8044 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 475 | ANN: testLoss: 0.7649 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 475 | ANN: testLoss: 1.1693 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 476 | ANN: trainLoss: 0.0348 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 476 | ANN: trainLoss: 0.0720 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 476 | ANN: trainLoss: 0.0624 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 476 | ANN: trainLoss: 0.0608 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 476 | ANN: trainLoss: 0.0641 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 476 | ANN: trainLoss: 0.0724 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 476 | ANN: trainLoss: 0.0646 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 476 | ANN: trainLoss: 0.0622 | trainAcc: 97.6562% (500/512)\n",
            "8 13 Epoch: 476 | ANN: trainLoss: 0.0603 | trainAcc: 97.9167% (564/576)\n",
            "9 13 Epoch: 476 | ANN: trainLoss: 0.0648 | trainAcc: 97.6562% (625/640)\n",
            "10 13 Epoch: 476 | ANN: trainLoss: 0.0654 | trainAcc: 97.5852% (687/704)\n",
            "11 13 Epoch: 476 | ANN: trainLoss: 0.0636 | trainAcc: 97.6562% (750/768)\n",
            "12 13 Epoch: 476 | ANN: trainLoss: 0.0603 | trainAcc: 97.6684% (754/772)\n",
            "0 4 Epoch: 476 | ANN: testLoss: 0.6418 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 476 | ANN: testLoss: 0.7453 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 476 | ANN: testLoss: 0.7424 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 476 | ANN: testLoss: 1.4722 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 477 | ANN: trainLoss: 0.0351 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 477 | ANN: trainLoss: 0.0309 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 477 | ANN: trainLoss: 0.0335 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 477 | ANN: trainLoss: 0.0308 | trainAcc: 99.6094% (255/256)\n",
            "4 13 Epoch: 477 | ANN: trainLoss: 0.0306 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 477 | ANN: trainLoss: 0.0326 | trainAcc: 99.4792% (382/384)\n",
            "6 13 Epoch: 477 | ANN: trainLoss: 0.0392 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 477 | ANN: trainLoss: 0.0409 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 477 | ANN: trainLoss: 0.0453 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 477 | ANN: trainLoss: 0.0430 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 477 | ANN: trainLoss: 0.0434 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 477 | ANN: trainLoss: 0.0415 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 477 | ANN: trainLoss: 0.0850 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 477 | ANN: testLoss: 1.1282 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 477 | ANN: testLoss: 0.8619 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 477 | ANN: testLoss: 0.7927 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 477 | ANN: testLoss: 0.5952 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 478 | ANN: trainLoss: 0.0402 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 478 | ANN: trainLoss: 0.0447 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 478 | ANN: trainLoss: 0.0372 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 478 | ANN: trainLoss: 0.0429 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 478 | ANN: trainLoss: 0.0373 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 478 | ANN: trainLoss: 0.0342 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 478 | ANN: trainLoss: 0.0338 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 478 | ANN: trainLoss: 0.0355 | trainAcc: 99.4141% (509/512)\n",
            "8 13 Epoch: 478 | ANN: trainLoss: 0.0354 | trainAcc: 99.3056% (572/576)\n",
            "9 13 Epoch: 478 | ANN: trainLoss: 0.0379 | trainAcc: 99.0625% (634/640)\n",
            "10 13 Epoch: 478 | ANN: trainLoss: 0.0390 | trainAcc: 99.0057% (697/704)\n",
            "11 13 Epoch: 478 | ANN: trainLoss: 0.0408 | trainAcc: 98.9583% (760/768)\n",
            "12 13 Epoch: 478 | ANN: trainLoss: 0.0415 | trainAcc: 98.9637% (764/772)\n",
            "0 4 Epoch: 478 | ANN: testLoss: 0.7736 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 478 | ANN: testLoss: 0.8380 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 478 | ANN: testLoss: 0.7638 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 478 | ANN: testLoss: 0.7570 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 479 | ANN: trainLoss: 0.0423 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 479 | ANN: trainLoss: 0.0379 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 479 | ANN: trainLoss: 0.0380 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 479 | ANN: trainLoss: 0.0449 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 479 | ANN: trainLoss: 0.0412 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 479 | ANN: trainLoss: 0.0377 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 479 | ANN: trainLoss: 0.0363 | trainAcc: 99.1071% (444/448)\n",
            "7 13 Epoch: 479 | ANN: trainLoss: 0.0370 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 479 | ANN: trainLoss: 0.0356 | trainAcc: 99.1319% (571/576)\n",
            "9 13 Epoch: 479 | ANN: trainLoss: 0.0362 | trainAcc: 99.0625% (634/640)\n",
            "10 13 Epoch: 479 | ANN: trainLoss: 0.0361 | trainAcc: 99.0057% (697/704)\n",
            "11 13 Epoch: 479 | ANN: trainLoss: 0.0364 | trainAcc: 99.0885% (761/768)\n",
            "12 13 Epoch: 479 | ANN: trainLoss: 0.0639 | trainAcc: 98.9637% (764/772)\n",
            "0 4 Epoch: 479 | ANN: testLoss: 0.8695 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 479 | ANN: testLoss: 0.8839 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 479 | ANN: testLoss: 0.7863 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 479 | ANN: testLoss: 0.5898 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 480 | ANN: trainLoss: 0.0748 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 480 | ANN: trainLoss: 0.0505 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 480 | ANN: trainLoss: 0.0457 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 480 | ANN: trainLoss: 0.0381 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 480 | ANN: trainLoss: 0.0429 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 480 | ANN: trainLoss: 0.0418 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 480 | ANN: trainLoss: 0.0385 | trainAcc: 99.1071% (444/448)\n",
            "7 13 Epoch: 480 | ANN: trainLoss: 0.0358 | trainAcc: 99.2188% (508/512)\n",
            "8 13 Epoch: 480 | ANN: trainLoss: 0.0347 | trainAcc: 99.3056% (572/576)\n",
            "9 13 Epoch: 480 | ANN: trainLoss: 0.0355 | trainAcc: 99.2188% (635/640)\n",
            "10 13 Epoch: 480 | ANN: trainLoss: 0.0341 | trainAcc: 99.2898% (699/704)\n",
            "11 13 Epoch: 480 | ANN: trainLoss: 0.0324 | trainAcc: 99.3490% (763/768)\n",
            "12 13 Epoch: 480 | ANN: trainLoss: 0.0311 | trainAcc: 99.3523% (767/772)\n",
            "0 4 Epoch: 480 | ANN: testLoss: 0.5934 | testAcc: 85.9375% (55/64)\n",
            "1 4 Epoch: 480 | ANN: testLoss: 0.6545 | testAcc: 83.5938% (107/128)\n",
            "2 4 Epoch: 480 | ANN: testLoss: 0.7753 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 480 | ANN: testLoss: 0.5952 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 481 | ANN: trainLoss: 0.0684 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 481 | ANN: trainLoss: 0.0552 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 481 | ANN: trainLoss: 0.0467 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 481 | ANN: trainLoss: 0.0476 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 481 | ANN: trainLoss: 0.0474 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 481 | ANN: trainLoss: 0.0510 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 481 | ANN: trainLoss: 0.0473 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 481 | ANN: trainLoss: 0.0509 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 481 | ANN: trainLoss: 0.0506 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 481 | ANN: trainLoss: 0.0513 | trainAcc: 98.1250% (628/640)\n",
            "10 13 Epoch: 481 | ANN: trainLoss: 0.0480 | trainAcc: 98.2955% (692/704)\n",
            "11 13 Epoch: 481 | ANN: trainLoss: 0.0511 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 481 | ANN: trainLoss: 0.0529 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 481 | ANN: testLoss: 0.9189 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 481 | ANN: testLoss: 0.7914 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 481 | ANN: testLoss: 0.7603 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 481 | ANN: testLoss: 0.5775 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 482 | ANN: trainLoss: 0.0979 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 482 | ANN: trainLoss: 0.0730 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 482 | ANN: trainLoss: 0.0666 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 482 | ANN: trainLoss: 0.0551 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 482 | ANN: trainLoss: 0.0483 | trainAcc: 98.1250% (314/320)\n",
            "5 13 Epoch: 482 | ANN: trainLoss: 0.0446 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 482 | ANN: trainLoss: 0.0437 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 482 | ANN: trainLoss: 0.0460 | trainAcc: 98.2422% (503/512)\n",
            "8 13 Epoch: 482 | ANN: trainLoss: 0.0474 | trainAcc: 98.0903% (565/576)\n",
            "9 13 Epoch: 482 | ANN: trainLoss: 0.0511 | trainAcc: 98.1250% (628/640)\n",
            "10 13 Epoch: 482 | ANN: trainLoss: 0.0509 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 482 | ANN: trainLoss: 0.0524 | trainAcc: 98.1771% (754/768)\n",
            "12 13 Epoch: 482 | ANN: trainLoss: 0.0611 | trainAcc: 98.1865% (758/772)\n",
            "0 4 Epoch: 482 | ANN: testLoss: 0.7047 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 482 | ANN: testLoss: 0.7679 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 482 | ANN: testLoss: 0.7664 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 482 | ANN: testLoss: 0.5749 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 483 | ANN: trainLoss: 0.0845 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 483 | ANN: trainLoss: 0.0986 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 483 | ANN: trainLoss: 0.0977 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 483 | ANN: trainLoss: 0.0869 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 483 | ANN: trainLoss: 0.0842 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 483 | ANN: trainLoss: 0.0841 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 483 | ANN: trainLoss: 0.0745 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 483 | ANN: trainLoss: 0.0732 | trainAcc: 96.6797% (495/512)\n",
            "8 13 Epoch: 483 | ANN: trainLoss: 0.0735 | trainAcc: 96.7014% (557/576)\n",
            "9 13 Epoch: 483 | ANN: trainLoss: 0.0716 | trainAcc: 96.7188% (619/640)\n",
            "10 13 Epoch: 483 | ANN: trainLoss: 0.0681 | trainAcc: 97.0170% (683/704)\n",
            "11 13 Epoch: 483 | ANN: trainLoss: 0.0657 | trainAcc: 97.1354% (746/768)\n",
            "12 13 Epoch: 483 | ANN: trainLoss: 0.0925 | trainAcc: 96.8912% (748/772)\n",
            "0 4 Epoch: 483 | ANN: testLoss: 0.7389 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 483 | ANN: testLoss: 0.8420 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 483 | ANN: testLoss: 0.7616 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 483 | ANN: testLoss: 0.6011 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 484 | ANN: trainLoss: 0.0248 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 484 | ANN: trainLoss: 0.0529 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 484 | ANN: trainLoss: 0.0455 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 484 | ANN: trainLoss: 0.0511 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 484 | ANN: trainLoss: 0.0538 | trainAcc: 97.5000% (312/320)\n",
            "5 13 Epoch: 484 | ANN: trainLoss: 0.0524 | trainAcc: 97.3958% (374/384)\n",
            "6 13 Epoch: 484 | ANN: trainLoss: 0.0470 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 484 | ANN: trainLoss: 0.0452 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 484 | ANN: trainLoss: 0.0438 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 484 | ANN: trainLoss: 0.0420 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 484 | ANN: trainLoss: 0.0407 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 484 | ANN: trainLoss: 0.0413 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 484 | ANN: trainLoss: 0.0389 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 484 | ANN: testLoss: 0.5478 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 484 | ANN: testLoss: 0.7135 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 484 | ANN: testLoss: 0.7557 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 484 | ANN: testLoss: 0.5671 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 485 | ANN: trainLoss: 0.0256 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 485 | ANN: trainLoss: 0.0389 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 485 | ANN: trainLoss: 0.0437 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 485 | ANN: trainLoss: 0.0378 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 485 | ANN: trainLoss: 0.0352 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 485 | ANN: trainLoss: 0.0340 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 485 | ANN: trainLoss: 0.0345 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 485 | ANN: trainLoss: 0.0355 | trainAcc: 99.2188% (508/512)\n",
            "8 13 Epoch: 485 | ANN: trainLoss: 0.0349 | trainAcc: 99.3056% (572/576)\n",
            "9 13 Epoch: 485 | ANN: trainLoss: 0.0371 | trainAcc: 98.9062% (633/640)\n",
            "10 13 Epoch: 485 | ANN: trainLoss: 0.0354 | trainAcc: 99.0057% (697/704)\n",
            "11 13 Epoch: 485 | ANN: trainLoss: 0.0347 | trainAcc: 98.9583% (760/768)\n",
            "12 13 Epoch: 485 | ANN: trainLoss: 0.1531 | trainAcc: 98.7047% (762/772)\n",
            "0 4 Epoch: 485 | ANN: testLoss: 0.5105 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 485 | ANN: testLoss: 0.6548 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 485 | ANN: testLoss: 0.7972 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 485 | ANN: testLoss: 0.5980 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 486 | ANN: trainLoss: 0.0324 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 486 | ANN: trainLoss: 0.0242 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 486 | ANN: trainLoss: 0.0268 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 486 | ANN: trainLoss: 0.0336 | trainAcc: 99.2188% (254/256)\n",
            "4 13 Epoch: 486 | ANN: trainLoss: 0.0310 | trainAcc: 99.3750% (318/320)\n",
            "5 13 Epoch: 486 | ANN: trainLoss: 0.0369 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 486 | ANN: trainLoss: 0.0347 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 486 | ANN: trainLoss: 0.0359 | trainAcc: 99.4141% (509/512)\n",
            "8 13 Epoch: 486 | ANN: trainLoss: 0.0362 | trainAcc: 99.4792% (573/576)\n",
            "9 13 Epoch: 486 | ANN: trainLoss: 0.0362 | trainAcc: 99.3750% (636/640)\n",
            "10 13 Epoch: 486 | ANN: trainLoss: 0.0362 | trainAcc: 99.4318% (700/704)\n",
            "11 13 Epoch: 486 | ANN: trainLoss: 0.0371 | trainAcc: 99.4792% (764/768)\n",
            "12 13 Epoch: 486 | ANN: trainLoss: 0.0400 | trainAcc: 99.4819% (768/772)\n",
            "0 4 Epoch: 486 | ANN: testLoss: 0.6033 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 486 | ANN: testLoss: 0.7125 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 486 | ANN: testLoss: 0.7704 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 486 | ANN: testLoss: 1.1172 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 487 | ANN: trainLoss: 0.0743 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 487 | ANN: trainLoss: 0.0609 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 487 | ANN: trainLoss: 0.0471 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 487 | ANN: trainLoss: 0.0506 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 487 | ANN: trainLoss: 0.0514 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 487 | ANN: trainLoss: 0.0461 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 487 | ANN: trainLoss: 0.0465 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 487 | ANN: trainLoss: 0.0480 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 487 | ANN: trainLoss: 0.0437 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 487 | ANN: trainLoss: 0.0420 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 487 | ANN: trainLoss: 0.0408 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 487 | ANN: trainLoss: 0.0431 | trainAcc: 98.3073% (755/768)\n",
            "12 13 Epoch: 487 | ANN: trainLoss: 0.0478 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 487 | ANN: testLoss: 0.8368 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 487 | ANN: testLoss: 0.7427 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 487 | ANN: testLoss: 0.7670 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 487 | ANN: testLoss: 0.5770 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 488 | ANN: trainLoss: 0.0261 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 488 | ANN: trainLoss: 0.0494 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 488 | ANN: trainLoss: 0.0383 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 488 | ANN: trainLoss: 0.0462 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 488 | ANN: trainLoss: 0.0456 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 488 | ANN: trainLoss: 0.0530 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 488 | ANN: trainLoss: 0.0511 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 488 | ANN: trainLoss: 0.0496 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 488 | ANN: trainLoss: 0.0477 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 488 | ANN: trainLoss: 0.0515 | trainAcc: 98.4375% (630/640)\n",
            "10 13 Epoch: 488 | ANN: trainLoss: 0.0503 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 488 | ANN: trainLoss: 0.0580 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 488 | ANN: trainLoss: 0.0563 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 488 | ANN: testLoss: 0.6769 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 488 | ANN: testLoss: 0.7617 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 488 | ANN: testLoss: 0.7670 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 488 | ANN: testLoss: 0.5760 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 489 | ANN: trainLoss: 0.0258 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 489 | ANN: trainLoss: 0.0219 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 489 | ANN: trainLoss: 0.0407 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 489 | ANN: trainLoss: 0.0462 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 489 | ANN: trainLoss: 0.0505 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 489 | ANN: trainLoss: 0.0491 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 489 | ANN: trainLoss: 0.0555 | trainAcc: 97.7679% (438/448)\n",
            "7 13 Epoch: 489 | ANN: trainLoss: 0.0540 | trainAcc: 98.0469% (502/512)\n",
            "8 13 Epoch: 489 | ANN: trainLoss: 0.0524 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 489 | ANN: trainLoss: 0.0537 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 489 | ANN: trainLoss: 0.0548 | trainAcc: 98.1534% (691/704)\n",
            "11 13 Epoch: 489 | ANN: trainLoss: 0.0541 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 489 | ANN: trainLoss: 0.0508 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 489 | ANN: testLoss: 0.5758 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 489 | ANN: testLoss: 0.8367 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 489 | ANN: testLoss: 0.7811 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 489 | ANN: testLoss: 0.6058 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 490 | ANN: trainLoss: 0.0579 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 490 | ANN: trainLoss: 0.0414 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 490 | ANN: trainLoss: 0.0446 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 490 | ANN: trainLoss: 0.0420 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 490 | ANN: trainLoss: 0.0400 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 490 | ANN: trainLoss: 0.0481 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 490 | ANN: trainLoss: 0.0494 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 490 | ANN: trainLoss: 0.0473 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 490 | ANN: trainLoss: 0.0448 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 490 | ANN: trainLoss: 0.0447 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 490 | ANN: trainLoss: 0.0450 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 490 | ANN: trainLoss: 0.0450 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 490 | ANN: trainLoss: 0.1005 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 490 | ANN: testLoss: 0.8777 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 490 | ANN: testLoss: 0.7587 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 490 | ANN: testLoss: 0.7687 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 490 | ANN: testLoss: 0.5766 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 491 | ANN: trainLoss: 0.0442 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 491 | ANN: trainLoss: 0.0401 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 491 | ANN: trainLoss: 0.0329 | trainAcc: 100.0000% (192/192)\n",
            "3 13 Epoch: 491 | ANN: trainLoss: 0.0307 | trainAcc: 100.0000% (256/256)\n",
            "4 13 Epoch: 491 | ANN: trainLoss: 0.0326 | trainAcc: 100.0000% (320/320)\n",
            "5 13 Epoch: 491 | ANN: trainLoss: 0.0379 | trainAcc: 99.4792% (382/384)\n",
            "6 13 Epoch: 491 | ANN: trainLoss: 0.0401 | trainAcc: 99.3304% (445/448)\n",
            "7 13 Epoch: 491 | ANN: trainLoss: 0.0399 | trainAcc: 99.2188% (508/512)\n",
            "8 13 Epoch: 491 | ANN: trainLoss: 0.0370 | trainAcc: 99.3056% (572/576)\n",
            "9 13 Epoch: 491 | ANN: trainLoss: 0.0364 | trainAcc: 99.3750% (636/640)\n",
            "10 13 Epoch: 491 | ANN: trainLoss: 0.0349 | trainAcc: 99.4318% (700/704)\n",
            "11 13 Epoch: 491 | ANN: trainLoss: 0.0351 | trainAcc: 99.3490% (763/768)\n",
            "12 13 Epoch: 491 | ANN: trainLoss: 0.0333 | trainAcc: 99.3523% (767/772)\n",
            "0 4 Epoch: 491 | ANN: testLoss: 0.5179 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 491 | ANN: testLoss: 0.7810 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 491 | ANN: testLoss: 0.7710 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 491 | ANN: testLoss: 0.5783 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 492 | ANN: trainLoss: 0.0432 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 492 | ANN: trainLoss: 0.0436 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 492 | ANN: trainLoss: 0.0557 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 492 | ANN: trainLoss: 0.0464 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 492 | ANN: trainLoss: 0.0493 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 492 | ANN: trainLoss: 0.0450 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 492 | ANN: trainLoss: 0.0424 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 492 | ANN: trainLoss: 0.0404 | trainAcc: 98.6328% (505/512)\n",
            "8 13 Epoch: 492 | ANN: trainLoss: 0.0403 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 492 | ANN: trainLoss: 0.0464 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 492 | ANN: trainLoss: 0.0460 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 492 | ANN: trainLoss: 0.0455 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 492 | ANN: trainLoss: 0.0424 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 492 | ANN: testLoss: 1.0205 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 492 | ANN: testLoss: 0.8348 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 492 | ANN: testLoss: 0.7854 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 492 | ANN: testLoss: 0.5891 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 493 | ANN: trainLoss: 0.0522 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 493 | ANN: trainLoss: 0.0626 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 493 | ANN: trainLoss: 0.0536 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 493 | ANN: trainLoss: 0.0458 | trainAcc: 99.6094% (255/256)\n",
            "4 13 Epoch: 493 | ANN: trainLoss: 0.0535 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 493 | ANN: trainLoss: 0.0542 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 493 | ANN: trainLoss: 0.0589 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 493 | ANN: trainLoss: 0.0570 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 493 | ANN: trainLoss: 0.0574 | trainAcc: 98.2639% (566/576)\n",
            "9 13 Epoch: 493 | ANN: trainLoss: 0.0575 | trainAcc: 98.2812% (629/640)\n",
            "10 13 Epoch: 493 | ANN: trainLoss: 0.0554 | trainAcc: 98.4375% (693/704)\n",
            "11 13 Epoch: 493 | ANN: trainLoss: 0.0544 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 493 | ANN: trainLoss: 0.1147 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 493 | ANN: testLoss: 0.6600 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 493 | ANN: testLoss: 0.8281 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 493 | ANN: testLoss: 0.7599 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 493 | ANN: testLoss: 0.5795 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 494 | ANN: trainLoss: 0.0518 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 494 | ANN: trainLoss: 0.0439 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 494 | ANN: trainLoss: 0.0438 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 494 | ANN: trainLoss: 0.0542 | trainAcc: 98.4375% (252/256)\n",
            "4 13 Epoch: 494 | ANN: trainLoss: 0.0519 | trainAcc: 98.4375% (315/320)\n",
            "5 13 Epoch: 494 | ANN: trainLoss: 0.0490 | trainAcc: 98.4375% (378/384)\n",
            "6 13 Epoch: 494 | ANN: trainLoss: 0.0494 | trainAcc: 98.4375% (441/448)\n",
            "7 13 Epoch: 494 | ANN: trainLoss: 0.0485 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 494 | ANN: trainLoss: 0.0491 | trainAcc: 98.4375% (567/576)\n",
            "9 13 Epoch: 494 | ANN: trainLoss: 0.0467 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 494 | ANN: trainLoss: 0.0455 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 494 | ANN: trainLoss: 0.0471 | trainAcc: 98.4375% (756/768)\n",
            "12 13 Epoch: 494 | ANN: trainLoss: 0.1301 | trainAcc: 98.3161% (759/772)\n",
            "0 4 Epoch: 494 | ANN: testLoss: 0.7082 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 494 | ANN: testLoss: 0.7377 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 494 | ANN: testLoss: 0.7545 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 494 | ANN: testLoss: 1.0735 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 495 | ANN: trainLoss: 0.0297 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 495 | ANN: trainLoss: 0.0444 | trainAcc: 99.2188% (127/128)\n",
            "2 13 Epoch: 495 | ANN: trainLoss: 0.0385 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 495 | ANN: trainLoss: 0.0364 | trainAcc: 99.6094% (255/256)\n",
            "4 13 Epoch: 495 | ANN: trainLoss: 0.0448 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 495 | ANN: trainLoss: 0.0466 | trainAcc: 99.2188% (381/384)\n",
            "6 13 Epoch: 495 | ANN: trainLoss: 0.0470 | trainAcc: 99.1071% (444/448)\n",
            "7 13 Epoch: 495 | ANN: trainLoss: 0.0476 | trainAcc: 98.8281% (506/512)\n",
            "8 13 Epoch: 495 | ANN: trainLoss: 0.0490 | trainAcc: 98.7847% (569/576)\n",
            "9 13 Epoch: 495 | ANN: trainLoss: 0.0493 | trainAcc: 98.7500% (632/640)\n",
            "10 13 Epoch: 495 | ANN: trainLoss: 0.0510 | trainAcc: 98.5795% (694/704)\n",
            "11 13 Epoch: 495 | ANN: trainLoss: 0.0516 | trainAcc: 98.5677% (757/768)\n",
            "12 13 Epoch: 495 | ANN: trainLoss: 0.0763 | trainAcc: 98.4456% (760/772)\n",
            "0 4 Epoch: 495 | ANN: testLoss: 0.6632 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 495 | ANN: testLoss: 0.7876 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 495 | ANN: testLoss: 0.7829 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 495 | ANN: testLoss: 0.5873 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 496 | ANN: trainLoss: 0.0255 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 496 | ANN: trainLoss: 0.0206 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 496 | ANN: trainLoss: 0.0316 | trainAcc: 98.9583% (190/192)\n",
            "3 13 Epoch: 496 | ANN: trainLoss: 0.0342 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 496 | ANN: trainLoss: 0.0355 | trainAcc: 98.7500% (316/320)\n",
            "5 13 Epoch: 496 | ANN: trainLoss: 0.0329 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 496 | ANN: trainLoss: 0.0355 | trainAcc: 99.1071% (444/448)\n",
            "7 13 Epoch: 496 | ANN: trainLoss: 0.0337 | trainAcc: 99.2188% (508/512)\n",
            "8 13 Epoch: 496 | ANN: trainLoss: 0.0324 | trainAcc: 99.3056% (572/576)\n",
            "9 13 Epoch: 496 | ANN: trainLoss: 0.0341 | trainAcc: 99.0625% (634/640)\n",
            "10 13 Epoch: 496 | ANN: trainLoss: 0.0332 | trainAcc: 99.1477% (698/704)\n",
            "11 13 Epoch: 496 | ANN: trainLoss: 0.0330 | trainAcc: 99.0885% (761/768)\n",
            "12 13 Epoch: 496 | ANN: trainLoss: 0.1023 | trainAcc: 98.8342% (763/772)\n",
            "0 4 Epoch: 496 | ANN: testLoss: 0.6613 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 496 | ANN: testLoss: 0.7483 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 496 | ANN: testLoss: 0.7748 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 496 | ANN: testLoss: 0.5811 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 497 | ANN: trainLoss: 0.0825 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 497 | ANN: trainLoss: 0.0502 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 497 | ANN: trainLoss: 0.0505 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 497 | ANN: trainLoss: 0.0534 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 497 | ANN: trainLoss: 0.0571 | trainAcc: 97.8125% (313/320)\n",
            "5 13 Epoch: 497 | ANN: trainLoss: 0.0514 | trainAcc: 98.1771% (377/384)\n",
            "6 13 Epoch: 497 | ANN: trainLoss: 0.0524 | trainAcc: 98.2143% (440/448)\n",
            "7 13 Epoch: 497 | ANN: trainLoss: 0.0484 | trainAcc: 98.4375% (504/512)\n",
            "8 13 Epoch: 497 | ANN: trainLoss: 0.0456 | trainAcc: 98.6111% (568/576)\n",
            "9 13 Epoch: 497 | ANN: trainLoss: 0.0474 | trainAcc: 98.5938% (631/640)\n",
            "10 13 Epoch: 497 | ANN: trainLoss: 0.0541 | trainAcc: 98.2955% (692/704)\n",
            "11 13 Epoch: 497 | ANN: trainLoss: 0.0581 | trainAcc: 98.0469% (753/768)\n",
            "12 13 Epoch: 497 | ANN: trainLoss: 0.0558 | trainAcc: 98.0570% (757/772)\n",
            "0 4 Epoch: 497 | ANN: testLoss: 0.6990 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 497 | ANN: testLoss: 0.8051 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 497 | ANN: testLoss: 0.7344 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 497 | ANN: testLoss: 1.2918 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 498 | ANN: trainLoss: 0.0239 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 498 | ANN: trainLoss: 0.0278 | trainAcc: 100.0000% (128/128)\n",
            "2 13 Epoch: 498 | ANN: trainLoss: 0.0295 | trainAcc: 99.4792% (191/192)\n",
            "3 13 Epoch: 498 | ANN: trainLoss: 0.0266 | trainAcc: 99.6094% (255/256)\n",
            "4 13 Epoch: 498 | ANN: trainLoss: 0.0268 | trainAcc: 99.6875% (319/320)\n",
            "5 13 Epoch: 498 | ANN: trainLoss: 0.0287 | trainAcc: 99.4792% (382/384)\n",
            "6 13 Epoch: 498 | ANN: trainLoss: 0.0273 | trainAcc: 99.5536% (446/448)\n",
            "7 13 Epoch: 498 | ANN: trainLoss: 0.0314 | trainAcc: 99.4141% (509/512)\n",
            "8 13 Epoch: 498 | ANN: trainLoss: 0.0302 | trainAcc: 99.4792% (573/576)\n",
            "9 13 Epoch: 498 | ANN: trainLoss: 0.0326 | trainAcc: 99.3750% (636/640)\n",
            "10 13 Epoch: 498 | ANN: trainLoss: 0.0319 | trainAcc: 99.4318% (700/704)\n",
            "11 13 Epoch: 498 | ANN: trainLoss: 0.0307 | trainAcc: 99.4792% (764/768)\n",
            "12 13 Epoch: 498 | ANN: trainLoss: 0.0284 | trainAcc: 99.4819% (768/772)\n",
            "0 4 Epoch: 498 | ANN: testLoss: 0.9308 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 498 | ANN: testLoss: 0.7806 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 498 | ANN: testLoss: 0.7487 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 498 | ANN: testLoss: 0.5615 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 499 | ANN: trainLoss: 0.0504 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 499 | ANN: trainLoss: 0.0370 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 499 | ANN: trainLoss: 0.0358 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 499 | ANN: trainLoss: 0.0318 | trainAcc: 98.8281% (253/256)\n",
            "4 13 Epoch: 499 | ANN: trainLoss: 0.0316 | trainAcc: 99.0625% (317/320)\n",
            "5 13 Epoch: 499 | ANN: trainLoss: 0.0350 | trainAcc: 98.9583% (380/384)\n",
            "6 13 Epoch: 499 | ANN: trainLoss: 0.0354 | trainAcc: 98.8839% (443/448)\n",
            "7 13 Epoch: 499 | ANN: trainLoss: 0.0356 | trainAcc: 99.0234% (507/512)\n",
            "8 13 Epoch: 499 | ANN: trainLoss: 0.0349 | trainAcc: 99.1319% (571/576)\n",
            "9 13 Epoch: 499 | ANN: trainLoss: 0.0335 | trainAcc: 99.2188% (635/640)\n",
            "10 13 Epoch: 499 | ANN: trainLoss: 0.0320 | trainAcc: 99.2898% (699/704)\n",
            "11 13 Epoch: 499 | ANN: trainLoss: 0.0403 | trainAcc: 98.6979% (758/768)\n",
            "12 13 Epoch: 499 | ANN: trainLoss: 0.1008 | trainAcc: 98.5751% (761/772)\n",
            "0 4 Epoch: 499 | ANN: testLoss: 0.9575 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 499 | ANN: testLoss: 0.8825 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 499 | ANN: testLoss: 0.7619 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 499 | ANN: testLoss: 0.5714 | testAcc: 77.7202% (150/193)\n",
            "---------------------------------------------\n",
            "Converting using MaxNorm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 339.61it/s]\n",
            "100%|██████████| 13/13 [00:00<00:00, 421.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ANN accuracy: Test: 77.7200%\n",
            "SNN accuracy: max_norm: 76.6839%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LENet_FCL to SNN Conversion Framework execution\n",
        "\n",
        "# Hyperparameters\n",
        "#EPOCHS = 300\n",
        "#BATCH_SIZE = 128\n",
        "#TIME_STEPS = 100  # T for SNN\n",
        "#TEST_SIZE = 0.2\n",
        "#DROP_OUT = 0.5\n",
        "\n",
        "\n",
        "# Split the data\n",
        "print(f\"{100 - (TEST_SIZE * 100)}% of the dataset is used for training and {TEST_SIZE * 100}% is used for testing.\")\n",
        "train_data, test_data, train_label, test_label = train_test_split(datasetX, datasetY, test_size=TEST_SIZE, shuffle=True,\n",
        "                                                                  random_state=0)\n",
        "\n",
        "# Initialize model\n",
        "cnn_model_lenet_fcl = LENet_FCL(classes_num=3, channel_count=channel_count, drop_out = DROP_OUT).to(device)\n",
        "cnn_model_lenet_fcl.apply(initialize_weights)\n",
        "\n",
        "# Train CNN model\n",
        "train_acc, test_acc,  cnn_model_lenet_fcl = train_ann(cnn_model_lenet_fcl, train_data, train_label, test_data, test_label,\n",
        "                                              ep=EPOCHS, batch=BATCH_SIZE)\n",
        "max_norm_acc = anntosnn( cnn_model_lenet_fcl, train_data, train_label, test_data, test_label,\n",
        "                        batch=BATCH_SIZE, T=TIME_STEPS)\n",
        "snn_model_lenet_fcl = ann2snn.Converter(mode='max', dataloader=data_loader(train_data, train_label, batch=BATCH_SIZE))(cnn_model_lenet_fcl)\n",
        "\n",
        "print('\\n')\n",
        "print('ANN accuracy: Test: %.4f%%' % (test_acc * 100))\n",
        "print('SNN accuracy: max_norm: %.4f%%' % (max_norm_acc[-1] * 100))"
      ],
      "metadata": {
        "id": "AeiiLk4TQp-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "outputId": "dd8e5c02-c382-4d9e-cf2c-a50b9c7dd901"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "3 4 Epoch: 236 | ANN: testLoss: 0.3717 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 237 | ANN: trainLoss: 0.1369 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 237 | ANN: trainLoss: 0.1788 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 237 | ANN: trainLoss: 0.1963 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 237 | ANN: trainLoss: 0.2231 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 237 | ANN: trainLoss: 0.2727 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 237 | ANN: trainLoss: 0.2693 | trainAcc: 89.5833% (344/384)\n",
            "6 13 Epoch: 237 | ANN: trainLoss: 0.2688 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 237 | ANN: trainLoss: 0.2597 | trainAcc: 90.4297% (463/512)\n",
            "8 13 Epoch: 237 | ANN: trainLoss: 0.2869 | trainAcc: 89.5833% (516/576)\n",
            "9 13 Epoch: 237 | ANN: trainLoss: 0.3044 | trainAcc: 88.2812% (565/640)\n",
            "10 13 Epoch: 237 | ANN: trainLoss: 0.3153 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 237 | ANN: trainLoss: 0.3159 | trainAcc: 88.1510% (677/768)\n",
            "12 13 Epoch: 237 | ANN: trainLoss: 0.2975 | trainAcc: 88.2124% (681/772)\n",
            "0 4 Epoch: 237 | ANN: testLoss: 0.5763 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 237 | ANN: testLoss: 0.5064 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 237 | ANN: testLoss: 0.4937 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 237 | ANN: testLoss: 0.3843 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 238 | ANN: trainLoss: 0.1874 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 238 | ANN: trainLoss: 0.2641 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 238 | ANN: trainLoss: 0.2768 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 238 | ANN: trainLoss: 0.2939 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 238 | ANN: trainLoss: 0.3028 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 238 | ANN: trainLoss: 0.2944 | trainAcc: 86.7188% (333/384)\n",
            "6 13 Epoch: 238 | ANN: trainLoss: 0.2878 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 238 | ANN: trainLoss: 0.2876 | trainAcc: 87.6953% (449/512)\n",
            "8 13 Epoch: 238 | ANN: trainLoss: 0.2801 | trainAcc: 88.0208% (507/576)\n",
            "9 13 Epoch: 238 | ANN: trainLoss: 0.2879 | trainAcc: 87.6562% (561/640)\n",
            "10 13 Epoch: 238 | ANN: trainLoss: 0.2818 | trainAcc: 87.7841% (618/704)\n",
            "11 13 Epoch: 238 | ANN: trainLoss: 0.2783 | trainAcc: 88.1510% (677/768)\n",
            "12 13 Epoch: 238 | ANN: trainLoss: 0.3218 | trainAcc: 88.0829% (680/772)\n",
            "0 4 Epoch: 238 | ANN: testLoss: 0.3798 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 238 | ANN: testLoss: 0.4494 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 238 | ANN: testLoss: 0.4711 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 238 | ANN: testLoss: 0.3584 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 239 | ANN: trainLoss: 0.2566 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 239 | ANN: trainLoss: 0.2461 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 239 | ANN: trainLoss: 0.2627 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 239 | ANN: trainLoss: 0.2556 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 239 | ANN: trainLoss: 0.2847 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 239 | ANN: trainLoss: 0.3073 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 239 | ANN: trainLoss: 0.2973 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 239 | ANN: trainLoss: 0.3048 | trainAcc: 87.8906% (450/512)\n",
            "8 13 Epoch: 239 | ANN: trainLoss: 0.3152 | trainAcc: 87.5000% (504/576)\n",
            "9 13 Epoch: 239 | ANN: trainLoss: 0.3082 | trainAcc: 87.6562% (561/640)\n",
            "10 13 Epoch: 239 | ANN: trainLoss: 0.3148 | trainAcc: 87.6420% (617/704)\n",
            "11 13 Epoch: 239 | ANN: trainLoss: 0.3178 | trainAcc: 87.5000% (672/768)\n",
            "12 13 Epoch: 239 | ANN: trainLoss: 0.3771 | trainAcc: 87.3057% (674/772)\n",
            "0 4 Epoch: 239 | ANN: testLoss: 0.5191 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 239 | ANN: testLoss: 0.5051 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 239 | ANN: testLoss: 0.5484 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 239 | ANN: testLoss: 0.5698 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 240 | ANN: trainLoss: 0.2531 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 240 | ANN: trainLoss: 0.2853 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 240 | ANN: trainLoss: 0.3047 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 240 | ANN: trainLoss: 0.2954 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 240 | ANN: trainLoss: 0.3054 | trainAcc: 87.8125% (281/320)\n",
            "5 13 Epoch: 240 | ANN: trainLoss: 0.3197 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 240 | ANN: trainLoss: 0.3246 | trainAcc: 87.0536% (390/448)\n",
            "7 13 Epoch: 240 | ANN: trainLoss: 0.3308 | trainAcc: 87.3047% (447/512)\n",
            "8 13 Epoch: 240 | ANN: trainLoss: 0.3268 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 240 | ANN: trainLoss: 0.3438 | trainAcc: 87.1875% (558/640)\n",
            "10 13 Epoch: 240 | ANN: trainLoss: 0.3370 | trainAcc: 87.2159% (614/704)\n",
            "11 13 Epoch: 240 | ANN: trainLoss: 0.3390 | trainAcc: 87.2396% (670/768)\n",
            "12 13 Epoch: 240 | ANN: trainLoss: 0.3427 | trainAcc: 87.1762% (673/772)\n",
            "0 4 Epoch: 240 | ANN: testLoss: 0.4769 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 240 | ANN: testLoss: 0.5003 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 240 | ANN: testLoss: 0.5018 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 240 | ANN: testLoss: 0.6692 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 241 | ANN: trainLoss: 0.2900 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 241 | ANN: trainLoss: 0.2571 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 241 | ANN: trainLoss: 0.2773 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 241 | ANN: trainLoss: 0.2868 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 241 | ANN: trainLoss: 0.2906 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 241 | ANN: trainLoss: 0.2882 | trainAcc: 89.5833% (344/384)\n",
            "6 13 Epoch: 241 | ANN: trainLoss: 0.2838 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 241 | ANN: trainLoss: 0.2786 | trainAcc: 89.8438% (460/512)\n",
            "8 13 Epoch: 241 | ANN: trainLoss: 0.2836 | trainAcc: 89.5833% (516/576)\n",
            "9 13 Epoch: 241 | ANN: trainLoss: 0.2989 | trainAcc: 88.9062% (569/640)\n",
            "10 13 Epoch: 241 | ANN: trainLoss: 0.2989 | trainAcc: 89.0625% (627/704)\n",
            "11 13 Epoch: 241 | ANN: trainLoss: 0.3055 | trainAcc: 88.8021% (682/768)\n",
            "12 13 Epoch: 241 | ANN: trainLoss: 0.3095 | trainAcc: 88.7306% (685/772)\n",
            "0 4 Epoch: 241 | ANN: testLoss: 0.5923 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 241 | ANN: testLoss: 0.5198 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 241 | ANN: testLoss: 0.4957 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 241 | ANN: testLoss: 0.5217 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 242 | ANN: trainLoss: 0.4170 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 242 | ANN: trainLoss: 0.4132 | trainAcc: 84.3750% (108/128)\n",
            "2 13 Epoch: 242 | ANN: trainLoss: 0.3807 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 242 | ANN: trainLoss: 0.3502 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 242 | ANN: trainLoss: 0.3164 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 242 | ANN: trainLoss: 0.3519 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 242 | ANN: trainLoss: 0.3369 | trainAcc: 86.6071% (388/448)\n",
            "7 13 Epoch: 242 | ANN: trainLoss: 0.3146 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 242 | ANN: trainLoss: 0.3071 | trainAcc: 87.6736% (505/576)\n",
            "9 13 Epoch: 242 | ANN: trainLoss: 0.3043 | trainAcc: 87.6562% (561/640)\n",
            "10 13 Epoch: 242 | ANN: trainLoss: 0.3035 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 242 | ANN: trainLoss: 0.2982 | trainAcc: 88.4115% (679/768)\n",
            "12 13 Epoch: 242 | ANN: trainLoss: 0.3080 | trainAcc: 88.3420% (682/772)\n",
            "0 4 Epoch: 242 | ANN: testLoss: 0.4824 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 242 | ANN: testLoss: 0.4603 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 242 | ANN: testLoss: 0.4637 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 242 | ANN: testLoss: 0.3659 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 243 | ANN: trainLoss: 0.2797 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 243 | ANN: trainLoss: 0.2494 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 243 | ANN: trainLoss: 0.2645 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 243 | ANN: trainLoss: 0.3016 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 243 | ANN: trainLoss: 0.2948 | trainAcc: 86.5625% (277/320)\n",
            "5 13 Epoch: 243 | ANN: trainLoss: 0.2840 | trainAcc: 87.5000% (336/384)\n",
            "6 13 Epoch: 243 | ANN: trainLoss: 0.2693 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 243 | ANN: trainLoss: 0.2655 | trainAcc: 88.4766% (453/512)\n",
            "8 13 Epoch: 243 | ANN: trainLoss: 0.2667 | trainAcc: 88.7153% (511/576)\n",
            "9 13 Epoch: 243 | ANN: trainLoss: 0.2623 | trainAcc: 88.7500% (568/640)\n",
            "10 13 Epoch: 243 | ANN: trainLoss: 0.2621 | trainAcc: 89.0625% (627/704)\n",
            "11 13 Epoch: 243 | ANN: trainLoss: 0.2577 | trainAcc: 89.3229% (686/768)\n",
            "12 13 Epoch: 243 | ANN: trainLoss: 0.2496 | trainAcc: 89.3782% (690/772)\n",
            "0 4 Epoch: 243 | ANN: testLoss: 0.5149 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 243 | ANN: testLoss: 0.4884 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 243 | ANN: testLoss: 0.5279 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 243 | ANN: testLoss: 0.3965 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 244 | ANN: trainLoss: 0.2209 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 244 | ANN: trainLoss: 0.2812 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 244 | ANN: trainLoss: 0.2735 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 244 | ANN: trainLoss: 0.2761 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 244 | ANN: trainLoss: 0.2729 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 244 | ANN: trainLoss: 0.2777 | trainAcc: 86.7188% (333/384)\n",
            "6 13 Epoch: 244 | ANN: trainLoss: 0.2706 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 244 | ANN: trainLoss: 0.2628 | trainAcc: 88.2812% (452/512)\n",
            "8 13 Epoch: 244 | ANN: trainLoss: 0.2746 | trainAcc: 88.7153% (511/576)\n",
            "9 13 Epoch: 244 | ANN: trainLoss: 0.2675 | trainAcc: 89.2188% (571/640)\n",
            "10 13 Epoch: 244 | ANN: trainLoss: 0.2608 | trainAcc: 89.4886% (630/704)\n",
            "11 13 Epoch: 244 | ANN: trainLoss: 0.2526 | trainAcc: 89.8438% (690/768)\n",
            "12 13 Epoch: 244 | ANN: trainLoss: 0.3499 | trainAcc: 89.6373% (692/772)\n",
            "0 4 Epoch: 244 | ANN: testLoss: 0.4660 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 244 | ANN: testLoss: 0.4676 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 244 | ANN: testLoss: 0.4815 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 244 | ANN: testLoss: 0.5647 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 245 | ANN: trainLoss: 0.2687 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 245 | ANN: trainLoss: 0.3086 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 245 | ANN: trainLoss: 0.2818 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 245 | ANN: trainLoss: 0.2787 | trainAcc: 88.6719% (227/256)\n",
            "4 13 Epoch: 245 | ANN: trainLoss: 0.2862 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 245 | ANN: trainLoss: 0.2867 | trainAcc: 88.2812% (339/384)\n",
            "6 13 Epoch: 245 | ANN: trainLoss: 0.2731 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 245 | ANN: trainLoss: 0.2696 | trainAcc: 89.2578% (457/512)\n",
            "8 13 Epoch: 245 | ANN: trainLoss: 0.2673 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 245 | ANN: trainLoss: 0.2655 | trainAcc: 89.6875% (574/640)\n",
            "10 13 Epoch: 245 | ANN: trainLoss: 0.2707 | trainAcc: 89.3466% (629/704)\n",
            "11 13 Epoch: 245 | ANN: trainLoss: 0.2725 | trainAcc: 89.1927% (685/768)\n",
            "12 13 Epoch: 245 | ANN: trainLoss: 0.2552 | trainAcc: 89.2487% (689/772)\n",
            "0 4 Epoch: 245 | ANN: testLoss: 0.4256 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 245 | ANN: testLoss: 0.4921 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 245 | ANN: testLoss: 0.4851 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 245 | ANN: testLoss: 0.3678 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 246 | ANN: trainLoss: 0.2481 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 246 | ANN: trainLoss: 0.2278 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 246 | ANN: trainLoss: 0.2056 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 246 | ANN: trainLoss: 0.2246 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 246 | ANN: trainLoss: 0.2211 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 246 | ANN: trainLoss: 0.2375 | trainAcc: 90.3646% (347/384)\n",
            "6 13 Epoch: 246 | ANN: trainLoss: 0.2318 | trainAcc: 90.8482% (407/448)\n",
            "7 13 Epoch: 246 | ANN: trainLoss: 0.2474 | trainAcc: 90.2344% (462/512)\n",
            "8 13 Epoch: 246 | ANN: trainLoss: 0.2515 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 246 | ANN: trainLoss: 0.2519 | trainAcc: 89.8438% (575/640)\n",
            "10 13 Epoch: 246 | ANN: trainLoss: 0.2518 | trainAcc: 89.9148% (633/704)\n",
            "11 13 Epoch: 246 | ANN: trainLoss: 0.2535 | trainAcc: 89.7135% (689/768)\n",
            "12 13 Epoch: 246 | ANN: trainLoss: 0.2583 | trainAcc: 89.6373% (692/772)\n",
            "0 4 Epoch: 246 | ANN: testLoss: 0.4325 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 246 | ANN: testLoss: 0.4886 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 246 | ANN: testLoss: 0.4652 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 246 | ANN: testLoss: 0.3572 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 247 | ANN: trainLoss: 0.3687 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 247 | ANN: trainLoss: 0.3129 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 247 | ANN: trainLoss: 0.3114 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 247 | ANN: trainLoss: 0.2951 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 247 | ANN: trainLoss: 0.2804 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 247 | ANN: trainLoss: 0.2962 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 247 | ANN: trainLoss: 0.2915 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 247 | ANN: trainLoss: 0.2810 | trainAcc: 89.8438% (460/512)\n",
            "8 13 Epoch: 247 | ANN: trainLoss: 0.2659 | trainAcc: 90.4514% (521/576)\n",
            "9 13 Epoch: 247 | ANN: trainLoss: 0.2706 | trainAcc: 90.4688% (579/640)\n",
            "10 13 Epoch: 247 | ANN: trainLoss: 0.2624 | trainAcc: 90.6250% (638/704)\n",
            "11 13 Epoch: 247 | ANN: trainLoss: 0.2682 | trainAcc: 90.4948% (695/768)\n",
            "12 13 Epoch: 247 | ANN: trainLoss: 0.2820 | trainAcc: 90.2850% (697/772)\n",
            "0 4 Epoch: 247 | ANN: testLoss: 0.4249 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 247 | ANN: testLoss: 0.4625 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 247 | ANN: testLoss: 0.4561 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 247 | ANN: testLoss: 0.4265 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 248 | ANN: trainLoss: 0.2705 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 248 | ANN: trainLoss: 0.2606 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 248 | ANN: trainLoss: 0.2370 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 248 | ANN: trainLoss: 0.2297 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 248 | ANN: trainLoss: 0.2368 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 248 | ANN: trainLoss: 0.2582 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 248 | ANN: trainLoss: 0.2487 | trainAcc: 90.8482% (407/448)\n",
            "7 13 Epoch: 248 | ANN: trainLoss: 0.2478 | trainAcc: 91.2109% (467/512)\n",
            "8 13 Epoch: 248 | ANN: trainLoss: 0.2524 | trainAcc: 90.7986% (523/576)\n",
            "9 13 Epoch: 248 | ANN: trainLoss: 0.2574 | trainAcc: 90.3125% (578/640)\n",
            "10 13 Epoch: 248 | ANN: trainLoss: 0.2625 | trainAcc: 90.0568% (634/704)\n",
            "11 13 Epoch: 248 | ANN: trainLoss: 0.2658 | trainAcc: 89.5833% (688/768)\n",
            "12 13 Epoch: 248 | ANN: trainLoss: 0.2572 | trainAcc: 89.6373% (692/772)\n",
            "0 4 Epoch: 248 | ANN: testLoss: 0.4224 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 248 | ANN: testLoss: 0.4058 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 248 | ANN: testLoss: 0.4487 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 248 | ANN: testLoss: 0.3992 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 249 | ANN: trainLoss: 0.2668 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 249 | ANN: trainLoss: 0.2210 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 249 | ANN: trainLoss: 0.2329 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 249 | ANN: trainLoss: 0.2617 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 249 | ANN: trainLoss: 0.2873 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 249 | ANN: trainLoss: 0.2918 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 249 | ANN: trainLoss: 0.2957 | trainAcc: 87.2768% (391/448)\n",
            "7 13 Epoch: 249 | ANN: trainLoss: 0.2859 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 249 | ANN: trainLoss: 0.2759 | trainAcc: 88.0208% (507/576)\n",
            "9 13 Epoch: 249 | ANN: trainLoss: 0.2645 | trainAcc: 88.7500% (568/640)\n",
            "10 13 Epoch: 249 | ANN: trainLoss: 0.2619 | trainAcc: 88.6364% (624/704)\n",
            "11 13 Epoch: 249 | ANN: trainLoss: 0.2646 | trainAcc: 88.5417% (680/768)\n",
            "12 13 Epoch: 249 | ANN: trainLoss: 0.4217 | trainAcc: 88.3420% (682/772)\n",
            "0 4 Epoch: 249 | ANN: testLoss: 0.4065 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 249 | ANN: testLoss: 0.4675 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 249 | ANN: testLoss: 0.4521 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 249 | ANN: testLoss: 0.4618 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 250 | ANN: trainLoss: 0.2194 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 250 | ANN: trainLoss: 0.2487 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 250 | ANN: trainLoss: 0.2504 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 250 | ANN: trainLoss: 0.2390 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 250 | ANN: trainLoss: 0.2402 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 250 | ANN: trainLoss: 0.2342 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 250 | ANN: trainLoss: 0.2276 | trainAcc: 90.4018% (405/448)\n",
            "7 13 Epoch: 250 | ANN: trainLoss: 0.2258 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 250 | ANN: trainLoss: 0.2286 | trainAcc: 90.4514% (521/576)\n",
            "9 13 Epoch: 250 | ANN: trainLoss: 0.2385 | trainAcc: 89.8438% (575/640)\n",
            "10 13 Epoch: 250 | ANN: trainLoss: 0.2536 | trainAcc: 89.2045% (628/704)\n",
            "11 13 Epoch: 250 | ANN: trainLoss: 0.2545 | trainAcc: 89.3229% (686/768)\n",
            "12 13 Epoch: 250 | ANN: trainLoss: 0.2471 | trainAcc: 89.3782% (690/772)\n",
            "0 4 Epoch: 250 | ANN: testLoss: 0.3926 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 250 | ANN: testLoss: 0.4471 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 250 | ANN: testLoss: 0.4636 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 250 | ANN: testLoss: 0.3557 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 251 | ANN: trainLoss: 0.4199 | trainAcc: 78.1250% (50/64)\n",
            "1 13 Epoch: 251 | ANN: trainLoss: 0.3322 | trainAcc: 82.8125% (106/128)\n",
            "2 13 Epoch: 251 | ANN: trainLoss: 0.3236 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 251 | ANN: trainLoss: 0.2958 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 251 | ANN: trainLoss: 0.2856 | trainAcc: 86.8750% (278/320)\n",
            "5 13 Epoch: 251 | ANN: trainLoss: 0.2964 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 251 | ANN: trainLoss: 0.3033 | trainAcc: 86.1607% (386/448)\n",
            "7 13 Epoch: 251 | ANN: trainLoss: 0.3086 | trainAcc: 85.7422% (439/512)\n",
            "8 13 Epoch: 251 | ANN: trainLoss: 0.2994 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 251 | ANN: trainLoss: 0.3023 | trainAcc: 86.5625% (554/640)\n",
            "10 13 Epoch: 251 | ANN: trainLoss: 0.2943 | trainAcc: 87.0739% (613/704)\n",
            "11 13 Epoch: 251 | ANN: trainLoss: 0.2970 | trainAcc: 87.3698% (671/768)\n",
            "12 13 Epoch: 251 | ANN: trainLoss: 0.2834 | trainAcc: 87.4352% (675/772)\n",
            "0 4 Epoch: 251 | ANN: testLoss: 0.4434 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 251 | ANN: testLoss: 0.4391 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 251 | ANN: testLoss: 0.4446 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 251 | ANN: testLoss: 0.5250 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 252 | ANN: trainLoss: 0.1580 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 252 | ANN: trainLoss: 0.1830 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 252 | ANN: trainLoss: 0.1856 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 252 | ANN: trainLoss: 0.1880 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 252 | ANN: trainLoss: 0.2042 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 252 | ANN: trainLoss: 0.2205 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 252 | ANN: trainLoss: 0.2230 | trainAcc: 89.7321% (402/448)\n",
            "7 13 Epoch: 252 | ANN: trainLoss: 0.2227 | trainAcc: 89.6484% (459/512)\n",
            "8 13 Epoch: 252 | ANN: trainLoss: 0.2235 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 252 | ANN: trainLoss: 0.2335 | trainAcc: 90.0000% (576/640)\n",
            "10 13 Epoch: 252 | ANN: trainLoss: 0.2413 | trainAcc: 90.0568% (634/704)\n",
            "11 13 Epoch: 252 | ANN: trainLoss: 0.2450 | trainAcc: 89.7135% (689/768)\n",
            "12 13 Epoch: 252 | ANN: trainLoss: 0.3719 | trainAcc: 89.3782% (690/772)\n",
            "0 4 Epoch: 252 | ANN: testLoss: 0.3510 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 252 | ANN: testLoss: 0.4449 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 252 | ANN: testLoss: 0.4482 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 252 | ANN: testLoss: 0.3362 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 253 | ANN: trainLoss: 0.3063 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 253 | ANN: trainLoss: 0.2578 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 253 | ANN: trainLoss: 0.3226 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 253 | ANN: trainLoss: 0.3165 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 253 | ANN: trainLoss: 0.3107 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 253 | ANN: trainLoss: 0.3079 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 253 | ANN: trainLoss: 0.2971 | trainAcc: 88.3929% (396/448)\n",
            "7 13 Epoch: 253 | ANN: trainLoss: 0.2980 | trainAcc: 88.0859% (451/512)\n",
            "8 13 Epoch: 253 | ANN: trainLoss: 0.3009 | trainAcc: 88.0208% (507/576)\n",
            "9 13 Epoch: 253 | ANN: trainLoss: 0.2914 | trainAcc: 88.7500% (568/640)\n",
            "10 13 Epoch: 253 | ANN: trainLoss: 0.2897 | trainAcc: 88.4943% (623/704)\n",
            "11 13 Epoch: 253 | ANN: trainLoss: 0.2813 | trainAcc: 88.8021% (682/768)\n",
            "12 13 Epoch: 253 | ANN: trainLoss: 0.3602 | trainAcc: 88.6010% (684/772)\n",
            "0 4 Epoch: 253 | ANN: testLoss: 0.3630 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 253 | ANN: testLoss: 0.4651 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 253 | ANN: testLoss: 0.4508 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 253 | ANN: testLoss: 0.5144 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 254 | ANN: trainLoss: 0.2353 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 254 | ANN: trainLoss: 0.2172 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 254 | ANN: trainLoss: 0.2438 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 254 | ANN: trainLoss: 0.2795 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 254 | ANN: trainLoss: 0.2807 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 254 | ANN: trainLoss: 0.2924 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 254 | ANN: trainLoss: 0.2949 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 254 | ANN: trainLoss: 0.2857 | trainAcc: 88.0859% (451/512)\n",
            "8 13 Epoch: 254 | ANN: trainLoss: 0.3010 | trainAcc: 87.5000% (504/576)\n",
            "9 13 Epoch: 254 | ANN: trainLoss: 0.3064 | trainAcc: 87.0312% (557/640)\n",
            "10 13 Epoch: 254 | ANN: trainLoss: 0.3198 | trainAcc: 86.7898% (611/704)\n",
            "11 13 Epoch: 254 | ANN: trainLoss: 0.3170 | trainAcc: 86.9792% (668/768)\n",
            "12 13 Epoch: 254 | ANN: trainLoss: 0.3103 | trainAcc: 86.9171% (671/772)\n",
            "0 4 Epoch: 254 | ANN: testLoss: 0.4452 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 254 | ANN: testLoss: 0.4722 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 254 | ANN: testLoss: 0.4841 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 254 | ANN: testLoss: 0.5516 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 255 | ANN: trainLoss: 0.3261 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 255 | ANN: trainLoss: 0.3219 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 255 | ANN: trainLoss: 0.3200 | trainAcc: 85.4167% (164/192)\n",
            "3 13 Epoch: 255 | ANN: trainLoss: 0.3167 | trainAcc: 85.9375% (220/256)\n",
            "4 13 Epoch: 255 | ANN: trainLoss: 0.3184 | trainAcc: 85.9375% (275/320)\n",
            "5 13 Epoch: 255 | ANN: trainLoss: 0.3259 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 255 | ANN: trainLoss: 0.3075 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 255 | ANN: trainLoss: 0.3129 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 255 | ANN: trainLoss: 0.3101 | trainAcc: 87.1528% (502/576)\n",
            "9 13 Epoch: 255 | ANN: trainLoss: 0.2998 | trainAcc: 87.8125% (562/640)\n",
            "10 13 Epoch: 255 | ANN: trainLoss: 0.2998 | trainAcc: 87.6420% (617/704)\n",
            "11 13 Epoch: 255 | ANN: trainLoss: 0.3044 | trainAcc: 87.5000% (672/768)\n",
            "12 13 Epoch: 255 | ANN: trainLoss: 0.3181 | trainAcc: 87.4352% (675/772)\n",
            "0 4 Epoch: 255 | ANN: testLoss: 0.5082 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 255 | ANN: testLoss: 0.5589 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 255 | ANN: testLoss: 0.5045 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 255 | ANN: testLoss: 0.3800 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 256 | ANN: trainLoss: 0.3172 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 256 | ANN: trainLoss: 0.2548 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 256 | ANN: trainLoss: 0.2167 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 256 | ANN: trainLoss: 0.2526 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 256 | ANN: trainLoss: 0.2827 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 256 | ANN: trainLoss: 0.2801 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 256 | ANN: trainLoss: 0.2856 | trainAcc: 88.3929% (396/448)\n",
            "7 13 Epoch: 256 | ANN: trainLoss: 0.2691 | trainAcc: 89.2578% (457/512)\n",
            "8 13 Epoch: 256 | ANN: trainLoss: 0.2786 | trainAcc: 89.4097% (515/576)\n",
            "9 13 Epoch: 256 | ANN: trainLoss: 0.2776 | trainAcc: 89.0625% (570/640)\n",
            "10 13 Epoch: 256 | ANN: trainLoss: 0.2794 | trainAcc: 88.7784% (625/704)\n",
            "11 13 Epoch: 256 | ANN: trainLoss: 0.2806 | trainAcc: 88.5417% (680/768)\n",
            "12 13 Epoch: 256 | ANN: trainLoss: 0.2942 | trainAcc: 88.4715% (683/772)\n",
            "0 4 Epoch: 256 | ANN: testLoss: 0.4877 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 256 | ANN: testLoss: 0.5258 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 256 | ANN: testLoss: 0.5050 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 256 | ANN: testLoss: 0.5642 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 257 | ANN: trainLoss: 0.3692 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 257 | ANN: trainLoss: 0.3176 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 257 | ANN: trainLoss: 0.3623 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 257 | ANN: trainLoss: 0.3781 | trainAcc: 87.1094% (223/256)\n",
            "4 13 Epoch: 257 | ANN: trainLoss: 0.3795 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 257 | ANN: trainLoss: 0.3475 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 257 | ANN: trainLoss: 0.3643 | trainAcc: 87.2768% (391/448)\n",
            "7 13 Epoch: 257 | ANN: trainLoss: 0.3535 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 257 | ANN: trainLoss: 0.3410 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 257 | ANN: trainLoss: 0.3272 | trainAcc: 88.2812% (565/640)\n",
            "10 13 Epoch: 257 | ANN: trainLoss: 0.3248 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 257 | ANN: trainLoss: 0.3319 | trainAcc: 87.6302% (673/768)\n",
            "12 13 Epoch: 257 | ANN: trainLoss: 0.3248 | trainAcc: 87.6943% (677/772)\n",
            "0 4 Epoch: 257 | ANN: testLoss: 0.5763 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 257 | ANN: testLoss: 0.5124 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 257 | ANN: testLoss: 0.4775 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 257 | ANN: testLoss: 0.3703 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 258 | ANN: trainLoss: 0.2417 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 258 | ANN: trainLoss: 0.2348 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 258 | ANN: trainLoss: 0.2961 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 258 | ANN: trainLoss: 0.2911 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 258 | ANN: trainLoss: 0.2834 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 258 | ANN: trainLoss: 0.2999 | trainAcc: 88.0208% (338/384)\n",
            "6 13 Epoch: 258 | ANN: trainLoss: 0.2976 | trainAcc: 88.3929% (396/448)\n",
            "7 13 Epoch: 258 | ANN: trainLoss: 0.2845 | trainAcc: 89.2578% (457/512)\n",
            "8 13 Epoch: 258 | ANN: trainLoss: 0.2883 | trainAcc: 89.4097% (515/576)\n",
            "9 13 Epoch: 258 | ANN: trainLoss: 0.2847 | trainAcc: 89.5312% (573/640)\n",
            "10 13 Epoch: 258 | ANN: trainLoss: 0.2895 | trainAcc: 89.2045% (628/704)\n",
            "11 13 Epoch: 258 | ANN: trainLoss: 0.2921 | trainAcc: 88.9323% (683/768)\n",
            "12 13 Epoch: 258 | ANN: trainLoss: 0.2932 | trainAcc: 88.8601% (686/772)\n",
            "0 4 Epoch: 258 | ANN: testLoss: 0.4541 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 258 | ANN: testLoss: 0.4951 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 258 | ANN: testLoss: 0.4702 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 258 | ANN: testLoss: 0.3543 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 259 | ANN: trainLoss: 0.2006 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 259 | ANN: trainLoss: 0.2913 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 259 | ANN: trainLoss: 0.2942 | trainAcc: 85.9375% (165/192)\n",
            "3 13 Epoch: 259 | ANN: trainLoss: 0.2814 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 259 | ANN: trainLoss: 0.2859 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 259 | ANN: trainLoss: 0.2894 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 259 | ANN: trainLoss: 0.2950 | trainAcc: 88.3929% (396/448)\n",
            "7 13 Epoch: 259 | ANN: trainLoss: 0.2821 | trainAcc: 89.2578% (457/512)\n",
            "8 13 Epoch: 259 | ANN: trainLoss: 0.2661 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 259 | ANN: trainLoss: 0.2768 | trainAcc: 89.6875% (574/640)\n",
            "10 13 Epoch: 259 | ANN: trainLoss: 0.2733 | trainAcc: 90.0568% (634/704)\n",
            "11 13 Epoch: 259 | ANN: trainLoss: 0.2709 | trainAcc: 89.9740% (691/768)\n",
            "12 13 Epoch: 259 | ANN: trainLoss: 0.2624 | trainAcc: 90.0259% (695/772)\n",
            "0 4 Epoch: 259 | ANN: testLoss: 0.6690 | testAcc: 62.5000% (40/64)\n",
            "1 4 Epoch: 259 | ANN: testLoss: 0.5352 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 259 | ANN: testLoss: 0.4716 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 259 | ANN: testLoss: 0.5863 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 260 | ANN: trainLoss: 0.3037 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 260 | ANN: trainLoss: 0.2529 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 260 | ANN: trainLoss: 0.2649 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 260 | ANN: trainLoss: 0.2722 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 260 | ANN: trainLoss: 0.2588 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 260 | ANN: trainLoss: 0.2512 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 260 | ANN: trainLoss: 0.2573 | trainAcc: 90.1786% (404/448)\n",
            "7 13 Epoch: 260 | ANN: trainLoss: 0.2511 | trainAcc: 90.8203% (465/512)\n",
            "8 13 Epoch: 260 | ANN: trainLoss: 0.2617 | trainAcc: 90.7986% (523/576)\n",
            "9 13 Epoch: 260 | ANN: trainLoss: 0.2637 | trainAcc: 90.7812% (581/640)\n",
            "10 13 Epoch: 260 | ANN: trainLoss: 0.2573 | trainAcc: 90.6250% (638/704)\n",
            "11 13 Epoch: 260 | ANN: trainLoss: 0.2533 | trainAcc: 90.7552% (697/768)\n",
            "12 13 Epoch: 260 | ANN: trainLoss: 0.2649 | trainAcc: 90.6736% (700/772)\n",
            "0 4 Epoch: 260 | ANN: testLoss: 0.5305 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 260 | ANN: testLoss: 0.4761 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 260 | ANN: testLoss: 0.4684 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 260 | ANN: testLoss: 0.3612 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 261 | ANN: trainLoss: 0.2807 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 261 | ANN: trainLoss: 0.2272 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 261 | ANN: trainLoss: 0.2382 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 261 | ANN: trainLoss: 0.2211 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 261 | ANN: trainLoss: 0.2427 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 261 | ANN: trainLoss: 0.2358 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 261 | ANN: trainLoss: 0.2282 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 261 | ANN: trainLoss: 0.2179 | trainAcc: 92.5781% (474/512)\n",
            "8 13 Epoch: 261 | ANN: trainLoss: 0.2211 | trainAcc: 92.0139% (530/576)\n",
            "9 13 Epoch: 261 | ANN: trainLoss: 0.2267 | trainAcc: 91.5625% (586/640)\n",
            "10 13 Epoch: 261 | ANN: trainLoss: 0.2311 | trainAcc: 91.4773% (644/704)\n",
            "11 13 Epoch: 261 | ANN: trainLoss: 0.2384 | trainAcc: 91.1458% (700/768)\n",
            "12 13 Epoch: 261 | ANN: trainLoss: 0.2284 | trainAcc: 91.1917% (704/772)\n",
            "0 4 Epoch: 261 | ANN: testLoss: 0.5114 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 261 | ANN: testLoss: 0.4967 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 261 | ANN: testLoss: 0.4639 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 261 | ANN: testLoss: 0.3479 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 262 | ANN: trainLoss: 0.2279 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 262 | ANN: trainLoss: 0.2318 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 262 | ANN: trainLoss: 0.2127 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 262 | ANN: trainLoss: 0.2259 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 262 | ANN: trainLoss: 0.2282 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 262 | ANN: trainLoss: 0.2328 | trainAcc: 89.0625% (342/384)\n",
            "6 13 Epoch: 262 | ANN: trainLoss: 0.2352 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 262 | ANN: trainLoss: 0.2428 | trainAcc: 88.8672% (455/512)\n",
            "8 13 Epoch: 262 | ANN: trainLoss: 0.2487 | trainAcc: 89.0625% (513/576)\n",
            "9 13 Epoch: 262 | ANN: trainLoss: 0.2425 | trainAcc: 89.2188% (571/640)\n",
            "10 13 Epoch: 262 | ANN: trainLoss: 0.2387 | trainAcc: 89.7727% (632/704)\n",
            "11 13 Epoch: 262 | ANN: trainLoss: 0.2400 | trainAcc: 89.5833% (688/768)\n",
            "12 13 Epoch: 262 | ANN: trainLoss: 0.3327 | trainAcc: 89.5078% (691/772)\n",
            "0 4 Epoch: 262 | ANN: testLoss: 0.6636 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 262 | ANN: testLoss: 0.6417 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 262 | ANN: testLoss: 0.5691 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 262 | ANN: testLoss: 0.5093 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 263 | ANN: trainLoss: 0.2760 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 263 | ANN: trainLoss: 0.3503 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 263 | ANN: trainLoss: 0.3287 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 263 | ANN: trainLoss: 0.3238 | trainAcc: 86.7188% (222/256)\n",
            "4 13 Epoch: 263 | ANN: trainLoss: 0.3590 | trainAcc: 86.2500% (276/320)\n",
            "5 13 Epoch: 263 | ANN: trainLoss: 0.3807 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 263 | ANN: trainLoss: 0.3982 | trainAcc: 86.1607% (386/448)\n",
            "7 13 Epoch: 263 | ANN: trainLoss: 0.4033 | trainAcc: 86.3281% (442/512)\n",
            "8 13 Epoch: 263 | ANN: trainLoss: 0.4056 | trainAcc: 86.2847% (497/576)\n",
            "9 13 Epoch: 263 | ANN: trainLoss: 0.4445 | trainAcc: 84.5312% (541/640)\n",
            "10 13 Epoch: 263 | ANN: trainLoss: 0.4363 | trainAcc: 84.6591% (596/704)\n",
            "11 13 Epoch: 263 | ANN: trainLoss: 0.4372 | trainAcc: 84.6354% (650/768)\n",
            "12 13 Epoch: 263 | ANN: trainLoss: 0.4573 | trainAcc: 84.5855% (653/772)\n",
            "0 4 Epoch: 263 | ANN: testLoss: 0.6768 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 263 | ANN: testLoss: 0.6544 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 263 | ANN: testLoss: 0.6229 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 263 | ANN: testLoss: 0.5425 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 264 | ANN: trainLoss: 0.5149 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 264 | ANN: trainLoss: 0.3602 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 264 | ANN: trainLoss: 0.3597 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 264 | ANN: trainLoss: 0.3522 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 264 | ANN: trainLoss: 0.3187 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 264 | ANN: trainLoss: 0.3294 | trainAcc: 85.9375% (330/384)\n",
            "6 13 Epoch: 264 | ANN: trainLoss: 0.3298 | trainAcc: 85.9375% (385/448)\n",
            "7 13 Epoch: 264 | ANN: trainLoss: 0.3334 | trainAcc: 85.3516% (437/512)\n",
            "8 13 Epoch: 264 | ANN: trainLoss: 0.3496 | trainAcc: 84.8958% (489/576)\n",
            "9 13 Epoch: 264 | ANN: trainLoss: 0.3424 | trainAcc: 85.1562% (545/640)\n",
            "10 13 Epoch: 264 | ANN: trainLoss: 0.3605 | trainAcc: 84.8011% (597/704)\n",
            "11 13 Epoch: 264 | ANN: trainLoss: 0.3575 | trainAcc: 84.8958% (652/768)\n",
            "12 13 Epoch: 264 | ANN: trainLoss: 0.3521 | trainAcc: 84.8446% (655/772)\n",
            "0 4 Epoch: 264 | ANN: testLoss: 0.8165 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 264 | ANN: testLoss: 0.7439 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 264 | ANN: testLoss: 0.7189 | testAcc: 71.8750% (138/192)\n",
            "3 4 Epoch: 264 | ANN: testLoss: 0.5392 | testAcc: 72.0207% (139/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 265 | ANN: trainLoss: 0.3332 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 265 | ANN: trainLoss: 0.3135 | trainAcc: 85.1562% (109/128)\n",
            "2 13 Epoch: 265 | ANN: trainLoss: 0.3327 | trainAcc: 84.8958% (163/192)\n",
            "3 13 Epoch: 265 | ANN: trainLoss: 0.3514 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 265 | ANN: trainLoss: 0.3505 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 265 | ANN: trainLoss: 0.3352 | trainAcc: 85.6771% (329/384)\n",
            "6 13 Epoch: 265 | ANN: trainLoss: 0.3276 | trainAcc: 86.3839% (387/448)\n",
            "7 13 Epoch: 265 | ANN: trainLoss: 0.3468 | trainAcc: 85.9375% (440/512)\n",
            "8 13 Epoch: 265 | ANN: trainLoss: 0.3451 | trainAcc: 85.9375% (495/576)\n",
            "9 13 Epoch: 265 | ANN: trainLoss: 0.3400 | trainAcc: 86.0938% (551/640)\n",
            "10 13 Epoch: 265 | ANN: trainLoss: 0.3422 | trainAcc: 85.9375% (605/704)\n",
            "11 13 Epoch: 265 | ANN: trainLoss: 0.3319 | trainAcc: 86.1979% (662/768)\n",
            "12 13 Epoch: 265 | ANN: trainLoss: 0.3218 | trainAcc: 86.2694% (666/772)\n",
            "0 4 Epoch: 265 | ANN: testLoss: 0.5062 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 265 | ANN: testLoss: 0.5532 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 265 | ANN: testLoss: 0.6206 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 265 | ANN: testLoss: 0.4656 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 266 | ANN: trainLoss: 0.1867 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 266 | ANN: trainLoss: 0.2635 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 266 | ANN: trainLoss: 0.2271 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 266 | ANN: trainLoss: 0.2673 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 266 | ANN: trainLoss: 0.2595 | trainAcc: 90.0000% (288/320)\n",
            "5 13 Epoch: 266 | ANN: trainLoss: 0.2808 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 266 | ANN: trainLoss: 0.2795 | trainAcc: 88.1696% (395/448)\n",
            "7 13 Epoch: 266 | ANN: trainLoss: 0.2841 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 266 | ANN: trainLoss: 0.3002 | trainAcc: 87.3264% (503/576)\n",
            "9 13 Epoch: 266 | ANN: trainLoss: 0.2951 | trainAcc: 87.1875% (558/640)\n",
            "10 13 Epoch: 266 | ANN: trainLoss: 0.2959 | trainAcc: 86.6477% (610/704)\n",
            "11 13 Epoch: 266 | ANN: trainLoss: 0.2983 | trainAcc: 86.1979% (662/768)\n",
            "12 13 Epoch: 266 | ANN: trainLoss: 0.2872 | trainAcc: 86.2694% (666/772)\n",
            "0 4 Epoch: 266 | ANN: testLoss: 0.5588 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 266 | ANN: testLoss: 0.5549 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 266 | ANN: testLoss: 0.5639 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 266 | ANN: testLoss: 0.4579 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 267 | ANN: trainLoss: 0.2950 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 267 | ANN: trainLoss: 0.2801 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 267 | ANN: trainLoss: 0.2648 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 267 | ANN: trainLoss: 0.2715 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 267 | ANN: trainLoss: 0.2697 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 267 | ANN: trainLoss: 0.2669 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 267 | ANN: trainLoss: 0.2639 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 267 | ANN: trainLoss: 0.2703 | trainAcc: 89.4531% (458/512)\n",
            "8 13 Epoch: 267 | ANN: trainLoss: 0.2610 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 267 | ANN: trainLoss: 0.2534 | trainAcc: 90.1562% (577/640)\n",
            "10 13 Epoch: 267 | ANN: trainLoss: 0.2551 | trainAcc: 89.9148% (633/704)\n",
            "11 13 Epoch: 267 | ANN: trainLoss: 0.2535 | trainAcc: 89.9740% (691/768)\n",
            "12 13 Epoch: 267 | ANN: trainLoss: 0.2409 | trainAcc: 90.0259% (695/772)\n",
            "0 4 Epoch: 267 | ANN: testLoss: 0.4207 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 267 | ANN: testLoss: 0.4999 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 267 | ANN: testLoss: 0.5064 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 267 | ANN: testLoss: 0.4535 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 268 | ANN: trainLoss: 0.2832 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 268 | ANN: trainLoss: 0.2397 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 268 | ANN: trainLoss: 0.2376 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 268 | ANN: trainLoss: 0.2471 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 268 | ANN: trainLoss: 0.2430 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 268 | ANN: trainLoss: 0.2444 | trainAcc: 91.4062% (351/384)\n",
            "6 13 Epoch: 268 | ANN: trainLoss: 0.2430 | trainAcc: 91.5179% (410/448)\n",
            "7 13 Epoch: 268 | ANN: trainLoss: 0.2585 | trainAcc: 90.8203% (465/512)\n",
            "8 13 Epoch: 268 | ANN: trainLoss: 0.2517 | trainAcc: 90.9722% (524/576)\n",
            "9 13 Epoch: 268 | ANN: trainLoss: 0.2453 | trainAcc: 91.0938% (583/640)\n",
            "10 13 Epoch: 268 | ANN: trainLoss: 0.2435 | trainAcc: 91.0511% (641/704)\n",
            "11 13 Epoch: 268 | ANN: trainLoss: 0.2417 | trainAcc: 91.2760% (701/768)\n",
            "12 13 Epoch: 268 | ANN: trainLoss: 0.2620 | trainAcc: 91.1917% (704/772)\n",
            "0 4 Epoch: 268 | ANN: testLoss: 0.5121 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 268 | ANN: testLoss: 0.4793 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 268 | ANN: testLoss: 0.4961 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 268 | ANN: testLoss: 0.4565 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 269 | ANN: trainLoss: 0.2233 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 269 | ANN: trainLoss: 0.3006 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 269 | ANN: trainLoss: 0.2608 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 269 | ANN: trainLoss: 0.2257 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 269 | ANN: trainLoss: 0.2230 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 269 | ANN: trainLoss: 0.2296 | trainAcc: 90.6250% (348/384)\n",
            "6 13 Epoch: 269 | ANN: trainLoss: 0.2428 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 269 | ANN: trainLoss: 0.2381 | trainAcc: 89.8438% (460/512)\n",
            "8 13 Epoch: 269 | ANN: trainLoss: 0.2519 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 269 | ANN: trainLoss: 0.2596 | trainAcc: 88.4375% (566/640)\n",
            "10 13 Epoch: 269 | ANN: trainLoss: 0.2703 | trainAcc: 88.2102% (621/704)\n",
            "11 13 Epoch: 269 | ANN: trainLoss: 0.2754 | trainAcc: 88.4115% (679/768)\n",
            "12 13 Epoch: 269 | ANN: trainLoss: 0.4788 | trainAcc: 88.2124% (681/772)\n",
            "0 4 Epoch: 269 | ANN: testLoss: 0.5740 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 269 | ANN: testLoss: 0.5426 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 269 | ANN: testLoss: 0.5251 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 269 | ANN: testLoss: 0.6319 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 270 | ANN: trainLoss: 0.1917 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 270 | ANN: trainLoss: 0.2646 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 270 | ANN: trainLoss: 0.2651 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 270 | ANN: trainLoss: 0.2671 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 270 | ANN: trainLoss: 0.2841 | trainAcc: 90.0000% (288/320)\n",
            "5 13 Epoch: 270 | ANN: trainLoss: 0.3013 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 270 | ANN: trainLoss: 0.2967 | trainAcc: 89.7321% (402/448)\n",
            "7 13 Epoch: 270 | ANN: trainLoss: 0.3001 | trainAcc: 89.8438% (460/512)\n",
            "8 13 Epoch: 270 | ANN: trainLoss: 0.2922 | trainAcc: 90.2778% (520/576)\n",
            "9 13 Epoch: 270 | ANN: trainLoss: 0.3007 | trainAcc: 90.0000% (576/640)\n",
            "10 13 Epoch: 270 | ANN: trainLoss: 0.3200 | trainAcc: 88.9205% (626/704)\n",
            "11 13 Epoch: 270 | ANN: trainLoss: 0.3165 | trainAcc: 89.0625% (684/768)\n",
            "12 13 Epoch: 270 | ANN: trainLoss: 0.4632 | trainAcc: 88.7306% (685/772)\n",
            "0 4 Epoch: 270 | ANN: testLoss: 0.6150 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 270 | ANN: testLoss: 0.5591 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 270 | ANN: testLoss: 0.5350 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 270 | ANN: testLoss: 0.6392 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 271 | ANN: trainLoss: 0.2655 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 271 | ANN: trainLoss: 0.2678 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 271 | ANN: trainLoss: 0.2708 | trainAcc: 86.9792% (167/192)\n",
            "3 13 Epoch: 271 | ANN: trainLoss: 0.2734 | trainAcc: 86.3281% (221/256)\n",
            "4 13 Epoch: 271 | ANN: trainLoss: 0.2826 | trainAcc: 85.6250% (274/320)\n",
            "5 13 Epoch: 271 | ANN: trainLoss: 0.2783 | trainAcc: 86.1979% (331/384)\n",
            "6 13 Epoch: 271 | ANN: trainLoss: 0.2783 | trainAcc: 86.8304% (389/448)\n",
            "7 13 Epoch: 271 | ANN: trainLoss: 0.2774 | trainAcc: 87.1094% (446/512)\n",
            "8 13 Epoch: 271 | ANN: trainLoss: 0.2979 | trainAcc: 86.4583% (498/576)\n",
            "9 13 Epoch: 271 | ANN: trainLoss: 0.3082 | trainAcc: 86.4062% (553/640)\n",
            "10 13 Epoch: 271 | ANN: trainLoss: 0.3213 | trainAcc: 85.7955% (604/704)\n",
            "11 13 Epoch: 271 | ANN: trainLoss: 0.3182 | trainAcc: 85.9375% (660/768)\n",
            "12 13 Epoch: 271 | ANN: trainLoss: 0.3211 | trainAcc: 85.8808% (663/772)\n",
            "0 4 Epoch: 271 | ANN: testLoss: 0.5460 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 271 | ANN: testLoss: 0.5078 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 271 | ANN: testLoss: 0.4907 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 271 | ANN: testLoss: 0.3681 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 272 | ANN: trainLoss: 0.2558 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 272 | ANN: trainLoss: 0.2656 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 272 | ANN: trainLoss: 0.2275 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 272 | ANN: trainLoss: 0.2475 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 272 | ANN: trainLoss: 0.2528 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 272 | ANN: trainLoss: 0.2768 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 272 | ANN: trainLoss: 0.2832 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 272 | ANN: trainLoss: 0.2850 | trainAcc: 88.6719% (454/512)\n",
            "8 13 Epoch: 272 | ANN: trainLoss: 0.2875 | trainAcc: 88.8889% (512/576)\n",
            "9 13 Epoch: 272 | ANN: trainLoss: 0.3064 | trainAcc: 87.5000% (560/640)\n",
            "10 13 Epoch: 272 | ANN: trainLoss: 0.2973 | trainAcc: 88.3523% (622/704)\n",
            "11 13 Epoch: 272 | ANN: trainLoss: 0.3009 | trainAcc: 88.0208% (676/768)\n",
            "12 13 Epoch: 272 | ANN: trainLoss: 0.5561 | trainAcc: 87.6943% (677/772)\n",
            "0 4 Epoch: 272 | ANN: testLoss: 0.6932 | testAcc: 64.0625% (41/64)\n",
            "1 4 Epoch: 272 | ANN: testLoss: 0.5548 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 272 | ANN: testLoss: 0.5173 | testAcc: 73.9583% (142/192)\n",
            "3 4 Epoch: 272 | ANN: testLoss: 0.4008 | testAcc: 74.0933% (143/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 273 | ANN: trainLoss: 0.3186 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 273 | ANN: trainLoss: 0.2940 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 273 | ANN: trainLoss: 0.3059 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 273 | ANN: trainLoss: 0.2968 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 273 | ANN: trainLoss: 0.2859 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 273 | ANN: trainLoss: 0.2711 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 273 | ANN: trainLoss: 0.2720 | trainAcc: 89.7321% (402/448)\n",
            "7 13 Epoch: 273 | ANN: trainLoss: 0.2755 | trainAcc: 89.8438% (460/512)\n",
            "8 13 Epoch: 273 | ANN: trainLoss: 0.2757 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 273 | ANN: trainLoss: 0.2775 | trainAcc: 89.8438% (575/640)\n",
            "10 13 Epoch: 273 | ANN: trainLoss: 0.2791 | trainAcc: 89.9148% (633/704)\n",
            "11 13 Epoch: 273 | ANN: trainLoss: 0.2745 | trainAcc: 89.7135% (689/768)\n",
            "12 13 Epoch: 273 | ANN: trainLoss: 0.2822 | trainAcc: 89.6373% (692/772)\n",
            "0 4 Epoch: 273 | ANN: testLoss: 0.5513 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 273 | ANN: testLoss: 0.5657 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 273 | ANN: testLoss: 0.5520 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 273 | ANN: testLoss: 0.5658 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 274 | ANN: trainLoss: 0.2375 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 274 | ANN: trainLoss: 0.3021 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 274 | ANN: trainLoss: 0.3193 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 274 | ANN: trainLoss: 0.3123 | trainAcc: 87.8906% (225/256)\n",
            "4 13 Epoch: 274 | ANN: trainLoss: 0.3068 | trainAcc: 87.5000% (280/320)\n",
            "5 13 Epoch: 274 | ANN: trainLoss: 0.3284 | trainAcc: 87.2396% (335/384)\n",
            "6 13 Epoch: 274 | ANN: trainLoss: 0.3262 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 274 | ANN: trainLoss: 0.3223 | trainAcc: 87.6953% (449/512)\n",
            "8 13 Epoch: 274 | ANN: trainLoss: 0.3178 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 274 | ANN: trainLoss: 0.3022 | trainAcc: 88.5938% (567/640)\n",
            "10 13 Epoch: 274 | ANN: trainLoss: 0.2981 | trainAcc: 88.6364% (624/704)\n",
            "11 13 Epoch: 274 | ANN: trainLoss: 0.2912 | trainAcc: 88.8021% (682/768)\n",
            "12 13 Epoch: 274 | ANN: trainLoss: 0.3875 | trainAcc: 88.6010% (684/772)\n",
            "0 4 Epoch: 274 | ANN: testLoss: 0.6217 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 274 | ANN: testLoss: 0.5560 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 274 | ANN: testLoss: 0.5066 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 274 | ANN: testLoss: 0.4672 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 275 | ANN: trainLoss: 0.3721 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 275 | ANN: trainLoss: 0.3160 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 275 | ANN: trainLoss: 0.3197 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 275 | ANN: trainLoss: 0.2806 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 275 | ANN: trainLoss: 0.2828 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 275 | ANN: trainLoss: 0.2742 | trainAcc: 90.6250% (348/384)\n",
            "6 13 Epoch: 275 | ANN: trainLoss: 0.2756 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 275 | ANN: trainLoss: 0.2780 | trainAcc: 89.4531% (458/512)\n",
            "8 13 Epoch: 275 | ANN: trainLoss: 0.2763 | trainAcc: 89.5833% (516/576)\n",
            "9 13 Epoch: 275 | ANN: trainLoss: 0.2728 | trainAcc: 90.0000% (576/640)\n",
            "10 13 Epoch: 275 | ANN: trainLoss: 0.2708 | trainAcc: 90.0568% (634/704)\n",
            "11 13 Epoch: 275 | ANN: trainLoss: 0.2942 | trainAcc: 88.9323% (683/768)\n",
            "12 13 Epoch: 275 | ANN: trainLoss: 0.4614 | trainAcc: 88.8601% (686/772)\n",
            "0 4 Epoch: 275 | ANN: testLoss: 0.4780 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 275 | ANN: testLoss: 0.5094 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 275 | ANN: testLoss: 0.4742 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 275 | ANN: testLoss: 0.4628 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 276 | ANN: trainLoss: 0.3379 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 276 | ANN: trainLoss: 0.3054 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 276 | ANN: trainLoss: 0.3258 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 276 | ANN: trainLoss: 0.3155 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 276 | ANN: trainLoss: 0.3111 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 276 | ANN: trainLoss: 0.3055 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 276 | ANN: trainLoss: 0.3080 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 276 | ANN: trainLoss: 0.2978 | trainAcc: 88.2812% (452/512)\n",
            "8 13 Epoch: 276 | ANN: trainLoss: 0.2991 | trainAcc: 88.0208% (507/576)\n",
            "9 13 Epoch: 276 | ANN: trainLoss: 0.3101 | trainAcc: 87.3438% (559/640)\n",
            "10 13 Epoch: 276 | ANN: trainLoss: 0.3070 | trainAcc: 87.6420% (617/704)\n",
            "11 13 Epoch: 276 | ANN: trainLoss: 0.3038 | trainAcc: 87.3698% (671/768)\n",
            "12 13 Epoch: 276 | ANN: trainLoss: 0.4270 | trainAcc: 87.0466% (672/772)\n",
            "0 4 Epoch: 276 | ANN: testLoss: 0.5130 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 276 | ANN: testLoss: 0.4837 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 276 | ANN: testLoss: 0.4746 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 276 | ANN: testLoss: 0.4217 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 277 | ANN: trainLoss: 0.3466 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 277 | ANN: trainLoss: 0.2894 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 277 | ANN: trainLoss: 0.2935 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 277 | ANN: trainLoss: 0.2986 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 277 | ANN: trainLoss: 0.3219 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 277 | ANN: trainLoss: 0.3314 | trainAcc: 86.9792% (334/384)\n",
            "6 13 Epoch: 277 | ANN: trainLoss: 0.3465 | trainAcc: 86.1607% (386/448)\n",
            "7 13 Epoch: 277 | ANN: trainLoss: 0.3334 | trainAcc: 86.5234% (443/512)\n",
            "8 13 Epoch: 277 | ANN: trainLoss: 0.3168 | trainAcc: 87.5000% (504/576)\n",
            "9 13 Epoch: 277 | ANN: trainLoss: 0.3107 | trainAcc: 87.3438% (559/640)\n",
            "10 13 Epoch: 277 | ANN: trainLoss: 0.3072 | trainAcc: 87.5000% (616/704)\n",
            "11 13 Epoch: 277 | ANN: trainLoss: 0.2999 | trainAcc: 87.8906% (675/768)\n",
            "12 13 Epoch: 277 | ANN: trainLoss: 0.3216 | trainAcc: 87.6943% (677/772)\n",
            "0 4 Epoch: 277 | ANN: testLoss: 0.6159 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 277 | ANN: testLoss: 0.5275 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 277 | ANN: testLoss: 0.5411 | testAcc: 72.9167% (140/192)\n",
            "3 4 Epoch: 277 | ANN: testLoss: 0.7007 | testAcc: 72.5389% (140/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 278 | ANN: trainLoss: 0.3273 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 278 | ANN: trainLoss: 0.2670 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 278 | ANN: trainLoss: 0.2388 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 278 | ANN: trainLoss: 0.2870 | trainAcc: 90.2344% (231/256)\n",
            "4 13 Epoch: 278 | ANN: trainLoss: 0.2733 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 278 | ANN: trainLoss: 0.2898 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 278 | ANN: trainLoss: 0.3081 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 278 | ANN: trainLoss: 0.3062 | trainAcc: 89.0625% (456/512)\n",
            "8 13 Epoch: 278 | ANN: trainLoss: 0.3311 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 278 | ANN: trainLoss: 0.3291 | trainAcc: 87.5000% (560/640)\n",
            "10 13 Epoch: 278 | ANN: trainLoss: 0.3154 | trainAcc: 88.3523% (622/704)\n",
            "11 13 Epoch: 278 | ANN: trainLoss: 0.3158 | trainAcc: 88.4115% (679/768)\n",
            "12 13 Epoch: 278 | ANN: trainLoss: 0.3074 | trainAcc: 88.4715% (683/772)\n",
            "0 4 Epoch: 278 | ANN: testLoss: 0.4744 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 278 | ANN: testLoss: 0.4734 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 278 | ANN: testLoss: 0.5011 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 278 | ANN: testLoss: 0.6914 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 279 | ANN: trainLoss: 0.4448 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 279 | ANN: trainLoss: 0.4228 | trainAcc: 83.5938% (107/128)\n",
            "2 13 Epoch: 279 | ANN: trainLoss: 0.3507 | trainAcc: 86.4583% (166/192)\n",
            "3 13 Epoch: 279 | ANN: trainLoss: 0.3609 | trainAcc: 83.9844% (215/256)\n",
            "4 13 Epoch: 279 | ANN: trainLoss: 0.3380 | trainAcc: 84.6875% (271/320)\n",
            "5 13 Epoch: 279 | ANN: trainLoss: 0.3371 | trainAcc: 84.1146% (323/384)\n",
            "6 13 Epoch: 279 | ANN: trainLoss: 0.3119 | trainAcc: 85.7143% (384/448)\n",
            "7 13 Epoch: 279 | ANN: trainLoss: 0.3188 | trainAcc: 85.5469% (438/512)\n",
            "8 13 Epoch: 279 | ANN: trainLoss: 0.3164 | trainAcc: 85.7639% (494/576)\n",
            "9 13 Epoch: 279 | ANN: trainLoss: 0.3136 | trainAcc: 85.9375% (550/640)\n",
            "10 13 Epoch: 279 | ANN: trainLoss: 0.3086 | trainAcc: 86.2216% (607/704)\n",
            "11 13 Epoch: 279 | ANN: trainLoss: 0.2998 | trainAcc: 86.9792% (668/768)\n",
            "12 13 Epoch: 279 | ANN: trainLoss: 0.2798 | trainAcc: 87.0466% (672/772)\n",
            "0 4 Epoch: 279 | ANN: testLoss: 0.5236 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 279 | ANN: testLoss: 0.5070 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 279 | ANN: testLoss: 0.4785 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 279 | ANN: testLoss: 0.4126 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 280 | ANN: trainLoss: 0.1790 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 280 | ANN: trainLoss: 0.2020 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 280 | ANN: trainLoss: 0.2226 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 280 | ANN: trainLoss: 0.2129 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 280 | ANN: trainLoss: 0.2402 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 280 | ANN: trainLoss: 0.2540 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 280 | ANN: trainLoss: 0.2546 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 280 | ANN: trainLoss: 0.2566 | trainAcc: 89.8438% (460/512)\n",
            "8 13 Epoch: 280 | ANN: trainLoss: 0.2629 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 280 | ANN: trainLoss: 0.2691 | trainAcc: 89.3750% (572/640)\n",
            "10 13 Epoch: 280 | ANN: trainLoss: 0.2727 | trainAcc: 89.6307% (631/704)\n",
            "11 13 Epoch: 280 | ANN: trainLoss: 0.2694 | trainAcc: 89.7135% (689/768)\n",
            "12 13 Epoch: 280 | ANN: trainLoss: 0.2762 | trainAcc: 89.6373% (692/772)\n",
            "0 4 Epoch: 280 | ANN: testLoss: 0.4990 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 280 | ANN: testLoss: 0.5294 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 280 | ANN: testLoss: 0.4835 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 280 | ANN: testLoss: 0.3705 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 281 | ANN: trainLoss: 0.1929 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 281 | ANN: trainLoss: 0.2607 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 281 | ANN: trainLoss: 0.2623 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 281 | ANN: trainLoss: 0.2718 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 281 | ANN: trainLoss: 0.2829 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 281 | ANN: trainLoss: 0.2744 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 281 | ANN: trainLoss: 0.2776 | trainAcc: 88.6161% (397/448)\n",
            "7 13 Epoch: 281 | ANN: trainLoss: 0.2785 | trainAcc: 88.6719% (454/512)\n",
            "8 13 Epoch: 281 | ANN: trainLoss: 0.2751 | trainAcc: 88.5417% (510/576)\n",
            "9 13 Epoch: 281 | ANN: trainLoss: 0.2802 | trainAcc: 88.1250% (564/640)\n",
            "10 13 Epoch: 281 | ANN: trainLoss: 0.2848 | trainAcc: 88.0682% (620/704)\n",
            "11 13 Epoch: 281 | ANN: trainLoss: 0.2768 | trainAcc: 88.5417% (680/768)\n",
            "12 13 Epoch: 281 | ANN: trainLoss: 0.2593 | trainAcc: 88.6010% (684/772)\n",
            "0 4 Epoch: 281 | ANN: testLoss: 0.4882 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 281 | ANN: testLoss: 0.4419 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 281 | ANN: testLoss: 0.4627 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 281 | ANN: testLoss: 0.7239 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 282 | ANN: trainLoss: 0.2334 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 282 | ANN: trainLoss: 0.2679 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 282 | ANN: trainLoss: 0.2865 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 282 | ANN: trainLoss: 0.3178 | trainAcc: 87.5000% (224/256)\n",
            "4 13 Epoch: 282 | ANN: trainLoss: 0.3054 | trainAcc: 88.1250% (282/320)\n",
            "5 13 Epoch: 282 | ANN: trainLoss: 0.3161 | trainAcc: 87.7604% (337/384)\n",
            "6 13 Epoch: 282 | ANN: trainLoss: 0.3181 | trainAcc: 87.5000% (392/448)\n",
            "7 13 Epoch: 282 | ANN: trainLoss: 0.2964 | trainAcc: 88.2812% (452/512)\n",
            "8 13 Epoch: 282 | ANN: trainLoss: 0.2943 | trainAcc: 88.3681% (509/576)\n",
            "9 13 Epoch: 282 | ANN: trainLoss: 0.2921 | trainAcc: 88.5938% (567/640)\n",
            "10 13 Epoch: 282 | ANN: trainLoss: 0.2843 | trainAcc: 88.7784% (625/704)\n",
            "11 13 Epoch: 282 | ANN: trainLoss: 0.2828 | trainAcc: 88.8021% (682/768)\n",
            "12 13 Epoch: 282 | ANN: trainLoss: 0.2641 | trainAcc: 88.8601% (686/772)\n",
            "0 4 Epoch: 282 | ANN: testLoss: 0.4389 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 282 | ANN: testLoss: 0.4712 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 282 | ANN: testLoss: 0.4564 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 282 | ANN: testLoss: 0.3802 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 283 | ANN: trainLoss: 0.1460 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 283 | ANN: trainLoss: 0.2072 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 283 | ANN: trainLoss: 0.2405 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 283 | ANN: trainLoss: 0.2267 | trainAcc: 90.2344% (231/256)\n",
            "4 13 Epoch: 283 | ANN: trainLoss: 0.2299 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 283 | ANN: trainLoss: 0.2294 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 283 | ANN: trainLoss: 0.2278 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 283 | ANN: trainLoss: 0.2326 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 283 | ANN: trainLoss: 0.2445 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 283 | ANN: trainLoss: 0.2470 | trainAcc: 90.4688% (579/640)\n",
            "10 13 Epoch: 283 | ANN: trainLoss: 0.2484 | trainAcc: 90.1989% (635/704)\n",
            "11 13 Epoch: 283 | ANN: trainLoss: 0.2459 | trainAcc: 90.3646% (694/768)\n",
            "12 13 Epoch: 283 | ANN: trainLoss: 0.2611 | trainAcc: 90.2850% (697/772)\n",
            "0 4 Epoch: 283 | ANN: testLoss: 0.4510 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 283 | ANN: testLoss: 0.5335 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 283 | ANN: testLoss: 0.4694 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 283 | ANN: testLoss: 0.5749 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 284 | ANN: trainLoss: 0.2526 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 284 | ANN: trainLoss: 0.2929 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 284 | ANN: trainLoss: 0.2706 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 284 | ANN: trainLoss: 0.2703 | trainAcc: 88.2812% (226/256)\n",
            "4 13 Epoch: 284 | ANN: trainLoss: 0.2826 | trainAcc: 88.4375% (283/320)\n",
            "5 13 Epoch: 284 | ANN: trainLoss: 0.2728 | trainAcc: 88.8021% (341/384)\n",
            "6 13 Epoch: 284 | ANN: trainLoss: 0.2697 | trainAcc: 89.0625% (399/448)\n",
            "7 13 Epoch: 284 | ANN: trainLoss: 0.2731 | trainAcc: 88.6719% (454/512)\n",
            "8 13 Epoch: 284 | ANN: trainLoss: 0.2658 | trainAcc: 88.7153% (511/576)\n",
            "9 13 Epoch: 284 | ANN: trainLoss: 0.2610 | trainAcc: 88.9062% (569/640)\n",
            "10 13 Epoch: 284 | ANN: trainLoss: 0.2587 | trainAcc: 89.0625% (627/704)\n",
            "11 13 Epoch: 284 | ANN: trainLoss: 0.2576 | trainAcc: 88.9323% (683/768)\n",
            "12 13 Epoch: 284 | ANN: trainLoss: 0.2928 | trainAcc: 88.8601% (686/772)\n",
            "0 4 Epoch: 284 | ANN: testLoss: 0.4019 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 284 | ANN: testLoss: 0.4244 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 284 | ANN: testLoss: 0.4533 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 284 | ANN: testLoss: 0.4600 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 285 | ANN: trainLoss: 0.2789 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 285 | ANN: trainLoss: 0.2360 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 285 | ANN: trainLoss: 0.2524 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 285 | ANN: trainLoss: 0.2339 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 285 | ANN: trainLoss: 0.2320 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 285 | ANN: trainLoss: 0.2569 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 285 | ANN: trainLoss: 0.2610 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 285 | ANN: trainLoss: 0.2484 | trainAcc: 90.4297% (463/512)\n",
            "8 13 Epoch: 285 | ANN: trainLoss: 0.2626 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 285 | ANN: trainLoss: 0.2634 | trainAcc: 90.1562% (577/640)\n",
            "10 13 Epoch: 285 | ANN: trainLoss: 0.2620 | trainAcc: 90.1989% (635/704)\n",
            "11 13 Epoch: 285 | ANN: trainLoss: 0.2618 | trainAcc: 90.2344% (693/768)\n",
            "12 13 Epoch: 285 | ANN: trainLoss: 0.2992 | trainAcc: 90.1554% (696/772)\n",
            "0 4 Epoch: 285 | ANN: testLoss: 0.4015 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 285 | ANN: testLoss: 0.4386 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 285 | ANN: testLoss: 0.4433 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 285 | ANN: testLoss: 0.5975 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 286 | ANN: trainLoss: 0.3530 | trainAcc: 79.6875% (51/64)\n",
            "1 13 Epoch: 286 | ANN: trainLoss: 0.2454 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 286 | ANN: trainLoss: 0.2468 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 286 | ANN: trainLoss: 0.2285 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 286 | ANN: trainLoss: 0.2323 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 286 | ANN: trainLoss: 0.2230 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 286 | ANN: trainLoss: 0.2312 | trainAcc: 90.8482% (407/448)\n",
            "7 13 Epoch: 286 | ANN: trainLoss: 0.2452 | trainAcc: 90.4297% (463/512)\n",
            "8 13 Epoch: 286 | ANN: trainLoss: 0.2409 | trainAcc: 90.9722% (524/576)\n",
            "9 13 Epoch: 286 | ANN: trainLoss: 0.2457 | trainAcc: 91.0938% (583/640)\n",
            "10 13 Epoch: 286 | ANN: trainLoss: 0.2457 | trainAcc: 90.7670% (639/704)\n",
            "11 13 Epoch: 286 | ANN: trainLoss: 0.2403 | trainAcc: 90.8854% (698/768)\n",
            "12 13 Epoch: 286 | ANN: trainLoss: 0.2366 | trainAcc: 90.9326% (702/772)\n",
            "0 4 Epoch: 286 | ANN: testLoss: 0.4471 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 286 | ANN: testLoss: 0.4410 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 286 | ANN: testLoss: 0.4282 | testAcc: 79.6875% (153/192)\n",
            "3 4 Epoch: 286 | ANN: testLoss: 0.6753 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 287 | ANN: trainLoss: 0.2317 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 287 | ANN: trainLoss: 0.2531 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 287 | ANN: trainLoss: 0.2489 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 287 | ANN: trainLoss: 0.2380 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 287 | ANN: trainLoss: 0.2323 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 287 | ANN: trainLoss: 0.2233 | trainAcc: 92.4479% (355/384)\n",
            "6 13 Epoch: 287 | ANN: trainLoss: 0.2240 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 287 | ANN: trainLoss: 0.2171 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 287 | ANN: trainLoss: 0.2407 | trainAcc: 91.1458% (525/576)\n",
            "9 13 Epoch: 287 | ANN: trainLoss: 0.2345 | trainAcc: 91.4062% (585/640)\n",
            "10 13 Epoch: 287 | ANN: trainLoss: 0.2267 | trainAcc: 91.6193% (645/704)\n",
            "11 13 Epoch: 287 | ANN: trainLoss: 0.2227 | trainAcc: 91.6667% (704/768)\n",
            "12 13 Epoch: 287 | ANN: trainLoss: 0.2114 | trainAcc: 91.7098% (708/772)\n",
            "0 4 Epoch: 287 | ANN: testLoss: 0.3889 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 287 | ANN: testLoss: 0.3932 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 287 | ANN: testLoss: 0.4363 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 287 | ANN: testLoss: 0.4384 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 288 | ANN: trainLoss: 0.2120 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 288 | ANN: trainLoss: 0.2212 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 288 | ANN: trainLoss: 0.2108 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 288 | ANN: trainLoss: 0.1991 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 288 | ANN: trainLoss: 0.1894 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 288 | ANN: trainLoss: 0.2048 | trainAcc: 91.4062% (351/384)\n",
            "6 13 Epoch: 288 | ANN: trainLoss: 0.2227 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 288 | ANN: trainLoss: 0.2239 | trainAcc: 91.2109% (467/512)\n",
            "8 13 Epoch: 288 | ANN: trainLoss: 0.2219 | trainAcc: 91.4931% (527/576)\n",
            "9 13 Epoch: 288 | ANN: trainLoss: 0.2297 | trainAcc: 90.9375% (582/640)\n",
            "10 13 Epoch: 288 | ANN: trainLoss: 0.2223 | trainAcc: 91.3352% (643/704)\n",
            "11 13 Epoch: 288 | ANN: trainLoss: 0.2319 | trainAcc: 90.8854% (698/768)\n",
            "12 13 Epoch: 288 | ANN: trainLoss: 0.2719 | trainAcc: 90.6736% (700/772)\n",
            "0 4 Epoch: 288 | ANN: testLoss: 0.3986 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 288 | ANN: testLoss: 0.4061 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 288 | ANN: testLoss: 0.4302 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 288 | ANN: testLoss: 0.3226 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 289 | ANN: trainLoss: 0.1825 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 289 | ANN: trainLoss: 0.2134 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 289 | ANN: trainLoss: 0.2041 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 289 | ANN: trainLoss: 0.2018 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 289 | ANN: trainLoss: 0.2192 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 289 | ANN: trainLoss: 0.2180 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 289 | ANN: trainLoss: 0.2239 | trainAcc: 92.1875% (413/448)\n",
            "7 13 Epoch: 289 | ANN: trainLoss: 0.2275 | trainAcc: 92.1875% (472/512)\n",
            "8 13 Epoch: 289 | ANN: trainLoss: 0.2334 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 289 | ANN: trainLoss: 0.2454 | trainAcc: 90.4688% (579/640)\n",
            "10 13 Epoch: 289 | ANN: trainLoss: 0.2380 | trainAcc: 90.9091% (640/704)\n",
            "11 13 Epoch: 289 | ANN: trainLoss: 0.2387 | trainAcc: 90.8854% (698/768)\n",
            "12 13 Epoch: 289 | ANN: trainLoss: 0.2367 | trainAcc: 90.9326% (702/772)\n",
            "0 4 Epoch: 289 | ANN: testLoss: 0.4957 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 289 | ANN: testLoss: 0.4356 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 289 | ANN: testLoss: 0.4273 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 289 | ANN: testLoss: 0.3233 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 290 | ANN: trainLoss: 0.2061 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 290 | ANN: trainLoss: 0.2198 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 290 | ANN: trainLoss: 0.2197 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 290 | ANN: trainLoss: 0.2082 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 290 | ANN: trainLoss: 0.2122 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 290 | ANN: trainLoss: 0.2004 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 290 | ANN: trainLoss: 0.2106 | trainAcc: 91.7411% (411/448)\n",
            "7 13 Epoch: 290 | ANN: trainLoss: 0.2341 | trainAcc: 90.8203% (465/512)\n",
            "8 13 Epoch: 290 | ANN: trainLoss: 0.2308 | trainAcc: 91.1458% (525/576)\n",
            "9 13 Epoch: 290 | ANN: trainLoss: 0.2332 | trainAcc: 90.9375% (582/640)\n",
            "10 13 Epoch: 290 | ANN: trainLoss: 0.2429 | trainAcc: 90.3409% (636/704)\n",
            "11 13 Epoch: 290 | ANN: trainLoss: 0.2427 | trainAcc: 90.3646% (694/768)\n",
            "12 13 Epoch: 290 | ANN: trainLoss: 0.2698 | trainAcc: 90.2850% (697/772)\n",
            "0 4 Epoch: 290 | ANN: testLoss: 0.4743 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 290 | ANN: testLoss: 0.4667 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 290 | ANN: testLoss: 0.4484 | testAcc: 80.7292% (155/192)\n",
            "3 4 Epoch: 290 | ANN: testLoss: 0.4037 | testAcc: 80.8290% (156/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 291 | ANN: trainLoss: 0.2403 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 291 | ANN: trainLoss: 0.2791 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 291 | ANN: trainLoss: 0.2634 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 291 | ANN: trainLoss: 0.2446 | trainAcc: 90.2344% (231/256)\n",
            "4 13 Epoch: 291 | ANN: trainLoss: 0.2428 | trainAcc: 89.0625% (285/320)\n",
            "5 13 Epoch: 291 | ANN: trainLoss: 0.2282 | trainAcc: 90.3646% (347/384)\n",
            "6 13 Epoch: 291 | ANN: trainLoss: 0.2340 | trainAcc: 90.4018% (405/448)\n",
            "7 13 Epoch: 291 | ANN: trainLoss: 0.2513 | trainAcc: 90.4297% (463/512)\n",
            "8 13 Epoch: 291 | ANN: trainLoss: 0.2694 | trainAcc: 89.9306% (518/576)\n",
            "9 13 Epoch: 291 | ANN: trainLoss: 0.2677 | trainAcc: 90.0000% (576/640)\n",
            "10 13 Epoch: 291 | ANN: trainLoss: 0.2560 | trainAcc: 90.6250% (638/704)\n",
            "11 13 Epoch: 291 | ANN: trainLoss: 0.2489 | trainAcc: 91.1458% (700/768)\n",
            "12 13 Epoch: 291 | ANN: trainLoss: 0.2399 | trainAcc: 91.1917% (704/772)\n",
            "0 4 Epoch: 291 | ANN: testLoss: 0.4052 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 291 | ANN: testLoss: 0.3989 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 291 | ANN: testLoss: 0.4569 | testAcc: 80.2083% (154/192)\n",
            "3 4 Epoch: 291 | ANN: testLoss: 0.9380 | testAcc: 79.7927% (154/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 292 | ANN: trainLoss: 0.1821 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 292 | ANN: trainLoss: 0.2317 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 292 | ANN: trainLoss: 0.2489 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 292 | ANN: trainLoss: 0.2551 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 292 | ANN: trainLoss: 0.2545 | trainAcc: 88.7500% (284/320)\n",
            "5 13 Epoch: 292 | ANN: trainLoss: 0.2581 | trainAcc: 89.3229% (343/384)\n",
            "6 13 Epoch: 292 | ANN: trainLoss: 0.2498 | trainAcc: 89.9554% (403/448)\n",
            "7 13 Epoch: 292 | ANN: trainLoss: 0.2516 | trainAcc: 89.2578% (457/512)\n",
            "8 13 Epoch: 292 | ANN: trainLoss: 0.2488 | trainAcc: 89.4097% (515/576)\n",
            "9 13 Epoch: 292 | ANN: trainLoss: 0.2389 | trainAcc: 89.6875% (574/640)\n",
            "10 13 Epoch: 292 | ANN: trainLoss: 0.2400 | trainAcc: 89.4886% (630/704)\n",
            "11 13 Epoch: 292 | ANN: trainLoss: 0.2352 | trainAcc: 89.7135% (689/768)\n",
            "12 13 Epoch: 292 | ANN: trainLoss: 0.2415 | trainAcc: 89.6373% (692/772)\n",
            "0 4 Epoch: 292 | ANN: testLoss: 0.3995 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 292 | ANN: testLoss: 0.4126 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 292 | ANN: testLoss: 0.4559 | testAcc: 79.6875% (153/192)\n",
            "3 4 Epoch: 292 | ANN: testLoss: 0.5117 | testAcc: 79.7927% (154/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 293 | ANN: trainLoss: 0.2796 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 293 | ANN: trainLoss: 0.2525 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 293 | ANN: trainLoss: 0.2437 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 293 | ANN: trainLoss: 0.2364 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 293 | ANN: trainLoss: 0.2146 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 293 | ANN: trainLoss: 0.2222 | trainAcc: 91.4062% (351/384)\n",
            "6 13 Epoch: 293 | ANN: trainLoss: 0.2187 | trainAcc: 91.7411% (411/448)\n",
            "7 13 Epoch: 293 | ANN: trainLoss: 0.2103 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 293 | ANN: trainLoss: 0.2063 | trainAcc: 92.1875% (531/576)\n",
            "9 13 Epoch: 293 | ANN: trainLoss: 0.2153 | trainAcc: 91.8750% (588/640)\n",
            "10 13 Epoch: 293 | ANN: trainLoss: 0.2216 | trainAcc: 91.1932% (642/704)\n",
            "11 13 Epoch: 293 | ANN: trainLoss: 0.2198 | trainAcc: 91.2760% (701/768)\n",
            "12 13 Epoch: 293 | ANN: trainLoss: 0.2124 | trainAcc: 91.3212% (705/772)\n",
            "0 4 Epoch: 293 | ANN: testLoss: 0.4569 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 293 | ANN: testLoss: 0.4355 | testAcc: 82.0312% (105/128)\n",
            "2 4 Epoch: 293 | ANN: testLoss: 0.4573 | testAcc: 80.7292% (155/192)\n",
            "3 4 Epoch: 293 | ANN: testLoss: 0.5301 | testAcc: 80.3109% (155/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 294 | ANN: trainLoss: 0.1263 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 294 | ANN: trainLoss: 0.1721 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 294 | ANN: trainLoss: 0.1599 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 294 | ANN: trainLoss: 0.1732 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 294 | ANN: trainLoss: 0.1744 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 294 | ANN: trainLoss: 0.1672 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 294 | ANN: trainLoss: 0.1686 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 294 | ANN: trainLoss: 0.1780 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 294 | ANN: trainLoss: 0.1748 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 294 | ANN: trainLoss: 0.1840 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 294 | ANN: trainLoss: 0.1909 | trainAcc: 92.1875% (649/704)\n",
            "11 13 Epoch: 294 | ANN: trainLoss: 0.2025 | trainAcc: 91.6667% (704/768)\n",
            "12 13 Epoch: 294 | ANN: trainLoss: 0.2349 | trainAcc: 91.5803% (707/772)\n",
            "0 4 Epoch: 294 | ANN: testLoss: 0.3799 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 294 | ANN: testLoss: 0.3577 | testAcc: 82.8125% (106/128)\n",
            "2 4 Epoch: 294 | ANN: testLoss: 0.4484 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 294 | ANN: testLoss: 0.4664 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 295 | ANN: trainLoss: 0.1843 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 295 | ANN: trainLoss: 0.1876 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 295 | ANN: trainLoss: 0.1906 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 295 | ANN: trainLoss: 0.2079 | trainAcc: 90.2344% (231/256)\n",
            "4 13 Epoch: 295 | ANN: trainLoss: 0.2391 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 295 | ANN: trainLoss: 0.2299 | trainAcc: 90.6250% (348/384)\n",
            "6 13 Epoch: 295 | ANN: trainLoss: 0.2139 | trainAcc: 91.7411% (411/448)\n",
            "7 13 Epoch: 295 | ANN: trainLoss: 0.2201 | trainAcc: 91.7969% (470/512)\n",
            "8 13 Epoch: 295 | ANN: trainLoss: 0.2231 | trainAcc: 91.8403% (529/576)\n",
            "9 13 Epoch: 295 | ANN: trainLoss: 0.2285 | trainAcc: 91.8750% (588/640)\n",
            "10 13 Epoch: 295 | ANN: trainLoss: 0.2279 | trainAcc: 91.6193% (645/704)\n",
            "11 13 Epoch: 295 | ANN: trainLoss: 0.2352 | trainAcc: 91.1458% (700/768)\n",
            "12 13 Epoch: 295 | ANN: trainLoss: 0.2859 | trainAcc: 91.0622% (703/772)\n",
            "0 4 Epoch: 295 | ANN: testLoss: 0.4346 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 295 | ANN: testLoss: 0.4382 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 295 | ANN: testLoss: 0.4627 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 295 | ANN: testLoss: 0.3701 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 296 | ANN: trainLoss: 0.1738 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 296 | ANN: trainLoss: 0.1879 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 296 | ANN: trainLoss: 0.1865 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 296 | ANN: trainLoss: 0.1700 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 296 | ANN: trainLoss: 0.1963 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 296 | ANN: trainLoss: 0.2159 | trainAcc: 90.8854% (349/384)\n",
            "6 13 Epoch: 296 | ANN: trainLoss: 0.2236 | trainAcc: 91.2946% (409/448)\n",
            "7 13 Epoch: 296 | ANN: trainLoss: 0.2350 | trainAcc: 91.0156% (466/512)\n",
            "8 13 Epoch: 296 | ANN: trainLoss: 0.2304 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 296 | ANN: trainLoss: 0.2226 | trainAcc: 92.0312% (589/640)\n",
            "10 13 Epoch: 296 | ANN: trainLoss: 0.2221 | trainAcc: 92.1875% (649/704)\n",
            "11 13 Epoch: 296 | ANN: trainLoss: 0.2284 | trainAcc: 92.0573% (707/768)\n",
            "12 13 Epoch: 296 | ANN: trainLoss: 0.2367 | trainAcc: 91.9689% (710/772)\n",
            "0 4 Epoch: 296 | ANN: testLoss: 0.4272 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 296 | ANN: testLoss: 0.4195 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 296 | ANN: testLoss: 0.4647 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 296 | ANN: testLoss: 0.3485 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 297 | ANN: trainLoss: 0.2170 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 297 | ANN: trainLoss: 0.1725 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 297 | ANN: trainLoss: 0.1994 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 297 | ANN: trainLoss: 0.2077 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 297 | ANN: trainLoss: 0.2041 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 297 | ANN: trainLoss: 0.2105 | trainAcc: 90.8854% (349/384)\n",
            "6 13 Epoch: 297 | ANN: trainLoss: 0.2179 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 297 | ANN: trainLoss: 0.2169 | trainAcc: 91.2109% (467/512)\n",
            "8 13 Epoch: 297 | ANN: trainLoss: 0.2161 | trainAcc: 91.3194% (526/576)\n",
            "9 13 Epoch: 297 | ANN: trainLoss: 0.2368 | trainAcc: 90.9375% (582/640)\n",
            "10 13 Epoch: 297 | ANN: trainLoss: 0.2291 | trainAcc: 91.3352% (643/704)\n",
            "11 13 Epoch: 297 | ANN: trainLoss: 0.2295 | trainAcc: 91.4062% (702/768)\n",
            "12 13 Epoch: 297 | ANN: trainLoss: 0.2681 | trainAcc: 91.3212% (705/772)\n",
            "0 4 Epoch: 297 | ANN: testLoss: 0.5531 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 297 | ANN: testLoss: 0.4747 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 297 | ANN: testLoss: 0.4716 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 297 | ANN: testLoss: 0.3605 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 298 | ANN: trainLoss: 0.2546 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 298 | ANN: trainLoss: 0.2385 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 298 | ANN: trainLoss: 0.2832 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 298 | ANN: trainLoss: 0.2771 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 298 | ANN: trainLoss: 0.2506 | trainAcc: 89.3750% (286/320)\n",
            "5 13 Epoch: 298 | ANN: trainLoss: 0.2399 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 298 | ANN: trainLoss: 0.2316 | trainAcc: 90.4018% (405/448)\n",
            "7 13 Epoch: 298 | ANN: trainLoss: 0.2298 | trainAcc: 90.2344% (462/512)\n",
            "8 13 Epoch: 298 | ANN: trainLoss: 0.2338 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 298 | ANN: trainLoss: 0.2416 | trainAcc: 89.6875% (574/640)\n",
            "10 13 Epoch: 298 | ANN: trainLoss: 0.2377 | trainAcc: 89.9148% (633/704)\n",
            "11 13 Epoch: 298 | ANN: trainLoss: 0.2476 | trainAcc: 89.3229% (686/768)\n",
            "12 13 Epoch: 298 | ANN: trainLoss: 0.2628 | trainAcc: 89.2487% (689/772)\n",
            "0 4 Epoch: 298 | ANN: testLoss: 0.4018 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 298 | ANN: testLoss: 0.4524 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 298 | ANN: testLoss: 0.4836 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 298 | ANN: testLoss: 0.3685 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 299 | ANN: trainLoss: 0.1732 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 299 | ANN: trainLoss: 0.2167 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 299 | ANN: trainLoss: 0.2007 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 299 | ANN: trainLoss: 0.2324 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 299 | ANN: trainLoss: 0.2394 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 299 | ANN: trainLoss: 0.2165 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 299 | ANN: trainLoss: 0.2154 | trainAcc: 92.1875% (413/448)\n",
            "7 13 Epoch: 299 | ANN: trainLoss: 0.2096 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 299 | ANN: trainLoss: 0.2113 | trainAcc: 92.1875% (531/576)\n",
            "9 13 Epoch: 299 | ANN: trainLoss: 0.2157 | trainAcc: 92.3438% (591/640)\n",
            "10 13 Epoch: 299 | ANN: trainLoss: 0.2151 | trainAcc: 92.0455% (648/704)\n",
            "11 13 Epoch: 299 | ANN: trainLoss: 0.2138 | trainAcc: 92.0573% (707/768)\n",
            "12 13 Epoch: 299 | ANN: trainLoss: 0.2011 | trainAcc: 92.0984% (711/772)\n",
            "0 4 Epoch: 299 | ANN: testLoss: 0.3635 | testAcc: 85.9375% (55/64)\n",
            "1 4 Epoch: 299 | ANN: testLoss: 0.4607 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 299 | ANN: testLoss: 0.4562 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 299 | ANN: testLoss: 0.3422 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 300 | ANN: trainLoss: 0.2585 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 300 | ANN: trainLoss: 0.2230 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 300 | ANN: trainLoss: 0.2312 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 300 | ANN: trainLoss: 0.2294 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 300 | ANN: trainLoss: 0.2636 | trainAcc: 87.1875% (279/320)\n",
            "5 13 Epoch: 300 | ANN: trainLoss: 0.2667 | trainAcc: 86.7188% (333/384)\n",
            "6 13 Epoch: 300 | ANN: trainLoss: 0.2576 | trainAcc: 87.7232% (393/448)\n",
            "7 13 Epoch: 300 | ANN: trainLoss: 0.2669 | trainAcc: 87.5000% (448/512)\n",
            "8 13 Epoch: 300 | ANN: trainLoss: 0.2696 | trainAcc: 87.8472% (506/576)\n",
            "9 13 Epoch: 300 | ANN: trainLoss: 0.2671 | trainAcc: 88.4375% (566/640)\n",
            "10 13 Epoch: 300 | ANN: trainLoss: 0.2689 | trainAcc: 88.3523% (622/704)\n",
            "11 13 Epoch: 300 | ANN: trainLoss: 0.2624 | trainAcc: 88.6719% (681/768)\n",
            "12 13 Epoch: 300 | ANN: trainLoss: 0.2831 | trainAcc: 88.6010% (684/772)\n",
            "0 4 Epoch: 300 | ANN: testLoss: 0.4876 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 300 | ANN: testLoss: 0.4662 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 300 | ANN: testLoss: 0.4561 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 300 | ANN: testLoss: 0.3422 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 301 | ANN: trainLoss: 0.1958 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 301 | ANN: trainLoss: 0.1980 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 301 | ANN: trainLoss: 0.2419 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 301 | ANN: trainLoss: 0.2311 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 301 | ANN: trainLoss: 0.2142 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 301 | ANN: trainLoss: 0.2190 | trainAcc: 91.4062% (351/384)\n",
            "6 13 Epoch: 301 | ANN: trainLoss: 0.2178 | trainAcc: 91.2946% (409/448)\n",
            "7 13 Epoch: 301 | ANN: trainLoss: 0.2269 | trainAcc: 91.4062% (468/512)\n",
            "8 13 Epoch: 301 | ANN: trainLoss: 0.2228 | trainAcc: 92.0139% (530/576)\n",
            "9 13 Epoch: 301 | ANN: trainLoss: 0.2218 | trainAcc: 92.1875% (590/640)\n",
            "10 13 Epoch: 301 | ANN: trainLoss: 0.2152 | trainAcc: 92.3295% (650/704)\n",
            "11 13 Epoch: 301 | ANN: trainLoss: 0.2210 | trainAcc: 91.7969% (705/768)\n",
            "12 13 Epoch: 301 | ANN: trainLoss: 0.2358 | trainAcc: 91.7098% (708/772)\n",
            "0 4 Epoch: 301 | ANN: testLoss: 0.5802 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 301 | ANN: testLoss: 0.5112 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 301 | ANN: testLoss: 0.4690 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 301 | ANN: testLoss: 0.3525 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 302 | ANN: trainLoss: 0.1466 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 302 | ANN: trainLoss: 0.1363 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 302 | ANN: trainLoss: 0.1824 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 302 | ANN: trainLoss: 0.2308 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 302 | ANN: trainLoss: 0.2200 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 302 | ANN: trainLoss: 0.2229 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 302 | ANN: trainLoss: 0.2394 | trainAcc: 90.6250% (406/448)\n",
            "7 13 Epoch: 302 | ANN: trainLoss: 0.2330 | trainAcc: 90.4297% (463/512)\n",
            "8 13 Epoch: 302 | ANN: trainLoss: 0.2378 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 302 | ANN: trainLoss: 0.2275 | trainAcc: 90.7812% (581/640)\n",
            "10 13 Epoch: 302 | ANN: trainLoss: 0.2256 | trainAcc: 90.7670% (639/704)\n",
            "11 13 Epoch: 302 | ANN: trainLoss: 0.2240 | trainAcc: 91.0156% (699/768)\n",
            "12 13 Epoch: 302 | ANN: trainLoss: 0.2193 | trainAcc: 91.0622% (703/772)\n",
            "0 4 Epoch: 302 | ANN: testLoss: 0.6311 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 302 | ANN: testLoss: 0.5257 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 302 | ANN: testLoss: 0.4813 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 302 | ANN: testLoss: 0.6602 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 303 | ANN: trainLoss: 0.1619 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 303 | ANN: trainLoss: 0.2001 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 303 | ANN: trainLoss: 0.2125 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 303 | ANN: trainLoss: 0.1884 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 303 | ANN: trainLoss: 0.2238 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 303 | ANN: trainLoss: 0.2295 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 303 | ANN: trainLoss: 0.2282 | trainAcc: 91.5179% (410/448)\n",
            "7 13 Epoch: 303 | ANN: trainLoss: 0.2412 | trainAcc: 90.8203% (465/512)\n",
            "8 13 Epoch: 303 | ANN: trainLoss: 0.2495 | trainAcc: 90.4514% (521/576)\n",
            "9 13 Epoch: 303 | ANN: trainLoss: 0.2462 | trainAcc: 90.7812% (581/640)\n",
            "10 13 Epoch: 303 | ANN: trainLoss: 0.2399 | trainAcc: 91.0511% (641/704)\n",
            "11 13 Epoch: 303 | ANN: trainLoss: 0.2352 | trainAcc: 91.4062% (702/768)\n",
            "12 13 Epoch: 303 | ANN: trainLoss: 0.2874 | trainAcc: 91.1917% (704/772)\n",
            "0 4 Epoch: 303 | ANN: testLoss: 0.5117 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 303 | ANN: testLoss: 0.4518 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 303 | ANN: testLoss: 0.4486 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 303 | ANN: testLoss: 0.3565 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 304 | ANN: trainLoss: 0.1733 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 304 | ANN: trainLoss: 0.1918 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 304 | ANN: trainLoss: 0.1959 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 304 | ANN: trainLoss: 0.2186 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 304 | ANN: trainLoss: 0.2085 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 304 | ANN: trainLoss: 0.2116 | trainAcc: 90.3646% (347/384)\n",
            "6 13 Epoch: 304 | ANN: trainLoss: 0.2178 | trainAcc: 90.1786% (404/448)\n",
            "7 13 Epoch: 304 | ANN: trainLoss: 0.2178 | trainAcc: 90.0391% (461/512)\n",
            "8 13 Epoch: 304 | ANN: trainLoss: 0.2163 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 304 | ANN: trainLoss: 0.2124 | trainAcc: 90.6250% (580/640)\n",
            "10 13 Epoch: 304 | ANN: trainLoss: 0.2135 | trainAcc: 90.9091% (640/704)\n",
            "11 13 Epoch: 304 | ANN: trainLoss: 0.2303 | trainAcc: 90.4948% (695/768)\n",
            "12 13 Epoch: 304 | ANN: trainLoss: 0.2146 | trainAcc: 90.5440% (699/772)\n",
            "0 4 Epoch: 304 | ANN: testLoss: 0.5442 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 304 | ANN: testLoss: 0.5000 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 304 | ANN: testLoss: 0.4284 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 304 | ANN: testLoss: 0.3223 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 305 | ANN: trainLoss: 0.3468 | trainAcc: 81.2500% (52/64)\n",
            "1 13 Epoch: 305 | ANN: trainLoss: 0.2829 | trainAcc: 85.9375% (110/128)\n",
            "2 13 Epoch: 305 | ANN: trainLoss: 0.2683 | trainAcc: 87.5000% (168/192)\n",
            "3 13 Epoch: 305 | ANN: trainLoss: 0.2410 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 305 | ANN: trainLoss: 0.2275 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 305 | ANN: trainLoss: 0.2178 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 305 | ANN: trainLoss: 0.2175 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 305 | ANN: trainLoss: 0.2161 | trainAcc: 90.8203% (465/512)\n",
            "8 13 Epoch: 305 | ANN: trainLoss: 0.2123 | trainAcc: 91.4931% (527/576)\n",
            "9 13 Epoch: 305 | ANN: trainLoss: 0.2124 | trainAcc: 91.2500% (584/640)\n",
            "10 13 Epoch: 305 | ANN: trainLoss: 0.2183 | trainAcc: 91.1932% (642/704)\n",
            "11 13 Epoch: 305 | ANN: trainLoss: 0.2206 | trainAcc: 91.0156% (699/768)\n",
            "12 13 Epoch: 305 | ANN: trainLoss: 0.2105 | trainAcc: 91.0622% (703/772)\n",
            "0 4 Epoch: 305 | ANN: testLoss: 0.4163 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 305 | ANN: testLoss: 0.4338 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 305 | ANN: testLoss: 0.4532 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 305 | ANN: testLoss: 0.3559 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 306 | ANN: trainLoss: 0.2347 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 306 | ANN: trainLoss: 0.2317 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 306 | ANN: trainLoss: 0.2635 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 306 | ANN: trainLoss: 0.2442 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 306 | ANN: trainLoss: 0.2376 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 306 | ANN: trainLoss: 0.2206 | trainAcc: 92.4479% (355/384)\n",
            "6 13 Epoch: 306 | ANN: trainLoss: 0.2143 | trainAcc: 92.4107% (414/448)\n",
            "7 13 Epoch: 306 | ANN: trainLoss: 0.2294 | trainAcc: 91.4062% (468/512)\n",
            "8 13 Epoch: 306 | ANN: trainLoss: 0.2272 | trainAcc: 91.4931% (527/576)\n",
            "9 13 Epoch: 306 | ANN: trainLoss: 0.2239 | trainAcc: 91.5625% (586/640)\n",
            "10 13 Epoch: 306 | ANN: trainLoss: 0.2163 | trainAcc: 92.0455% (648/704)\n",
            "11 13 Epoch: 306 | ANN: trainLoss: 0.2136 | trainAcc: 92.3177% (709/768)\n",
            "12 13 Epoch: 306 | ANN: trainLoss: 0.2107 | trainAcc: 92.3575% (713/772)\n",
            "0 4 Epoch: 306 | ANN: testLoss: 0.4718 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 306 | ANN: testLoss: 0.4113 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 306 | ANN: testLoss: 0.4485 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 306 | ANN: testLoss: 0.3422 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 307 | ANN: trainLoss: 0.1931 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 307 | ANN: trainLoss: 0.2033 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 307 | ANN: trainLoss: 0.1986 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 307 | ANN: trainLoss: 0.2021 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 307 | ANN: trainLoss: 0.2078 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 307 | ANN: trainLoss: 0.2115 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 307 | ANN: trainLoss: 0.2045 | trainAcc: 92.1875% (413/448)\n",
            "7 13 Epoch: 307 | ANN: trainLoss: 0.2002 | trainAcc: 92.5781% (474/512)\n",
            "8 13 Epoch: 307 | ANN: trainLoss: 0.2004 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 307 | ANN: trainLoss: 0.1988 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 307 | ANN: trainLoss: 0.1913 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 307 | ANN: trainLoss: 0.1886 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 307 | ANN: trainLoss: 0.1827 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 307 | ANN: testLoss: 0.3908 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 307 | ANN: testLoss: 0.4552 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 307 | ANN: testLoss: 0.4360 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 307 | ANN: testLoss: 0.5735 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 308 | ANN: trainLoss: 0.1745 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 308 | ANN: trainLoss: 0.1988 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 308 | ANN: trainLoss: 0.1887 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 308 | ANN: trainLoss: 0.2010 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 308 | ANN: trainLoss: 0.2032 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 308 | ANN: trainLoss: 0.1875 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 308 | ANN: trainLoss: 0.1961 | trainAcc: 92.4107% (414/448)\n",
            "7 13 Epoch: 308 | ANN: trainLoss: 0.1974 | trainAcc: 91.9922% (471/512)\n",
            "8 13 Epoch: 308 | ANN: trainLoss: 0.1922 | trainAcc: 92.3611% (532/576)\n",
            "9 13 Epoch: 308 | ANN: trainLoss: 0.1860 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 308 | ANN: trainLoss: 0.1938 | trainAcc: 92.6136% (652/704)\n",
            "11 13 Epoch: 308 | ANN: trainLoss: 0.2052 | trainAcc: 91.5365% (703/768)\n",
            "12 13 Epoch: 308 | ANN: trainLoss: 0.1969 | trainAcc: 91.5803% (707/772)\n",
            "0 4 Epoch: 308 | ANN: testLoss: 0.3614 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 308 | ANN: testLoss: 0.4055 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 308 | ANN: testLoss: 0.4505 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 308 | ANN: testLoss: 0.3425 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 309 | ANN: trainLoss: 0.3043 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 309 | ANN: trainLoss: 0.2165 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 309 | ANN: trainLoss: 0.2003 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 309 | ANN: trainLoss: 0.2146 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 309 | ANN: trainLoss: 0.2149 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 309 | ANN: trainLoss: 0.2046 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 309 | ANN: trainLoss: 0.2012 | trainAcc: 91.7411% (411/448)\n",
            "7 13 Epoch: 309 | ANN: trainLoss: 0.1943 | trainAcc: 92.1875% (472/512)\n",
            "8 13 Epoch: 309 | ANN: trainLoss: 0.1985 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 309 | ANN: trainLoss: 0.1897 | trainAcc: 92.3438% (591/640)\n",
            "10 13 Epoch: 309 | ANN: trainLoss: 0.1952 | trainAcc: 92.1875% (649/704)\n",
            "11 13 Epoch: 309 | ANN: trainLoss: 0.1938 | trainAcc: 92.0573% (707/768)\n",
            "12 13 Epoch: 309 | ANN: trainLoss: 0.1928 | trainAcc: 92.0984% (711/772)\n",
            "0 4 Epoch: 309 | ANN: testLoss: 0.4715 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 309 | ANN: testLoss: 0.4859 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 309 | ANN: testLoss: 0.4658 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 309 | ANN: testLoss: 0.3870 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 310 | ANN: trainLoss: 0.2343 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 310 | ANN: trainLoss: 0.2514 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 310 | ANN: trainLoss: 0.2222 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 310 | ANN: trainLoss: 0.2012 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 310 | ANN: trainLoss: 0.2020 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 310 | ANN: trainLoss: 0.2067 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 310 | ANN: trainLoss: 0.2037 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 310 | ANN: trainLoss: 0.2128 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 310 | ANN: trainLoss: 0.2070 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 310 | ANN: trainLoss: 0.2081 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 310 | ANN: trainLoss: 0.2102 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 310 | ANN: trainLoss: 0.2082 | trainAcc: 92.9688% (714/768)\n",
            "12 13 Epoch: 310 | ANN: trainLoss: 0.2044 | trainAcc: 93.0052% (718/772)\n",
            "0 4 Epoch: 310 | ANN: testLoss: 0.3133 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 310 | ANN: testLoss: 0.4115 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 310 | ANN: testLoss: 0.4749 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 310 | ANN: testLoss: 0.6649 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 311 | ANN: trainLoss: 0.1136 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 311 | ANN: trainLoss: 0.2195 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 311 | ANN: trainLoss: 0.2276 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 311 | ANN: trainLoss: 0.2052 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 311 | ANN: trainLoss: 0.1874 | trainAcc: 92.5000% (296/320)\n",
            "5 13 Epoch: 311 | ANN: trainLoss: 0.1782 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 311 | ANN: trainLoss: 0.1669 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 311 | ANN: trainLoss: 0.1711 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 311 | ANN: trainLoss: 0.1912 | trainAcc: 92.0139% (530/576)\n",
            "9 13 Epoch: 311 | ANN: trainLoss: 0.1874 | trainAcc: 92.3438% (591/640)\n",
            "10 13 Epoch: 311 | ANN: trainLoss: 0.1808 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 311 | ANN: trainLoss: 0.1785 | trainAcc: 92.9688% (714/768)\n",
            "12 13 Epoch: 311 | ANN: trainLoss: 0.2556 | trainAcc: 92.7461% (716/772)\n",
            "0 4 Epoch: 311 | ANN: testLoss: 0.3060 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 311 | ANN: testLoss: 0.3700 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 311 | ANN: testLoss: 0.4582 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 311 | ANN: testLoss: 0.3591 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 312 | ANN: trainLoss: 0.2465 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 312 | ANN: trainLoss: 0.1725 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 312 | ANN: trainLoss: 0.2017 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 312 | ANN: trainLoss: 0.1947 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 312 | ANN: trainLoss: 0.1812 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 312 | ANN: trainLoss: 0.1799 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 312 | ANN: trainLoss: 0.2045 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 312 | ANN: trainLoss: 0.2049 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 312 | ANN: trainLoss: 0.2003 | trainAcc: 92.5347% (533/576)\n",
            "9 13 Epoch: 312 | ANN: trainLoss: 0.1976 | trainAcc: 92.5000% (592/640)\n",
            "10 13 Epoch: 312 | ANN: trainLoss: 0.1933 | trainAcc: 92.4716% (651/704)\n",
            "11 13 Epoch: 312 | ANN: trainLoss: 0.1897 | trainAcc: 92.5781% (711/768)\n",
            "12 13 Epoch: 312 | ANN: trainLoss: 0.2086 | trainAcc: 92.4870% (714/772)\n",
            "0 4 Epoch: 312 | ANN: testLoss: 0.4676 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 312 | ANN: testLoss: 0.5458 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 312 | ANN: testLoss: 0.5133 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 312 | ANN: testLoss: 0.3855 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 313 | ANN: trainLoss: 0.1217 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 313 | ANN: trainLoss: 0.1256 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 313 | ANN: trainLoss: 0.1501 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 313 | ANN: trainLoss: 0.1587 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 313 | ANN: trainLoss: 0.1645 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 313 | ANN: trainLoss: 0.1788 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 313 | ANN: trainLoss: 0.1743 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 313 | ANN: trainLoss: 0.1784 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 313 | ANN: trainLoss: 0.1771 | trainAcc: 93.5764% (539/576)\n",
            "9 13 Epoch: 313 | ANN: trainLoss: 0.1765 | trainAcc: 93.7500% (600/640)\n",
            "10 13 Epoch: 313 | ANN: trainLoss: 0.1802 | trainAcc: 93.6080% (659/704)\n",
            "11 13 Epoch: 313 | ANN: trainLoss: 0.1782 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 313 | ANN: trainLoss: 0.1813 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 313 | ANN: testLoss: 0.4577 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 313 | ANN: testLoss: 0.4970 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 313 | ANN: testLoss: 0.4580 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 313 | ANN: testLoss: 0.3524 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 314 | ANN: trainLoss: 0.2229 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 314 | ANN: trainLoss: 0.2090 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 314 | ANN: trainLoss: 0.2353 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 314 | ANN: trainLoss: 0.2674 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 314 | ANN: trainLoss: 0.2430 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 314 | ANN: trainLoss: 0.2419 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 314 | ANN: trainLoss: 0.2287 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 314 | ANN: trainLoss: 0.2269 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 314 | ANN: trainLoss: 0.2216 | trainAcc: 92.8819% (535/576)\n",
            "9 13 Epoch: 314 | ANN: trainLoss: 0.2253 | trainAcc: 92.5000% (592/640)\n",
            "10 13 Epoch: 314 | ANN: trainLoss: 0.2208 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 314 | ANN: trainLoss: 0.2179 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 314 | ANN: trainLoss: 0.2061 | trainAcc: 92.8756% (717/772)\n",
            "0 4 Epoch: 314 | ANN: testLoss: 0.5771 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 314 | ANN: testLoss: 0.4898 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 314 | ANN: testLoss: 0.4712 | testAcc: 79.6875% (153/192)\n",
            "3 4 Epoch: 314 | ANN: testLoss: 0.5206 | testAcc: 79.7927% (154/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 315 | ANN: trainLoss: 0.1946 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 315 | ANN: trainLoss: 0.1855 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 315 | ANN: trainLoss: 0.2120 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 315 | ANN: trainLoss: 0.2039 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 315 | ANN: trainLoss: 0.1874 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 315 | ANN: trainLoss: 0.1975 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 315 | ANN: trainLoss: 0.1919 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 315 | ANN: trainLoss: 0.2098 | trainAcc: 92.1875% (472/512)\n",
            "8 13 Epoch: 315 | ANN: trainLoss: 0.2041 | trainAcc: 92.5347% (533/576)\n",
            "9 13 Epoch: 315 | ANN: trainLoss: 0.2066 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 315 | ANN: trainLoss: 0.2090 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 315 | ANN: trainLoss: 0.2099 | trainAcc: 92.5781% (711/768)\n",
            "12 13 Epoch: 315 | ANN: trainLoss: 0.2532 | trainAcc: 92.3575% (713/772)\n",
            "0 4 Epoch: 315 | ANN: testLoss: 0.3602 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 315 | ANN: testLoss: 0.3936 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 315 | ANN: testLoss: 0.4900 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 315 | ANN: testLoss: 0.4178 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 316 | ANN: trainLoss: 0.3339 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 316 | ANN: trainLoss: 0.2729 | trainAcc: 86.7188% (111/128)\n",
            "2 13 Epoch: 316 | ANN: trainLoss: 0.2342 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 316 | ANN: trainLoss: 0.2038 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 316 | ANN: trainLoss: 0.1997 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 316 | ANN: trainLoss: 0.1993 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 316 | ANN: trainLoss: 0.2075 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 316 | ANN: trainLoss: 0.2203 | trainAcc: 91.0156% (466/512)\n",
            "8 13 Epoch: 316 | ANN: trainLoss: 0.2182 | trainAcc: 90.7986% (523/576)\n",
            "9 13 Epoch: 316 | ANN: trainLoss: 0.2203 | trainAcc: 90.7812% (581/640)\n",
            "10 13 Epoch: 316 | ANN: trainLoss: 0.2152 | trainAcc: 90.9091% (640/704)\n",
            "11 13 Epoch: 316 | ANN: trainLoss: 0.2162 | trainAcc: 91.0156% (699/768)\n",
            "12 13 Epoch: 316 | ANN: trainLoss: 0.2119 | trainAcc: 91.0622% (703/772)\n",
            "0 4 Epoch: 316 | ANN: testLoss: 0.5170 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 316 | ANN: testLoss: 0.4805 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 316 | ANN: testLoss: 0.5249 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 316 | ANN: testLoss: 0.3937 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 317 | ANN: trainLoss: 0.1916 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 317 | ANN: trainLoss: 0.2290 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 317 | ANN: trainLoss: 0.2204 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 317 | ANN: trainLoss: 0.2148 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 317 | ANN: trainLoss: 0.2160 | trainAcc: 92.5000% (296/320)\n",
            "5 13 Epoch: 317 | ANN: trainLoss: 0.2279 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 317 | ANN: trainLoss: 0.2114 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 317 | ANN: trainLoss: 0.2200 | trainAcc: 91.6016% (469/512)\n",
            "8 13 Epoch: 317 | ANN: trainLoss: 0.2267 | trainAcc: 91.1458% (525/576)\n",
            "9 13 Epoch: 317 | ANN: trainLoss: 0.2286 | trainAcc: 90.7812% (581/640)\n",
            "10 13 Epoch: 317 | ANN: trainLoss: 0.2235 | trainAcc: 90.9091% (640/704)\n",
            "11 13 Epoch: 317 | ANN: trainLoss: 0.2245 | trainAcc: 91.0156% (699/768)\n",
            "12 13 Epoch: 317 | ANN: trainLoss: 0.2210 | trainAcc: 91.0622% (703/772)\n",
            "0 4 Epoch: 317 | ANN: testLoss: 0.6230 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 317 | ANN: testLoss: 0.5100 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 317 | ANN: testLoss: 0.5119 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 317 | ANN: testLoss: 0.4546 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 318 | ANN: trainLoss: 0.1433 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 318 | ANN: trainLoss: 0.1472 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 318 | ANN: trainLoss: 0.1629 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 318 | ANN: trainLoss: 0.1892 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 318 | ANN: trainLoss: 0.1862 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 318 | ANN: trainLoss: 0.2045 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 318 | ANN: trainLoss: 0.2061 | trainAcc: 91.5179% (410/448)\n",
            "7 13 Epoch: 318 | ANN: trainLoss: 0.1999 | trainAcc: 91.9922% (471/512)\n",
            "8 13 Epoch: 318 | ANN: trainLoss: 0.2055 | trainAcc: 92.0139% (530/576)\n",
            "9 13 Epoch: 318 | ANN: trainLoss: 0.2004 | trainAcc: 92.5000% (592/640)\n",
            "10 13 Epoch: 318 | ANN: trainLoss: 0.2005 | trainAcc: 92.1875% (649/704)\n",
            "11 13 Epoch: 318 | ANN: trainLoss: 0.2044 | trainAcc: 92.0573% (707/768)\n",
            "12 13 Epoch: 318 | ANN: trainLoss: 0.3024 | trainAcc: 91.8394% (709/772)\n",
            "0 4 Epoch: 318 | ANN: testLoss: 0.4992 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 318 | ANN: testLoss: 0.4806 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 318 | ANN: testLoss: 0.4992 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 318 | ANN: testLoss: 0.4758 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 319 | ANN: trainLoss: 0.2573 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 319 | ANN: trainLoss: 0.1981 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 319 | ANN: trainLoss: 0.1847 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 319 | ANN: trainLoss: 0.2056 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 319 | ANN: trainLoss: 0.2000 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 319 | ANN: trainLoss: 0.2043 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 319 | ANN: trainLoss: 0.2031 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 319 | ANN: trainLoss: 0.2014 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 319 | ANN: trainLoss: 0.2013 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 319 | ANN: trainLoss: 0.2108 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 319 | ANN: trainLoss: 0.2076 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 319 | ANN: trainLoss: 0.2087 | trainAcc: 93.4896% (718/768)\n",
            "12 13 Epoch: 319 | ANN: trainLoss: 0.2489 | trainAcc: 93.2642% (720/772)\n",
            "0 4 Epoch: 319 | ANN: testLoss: 0.5352 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 319 | ANN: testLoss: 0.5866 | testAcc: 71.0938% (91/128)\n",
            "2 4 Epoch: 319 | ANN: testLoss: 0.5308 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 319 | ANN: testLoss: 0.4434 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 320 | ANN: trainLoss: 0.2262 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 320 | ANN: trainLoss: 0.2367 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 320 | ANN: trainLoss: 0.2168 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 320 | ANN: trainLoss: 0.2294 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 320 | ANN: trainLoss: 0.2445 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 320 | ANN: trainLoss: 0.2801 | trainAcc: 89.5833% (344/384)\n",
            "6 13 Epoch: 320 | ANN: trainLoss: 0.2850 | trainAcc: 88.8393% (398/448)\n",
            "7 13 Epoch: 320 | ANN: trainLoss: 0.2720 | trainAcc: 89.4531% (458/512)\n",
            "8 13 Epoch: 320 | ANN: trainLoss: 0.2687 | trainAcc: 89.5833% (516/576)\n",
            "9 13 Epoch: 320 | ANN: trainLoss: 0.2623 | trainAcc: 89.5312% (573/640)\n",
            "10 13 Epoch: 320 | ANN: trainLoss: 0.2518 | trainAcc: 90.0568% (634/704)\n",
            "11 13 Epoch: 320 | ANN: trainLoss: 0.2503 | trainAcc: 90.1042% (692/768)\n",
            "12 13 Epoch: 320 | ANN: trainLoss: 0.2518 | trainAcc: 90.1554% (696/772)\n",
            "0 4 Epoch: 320 | ANN: testLoss: 0.5764 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 320 | ANN: testLoss: 0.4895 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 320 | ANN: testLoss: 0.5161 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 320 | ANN: testLoss: 0.3871 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 321 | ANN: trainLoss: 0.2069 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 321 | ANN: trainLoss: 0.2169 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 321 | ANN: trainLoss: 0.2316 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 321 | ANN: trainLoss: 0.2041 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 321 | ANN: trainLoss: 0.2305 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 321 | ANN: trainLoss: 0.2202 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 321 | ANN: trainLoss: 0.2279 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 321 | ANN: trainLoss: 0.2377 | trainAcc: 91.6016% (469/512)\n",
            "8 13 Epoch: 321 | ANN: trainLoss: 0.2449 | trainAcc: 91.3194% (526/576)\n",
            "9 13 Epoch: 321 | ANN: trainLoss: 0.2426 | trainAcc: 91.5625% (586/640)\n",
            "10 13 Epoch: 321 | ANN: trainLoss: 0.2354 | trainAcc: 91.6193% (645/704)\n",
            "11 13 Epoch: 321 | ANN: trainLoss: 0.2345 | trainAcc: 91.7969% (705/768)\n",
            "12 13 Epoch: 321 | ANN: trainLoss: 0.2434 | trainAcc: 91.7098% (708/772)\n",
            "0 4 Epoch: 321 | ANN: testLoss: 0.5695 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 321 | ANN: testLoss: 0.5385 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 321 | ANN: testLoss: 0.4991 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 321 | ANN: testLoss: 0.5089 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 322 | ANN: trainLoss: 0.2611 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 322 | ANN: trainLoss: 0.2117 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 322 | ANN: trainLoss: 0.2204 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 322 | ANN: trainLoss: 0.2294 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 322 | ANN: trainLoss: 0.2289 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 322 | ANN: trainLoss: 0.2346 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 322 | ANN: trainLoss: 0.2297 | trainAcc: 91.7411% (411/448)\n",
            "7 13 Epoch: 322 | ANN: trainLoss: 0.2167 | trainAcc: 92.1875% (472/512)\n",
            "8 13 Epoch: 322 | ANN: trainLoss: 0.2303 | trainAcc: 92.0139% (530/576)\n",
            "9 13 Epoch: 322 | ANN: trainLoss: 0.2262 | trainAcc: 91.8750% (588/640)\n",
            "10 13 Epoch: 322 | ANN: trainLoss: 0.2302 | trainAcc: 91.9034% (647/704)\n",
            "11 13 Epoch: 322 | ANN: trainLoss: 0.2270 | trainAcc: 92.1875% (708/768)\n",
            "12 13 Epoch: 322 | ANN: trainLoss: 0.2259 | trainAcc: 92.2280% (712/772)\n",
            "0 4 Epoch: 322 | ANN: testLoss: 0.4662 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 322 | ANN: testLoss: 0.4803 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 322 | ANN: testLoss: 0.4831 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 322 | ANN: testLoss: 0.5148 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 323 | ANN: trainLoss: 0.1849 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 323 | ANN: trainLoss: 0.2222 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 323 | ANN: trainLoss: 0.1996 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 323 | ANN: trainLoss: 0.1973 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 323 | ANN: trainLoss: 0.2228 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 323 | ANN: trainLoss: 0.2241 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 323 | ANN: trainLoss: 0.2237 | trainAcc: 91.2946% (409/448)\n",
            "7 13 Epoch: 323 | ANN: trainLoss: 0.2238 | trainAcc: 91.4062% (468/512)\n",
            "8 13 Epoch: 323 | ANN: trainLoss: 0.2407 | trainAcc: 90.1042% (519/576)\n",
            "9 13 Epoch: 323 | ANN: trainLoss: 0.2434 | trainAcc: 89.6875% (574/640)\n",
            "10 13 Epoch: 323 | ANN: trainLoss: 0.2413 | trainAcc: 90.1989% (635/704)\n",
            "11 13 Epoch: 323 | ANN: trainLoss: 0.2400 | trainAcc: 90.2344% (693/768)\n",
            "12 13 Epoch: 323 | ANN: trainLoss: 0.2232 | trainAcc: 90.2850% (697/772)\n",
            "0 4 Epoch: 323 | ANN: testLoss: 0.4495 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 323 | ANN: testLoss: 0.4687 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 323 | ANN: testLoss: 0.4635 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 323 | ANN: testLoss: 0.5275 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 324 | ANN: trainLoss: 0.2958 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 324 | ANN: trainLoss: 0.2441 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 324 | ANN: trainLoss: 0.2280 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 324 | ANN: trainLoss: 0.2253 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 324 | ANN: trainLoss: 0.2169 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 324 | ANN: trainLoss: 0.2149 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 324 | ANN: trainLoss: 0.2078 | trainAcc: 92.4107% (414/448)\n",
            "7 13 Epoch: 324 | ANN: trainLoss: 0.2025 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 324 | ANN: trainLoss: 0.2064 | trainAcc: 92.1875% (531/576)\n",
            "9 13 Epoch: 324 | ANN: trainLoss: 0.2145 | trainAcc: 92.0312% (589/640)\n",
            "10 13 Epoch: 324 | ANN: trainLoss: 0.2240 | trainAcc: 91.4773% (644/704)\n",
            "11 13 Epoch: 324 | ANN: trainLoss: 0.2316 | trainAcc: 91.2760% (701/768)\n",
            "12 13 Epoch: 324 | ANN: trainLoss: 0.3545 | trainAcc: 91.0622% (703/772)\n",
            "0 4 Epoch: 324 | ANN: testLoss: 0.4920 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 324 | ANN: testLoss: 0.5037 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 324 | ANN: testLoss: 0.4609 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 324 | ANN: testLoss: 0.6587 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 325 | ANN: trainLoss: 0.2972 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 325 | ANN: trainLoss: 0.2599 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 325 | ANN: trainLoss: 0.2296 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 325 | ANN: trainLoss: 0.2398 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 325 | ANN: trainLoss: 0.2635 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 325 | ANN: trainLoss: 0.2570 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 325 | ANN: trainLoss: 0.2444 | trainAcc: 90.4018% (405/448)\n",
            "7 13 Epoch: 325 | ANN: trainLoss: 0.2336 | trainAcc: 90.8203% (465/512)\n",
            "8 13 Epoch: 325 | ANN: trainLoss: 0.2292 | trainAcc: 90.7986% (523/576)\n",
            "9 13 Epoch: 325 | ANN: trainLoss: 0.2340 | trainAcc: 90.4688% (579/640)\n",
            "10 13 Epoch: 325 | ANN: trainLoss: 0.2324 | trainAcc: 90.4830% (637/704)\n",
            "11 13 Epoch: 325 | ANN: trainLoss: 0.2285 | trainAcc: 90.7552% (697/768)\n",
            "12 13 Epoch: 325 | ANN: trainLoss: 0.2703 | trainAcc: 90.6736% (700/772)\n",
            "0 4 Epoch: 325 | ANN: testLoss: 0.4529 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 325 | ANN: testLoss: 0.4844 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 325 | ANN: testLoss: 0.5087 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 325 | ANN: testLoss: 0.3815 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 326 | ANN: trainLoss: 0.2757 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 326 | ANN: trainLoss: 0.2603 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 326 | ANN: trainLoss: 0.2401 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 326 | ANN: trainLoss: 0.2336 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 326 | ANN: trainLoss: 0.2319 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 326 | ANN: trainLoss: 0.2195 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 326 | ANN: trainLoss: 0.2403 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 326 | ANN: trainLoss: 0.2521 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 326 | ANN: trainLoss: 0.2468 | trainAcc: 90.9722% (524/576)\n",
            "9 13 Epoch: 326 | ANN: trainLoss: 0.2441 | trainAcc: 90.9375% (582/640)\n",
            "10 13 Epoch: 326 | ANN: trainLoss: 0.2450 | trainAcc: 90.6250% (638/704)\n",
            "11 13 Epoch: 326 | ANN: trainLoss: 0.2437 | trainAcc: 90.8854% (698/768)\n",
            "12 13 Epoch: 326 | ANN: trainLoss: 0.2370 | trainAcc: 90.9326% (702/772)\n",
            "0 4 Epoch: 326 | ANN: testLoss: 0.4711 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 326 | ANN: testLoss: 0.4954 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 326 | ANN: testLoss: 0.4732 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 326 | ANN: testLoss: 0.3639 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 327 | ANN: trainLoss: 0.2061 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 327 | ANN: trainLoss: 0.1787 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 327 | ANN: trainLoss: 0.1660 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 327 | ANN: trainLoss: 0.1890 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 327 | ANN: trainLoss: 0.1785 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 327 | ANN: trainLoss: 0.2182 | trainAcc: 92.4479% (355/384)\n",
            "6 13 Epoch: 327 | ANN: trainLoss: 0.2182 | trainAcc: 91.5179% (410/448)\n",
            "7 13 Epoch: 327 | ANN: trainLoss: 0.2220 | trainAcc: 91.7969% (470/512)\n",
            "8 13 Epoch: 327 | ANN: trainLoss: 0.2288 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 327 | ANN: trainLoss: 0.2334 | trainAcc: 91.2500% (584/640)\n",
            "10 13 Epoch: 327 | ANN: trainLoss: 0.2309 | trainAcc: 91.0511% (641/704)\n",
            "11 13 Epoch: 327 | ANN: trainLoss: 0.2316 | trainAcc: 90.7552% (697/768)\n",
            "12 13 Epoch: 327 | ANN: trainLoss: 0.2584 | trainAcc: 90.6736% (700/772)\n",
            "0 4 Epoch: 327 | ANN: testLoss: 0.4796 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 327 | ANN: testLoss: 0.4578 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 327 | ANN: testLoss: 0.4627 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 327 | ANN: testLoss: 0.7394 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 328 | ANN: trainLoss: 0.2301 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 328 | ANN: trainLoss: 0.1696 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 328 | ANN: trainLoss: 0.1923 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 328 | ANN: trainLoss: 0.1904 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 328 | ANN: trainLoss: 0.1902 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 328 | ANN: trainLoss: 0.1860 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 328 | ANN: trainLoss: 0.1808 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 328 | ANN: trainLoss: 0.1832 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 328 | ANN: trainLoss: 0.1873 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 328 | ANN: trainLoss: 0.1962 | trainAcc: 92.8125% (594/640)\n",
            "10 13 Epoch: 328 | ANN: trainLoss: 0.2004 | trainAcc: 92.4716% (651/704)\n",
            "11 13 Epoch: 328 | ANN: trainLoss: 0.2069 | trainAcc: 92.4479% (710/768)\n",
            "12 13 Epoch: 328 | ANN: trainLoss: 0.2162 | trainAcc: 92.3575% (713/772)\n",
            "0 4 Epoch: 328 | ANN: testLoss: 0.4687 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 328 | ANN: testLoss: 0.5107 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 328 | ANN: testLoss: 0.5104 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 328 | ANN: testLoss: 0.3854 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 329 | ANN: trainLoss: 0.2130 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 329 | ANN: trainLoss: 0.2081 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 329 | ANN: trainLoss: 0.1984 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 329 | ANN: trainLoss: 0.2098 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 329 | ANN: trainLoss: 0.2141 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 329 | ANN: trainLoss: 0.2191 | trainAcc: 90.8854% (349/384)\n",
            "6 13 Epoch: 329 | ANN: trainLoss: 0.2120 | trainAcc: 90.8482% (407/448)\n",
            "7 13 Epoch: 329 | ANN: trainLoss: 0.2074 | trainAcc: 90.8203% (465/512)\n",
            "8 13 Epoch: 329 | ANN: trainLoss: 0.2015 | trainAcc: 91.1458% (525/576)\n",
            "9 13 Epoch: 329 | ANN: trainLoss: 0.1923 | trainAcc: 91.7188% (587/640)\n",
            "10 13 Epoch: 329 | ANN: trainLoss: 0.1964 | trainAcc: 91.4773% (644/704)\n",
            "11 13 Epoch: 329 | ANN: trainLoss: 0.1957 | trainAcc: 91.4062% (702/768)\n",
            "12 13 Epoch: 329 | ANN: trainLoss: 0.2364 | trainAcc: 91.3212% (705/772)\n",
            "0 4 Epoch: 329 | ANN: testLoss: 0.3639 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 329 | ANN: testLoss: 0.4269 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 329 | ANN: testLoss: 0.4504 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 329 | ANN: testLoss: 0.3379 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 330 | ANN: trainLoss: 0.1724 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 330 | ANN: trainLoss: 0.1932 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 330 | ANN: trainLoss: 0.1615 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 330 | ANN: trainLoss: 0.2127 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 330 | ANN: trainLoss: 0.2333 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 330 | ANN: trainLoss: 0.2158 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 330 | ANN: trainLoss: 0.2230 | trainAcc: 91.5179% (410/448)\n",
            "7 13 Epoch: 330 | ANN: trainLoss: 0.2236 | trainAcc: 91.7969% (470/512)\n",
            "8 13 Epoch: 330 | ANN: trainLoss: 0.2388 | trainAcc: 91.1458% (525/576)\n",
            "9 13 Epoch: 330 | ANN: trainLoss: 0.2314 | trainAcc: 91.2500% (584/640)\n",
            "10 13 Epoch: 330 | ANN: trainLoss: 0.2370 | trainAcc: 90.6250% (638/704)\n",
            "11 13 Epoch: 330 | ANN: trainLoss: 0.2294 | trainAcc: 90.8854% (698/768)\n",
            "12 13 Epoch: 330 | ANN: trainLoss: 0.2370 | trainAcc: 90.8031% (701/772)\n",
            "0 4 Epoch: 330 | ANN: testLoss: 0.4323 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 330 | ANN: testLoss: 0.4726 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 330 | ANN: testLoss: 0.4413 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 330 | ANN: testLoss: 0.3313 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 331 | ANN: trainLoss: 0.2279 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 331 | ANN: trainLoss: 0.2676 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 331 | ANN: trainLoss: 0.2298 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 331 | ANN: trainLoss: 0.2275 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 331 | ANN: trainLoss: 0.2001 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 331 | ANN: trainLoss: 0.1933 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 331 | ANN: trainLoss: 0.1949 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 331 | ANN: trainLoss: 0.2025 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 331 | ANN: trainLoss: 0.2089 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 331 | ANN: trainLoss: 0.2161 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 331 | ANN: trainLoss: 0.2118 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 331 | ANN: trainLoss: 0.2103 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 331 | ANN: trainLoss: 0.3348 | trainAcc: 92.7461% (716/772)\n",
            "0 4 Epoch: 331 | ANN: testLoss: 0.5910 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 331 | ANN: testLoss: 0.4861 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 331 | ANN: testLoss: 0.4577 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 331 | ANN: testLoss: 0.3433 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 332 | ANN: trainLoss: 0.1435 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 332 | ANN: trainLoss: 0.1473 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 332 | ANN: trainLoss: 0.1679 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 332 | ANN: trainLoss: 0.1888 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 332 | ANN: trainLoss: 0.1918 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 332 | ANN: trainLoss: 0.1830 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 332 | ANN: trainLoss: 0.1944 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 332 | ANN: trainLoss: 0.1927 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 332 | ANN: trainLoss: 0.1893 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 332 | ANN: trainLoss: 0.1996 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 332 | ANN: trainLoss: 0.1943 | trainAcc: 93.1818% (656/704)\n",
            "11 13 Epoch: 332 | ANN: trainLoss: 0.1976 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 332 | ANN: trainLoss: 0.2325 | trainAcc: 92.7461% (716/772)\n",
            "0 4 Epoch: 332 | ANN: testLoss: 0.5221 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 332 | ANN: testLoss: 0.4936 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 332 | ANN: testLoss: 0.5152 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 332 | ANN: testLoss: 0.4127 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 333 | ANN: trainLoss: 0.1728 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 333 | ANN: trainLoss: 0.2096 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 333 | ANN: trainLoss: 0.2054 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 333 | ANN: trainLoss: 0.2115 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 333 | ANN: trainLoss: 0.2168 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 333 | ANN: trainLoss: 0.2091 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 333 | ANN: trainLoss: 0.2135 | trainAcc: 91.2946% (409/448)\n",
            "7 13 Epoch: 333 | ANN: trainLoss: 0.2090 | trainAcc: 91.4062% (468/512)\n",
            "8 13 Epoch: 333 | ANN: trainLoss: 0.2093 | trainAcc: 91.3194% (526/576)\n",
            "9 13 Epoch: 333 | ANN: trainLoss: 0.2098 | trainAcc: 91.5625% (586/640)\n",
            "10 13 Epoch: 333 | ANN: trainLoss: 0.2057 | trainAcc: 91.6193% (645/704)\n",
            "11 13 Epoch: 333 | ANN: trainLoss: 0.2040 | trainAcc: 91.6667% (704/768)\n",
            "12 13 Epoch: 333 | ANN: trainLoss: 0.2327 | trainAcc: 91.5803% (707/772)\n",
            "0 4 Epoch: 333 | ANN: testLoss: 0.4416 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 333 | ANN: testLoss: 0.3925 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 333 | ANN: testLoss: 0.4682 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 333 | ANN: testLoss: 0.3538 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 334 | ANN: trainLoss: 0.2162 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 334 | ANN: trainLoss: 0.2344 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 334 | ANN: trainLoss: 0.2040 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 334 | ANN: trainLoss: 0.1950 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 334 | ANN: trainLoss: 0.2008 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 334 | ANN: trainLoss: 0.2148 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 334 | ANN: trainLoss: 0.2134 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 334 | ANN: trainLoss: 0.2147 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 334 | ANN: trainLoss: 0.2067 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 334 | ANN: trainLoss: 0.2032 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 334 | ANN: trainLoss: 0.1968 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 334 | ANN: trainLoss: 0.1916 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 334 | ANN: trainLoss: 0.2545 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 334 | ANN: testLoss: 0.5081 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 334 | ANN: testLoss: 0.4666 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 334 | ANN: testLoss: 0.4902 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 334 | ANN: testLoss: 0.3682 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 335 | ANN: trainLoss: 0.1418 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 335 | ANN: trainLoss: 0.1475 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 335 | ANN: trainLoss: 0.1627 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 335 | ANN: trainLoss: 0.1482 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 335 | ANN: trainLoss: 0.1403 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 335 | ANN: trainLoss: 0.1754 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 335 | ANN: trainLoss: 0.1868 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 335 | ANN: trainLoss: 0.1861 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 335 | ANN: trainLoss: 0.1817 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 335 | ANN: trainLoss: 0.1886 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 335 | ANN: trainLoss: 0.1791 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 335 | ANN: trainLoss: 0.1774 | trainAcc: 94.4010% (725/768)\n",
            "12 13 Epoch: 335 | ANN: trainLoss: 0.1842 | trainAcc: 94.3005% (728/772)\n",
            "0 4 Epoch: 335 | ANN: testLoss: 0.4335 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 335 | ANN: testLoss: 0.5193 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 335 | ANN: testLoss: 0.4855 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 335 | ANN: testLoss: 0.3644 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 336 | ANN: trainLoss: 0.1571 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 336 | ANN: trainLoss: 0.1717 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 336 | ANN: trainLoss: 0.1461 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 336 | ANN: trainLoss: 0.1773 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 336 | ANN: trainLoss: 0.1744 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 336 | ANN: trainLoss: 0.1863 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 336 | ANN: trainLoss: 0.1929 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 336 | ANN: trainLoss: 0.1863 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 336 | ANN: trainLoss: 0.1851 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 336 | ANN: trainLoss: 0.1959 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 336 | ANN: trainLoss: 0.1997 | trainAcc: 92.6136% (652/704)\n",
            "11 13 Epoch: 336 | ANN: trainLoss: 0.2019 | trainAcc: 92.3177% (709/768)\n",
            "12 13 Epoch: 336 | ANN: trainLoss: 0.2182 | trainAcc: 92.2280% (712/772)\n",
            "0 4 Epoch: 336 | ANN: testLoss: 0.4375 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 336 | ANN: testLoss: 0.4396 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 336 | ANN: testLoss: 0.4755 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 336 | ANN: testLoss: 0.7780 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 337 | ANN: trainLoss: 0.2696 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 337 | ANN: trainLoss: 0.2270 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 337 | ANN: trainLoss: 0.2017 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 337 | ANN: trainLoss: 0.1942 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 337 | ANN: trainLoss: 0.1930 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 337 | ANN: trainLoss: 0.1917 | trainAcc: 92.4479% (355/384)\n",
            "6 13 Epoch: 337 | ANN: trainLoss: 0.1863 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 337 | ANN: trainLoss: 0.1971 | trainAcc: 91.9922% (471/512)\n",
            "8 13 Epoch: 337 | ANN: trainLoss: 0.2049 | trainAcc: 91.8403% (529/576)\n",
            "9 13 Epoch: 337 | ANN: trainLoss: 0.2069 | trainAcc: 91.5625% (586/640)\n",
            "10 13 Epoch: 337 | ANN: trainLoss: 0.2114 | trainAcc: 91.7614% (646/704)\n",
            "11 13 Epoch: 337 | ANN: trainLoss: 0.2089 | trainAcc: 91.7969% (705/768)\n",
            "12 13 Epoch: 337 | ANN: trainLoss: 0.2139 | trainAcc: 91.8394% (709/772)\n",
            "0 4 Epoch: 337 | ANN: testLoss: 0.3956 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 337 | ANN: testLoss: 0.3963 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 337 | ANN: testLoss: 0.4609 | testAcc: 79.6875% (153/192)\n",
            "3 4 Epoch: 337 | ANN: testLoss: 0.7191 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 338 | ANN: trainLoss: 0.2415 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 338 | ANN: trainLoss: 0.2067 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 338 | ANN: trainLoss: 0.2081 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 338 | ANN: trainLoss: 0.1860 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 338 | ANN: trainLoss: 0.2049 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 338 | ANN: trainLoss: 0.2081 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 338 | ANN: trainLoss: 0.2134 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 338 | ANN: trainLoss: 0.2162 | trainAcc: 91.2109% (467/512)\n",
            "8 13 Epoch: 338 | ANN: trainLoss: 0.2090 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 338 | ANN: trainLoss: 0.2089 | trainAcc: 91.7188% (587/640)\n",
            "10 13 Epoch: 338 | ANN: trainLoss: 0.2023 | trainAcc: 92.1875% (649/704)\n",
            "11 13 Epoch: 338 | ANN: trainLoss: 0.2098 | trainAcc: 91.9271% (706/768)\n",
            "12 13 Epoch: 338 | ANN: trainLoss: 0.2142 | trainAcc: 91.8394% (709/772)\n",
            "0 4 Epoch: 338 | ANN: testLoss: 0.3785 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 338 | ANN: testLoss: 0.4557 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 338 | ANN: testLoss: 0.4608 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 338 | ANN: testLoss: 0.6244 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 339 | ANN: trainLoss: 0.1279 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 339 | ANN: trainLoss: 0.1505 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 339 | ANN: trainLoss: 0.1832 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 339 | ANN: trainLoss: 0.1932 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 339 | ANN: trainLoss: 0.1878 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 339 | ANN: trainLoss: 0.2087 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 339 | ANN: trainLoss: 0.2028 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 339 | ANN: trainLoss: 0.2027 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 339 | ANN: trainLoss: 0.1922 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 339 | ANN: trainLoss: 0.2024 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 339 | ANN: trainLoss: 0.2159 | trainAcc: 92.3295% (650/704)\n",
            "11 13 Epoch: 339 | ANN: trainLoss: 0.2108 | trainAcc: 92.5781% (711/768)\n",
            "12 13 Epoch: 339 | ANN: trainLoss: 0.4358 | trainAcc: 92.4870% (714/772)\n",
            "0 4 Epoch: 339 | ANN: testLoss: 0.5086 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 339 | ANN: testLoss: 0.4305 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 339 | ANN: testLoss: 0.4670 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 339 | ANN: testLoss: 0.3518 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 340 | ANN: trainLoss: 0.1575 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 340 | ANN: trainLoss: 0.1904 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 340 | ANN: trainLoss: 0.2009 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 340 | ANN: trainLoss: 0.1819 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 340 | ANN: trainLoss: 0.1839 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 340 | ANN: trainLoss: 0.1814 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 340 | ANN: trainLoss: 0.1755 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 340 | ANN: trainLoss: 0.1750 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 340 | ANN: trainLoss: 0.1806 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 340 | ANN: trainLoss: 0.1782 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 340 | ANN: trainLoss: 0.1944 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 340 | ANN: trainLoss: 0.2042 | trainAcc: 92.7083% (712/768)\n",
            "12 13 Epoch: 340 | ANN: trainLoss: 0.2279 | trainAcc: 92.6166% (715/772)\n",
            "0 4 Epoch: 340 | ANN: testLoss: 0.5230 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 340 | ANN: testLoss: 0.5122 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 340 | ANN: testLoss: 0.5370 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 340 | ANN: testLoss: 0.4191 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 341 | ANN: trainLoss: 0.2815 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 341 | ANN: trainLoss: 0.2661 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 341 | ANN: trainLoss: 0.2338 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 341 | ANN: trainLoss: 0.2241 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 341 | ANN: trainLoss: 0.2458 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 341 | ANN: trainLoss: 0.2388 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 341 | ANN: trainLoss: 0.2346 | trainAcc: 91.2946% (409/448)\n",
            "7 13 Epoch: 341 | ANN: trainLoss: 0.2426 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 341 | ANN: trainLoss: 0.2386 | trainAcc: 90.4514% (521/576)\n",
            "9 13 Epoch: 341 | ANN: trainLoss: 0.2417 | trainAcc: 90.1562% (577/640)\n",
            "10 13 Epoch: 341 | ANN: trainLoss: 0.2357 | trainAcc: 90.3409% (636/704)\n",
            "11 13 Epoch: 341 | ANN: trainLoss: 0.2335 | trainAcc: 90.4948% (695/768)\n",
            "12 13 Epoch: 341 | ANN: trainLoss: 0.2917 | trainAcc: 90.4145% (698/772)\n",
            "0 4 Epoch: 341 | ANN: testLoss: 0.4675 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 341 | ANN: testLoss: 0.3965 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 341 | ANN: testLoss: 0.5119 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 341 | ANN: testLoss: 0.5844 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 342 | ANN: trainLoss: 0.1913 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 342 | ANN: trainLoss: 0.2207 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 342 | ANN: trainLoss: 0.2583 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 342 | ANN: trainLoss: 0.2281 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 342 | ANN: trainLoss: 0.2281 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 342 | ANN: trainLoss: 0.2377 | trainAcc: 89.8438% (345/384)\n",
            "6 13 Epoch: 342 | ANN: trainLoss: 0.2478 | trainAcc: 89.2857% (400/448)\n",
            "7 13 Epoch: 342 | ANN: trainLoss: 0.2370 | trainAcc: 90.2344% (462/512)\n",
            "8 13 Epoch: 342 | ANN: trainLoss: 0.2447 | trainAcc: 89.9306% (518/576)\n",
            "9 13 Epoch: 342 | ANN: trainLoss: 0.2409 | trainAcc: 90.1562% (577/640)\n",
            "10 13 Epoch: 342 | ANN: trainLoss: 0.2339 | trainAcc: 90.3409% (636/704)\n",
            "11 13 Epoch: 342 | ANN: trainLoss: 0.2314 | trainAcc: 90.7552% (697/768)\n",
            "12 13 Epoch: 342 | ANN: trainLoss: 0.3183 | trainAcc: 90.5440% (699/772)\n",
            "0 4 Epoch: 342 | ANN: testLoss: 0.3881 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 342 | ANN: testLoss: 0.4509 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 342 | ANN: testLoss: 0.5330 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 342 | ANN: testLoss: 0.3998 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 343 | ANN: trainLoss: 0.1764 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 343 | ANN: trainLoss: 0.1947 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 343 | ANN: trainLoss: 0.1841 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 343 | ANN: trainLoss: 0.2093 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 343 | ANN: trainLoss: 0.2190 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 343 | ANN: trainLoss: 0.2123 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 343 | ANN: trainLoss: 0.2197 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 343 | ANN: trainLoss: 0.2190 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 343 | ANN: trainLoss: 0.2206 | trainAcc: 90.2778% (520/576)\n",
            "9 13 Epoch: 343 | ANN: trainLoss: 0.2215 | trainAcc: 90.4688% (579/640)\n",
            "10 13 Epoch: 343 | ANN: trainLoss: 0.2185 | trainAcc: 90.7670% (639/704)\n",
            "11 13 Epoch: 343 | ANN: trainLoss: 0.2217 | trainAcc: 90.6250% (696/768)\n",
            "12 13 Epoch: 343 | ANN: trainLoss: 0.2147 | trainAcc: 90.6736% (700/772)\n",
            "0 4 Epoch: 343 | ANN: testLoss: 0.6760 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 343 | ANN: testLoss: 0.5729 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 343 | ANN: testLoss: 0.5376 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 343 | ANN: testLoss: 0.4081 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 344 | ANN: trainLoss: 0.1777 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 344 | ANN: trainLoss: 0.2062 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 344 | ANN: trainLoss: 0.1893 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 344 | ANN: trainLoss: 0.1773 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 344 | ANN: trainLoss: 0.1990 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 344 | ANN: trainLoss: 0.1973 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 344 | ANN: trainLoss: 0.2125 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 344 | ANN: trainLoss: 0.2032 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 344 | ANN: trainLoss: 0.2034 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 344 | ANN: trainLoss: 0.2045 | trainAcc: 92.8125% (594/640)\n",
            "10 13 Epoch: 344 | ANN: trainLoss: 0.2007 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 344 | ANN: trainLoss: 0.2008 | trainAcc: 92.9688% (714/768)\n",
            "12 13 Epoch: 344 | ANN: trainLoss: 0.3062 | trainAcc: 92.7461% (716/772)\n",
            "0 4 Epoch: 344 | ANN: testLoss: 0.5029 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 344 | ANN: testLoss: 0.5787 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 344 | ANN: testLoss: 0.5277 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 344 | ANN: testLoss: 0.7339 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 345 | ANN: trainLoss: 0.2809 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 345 | ANN: trainLoss: 0.2185 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 345 | ANN: trainLoss: 0.1903 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 345 | ANN: trainLoss: 0.2134 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 345 | ANN: trainLoss: 0.2051 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 345 | ANN: trainLoss: 0.2068 | trainAcc: 90.8854% (349/384)\n",
            "6 13 Epoch: 345 | ANN: trainLoss: 0.2079 | trainAcc: 90.6250% (406/448)\n",
            "7 13 Epoch: 345 | ANN: trainLoss: 0.2093 | trainAcc: 90.6250% (464/512)\n",
            "8 13 Epoch: 345 | ANN: trainLoss: 0.2213 | trainAcc: 89.7569% (517/576)\n",
            "9 13 Epoch: 345 | ANN: trainLoss: 0.2148 | trainAcc: 90.4688% (579/640)\n",
            "10 13 Epoch: 345 | ANN: trainLoss: 0.2194 | trainAcc: 90.4830% (637/704)\n",
            "11 13 Epoch: 345 | ANN: trainLoss: 0.2203 | trainAcc: 90.4948% (695/768)\n",
            "12 13 Epoch: 345 | ANN: trainLoss: 0.2264 | trainAcc: 90.5440% (699/772)\n",
            "0 4 Epoch: 345 | ANN: testLoss: 0.4765 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 345 | ANN: testLoss: 0.5457 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 345 | ANN: testLoss: 0.5264 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 345 | ANN: testLoss: 0.4263 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 346 | ANN: trainLoss: 0.1703 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 346 | ANN: trainLoss: 0.2420 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 346 | ANN: trainLoss: 0.2523 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 346 | ANN: trainLoss: 0.2866 | trainAcc: 89.8438% (230/256)\n",
            "4 13 Epoch: 346 | ANN: trainLoss: 0.2748 | trainAcc: 90.3125% (289/320)\n",
            "5 13 Epoch: 346 | ANN: trainLoss: 0.2731 | trainAcc: 90.3646% (347/384)\n",
            "6 13 Epoch: 346 | ANN: trainLoss: 0.2634 | trainAcc: 90.6250% (406/448)\n",
            "7 13 Epoch: 346 | ANN: trainLoss: 0.2574 | trainAcc: 90.4297% (463/512)\n",
            "8 13 Epoch: 346 | ANN: trainLoss: 0.2487 | trainAcc: 90.9722% (524/576)\n",
            "9 13 Epoch: 346 | ANN: trainLoss: 0.2502 | trainAcc: 91.2500% (584/640)\n",
            "10 13 Epoch: 346 | ANN: trainLoss: 0.2605 | trainAcc: 91.1932% (642/704)\n",
            "11 13 Epoch: 346 | ANN: trainLoss: 0.2692 | trainAcc: 90.3646% (694/768)\n",
            "12 13 Epoch: 346 | ANN: trainLoss: 0.2629 | trainAcc: 90.4145% (698/772)\n",
            "0 4 Epoch: 346 | ANN: testLoss: 0.5591 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 346 | ANN: testLoss: 0.5655 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 346 | ANN: testLoss: 0.5037 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 346 | ANN: testLoss: 0.7584 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 347 | ANN: trainLoss: 0.1784 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 347 | ANN: trainLoss: 0.1408 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 347 | ANN: trainLoss: 0.1855 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 347 | ANN: trainLoss: 0.2203 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 347 | ANN: trainLoss: 0.2287 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 347 | ANN: trainLoss: 0.2451 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 347 | ANN: trainLoss: 0.2449 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 347 | ANN: trainLoss: 0.2449 | trainAcc: 91.6016% (469/512)\n",
            "8 13 Epoch: 347 | ANN: trainLoss: 0.2398 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 347 | ANN: trainLoss: 0.2400 | trainAcc: 91.2500% (584/640)\n",
            "10 13 Epoch: 347 | ANN: trainLoss: 0.2410 | trainAcc: 91.1932% (642/704)\n",
            "11 13 Epoch: 347 | ANN: trainLoss: 0.2388 | trainAcc: 91.4062% (702/768)\n",
            "12 13 Epoch: 347 | ANN: trainLoss: 0.2338 | trainAcc: 91.4508% (706/772)\n",
            "0 4 Epoch: 347 | ANN: testLoss: 0.4213 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 347 | ANN: testLoss: 0.4542 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 347 | ANN: testLoss: 0.4963 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 347 | ANN: testLoss: 0.5804 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 348 | ANN: trainLoss: 0.2345 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 348 | ANN: trainLoss: 0.2257 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 348 | ANN: trainLoss: 0.2240 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 348 | ANN: trainLoss: 0.1971 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 348 | ANN: trainLoss: 0.2008 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 348 | ANN: trainLoss: 0.2006 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 348 | ANN: trainLoss: 0.1945 | trainAcc: 92.4107% (414/448)\n",
            "7 13 Epoch: 348 | ANN: trainLoss: 0.2091 | trainAcc: 91.7969% (470/512)\n",
            "8 13 Epoch: 348 | ANN: trainLoss: 0.2025 | trainAcc: 92.3611% (532/576)\n",
            "9 13 Epoch: 348 | ANN: trainLoss: 0.2044 | trainAcc: 92.0312% (589/640)\n",
            "10 13 Epoch: 348 | ANN: trainLoss: 0.2059 | trainAcc: 92.1875% (649/704)\n",
            "11 13 Epoch: 348 | ANN: trainLoss: 0.2092 | trainAcc: 91.9271% (706/768)\n",
            "12 13 Epoch: 348 | ANN: trainLoss: 0.1986 | trainAcc: 91.9689% (710/772)\n",
            "0 4 Epoch: 348 | ANN: testLoss: 0.4404 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 348 | ANN: testLoss: 0.4887 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 348 | ANN: testLoss: 0.4783 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 348 | ANN: testLoss: 0.5328 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 349 | ANN: trainLoss: 0.2203 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 349 | ANN: trainLoss: 0.2587 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 349 | ANN: trainLoss: 0.2661 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 349 | ANN: trainLoss: 0.2488 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 349 | ANN: trainLoss: 0.2286 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 349 | ANN: trainLoss: 0.2365 | trainAcc: 90.3646% (347/384)\n",
            "6 13 Epoch: 349 | ANN: trainLoss: 0.2171 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 349 | ANN: trainLoss: 0.2183 | trainAcc: 90.4297% (463/512)\n",
            "8 13 Epoch: 349 | ANN: trainLoss: 0.2163 | trainAcc: 90.4514% (521/576)\n",
            "9 13 Epoch: 349 | ANN: trainLoss: 0.2147 | trainAcc: 90.7812% (581/640)\n",
            "10 13 Epoch: 349 | ANN: trainLoss: 0.2168 | trainAcc: 90.6250% (638/704)\n",
            "11 13 Epoch: 349 | ANN: trainLoss: 0.2097 | trainAcc: 90.8854% (698/768)\n",
            "12 13 Epoch: 349 | ANN: trainLoss: 0.2133 | trainAcc: 90.8031% (701/772)\n",
            "0 4 Epoch: 349 | ANN: testLoss: 0.4553 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 349 | ANN: testLoss: 0.4243 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 349 | ANN: testLoss: 0.4725 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 349 | ANN: testLoss: 0.3668 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 350 | ANN: trainLoss: 0.1652 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 350 | ANN: trainLoss: 0.2144 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 350 | ANN: trainLoss: 0.2085 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 350 | ANN: trainLoss: 0.2263 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 350 | ANN: trainLoss: 0.2196 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 350 | ANN: trainLoss: 0.2084 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 350 | ANN: trainLoss: 0.2062 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 350 | ANN: trainLoss: 0.2009 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 350 | ANN: trainLoss: 0.2065 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 350 | ANN: trainLoss: 0.2152 | trainAcc: 92.5000% (592/640)\n",
            "10 13 Epoch: 350 | ANN: trainLoss: 0.2127 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 350 | ANN: trainLoss: 0.2069 | trainAcc: 93.0990% (715/768)\n",
            "12 13 Epoch: 350 | ANN: trainLoss: 0.1950 | trainAcc: 93.1347% (719/772)\n",
            "0 4 Epoch: 350 | ANN: testLoss: 0.4363 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 350 | ANN: testLoss: 0.4742 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 350 | ANN: testLoss: 0.4727 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 350 | ANN: testLoss: 0.6812 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 351 | ANN: trainLoss: 0.2423 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 351 | ANN: trainLoss: 0.3022 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 351 | ANN: trainLoss: 0.2831 | trainAcc: 88.5417% (170/192)\n",
            "3 13 Epoch: 351 | ANN: trainLoss: 0.2691 | trainAcc: 89.0625% (228/256)\n",
            "4 13 Epoch: 351 | ANN: trainLoss: 0.2446 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 351 | ANN: trainLoss: 0.2472 | trainAcc: 90.1042% (346/384)\n",
            "6 13 Epoch: 351 | ANN: trainLoss: 0.2355 | trainAcc: 90.8482% (407/448)\n",
            "7 13 Epoch: 351 | ANN: trainLoss: 0.2291 | trainAcc: 91.0156% (466/512)\n",
            "8 13 Epoch: 351 | ANN: trainLoss: 0.2307 | trainAcc: 90.2778% (520/576)\n",
            "9 13 Epoch: 351 | ANN: trainLoss: 0.2238 | trainAcc: 90.7812% (581/640)\n",
            "10 13 Epoch: 351 | ANN: trainLoss: 0.2168 | trainAcc: 91.0511% (641/704)\n",
            "11 13 Epoch: 351 | ANN: trainLoss: 0.2148 | trainAcc: 91.1458% (700/768)\n",
            "12 13 Epoch: 351 | ANN: trainLoss: 0.2758 | trainAcc: 91.0622% (703/772)\n",
            "0 4 Epoch: 351 | ANN: testLoss: 0.3623 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 351 | ANN: testLoss: 0.4019 | testAcc: 82.0312% (105/128)\n",
            "2 4 Epoch: 351 | ANN: testLoss: 0.4794 | testAcc: 79.6875% (153/192)\n",
            "3 4 Epoch: 351 | ANN: testLoss: 0.8163 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 352 | ANN: trainLoss: 0.1530 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 352 | ANN: trainLoss: 0.2120 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 352 | ANN: trainLoss: 0.2311 | trainAcc: 88.0208% (169/192)\n",
            "3 13 Epoch: 352 | ANN: trainLoss: 0.2158 | trainAcc: 89.4531% (229/256)\n",
            "4 13 Epoch: 352 | ANN: trainLoss: 0.2295 | trainAcc: 89.6875% (287/320)\n",
            "5 13 Epoch: 352 | ANN: trainLoss: 0.2257 | trainAcc: 90.3646% (347/384)\n",
            "6 13 Epoch: 352 | ANN: trainLoss: 0.2137 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 352 | ANN: trainLoss: 0.2188 | trainAcc: 91.0156% (466/512)\n",
            "8 13 Epoch: 352 | ANN: trainLoss: 0.2170 | trainAcc: 91.4931% (527/576)\n",
            "9 13 Epoch: 352 | ANN: trainLoss: 0.2174 | trainAcc: 91.4062% (585/640)\n",
            "10 13 Epoch: 352 | ANN: trainLoss: 0.2076 | trainAcc: 91.7614% (646/704)\n",
            "11 13 Epoch: 352 | ANN: trainLoss: 0.2086 | trainAcc: 91.7969% (705/768)\n",
            "12 13 Epoch: 352 | ANN: trainLoss: 0.3353 | trainAcc: 91.7098% (708/772)\n",
            "0 4 Epoch: 352 | ANN: testLoss: 0.4570 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 352 | ANN: testLoss: 0.5220 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 352 | ANN: testLoss: 0.5020 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 352 | ANN: testLoss: 0.5772 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 353 | ANN: trainLoss: 0.2386 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 353 | ANN: trainLoss: 0.2198 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 353 | ANN: trainLoss: 0.1915 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 353 | ANN: trainLoss: 0.1811 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 353 | ANN: trainLoss: 0.1885 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 353 | ANN: trainLoss: 0.1902 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 353 | ANN: trainLoss: 0.1925 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 353 | ANN: trainLoss: 0.1943 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 353 | ANN: trainLoss: 0.1979 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 353 | ANN: trainLoss: 0.1933 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 353 | ANN: trainLoss: 0.1920 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 353 | ANN: trainLoss: 0.1954 | trainAcc: 93.3594% (717/768)\n",
            "12 13 Epoch: 353 | ANN: trainLoss: 0.1889 | trainAcc: 93.3938% (721/772)\n",
            "0 4 Epoch: 353 | ANN: testLoss: 0.4773 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 353 | ANN: testLoss: 0.5025 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 353 | ANN: testLoss: 0.4739 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 353 | ANN: testLoss: 1.7221 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 354 | ANN: trainLoss: 0.0916 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 354 | ANN: trainLoss: 0.1316 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 354 | ANN: trainLoss: 0.1447 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 354 | ANN: trainLoss: 0.1471 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 354 | ANN: trainLoss: 0.1611 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 354 | ANN: trainLoss: 0.1733 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 354 | ANN: trainLoss: 0.1855 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 354 | ANN: trainLoss: 0.1819 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 354 | ANN: trainLoss: 0.1962 | trainAcc: 92.1875% (531/576)\n",
            "9 13 Epoch: 354 | ANN: trainLoss: 0.1966 | trainAcc: 92.5000% (592/640)\n",
            "10 13 Epoch: 354 | ANN: trainLoss: 0.1963 | trainAcc: 92.4716% (651/704)\n",
            "11 13 Epoch: 354 | ANN: trainLoss: 0.1936 | trainAcc: 92.4479% (710/768)\n",
            "12 13 Epoch: 354 | ANN: trainLoss: 0.2175 | trainAcc: 92.3575% (713/772)\n",
            "0 4 Epoch: 354 | ANN: testLoss: 0.4122 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 354 | ANN: testLoss: 0.4445 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 354 | ANN: testLoss: 0.4864 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 354 | ANN: testLoss: 0.3662 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 355 | ANN: trainLoss: 0.2243 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 355 | ANN: trainLoss: 0.2535 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 355 | ANN: trainLoss: 0.2160 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 355 | ANN: trainLoss: 0.1892 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 355 | ANN: trainLoss: 0.1663 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 355 | ANN: trainLoss: 0.1744 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 355 | ANN: trainLoss: 0.1660 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 355 | ANN: trainLoss: 0.1603 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 355 | ANN: trainLoss: 0.1639 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 355 | ANN: trainLoss: 0.1606 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 355 | ANN: trainLoss: 0.1662 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 355 | ANN: trainLoss: 0.1895 | trainAcc: 93.3594% (717/768)\n",
            "12 13 Epoch: 355 | ANN: trainLoss: 0.2761 | trainAcc: 93.0052% (718/772)\n",
            "0 4 Epoch: 355 | ANN: testLoss: 0.5124 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 355 | ANN: testLoss: 0.5543 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 355 | ANN: testLoss: 0.4894 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 355 | ANN: testLoss: 1.0661 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 356 | ANN: trainLoss: 0.1666 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 356 | ANN: trainLoss: 0.1950 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 356 | ANN: trainLoss: 0.1715 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 356 | ANN: trainLoss: 0.1837 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 356 | ANN: trainLoss: 0.1883 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 356 | ANN: trainLoss: 0.2031 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 356 | ANN: trainLoss: 0.2005 | trainAcc: 91.5179% (410/448)\n",
            "7 13 Epoch: 356 | ANN: trainLoss: 0.1938 | trainAcc: 91.7969% (470/512)\n",
            "8 13 Epoch: 356 | ANN: trainLoss: 0.1882 | trainAcc: 92.5347% (533/576)\n",
            "9 13 Epoch: 356 | ANN: trainLoss: 0.1942 | trainAcc: 92.3438% (591/640)\n",
            "10 13 Epoch: 356 | ANN: trainLoss: 0.2060 | trainAcc: 92.3295% (650/704)\n",
            "11 13 Epoch: 356 | ANN: trainLoss: 0.2061 | trainAcc: 92.3177% (709/768)\n",
            "12 13 Epoch: 356 | ANN: trainLoss: 0.1907 | trainAcc: 92.3575% (713/772)\n",
            "0 4 Epoch: 356 | ANN: testLoss: 0.4851 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 356 | ANN: testLoss: 0.4752 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 356 | ANN: testLoss: 0.4920 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 356 | ANN: testLoss: 0.4156 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 357 | ANN: trainLoss: 0.1887 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 357 | ANN: trainLoss: 0.1886 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 357 | ANN: trainLoss: 0.2103 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 357 | ANN: trainLoss: 0.2005 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 357 | ANN: trainLoss: 0.1972 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 357 | ANN: trainLoss: 0.1970 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 357 | ANN: trainLoss: 0.1943 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 357 | ANN: trainLoss: 0.1918 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 357 | ANN: trainLoss: 0.1927 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 357 | ANN: trainLoss: 0.1900 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 357 | ANN: trainLoss: 0.1958 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 357 | ANN: trainLoss: 0.2029 | trainAcc: 92.5781% (711/768)\n",
            "12 13 Epoch: 357 | ANN: trainLoss: 0.1968 | trainAcc: 92.6166% (715/772)\n",
            "0 4 Epoch: 357 | ANN: testLoss: 0.5166 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 357 | ANN: testLoss: 0.4904 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 357 | ANN: testLoss: 0.4995 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 357 | ANN: testLoss: 0.4403 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 358 | ANN: trainLoss: 0.1902 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 358 | ANN: trainLoss: 0.2280 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 358 | ANN: trainLoss: 0.1976 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 358 | ANN: trainLoss: 0.1995 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 358 | ANN: trainLoss: 0.1880 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 358 | ANN: trainLoss: 0.1830 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 358 | ANN: trainLoss: 0.1819 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 358 | ANN: trainLoss: 0.1762 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 358 | ANN: trainLoss: 0.1674 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 358 | ANN: trainLoss: 0.1806 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 358 | ANN: trainLoss: 0.1812 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 358 | ANN: trainLoss: 0.1880 | trainAcc: 92.5781% (711/768)\n",
            "12 13 Epoch: 358 | ANN: trainLoss: 0.2562 | trainAcc: 92.3575% (713/772)\n",
            "0 4 Epoch: 358 | ANN: testLoss: 0.3110 | testAcc: 90.6250% (58/64)\n",
            "1 4 Epoch: 358 | ANN: testLoss: 0.4464 | testAcc: 82.0312% (105/128)\n",
            "2 4 Epoch: 358 | ANN: testLoss: 0.4839 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 358 | ANN: testLoss: 0.3632 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 359 | ANN: trainLoss: 0.2068 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 359 | ANN: trainLoss: 0.1964 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 359 | ANN: trainLoss: 0.1809 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 359 | ANN: trainLoss: 0.1788 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 359 | ANN: trainLoss: 0.1893 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 359 | ANN: trainLoss: 0.1791 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 359 | ANN: trainLoss: 0.1712 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 359 | ANN: trainLoss: 0.1709 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 359 | ANN: trainLoss: 0.1651 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 359 | ANN: trainLoss: 0.1703 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 359 | ANN: trainLoss: 0.1733 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 359 | ANN: trainLoss: 0.1755 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 359 | ANN: trainLoss: 0.1886 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 359 | ANN: testLoss: 0.6074 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 359 | ANN: testLoss: 0.5197 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 359 | ANN: testLoss: 0.4830 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 359 | ANN: testLoss: 0.3667 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 360 | ANN: trainLoss: 0.1150 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 360 | ANN: trainLoss: 0.1202 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 360 | ANN: trainLoss: 0.1555 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 360 | ANN: trainLoss: 0.1481 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 360 | ANN: trainLoss: 0.1585 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 360 | ANN: trainLoss: 0.1610 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 360 | ANN: trainLoss: 0.1629 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 360 | ANN: trainLoss: 0.1654 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 360 | ANN: trainLoss: 0.1705 | trainAcc: 94.0972% (542/576)\n",
            "9 13 Epoch: 360 | ANN: trainLoss: 0.1699 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 360 | ANN: trainLoss: 0.1621 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 360 | ANN: trainLoss: 0.1630 | trainAcc: 94.4010% (725/768)\n",
            "12 13 Epoch: 360 | ANN: trainLoss: 0.1531 | trainAcc: 94.4301% (729/772)\n",
            "0 4 Epoch: 360 | ANN: testLoss: 0.4270 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 360 | ANN: testLoss: 0.4537 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 360 | ANN: testLoss: 0.4851 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 360 | ANN: testLoss: 0.5034 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 361 | ANN: trainLoss: 0.2022 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 361 | ANN: trainLoss: 0.1476 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 361 | ANN: trainLoss: 0.1361 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 361 | ANN: trainLoss: 0.1549 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 361 | ANN: trainLoss: 0.1618 | trainAcc: 92.5000% (296/320)\n",
            "5 13 Epoch: 361 | ANN: trainLoss: 0.1747 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 361 | ANN: trainLoss: 0.1804 | trainAcc: 92.4107% (414/448)\n",
            "7 13 Epoch: 361 | ANN: trainLoss: 0.1675 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 361 | ANN: trainLoss: 0.1693 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 361 | ANN: trainLoss: 0.1676 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 361 | ANN: trainLoss: 0.1764 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 361 | ANN: trainLoss: 0.1818 | trainAcc: 92.7083% (712/768)\n",
            "12 13 Epoch: 361 | ANN: trainLoss: 0.3333 | trainAcc: 92.4870% (714/772)\n",
            "0 4 Epoch: 361 | ANN: testLoss: 0.5077 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 361 | ANN: testLoss: 0.4746 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 361 | ANN: testLoss: 0.4758 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 361 | ANN: testLoss: 0.8725 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 362 | ANN: trainLoss: 0.1340 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 362 | ANN: trainLoss: 0.1674 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 362 | ANN: trainLoss: 0.1641 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 362 | ANN: trainLoss: 0.1494 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 362 | ANN: trainLoss: 0.1585 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 362 | ANN: trainLoss: 0.1697 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 362 | ANN: trainLoss: 0.1721 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 362 | ANN: trainLoss: 0.1644 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 362 | ANN: trainLoss: 0.1757 | trainAcc: 93.5764% (539/576)\n",
            "9 13 Epoch: 362 | ANN: trainLoss: 0.1760 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 362 | ANN: trainLoss: 0.1779 | trainAcc: 93.6080% (659/704)\n",
            "11 13 Epoch: 362 | ANN: trainLoss: 0.1745 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 362 | ANN: trainLoss: 0.1618 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 362 | ANN: testLoss: 0.5537 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 362 | ANN: testLoss: 0.5177 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 362 | ANN: testLoss: 0.4659 | testAcc: 79.6875% (153/192)\n",
            "3 4 Epoch: 362 | ANN: testLoss: 0.6724 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 363 | ANN: trainLoss: 0.1327 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 363 | ANN: trainLoss: 0.1440 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 363 | ANN: trainLoss: 0.2121 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 363 | ANN: trainLoss: 0.2133 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 363 | ANN: trainLoss: 0.2197 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 363 | ANN: trainLoss: 0.2157 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 363 | ANN: trainLoss: 0.2105 | trainAcc: 91.5179% (410/448)\n",
            "7 13 Epoch: 363 | ANN: trainLoss: 0.2237 | trainAcc: 91.2109% (467/512)\n",
            "8 13 Epoch: 363 | ANN: trainLoss: 0.2162 | trainAcc: 91.4931% (527/576)\n",
            "9 13 Epoch: 363 | ANN: trainLoss: 0.2099 | trainAcc: 91.5625% (586/640)\n",
            "10 13 Epoch: 363 | ANN: trainLoss: 0.2160 | trainAcc: 91.1932% (642/704)\n",
            "11 13 Epoch: 363 | ANN: trainLoss: 0.2049 | trainAcc: 91.6667% (704/768)\n",
            "12 13 Epoch: 363 | ANN: trainLoss: 0.2590 | trainAcc: 91.5803% (707/772)\n",
            "0 4 Epoch: 363 | ANN: testLoss: 0.4417 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 363 | ANN: testLoss: 0.4342 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 363 | ANN: testLoss: 0.4603 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 363 | ANN: testLoss: 0.6436 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 364 | ANN: trainLoss: 0.1763 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 364 | ANN: trainLoss: 0.2266 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 364 | ANN: trainLoss: 0.2411 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 364 | ANN: trainLoss: 0.2131 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 364 | ANN: trainLoss: 0.2089 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 364 | ANN: trainLoss: 0.2121 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 364 | ANN: trainLoss: 0.2036 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 364 | ANN: trainLoss: 0.2126 | trainAcc: 92.1875% (472/512)\n",
            "8 13 Epoch: 364 | ANN: trainLoss: 0.2116 | trainAcc: 92.3611% (532/576)\n",
            "9 13 Epoch: 364 | ANN: trainLoss: 0.2086 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 364 | ANN: trainLoss: 0.1997 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 364 | ANN: trainLoss: 0.2000 | trainAcc: 92.7083% (712/768)\n",
            "12 13 Epoch: 364 | ANN: trainLoss: 0.1914 | trainAcc: 92.7461% (716/772)\n",
            "0 4 Epoch: 364 | ANN: testLoss: 0.4935 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 364 | ANN: testLoss: 0.4673 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 364 | ANN: testLoss: 0.4706 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 364 | ANN: testLoss: 0.3992 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 365 | ANN: trainLoss: 0.1510 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 365 | ANN: trainLoss: 0.2379 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 365 | ANN: trainLoss: 0.2255 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 365 | ANN: trainLoss: 0.2050 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 365 | ANN: trainLoss: 0.2059 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 365 | ANN: trainLoss: 0.2000 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 365 | ANN: trainLoss: 0.2123 | trainAcc: 91.0714% (408/448)\n",
            "7 13 Epoch: 365 | ANN: trainLoss: 0.2082 | trainAcc: 91.4062% (468/512)\n",
            "8 13 Epoch: 365 | ANN: trainLoss: 0.2050 | trainAcc: 91.4931% (527/576)\n",
            "9 13 Epoch: 365 | ANN: trainLoss: 0.1993 | trainAcc: 91.8750% (588/640)\n",
            "10 13 Epoch: 365 | ANN: trainLoss: 0.2008 | trainAcc: 92.0455% (648/704)\n",
            "11 13 Epoch: 365 | ANN: trainLoss: 0.1970 | trainAcc: 92.3177% (709/768)\n",
            "12 13 Epoch: 365 | ANN: trainLoss: 0.1823 | trainAcc: 92.3575% (713/772)\n",
            "0 4 Epoch: 365 | ANN: testLoss: 0.5166 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 365 | ANN: testLoss: 0.4881 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 365 | ANN: testLoss: 0.4565 | testAcc: 79.6875% (153/192)\n",
            "3 4 Epoch: 365 | ANN: testLoss: 0.7533 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 366 | ANN: trainLoss: 0.2116 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 366 | ANN: trainLoss: 0.1916 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 366 | ANN: trainLoss: 0.1856 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 366 | ANN: trainLoss: 0.1694 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 366 | ANN: trainLoss: 0.2014 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 366 | ANN: trainLoss: 0.1940 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 366 | ANN: trainLoss: 0.1904 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 366 | ANN: trainLoss: 0.1969 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 366 | ANN: trainLoss: 0.1896 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 366 | ANN: trainLoss: 0.1892 | trainAcc: 93.7500% (600/640)\n",
            "10 13 Epoch: 366 | ANN: trainLoss: 0.1835 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 366 | ANN: trainLoss: 0.1773 | trainAcc: 94.2708% (724/768)\n",
            "12 13 Epoch: 366 | ANN: trainLoss: 0.2212 | trainAcc: 94.0415% (726/772)\n",
            "0 4 Epoch: 366 | ANN: testLoss: 0.3049 | testAcc: 87.5000% (56/64)\n",
            "1 4 Epoch: 366 | ANN: testLoss: 0.4346 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 366 | ANN: testLoss: 0.4651 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 366 | ANN: testLoss: 0.7194 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 367 | ANN: trainLoss: 0.2197 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 367 | ANN: trainLoss: 0.1862 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 367 | ANN: trainLoss: 0.1804 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 367 | ANN: trainLoss: 0.1774 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 367 | ANN: trainLoss: 0.1924 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 367 | ANN: trainLoss: 0.2166 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 367 | ANN: trainLoss: 0.2098 | trainAcc: 91.5179% (410/448)\n",
            "7 13 Epoch: 367 | ANN: trainLoss: 0.2011 | trainAcc: 91.9922% (471/512)\n",
            "8 13 Epoch: 367 | ANN: trainLoss: 0.2047 | trainAcc: 91.4931% (527/576)\n",
            "9 13 Epoch: 367 | ANN: trainLoss: 0.2002 | trainAcc: 91.5625% (586/640)\n",
            "10 13 Epoch: 367 | ANN: trainLoss: 0.2033 | trainAcc: 91.4773% (644/704)\n",
            "11 13 Epoch: 367 | ANN: trainLoss: 0.2008 | trainAcc: 91.6667% (704/768)\n",
            "12 13 Epoch: 367 | ANN: trainLoss: 0.2249 | trainAcc: 91.5803% (707/772)\n",
            "0 4 Epoch: 367 | ANN: testLoss: 0.3792 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 367 | ANN: testLoss: 0.4439 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 367 | ANN: testLoss: 0.4790 | testAcc: 80.7292% (155/192)\n",
            "3 4 Epoch: 367 | ANN: testLoss: 0.6560 | testAcc: 80.3109% (155/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 368 | ANN: trainLoss: 0.2043 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 368 | ANN: trainLoss: 0.1840 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 368 | ANN: trainLoss: 0.1603 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 368 | ANN: trainLoss: 0.1840 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 368 | ANN: trainLoss: 0.1885 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 368 | ANN: trainLoss: 0.1818 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 368 | ANN: trainLoss: 0.1738 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 368 | ANN: trainLoss: 0.1800 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 368 | ANN: trainLoss: 0.1775 | trainAcc: 92.3611% (532/576)\n",
            "9 13 Epoch: 368 | ANN: trainLoss: 0.1777 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 368 | ANN: trainLoss: 0.1733 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 368 | ANN: trainLoss: 0.1669 | trainAcc: 92.9688% (714/768)\n",
            "12 13 Epoch: 368 | ANN: trainLoss: 0.1626 | trainAcc: 93.0052% (718/772)\n",
            "0 4 Epoch: 368 | ANN: testLoss: 0.3065 | testAcc: 87.5000% (56/64)\n",
            "1 4 Epoch: 368 | ANN: testLoss: 0.4642 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 368 | ANN: testLoss: 0.4720 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 368 | ANN: testLoss: 0.3950 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 369 | ANN: trainLoss: 0.3702 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 369 | ANN: trainLoss: 0.2624 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 369 | ANN: trainLoss: 0.2123 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 369 | ANN: trainLoss: 0.2057 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 369 | ANN: trainLoss: 0.2006 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 369 | ANN: trainLoss: 0.2111 | trainAcc: 92.4479% (355/384)\n",
            "6 13 Epoch: 369 | ANN: trainLoss: 0.2223 | trainAcc: 92.4107% (414/448)\n",
            "7 13 Epoch: 369 | ANN: trainLoss: 0.2142 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 369 | ANN: trainLoss: 0.2036 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 369 | ANN: trainLoss: 0.2081 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 369 | ANN: trainLoss: 0.2021 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 369 | ANN: trainLoss: 0.1975 | trainAcc: 92.7083% (712/768)\n",
            "12 13 Epoch: 369 | ANN: trainLoss: 0.1867 | trainAcc: 92.7461% (716/772)\n",
            "0 4 Epoch: 369 | ANN: testLoss: 0.3565 | testAcc: 85.9375% (55/64)\n",
            "1 4 Epoch: 369 | ANN: testLoss: 0.3829 | testAcc: 82.8125% (106/128)\n",
            "2 4 Epoch: 369 | ANN: testLoss: 0.4637 | testAcc: 82.2917% (158/192)\n",
            "3 4 Epoch: 369 | ANN: testLoss: 0.4102 | testAcc: 82.3834% (159/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 370 | ANN: trainLoss: 0.1472 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 370 | ANN: trainLoss: 0.1530 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 370 | ANN: trainLoss: 0.1528 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 370 | ANN: trainLoss: 0.1341 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 370 | ANN: trainLoss: 0.1572 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 370 | ANN: trainLoss: 0.1597 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 370 | ANN: trainLoss: 0.1776 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 370 | ANN: trainLoss: 0.1755 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 370 | ANN: trainLoss: 0.1804 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 370 | ANN: trainLoss: 0.1813 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 370 | ANN: trainLoss: 0.1739 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 370 | ANN: trainLoss: 0.1718 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 370 | ANN: trainLoss: 0.1611 | trainAcc: 94.1710% (727/772)\n",
            "0 4 Epoch: 370 | ANN: testLoss: 0.3552 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 370 | ANN: testLoss: 0.4172 | testAcc: 82.0312% (105/128)\n",
            "2 4 Epoch: 370 | ANN: testLoss: 0.4527 | testAcc: 81.7708% (157/192)\n",
            "3 4 Epoch: 370 | ANN: testLoss: 0.7465 | testAcc: 81.3472% (157/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 371 | ANN: trainLoss: 0.2309 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 371 | ANN: trainLoss: 0.1902 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 371 | ANN: trainLoss: 0.1832 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 371 | ANN: trainLoss: 0.1840 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 371 | ANN: trainLoss: 0.1822 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 371 | ANN: trainLoss: 0.1827 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 371 | ANN: trainLoss: 0.1810 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 371 | ANN: trainLoss: 0.1931 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 371 | ANN: trainLoss: 0.1931 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 371 | ANN: trainLoss: 0.1894 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 371 | ANN: trainLoss: 0.1888 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 371 | ANN: trainLoss: 0.1896 | trainAcc: 93.3594% (717/768)\n",
            "12 13 Epoch: 371 | ANN: trainLoss: 0.1927 | trainAcc: 93.2642% (720/772)\n",
            "0 4 Epoch: 371 | ANN: testLoss: 0.3420 | testAcc: 89.0625% (57/64)\n",
            "1 4 Epoch: 371 | ANN: testLoss: 0.4315 | testAcc: 84.3750% (108/128)\n",
            "2 4 Epoch: 371 | ANN: testLoss: 0.4631 | testAcc: 81.2500% (156/192)\n",
            "3 4 Epoch: 371 | ANN: testLoss: 0.4540 | testAcc: 81.3472% (157/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 372 | ANN: trainLoss: 0.1385 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 372 | ANN: trainLoss: 0.1761 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 372 | ANN: trainLoss: 0.1783 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 372 | ANN: trainLoss: 0.1740 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 372 | ANN: trainLoss: 0.1710 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 372 | ANN: trainLoss: 0.1594 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 372 | ANN: trainLoss: 0.1684 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 372 | ANN: trainLoss: 0.1755 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 372 | ANN: trainLoss: 0.1834 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 372 | ANN: trainLoss: 0.1793 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 372 | ANN: trainLoss: 0.1781 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 372 | ANN: trainLoss: 0.1766 | trainAcc: 93.2292% (716/768)\n",
            "12 13 Epoch: 372 | ANN: trainLoss: 0.1739 | trainAcc: 93.2642% (720/772)\n",
            "0 4 Epoch: 372 | ANN: testLoss: 0.5740 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 372 | ANN: testLoss: 0.5295 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 372 | ANN: testLoss: 0.4827 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 372 | ANN: testLoss: 0.3621 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 373 | ANN: trainLoss: 0.3505 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 373 | ANN: trainLoss: 0.2896 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 373 | ANN: trainLoss: 0.2279 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 373 | ANN: trainLoss: 0.2119 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 373 | ANN: trainLoss: 0.2032 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 373 | ANN: trainLoss: 0.1973 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 373 | ANN: trainLoss: 0.2023 | trainAcc: 91.7411% (411/448)\n",
            "7 13 Epoch: 373 | ANN: trainLoss: 0.2010 | trainAcc: 91.9922% (471/512)\n",
            "8 13 Epoch: 373 | ANN: trainLoss: 0.1857 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 373 | ANN: trainLoss: 0.1811 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 373 | ANN: trainLoss: 0.1758 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 373 | ANN: trainLoss: 0.1712 | trainAcc: 93.4896% (718/768)\n",
            "12 13 Epoch: 373 | ANN: trainLoss: 0.1948 | trainAcc: 93.3938% (721/772)\n",
            "0 4 Epoch: 373 | ANN: testLoss: 0.5054 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 373 | ANN: testLoss: 0.4929 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 373 | ANN: testLoss: 0.4872 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 373 | ANN: testLoss: 0.3839 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 374 | ANN: trainLoss: 0.2419 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 374 | ANN: trainLoss: 0.2314 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 374 | ANN: trainLoss: 0.2117 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 374 | ANN: trainLoss: 0.2192 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 374 | ANN: trainLoss: 0.2168 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 374 | ANN: trainLoss: 0.2127 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 374 | ANN: trainLoss: 0.2074 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 374 | ANN: trainLoss: 0.2122 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 374 | ANN: trainLoss: 0.2025 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 374 | ANN: trainLoss: 0.2040 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 374 | ANN: trainLoss: 0.2087 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 374 | ANN: trainLoss: 0.2053 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 374 | ANN: trainLoss: 0.1933 | trainAcc: 92.8756% (717/772)\n",
            "0 4 Epoch: 374 | ANN: testLoss: 0.4816 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 374 | ANN: testLoss: 0.4865 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 374 | ANN: testLoss: 0.4886 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 374 | ANN: testLoss: 0.3671 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 375 | ANN: trainLoss: 0.1308 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 375 | ANN: trainLoss: 0.1697 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 375 | ANN: trainLoss: 0.1840 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 375 | ANN: trainLoss: 0.1850 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 375 | ANN: trainLoss: 0.1787 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 375 | ANN: trainLoss: 0.1705 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 375 | ANN: trainLoss: 0.1754 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 375 | ANN: trainLoss: 0.1735 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 375 | ANN: trainLoss: 0.1784 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 375 | ANN: trainLoss: 0.1725 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 375 | ANN: trainLoss: 0.1766 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 375 | ANN: trainLoss: 0.1704 | trainAcc: 94.4010% (725/768)\n",
            "12 13 Epoch: 375 | ANN: trainLoss: 0.1583 | trainAcc: 94.4301% (729/772)\n",
            "0 4 Epoch: 375 | ANN: testLoss: 0.5110 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 375 | ANN: testLoss: 0.5059 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 375 | ANN: testLoss: 0.4858 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 375 | ANN: testLoss: 0.4450 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 376 | ANN: trainLoss: 0.3208 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 376 | ANN: trainLoss: 0.2418 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 376 | ANN: trainLoss: 0.2105 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 376 | ANN: trainLoss: 0.1973 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 376 | ANN: trainLoss: 0.2170 | trainAcc: 92.5000% (296/320)\n",
            "5 13 Epoch: 376 | ANN: trainLoss: 0.2046 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 376 | ANN: trainLoss: 0.1973 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 376 | ANN: trainLoss: 0.1893 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 376 | ANN: trainLoss: 0.1929 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 376 | ANN: trainLoss: 0.1880 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 376 | ANN: trainLoss: 0.1845 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 376 | ANN: trainLoss: 0.1889 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 376 | ANN: trainLoss: 0.2300 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 376 | ANN: testLoss: 0.5582 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 376 | ANN: testLoss: 0.4870 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 376 | ANN: testLoss: 0.4972 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 376 | ANN: testLoss: 0.3729 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 377 | ANN: trainLoss: 0.2727 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 377 | ANN: trainLoss: 0.2213 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 377 | ANN: trainLoss: 0.1977 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 377 | ANN: trainLoss: 0.1862 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 377 | ANN: trainLoss: 0.1833 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 377 | ANN: trainLoss: 0.1841 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 377 | ANN: trainLoss: 0.1819 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 377 | ANN: trainLoss: 0.1908 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 377 | ANN: trainLoss: 0.1900 | trainAcc: 92.5347% (533/576)\n",
            "9 13 Epoch: 377 | ANN: trainLoss: 0.1828 | trainAcc: 92.8125% (594/640)\n",
            "10 13 Epoch: 377 | ANN: trainLoss: 0.1878 | trainAcc: 92.7557% (653/704)\n",
            "11 13 Epoch: 377 | ANN: trainLoss: 0.1786 | trainAcc: 93.2292% (716/768)\n",
            "12 13 Epoch: 377 | ANN: trainLoss: 0.1696 | trainAcc: 93.2642% (720/772)\n",
            "0 4 Epoch: 377 | ANN: testLoss: 0.5852 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 377 | ANN: testLoss: 0.5107 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 377 | ANN: testLoss: 0.4722 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 377 | ANN: testLoss: 0.3776 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 378 | ANN: trainLoss: 0.1483 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 378 | ANN: trainLoss: 0.1135 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 378 | ANN: trainLoss: 0.1207 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 378 | ANN: trainLoss: 0.1151 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 378 | ANN: trainLoss: 0.1384 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 378 | ANN: trainLoss: 0.1456 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 378 | ANN: trainLoss: 0.1576 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 378 | ANN: trainLoss: 0.1590 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 378 | ANN: trainLoss: 0.1626 | trainAcc: 94.0972% (542/576)\n",
            "9 13 Epoch: 378 | ANN: trainLoss: 0.1635 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 378 | ANN: trainLoss: 0.1729 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 378 | ANN: trainLoss: 0.1734 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 378 | ANN: trainLoss: 0.1686 | trainAcc: 93.6528% (723/772)\n",
            "0 4 Epoch: 378 | ANN: testLoss: 0.4678 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 378 | ANN: testLoss: 0.5171 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 378 | ANN: testLoss: 0.4630 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 378 | ANN: testLoss: 0.3474 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 379 | ANN: trainLoss: 0.2445 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 379 | ANN: trainLoss: 0.2214 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 379 | ANN: trainLoss: 0.1990 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 379 | ANN: trainLoss: 0.2198 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 379 | ANN: trainLoss: 0.2192 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 379 | ANN: trainLoss: 0.2079 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 379 | ANN: trainLoss: 0.1994 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 379 | ANN: trainLoss: 0.1976 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 379 | ANN: trainLoss: 0.1929 | trainAcc: 93.5764% (539/576)\n",
            "9 13 Epoch: 379 | ANN: trainLoss: 0.1923 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 379 | ANN: trainLoss: 0.1871 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 379 | ANN: trainLoss: 0.1811 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 379 | ANN: trainLoss: 0.2700 | trainAcc: 93.3938% (721/772)\n",
            "0 4 Epoch: 379 | ANN: testLoss: 0.3302 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 379 | ANN: testLoss: 0.5006 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 379 | ANN: testLoss: 0.4796 | testAcc: 79.6875% (153/192)\n",
            "3 4 Epoch: 379 | ANN: testLoss: 0.4096 | testAcc: 79.7927% (154/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 380 | ANN: trainLoss: 0.1543 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 380 | ANN: trainLoss: 0.1442 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 380 | ANN: trainLoss: 0.1473 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 380 | ANN: trainLoss: 0.1515 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 380 | ANN: trainLoss: 0.1425 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 380 | ANN: trainLoss: 0.1447 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 380 | ANN: trainLoss: 0.1458 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 380 | ANN: trainLoss: 0.1395 | trainAcc: 95.3125% (488/512)\n",
            "8 13 Epoch: 380 | ANN: trainLoss: 0.1442 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 380 | ANN: trainLoss: 0.1483 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 380 | ANN: trainLoss: 0.1533 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 380 | ANN: trainLoss: 0.1528 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 380 | ANN: trainLoss: 0.1550 | trainAcc: 94.8187% (732/772)\n",
            "0 4 Epoch: 380 | ANN: testLoss: 0.4815 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 380 | ANN: testLoss: 0.4489 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 380 | ANN: testLoss: 0.4958 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 380 | ANN: testLoss: 0.3784 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 381 | ANN: trainLoss: 0.0968 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 381 | ANN: trainLoss: 0.1042 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 381 | ANN: trainLoss: 0.0989 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 381 | ANN: trainLoss: 0.1134 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 381 | ANN: trainLoss: 0.1139 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 381 | ANN: trainLoss: 0.1264 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 381 | ANN: trainLoss: 0.1440 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 381 | ANN: trainLoss: 0.1498 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 381 | ANN: trainLoss: 0.1546 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 381 | ANN: trainLoss: 0.1560 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 381 | ANN: trainLoss: 0.1566 | trainAcc: 94.3182% (664/704)\n",
            "11 13 Epoch: 381 | ANN: trainLoss: 0.1622 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 381 | ANN: trainLoss: 0.2637 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 381 | ANN: testLoss: 0.4077 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 381 | ANN: testLoss: 0.5462 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 381 | ANN: testLoss: 0.5033 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 381 | ANN: testLoss: 0.6283 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 382 | ANN: trainLoss: 0.2701 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 382 | ANN: trainLoss: 0.1877 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 382 | ANN: trainLoss: 0.2085 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 382 | ANN: trainLoss: 0.1831 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 382 | ANN: trainLoss: 0.1723 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 382 | ANN: trainLoss: 0.1577 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 382 | ANN: trainLoss: 0.1636 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 382 | ANN: trainLoss: 0.1547 | trainAcc: 95.3125% (488/512)\n",
            "8 13 Epoch: 382 | ANN: trainLoss: 0.1662 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 382 | ANN: trainLoss: 0.1695 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 382 | ANN: trainLoss: 0.1671 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 382 | ANN: trainLoss: 0.1681 | trainAcc: 94.9219% (729/768)\n",
            "12 13 Epoch: 382 | ANN: trainLoss: 0.1760 | trainAcc: 94.9482% (733/772)\n",
            "0 4 Epoch: 382 | ANN: testLoss: 0.3336 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 382 | ANN: testLoss: 0.5562 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 382 | ANN: testLoss: 0.5263 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 382 | ANN: testLoss: 0.4592 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 383 | ANN: trainLoss: 0.2369 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 383 | ANN: trainLoss: 0.2194 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 383 | ANN: trainLoss: 0.2089 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 383 | ANN: trainLoss: 0.2090 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 383 | ANN: trainLoss: 0.2161 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 383 | ANN: trainLoss: 0.2130 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 383 | ANN: trainLoss: 0.2054 | trainAcc: 91.5179% (410/448)\n",
            "7 13 Epoch: 383 | ANN: trainLoss: 0.2013 | trainAcc: 92.1875% (472/512)\n",
            "8 13 Epoch: 383 | ANN: trainLoss: 0.1910 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 383 | ANN: trainLoss: 0.1876 | trainAcc: 92.8125% (594/640)\n",
            "10 13 Epoch: 383 | ANN: trainLoss: 0.1867 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 383 | ANN: trainLoss: 0.1864 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 383 | ANN: trainLoss: 0.1780 | trainAcc: 92.8756% (717/772)\n",
            "0 4 Epoch: 383 | ANN: testLoss: 0.5479 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 383 | ANN: testLoss: 0.4826 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 383 | ANN: testLoss: 0.5093 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 383 | ANN: testLoss: 0.3902 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 384 | ANN: trainLoss: 0.2577 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 384 | ANN: trainLoss: 0.1924 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 384 | ANN: trainLoss: 0.1901 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 384 | ANN: trainLoss: 0.2218 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 384 | ANN: trainLoss: 0.2118 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 384 | ANN: trainLoss: 0.1899 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 384 | ANN: trainLoss: 0.1884 | trainAcc: 92.1875% (413/448)\n",
            "7 13 Epoch: 384 | ANN: trainLoss: 0.1963 | trainAcc: 91.9922% (471/512)\n",
            "8 13 Epoch: 384 | ANN: trainLoss: 0.1935 | trainAcc: 92.3611% (532/576)\n",
            "9 13 Epoch: 384 | ANN: trainLoss: 0.1810 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 384 | ANN: trainLoss: 0.1744 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 384 | ANN: trainLoss: 0.1693 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 384 | ANN: trainLoss: 0.1769 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 384 | ANN: testLoss: 0.6556 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 384 | ANN: testLoss: 0.5770 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 384 | ANN: testLoss: 0.5172 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 384 | ANN: testLoss: 0.4420 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 385 | ANN: trainLoss: 0.1260 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 385 | ANN: trainLoss: 0.1235 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 385 | ANN: trainLoss: 0.1304 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 385 | ANN: trainLoss: 0.1314 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 385 | ANN: trainLoss: 0.1338 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 385 | ANN: trainLoss: 0.1446 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 385 | ANN: trainLoss: 0.1459 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 385 | ANN: trainLoss: 0.1575 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 385 | ANN: trainLoss: 0.1687 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 385 | ANN: trainLoss: 0.1745 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 385 | ANN: trainLoss: 0.1692 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 385 | ANN: trainLoss: 0.1717 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 385 | ANN: trainLoss: 0.1632 | trainAcc: 93.6528% (723/772)\n",
            "0 4 Epoch: 385 | ANN: testLoss: 0.5378 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 385 | ANN: testLoss: 0.5147 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 385 | ANN: testLoss: 0.4904 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 385 | ANN: testLoss: 0.7186 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 386 | ANN: trainLoss: 0.1138 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 386 | ANN: trainLoss: 0.2172 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 386 | ANN: trainLoss: 0.1908 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 386 | ANN: trainLoss: 0.1863 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 386 | ANN: trainLoss: 0.1772 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 386 | ANN: trainLoss: 0.1716 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 386 | ANN: trainLoss: 0.1712 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 386 | ANN: trainLoss: 0.1650 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 386 | ANN: trainLoss: 0.1722 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 386 | ANN: trainLoss: 0.1729 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 386 | ANN: trainLoss: 0.1660 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 386 | ANN: trainLoss: 0.1660 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 386 | ANN: trainLoss: 0.2905 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 386 | ANN: testLoss: 0.4630 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 386 | ANN: testLoss: 0.4307 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 386 | ANN: testLoss: 0.5014 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 386 | ANN: testLoss: 0.3762 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 387 | ANN: trainLoss: 0.1711 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 387 | ANN: trainLoss: 0.1404 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 387 | ANN: trainLoss: 0.1882 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 387 | ANN: trainLoss: 0.1775 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 387 | ANN: trainLoss: 0.1730 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 387 | ANN: trainLoss: 0.1631 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 387 | ANN: trainLoss: 0.1697 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 387 | ANN: trainLoss: 0.1764 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 387 | ANN: trainLoss: 0.1854 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 387 | ANN: trainLoss: 0.1798 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 387 | ANN: trainLoss: 0.1737 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 387 | ANN: trainLoss: 0.1686 | trainAcc: 94.2708% (724/768)\n",
            "12 13 Epoch: 387 | ANN: trainLoss: 0.1612 | trainAcc: 94.3005% (728/772)\n",
            "0 4 Epoch: 387 | ANN: testLoss: 0.5399 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 387 | ANN: testLoss: 0.4551 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 387 | ANN: testLoss: 0.4960 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 387 | ANN: testLoss: 0.3721 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 388 | ANN: trainLoss: 0.1593 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 388 | ANN: trainLoss: 0.1615 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 388 | ANN: trainLoss: 0.1891 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 388 | ANN: trainLoss: 0.1723 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 388 | ANN: trainLoss: 0.1921 | trainAcc: 92.5000% (296/320)\n",
            "5 13 Epoch: 388 | ANN: trainLoss: 0.2020 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 388 | ANN: trainLoss: 0.2045 | trainAcc: 92.4107% (414/448)\n",
            "7 13 Epoch: 388 | ANN: trainLoss: 0.1987 | trainAcc: 92.1875% (472/512)\n",
            "8 13 Epoch: 388 | ANN: trainLoss: 0.1929 | trainAcc: 92.3611% (532/576)\n",
            "9 13 Epoch: 388 | ANN: trainLoss: 0.1951 | trainAcc: 92.0312% (589/640)\n",
            "10 13 Epoch: 388 | ANN: trainLoss: 0.1946 | trainAcc: 91.9034% (647/704)\n",
            "11 13 Epoch: 388 | ANN: trainLoss: 0.2037 | trainAcc: 91.5365% (703/768)\n",
            "12 13 Epoch: 388 | ANN: trainLoss: 0.2159 | trainAcc: 91.4508% (706/772)\n",
            "0 4 Epoch: 388 | ANN: testLoss: 0.5361 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 388 | ANN: testLoss: 0.5158 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 388 | ANN: testLoss: 0.5009 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 388 | ANN: testLoss: 0.3919 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 389 | ANN: trainLoss: 0.3594 | trainAcc: 84.3750% (54/64)\n",
            "1 13 Epoch: 389 | ANN: trainLoss: 0.2586 | trainAcc: 88.2812% (113/128)\n",
            "2 13 Epoch: 389 | ANN: trainLoss: 0.2164 | trainAcc: 89.0625% (171/192)\n",
            "3 13 Epoch: 389 | ANN: trainLoss: 0.1995 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 389 | ANN: trainLoss: 0.2053 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 389 | ANN: trainLoss: 0.1903 | trainAcc: 91.4062% (351/384)\n",
            "6 13 Epoch: 389 | ANN: trainLoss: 0.1883 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 389 | ANN: trainLoss: 0.1903 | trainAcc: 91.7969% (470/512)\n",
            "8 13 Epoch: 389 | ANN: trainLoss: 0.1958 | trainAcc: 91.4931% (527/576)\n",
            "9 13 Epoch: 389 | ANN: trainLoss: 0.1921 | trainAcc: 91.4062% (585/640)\n",
            "10 13 Epoch: 389 | ANN: trainLoss: 0.1996 | trainAcc: 91.1932% (642/704)\n",
            "11 13 Epoch: 389 | ANN: trainLoss: 0.1945 | trainAcc: 91.4062% (702/768)\n",
            "12 13 Epoch: 389 | ANN: trainLoss: 0.1865 | trainAcc: 91.4508% (706/772)\n",
            "0 4 Epoch: 389 | ANN: testLoss: 0.4115 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 389 | ANN: testLoss: 0.4758 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 389 | ANN: testLoss: 0.5064 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 389 | ANN: testLoss: 0.4071 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 390 | ANN: trainLoss: 0.1584 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 390 | ANN: trainLoss: 0.2123 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 390 | ANN: trainLoss: 0.2193 | trainAcc: 90.6250% (174/192)\n",
            "3 13 Epoch: 390 | ANN: trainLoss: 0.1849 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 390 | ANN: trainLoss: 0.1637 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 390 | ANN: trainLoss: 0.1627 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 390 | ANN: trainLoss: 0.1638 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 390 | ANN: trainLoss: 0.1569 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 390 | ANN: trainLoss: 0.1561 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 390 | ANN: trainLoss: 0.1534 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 390 | ANN: trainLoss: 0.1549 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 390 | ANN: trainLoss: 0.1502 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 390 | ANN: trainLoss: 0.4228 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 390 | ANN: testLoss: 0.6392 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 390 | ANN: testLoss: 0.5663 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 390 | ANN: testLoss: 0.5101 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 390 | ANN: testLoss: 0.3883 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 391 | ANN: trainLoss: 0.0973 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 391 | ANN: trainLoss: 0.1509 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 391 | ANN: trainLoss: 0.1512 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 391 | ANN: trainLoss: 0.1426 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 391 | ANN: trainLoss: 0.1477 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 391 | ANN: trainLoss: 0.1475 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 391 | ANN: trainLoss: 0.1450 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 391 | ANN: trainLoss: 0.1438 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 391 | ANN: trainLoss: 0.1518 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 391 | ANN: trainLoss: 0.1517 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 391 | ANN: trainLoss: 0.1547 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 391 | ANN: trainLoss: 0.1585 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 391 | ANN: trainLoss: 0.1733 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 391 | ANN: testLoss: 0.6392 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 391 | ANN: testLoss: 0.5253 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 391 | ANN: testLoss: 0.4762 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 391 | ANN: testLoss: 1.2087 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 392 | ANN: trainLoss: 0.1597 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 392 | ANN: trainLoss: 0.1421 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 392 | ANN: trainLoss: 0.1535 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 392 | ANN: trainLoss: 0.1422 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 392 | ANN: trainLoss: 0.1431 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 392 | ANN: trainLoss: 0.1481 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 392 | ANN: trainLoss: 0.1640 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 392 | ANN: trainLoss: 0.1720 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 392 | ANN: trainLoss: 0.1811 | trainAcc: 92.5347% (533/576)\n",
            "9 13 Epoch: 392 | ANN: trainLoss: 0.1733 | trainAcc: 92.8125% (594/640)\n",
            "10 13 Epoch: 392 | ANN: trainLoss: 0.1645 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 392 | ANN: trainLoss: 0.1667 | trainAcc: 93.4896% (718/768)\n",
            "12 13 Epoch: 392 | ANN: trainLoss: 0.2783 | trainAcc: 93.2642% (720/772)\n",
            "0 4 Epoch: 392 | ANN: testLoss: 0.4708 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 392 | ANN: testLoss: 0.4581 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 392 | ANN: testLoss: 0.5012 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 392 | ANN: testLoss: 0.3950 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 393 | ANN: trainLoss: 0.2661 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 393 | ANN: trainLoss: 0.2302 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 393 | ANN: trainLoss: 0.2158 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 393 | ANN: trainLoss: 0.1979 | trainAcc: 91.4062% (234/256)\n",
            "4 13 Epoch: 393 | ANN: trainLoss: 0.1985 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 393 | ANN: trainLoss: 0.1895 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 393 | ANN: trainLoss: 0.1864 | trainAcc: 91.5179% (410/448)\n",
            "7 13 Epoch: 393 | ANN: trainLoss: 0.1754 | trainAcc: 92.1875% (472/512)\n",
            "8 13 Epoch: 393 | ANN: trainLoss: 0.1822 | trainAcc: 92.1875% (531/576)\n",
            "9 13 Epoch: 393 | ANN: trainLoss: 0.1733 | trainAcc: 92.8125% (594/640)\n",
            "10 13 Epoch: 393 | ANN: trainLoss: 0.1718 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 393 | ANN: trainLoss: 0.1670 | trainAcc: 93.3594% (717/768)\n",
            "12 13 Epoch: 393 | ANN: trainLoss: 0.1600 | trainAcc: 93.3938% (721/772)\n",
            "0 4 Epoch: 393 | ANN: testLoss: 0.5715 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 393 | ANN: testLoss: 0.4650 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 393 | ANN: testLoss: 0.5257 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 393 | ANN: testLoss: 0.5234 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 394 | ANN: trainLoss: 0.1246 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 394 | ANN: trainLoss: 0.1734 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 394 | ANN: trainLoss: 0.1720 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 394 | ANN: trainLoss: 0.1722 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 394 | ANN: trainLoss: 0.1717 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 394 | ANN: trainLoss: 0.1804 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 394 | ANN: trainLoss: 0.1710 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 394 | ANN: trainLoss: 0.1677 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 394 | ANN: trainLoss: 0.1667 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 394 | ANN: trainLoss: 0.1628 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 394 | ANN: trainLoss: 0.1616 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 394 | ANN: trainLoss: 0.1761 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 394 | ANN: trainLoss: 0.1880 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 394 | ANN: testLoss: 0.5661 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 394 | ANN: testLoss: 0.5550 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 394 | ANN: testLoss: 0.5221 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 394 | ANN: testLoss: 0.4334 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 395 | ANN: trainLoss: 0.1642 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 395 | ANN: trainLoss: 0.1580 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 395 | ANN: trainLoss: 0.1683 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 395 | ANN: trainLoss: 0.1838 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 395 | ANN: trainLoss: 0.1763 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 395 | ANN: trainLoss: 0.1722 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 395 | ANN: trainLoss: 0.1800 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 395 | ANN: trainLoss: 0.1747 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 395 | ANN: trainLoss: 0.1723 | trainAcc: 94.0972% (542/576)\n",
            "9 13 Epoch: 395 | ANN: trainLoss: 0.1712 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 395 | ANN: trainLoss: 0.1780 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 395 | ANN: trainLoss: 0.1785 | trainAcc: 93.4896% (718/768)\n",
            "12 13 Epoch: 395 | ANN: trainLoss: 0.1686 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 395 | ANN: testLoss: 0.5992 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 395 | ANN: testLoss: 0.5141 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 395 | ANN: testLoss: 0.5062 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 395 | ANN: testLoss: 0.5945 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 396 | ANN: trainLoss: 0.1715 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 396 | ANN: trainLoss: 0.1561 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 396 | ANN: trainLoss: 0.1546 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 396 | ANN: trainLoss: 0.1677 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 396 | ANN: trainLoss: 0.1932 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 396 | ANN: trainLoss: 0.1945 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 396 | ANN: trainLoss: 0.1814 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 396 | ANN: trainLoss: 0.1734 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 396 | ANN: trainLoss: 0.1794 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 396 | ANN: trainLoss: 0.1746 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 396 | ANN: trainLoss: 0.1793 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 396 | ANN: trainLoss: 0.1831 | trainAcc: 93.2292% (716/768)\n",
            "12 13 Epoch: 396 | ANN: trainLoss: 0.1968 | trainAcc: 93.1347% (719/772)\n",
            "0 4 Epoch: 396 | ANN: testLoss: 0.6327 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 396 | ANN: testLoss: 0.5492 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 396 | ANN: testLoss: 0.5128 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 396 | ANN: testLoss: 0.3851 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 397 | ANN: trainLoss: 0.2022 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 397 | ANN: trainLoss: 0.1862 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 397 | ANN: trainLoss: 0.1794 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 397 | ANN: trainLoss: 0.1930 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 397 | ANN: trainLoss: 0.1948 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 397 | ANN: trainLoss: 0.1920 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 397 | ANN: trainLoss: 0.1905 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 397 | ANN: trainLoss: 0.1844 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 397 | ANN: trainLoss: 0.1765 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 397 | ANN: trainLoss: 0.1730 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 397 | ANN: trainLoss: 0.1666 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 397 | ANN: trainLoss: 0.1635 | trainAcc: 94.2708% (724/768)\n",
            "12 13 Epoch: 397 | ANN: trainLoss: 0.1684 | trainAcc: 94.1710% (727/772)\n",
            "0 4 Epoch: 397 | ANN: testLoss: 0.5330 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 397 | ANN: testLoss: 0.5048 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 397 | ANN: testLoss: 0.5132 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 397 | ANN: testLoss: 0.5181 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 398 | ANN: trainLoss: 0.1661 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 398 | ANN: trainLoss: 0.1623 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 398 | ANN: trainLoss: 0.1552 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 398 | ANN: trainLoss: 0.1752 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 398 | ANN: trainLoss: 0.1768 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 398 | ANN: trainLoss: 0.1854 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 398 | ANN: trainLoss: 0.1760 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 398 | ANN: trainLoss: 0.1804 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 398 | ANN: trainLoss: 0.1770 | trainAcc: 93.5764% (539/576)\n",
            "9 13 Epoch: 398 | ANN: trainLoss: 0.1852 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 398 | ANN: trainLoss: 0.1878 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 398 | ANN: trainLoss: 0.1868 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 398 | ANN: trainLoss: 0.2248 | trainAcc: 92.6166% (715/772)\n",
            "0 4 Epoch: 398 | ANN: testLoss: 0.4054 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 398 | ANN: testLoss: 0.5211 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 398 | ANN: testLoss: 0.5223 | testAcc: 74.4792% (143/192)\n",
            "3 4 Epoch: 398 | ANN: testLoss: 0.3927 | testAcc: 74.6114% (144/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 399 | ANN: trainLoss: 0.1934 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 399 | ANN: trainLoss: 0.1841 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 399 | ANN: trainLoss: 0.2327 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 399 | ANN: trainLoss: 0.2344 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 399 | ANN: trainLoss: 0.2499 | trainAcc: 90.9375% (291/320)\n",
            "5 13 Epoch: 399 | ANN: trainLoss: 0.2482 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 399 | ANN: trainLoss: 0.2318 | trainAcc: 92.1875% (413/448)\n",
            "7 13 Epoch: 399 | ANN: trainLoss: 0.2198 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 399 | ANN: trainLoss: 0.2211 | trainAcc: 92.3611% (532/576)\n",
            "9 13 Epoch: 399 | ANN: trainLoss: 0.2114 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 399 | ANN: trainLoss: 0.2048 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 399 | ANN: trainLoss: 0.2059 | trainAcc: 92.4479% (710/768)\n",
            "12 13 Epoch: 399 | ANN: trainLoss: 0.2051 | trainAcc: 92.4870% (714/772)\n",
            "0 4 Epoch: 399 | ANN: testLoss: 0.4531 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 399 | ANN: testLoss: 0.4786 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 399 | ANN: testLoss: 0.5335 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 399 | ANN: testLoss: 0.4072 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 400 | ANN: trainLoss: 0.1523 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 400 | ANN: trainLoss: 0.1634 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 400 | ANN: trainLoss: 0.1443 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 400 | ANN: trainLoss: 0.1521 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 400 | ANN: trainLoss: 0.1470 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 400 | ANN: trainLoss: 0.1530 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 400 | ANN: trainLoss: 0.1518 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 400 | ANN: trainLoss: 0.1393 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 400 | ANN: trainLoss: 0.1436 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 400 | ANN: trainLoss: 0.1432 | trainAcc: 94.3750% (604/640)\n",
            "10 13 Epoch: 400 | ANN: trainLoss: 0.1594 | trainAcc: 93.6080% (659/704)\n",
            "11 13 Epoch: 400 | ANN: trainLoss: 0.1603 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 400 | ANN: trainLoss: 0.1597 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 400 | ANN: testLoss: 0.4481 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 400 | ANN: testLoss: 0.5208 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 400 | ANN: testLoss: 0.5200 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 400 | ANN: testLoss: 0.3925 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 401 | ANN: trainLoss: 0.3158 | trainAcc: 85.9375% (55/64)\n",
            "1 13 Epoch: 401 | ANN: trainLoss: 0.2428 | trainAcc: 89.0625% (114/128)\n",
            "2 13 Epoch: 401 | ANN: trainLoss: 0.2461 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 401 | ANN: trainLoss: 0.2150 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 401 | ANN: trainLoss: 0.2098 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 401 | ANN: trainLoss: 0.2127 | trainAcc: 91.6667% (352/384)\n",
            "6 13 Epoch: 401 | ANN: trainLoss: 0.2159 | trainAcc: 91.5179% (410/448)\n",
            "7 13 Epoch: 401 | ANN: trainLoss: 0.2157 | trainAcc: 91.4062% (468/512)\n",
            "8 13 Epoch: 401 | ANN: trainLoss: 0.2085 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 401 | ANN: trainLoss: 0.1978 | trainAcc: 92.0312% (589/640)\n",
            "10 13 Epoch: 401 | ANN: trainLoss: 0.1988 | trainAcc: 91.9034% (647/704)\n",
            "11 13 Epoch: 401 | ANN: trainLoss: 0.1973 | trainAcc: 92.0573% (707/768)\n",
            "12 13 Epoch: 401 | ANN: trainLoss: 0.2014 | trainAcc: 91.9689% (710/772)\n",
            "0 4 Epoch: 401 | ANN: testLoss: 0.3766 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 401 | ANN: testLoss: 0.4600 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 401 | ANN: testLoss: 0.4987 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 401 | ANN: testLoss: 0.3741 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 402 | ANN: trainLoss: 0.1217 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 402 | ANN: trainLoss: 0.1614 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 402 | ANN: trainLoss: 0.1546 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 402 | ANN: trainLoss: 0.1496 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 402 | ANN: trainLoss: 0.1447 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 402 | ANN: trainLoss: 0.1398 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 402 | ANN: trainLoss: 0.1522 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 402 | ANN: trainLoss: 0.1642 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 402 | ANN: trainLoss: 0.1626 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 402 | ANN: trainLoss: 0.1625 | trainAcc: 93.7500% (600/640)\n",
            "10 13 Epoch: 402 | ANN: trainLoss: 0.1594 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 402 | ANN: trainLoss: 0.1609 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 402 | ANN: trainLoss: 0.1511 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 402 | ANN: testLoss: 0.5048 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 402 | ANN: testLoss: 0.4380 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 402 | ANN: testLoss: 0.4847 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 402 | ANN: testLoss: 0.4057 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 403 | ANN: trainLoss: 0.2604 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 403 | ANN: trainLoss: 0.1970 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 403 | ANN: trainLoss: 0.1742 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 403 | ANN: trainLoss: 0.1733 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 403 | ANN: trainLoss: 0.1648 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 403 | ANN: trainLoss: 0.1652 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 403 | ANN: trainLoss: 0.1684 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 403 | ANN: trainLoss: 0.1751 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 403 | ANN: trainLoss: 0.1865 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 403 | ANN: trainLoss: 0.1810 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 403 | ANN: trainLoss: 0.1695 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 403 | ANN: trainLoss: 0.1670 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 403 | ANN: trainLoss: 0.2410 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 403 | ANN: testLoss: 0.4798 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 403 | ANN: testLoss: 0.4138 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 403 | ANN: testLoss: 0.4827 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 403 | ANN: testLoss: 0.3986 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 404 | ANN: trainLoss: 0.1044 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 404 | ANN: trainLoss: 0.1832 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 404 | ANN: trainLoss: 0.2043 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 404 | ANN: trainLoss: 0.2035 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 404 | ANN: trainLoss: 0.1974 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 404 | ANN: trainLoss: 0.1738 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 404 | ANN: trainLoss: 0.1704 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 404 | ANN: trainLoss: 0.1637 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 404 | ANN: trainLoss: 0.1733 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 404 | ANN: trainLoss: 0.1705 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 404 | ANN: trainLoss: 0.1737 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 404 | ANN: trainLoss: 0.1715 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 404 | ANN: trainLoss: 0.1828 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 404 | ANN: testLoss: 0.4455 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 404 | ANN: testLoss: 0.4556 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 404 | ANN: testLoss: 0.4734 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 404 | ANN: testLoss: 1.0014 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 405 | ANN: trainLoss: 0.1514 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 405 | ANN: trainLoss: 0.1897 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 405 | ANN: trainLoss: 0.1646 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 405 | ANN: trainLoss: 0.1847 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 405 | ANN: trainLoss: 0.1696 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 405 | ANN: trainLoss: 0.1571 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 405 | ANN: trainLoss: 0.1489 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 405 | ANN: trainLoss: 0.1616 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 405 | ANN: trainLoss: 0.1601 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 405 | ANN: trainLoss: 0.1623 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 405 | ANN: trainLoss: 0.1684 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 405 | ANN: trainLoss: 0.1667 | trainAcc: 92.9688% (714/768)\n",
            "12 13 Epoch: 405 | ANN: trainLoss: 0.1587 | trainAcc: 93.0052% (718/772)\n",
            "0 4 Epoch: 405 | ANN: testLoss: 0.5283 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 405 | ANN: testLoss: 0.5272 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 405 | ANN: testLoss: 0.4810 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 405 | ANN: testLoss: 0.3938 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 406 | ANN: trainLoss: 0.1577 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 406 | ANN: trainLoss: 0.1782 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 406 | ANN: trainLoss: 0.1778 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 406 | ANN: trainLoss: 0.1725 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 406 | ANN: trainLoss: 0.1773 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 406 | ANN: trainLoss: 0.1766 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 406 | ANN: trainLoss: 0.1750 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 406 | ANN: trainLoss: 0.1839 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 406 | ANN: trainLoss: 0.1739 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 406 | ANN: trainLoss: 0.1666 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 406 | ANN: trainLoss: 0.1641 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 406 | ANN: trainLoss: 0.1618 | trainAcc: 93.4896% (718/768)\n",
            "12 13 Epoch: 406 | ANN: trainLoss: 0.1540 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 406 | ANN: testLoss: 0.5546 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 406 | ANN: testLoss: 0.5262 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 406 | ANN: testLoss: 0.4833 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 406 | ANN: testLoss: 0.4243 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 407 | ANN: trainLoss: 0.1242 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 407 | ANN: trainLoss: 0.1189 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 407 | ANN: trainLoss: 0.1516 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 407 | ANN: trainLoss: 0.1595 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 407 | ANN: trainLoss: 0.1543 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 407 | ANN: trainLoss: 0.1468 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 407 | ANN: trainLoss: 0.1523 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 407 | ANN: trainLoss: 0.1480 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 407 | ANN: trainLoss: 0.1430 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 407 | ANN: trainLoss: 0.1417 | trainAcc: 95.0000% (608/640)\n",
            "10 13 Epoch: 407 | ANN: trainLoss: 0.1574 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 407 | ANN: trainLoss: 0.1515 | trainAcc: 94.4010% (725/768)\n",
            "12 13 Epoch: 407 | ANN: trainLoss: 0.1904 | trainAcc: 94.1710% (727/772)\n",
            "0 4 Epoch: 407 | ANN: testLoss: 0.4402 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 407 | ANN: testLoss: 0.4075 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 407 | ANN: testLoss: 0.4807 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 407 | ANN: testLoss: 0.3673 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 408 | ANN: trainLoss: 0.3276 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 408 | ANN: trainLoss: 0.2187 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 408 | ANN: trainLoss: 0.1942 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 408 | ANN: trainLoss: 0.1785 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 408 | ANN: trainLoss: 0.1692 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 408 | ANN: trainLoss: 0.1715 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 408 | ANN: trainLoss: 0.1749 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 408 | ANN: trainLoss: 0.1785 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 408 | ANN: trainLoss: 0.1794 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 408 | ANN: trainLoss: 0.1724 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 408 | ANN: trainLoss: 0.1759 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 408 | ANN: trainLoss: 0.1710 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 408 | ANN: trainLoss: 0.1783 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 408 | ANN: testLoss: 0.4357 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 408 | ANN: testLoss: 0.4870 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 408 | ANN: testLoss: 0.4794 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 408 | ANN: testLoss: 0.4288 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 409 | ANN: trainLoss: 0.2831 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 409 | ANN: trainLoss: 0.2014 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 409 | ANN: trainLoss: 0.1894 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 409 | ANN: trainLoss: 0.1885 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 409 | ANN: trainLoss: 0.1756 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 409 | ANN: trainLoss: 0.1734 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 409 | ANN: trainLoss: 0.1599 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 409 | ANN: trainLoss: 0.1672 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 409 | ANN: trainLoss: 0.1794 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 409 | ANN: trainLoss: 0.1745 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 409 | ANN: trainLoss: 0.1719 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 409 | ANN: trainLoss: 0.1741 | trainAcc: 94.2708% (724/768)\n",
            "12 13 Epoch: 409 | ANN: trainLoss: 0.1883 | trainAcc: 94.1710% (727/772)\n",
            "0 4 Epoch: 409 | ANN: testLoss: 0.5303 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 409 | ANN: testLoss: 0.5022 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 409 | ANN: testLoss: 0.4636 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 409 | ANN: testLoss: 1.5162 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 410 | ANN: trainLoss: 0.1098 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 410 | ANN: trainLoss: 0.1229 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 410 | ANN: trainLoss: 0.1654 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 410 | ANN: trainLoss: 0.1633 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 410 | ANN: trainLoss: 0.1511 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 410 | ANN: trainLoss: 0.1553 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 410 | ANN: trainLoss: 0.1577 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 410 | ANN: trainLoss: 0.1596 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 410 | ANN: trainLoss: 0.1546 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 410 | ANN: trainLoss: 0.1564 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 410 | ANN: trainLoss: 0.1580 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 410 | ANN: trainLoss: 0.1564 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 410 | ANN: trainLoss: 0.1560 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 410 | ANN: testLoss: 0.4375 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 410 | ANN: testLoss: 0.5085 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 410 | ANN: testLoss: 0.4890 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 410 | ANN: testLoss: 0.3826 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 411 | ANN: trainLoss: 0.1144 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 411 | ANN: trainLoss: 0.1232 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 411 | ANN: trainLoss: 0.1303 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 411 | ANN: trainLoss: 0.1585 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 411 | ANN: trainLoss: 0.1627 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 411 | ANN: trainLoss: 0.1557 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 411 | ANN: trainLoss: 0.1569 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 411 | ANN: trainLoss: 0.1616 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 411 | ANN: trainLoss: 0.1670 | trainAcc: 94.0972% (542/576)\n",
            "9 13 Epoch: 411 | ANN: trainLoss: 0.1594 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 411 | ANN: trainLoss: 0.1659 | trainAcc: 94.3182% (664/704)\n",
            "11 13 Epoch: 411 | ANN: trainLoss: 0.1679 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 411 | ANN: trainLoss: 0.1827 | trainAcc: 94.0415% (726/772)\n",
            "0 4 Epoch: 411 | ANN: testLoss: 0.6909 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 411 | ANN: testLoss: 0.5546 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 411 | ANN: testLoss: 0.4811 | testAcc: 79.6875% (153/192)\n",
            "3 4 Epoch: 411 | ANN: testLoss: 0.7511 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 412 | ANN: trainLoss: 0.2021 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 412 | ANN: trainLoss: 0.1446 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 412 | ANN: trainLoss: 0.1377 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 412 | ANN: trainLoss: 0.1448 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 412 | ANN: trainLoss: 0.1381 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 412 | ANN: trainLoss: 0.1446 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 412 | ANN: trainLoss: 0.1532 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 412 | ANN: trainLoss: 0.1532 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 412 | ANN: trainLoss: 0.1534 | trainAcc: 93.5764% (539/576)\n",
            "9 13 Epoch: 412 | ANN: trainLoss: 0.1473 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 412 | ANN: trainLoss: 0.1474 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 412 | ANN: trainLoss: 0.1464 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 412 | ANN: trainLoss: 0.1674 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 412 | ANN: testLoss: 0.5056 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 412 | ANN: testLoss: 0.4652 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 412 | ANN: testLoss: 0.5040 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 412 | ANN: testLoss: 0.3787 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 413 | ANN: trainLoss: 0.2196 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 413 | ANN: trainLoss: 0.1792 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 413 | ANN: trainLoss: 0.1674 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 413 | ANN: trainLoss: 0.1546 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 413 | ANN: trainLoss: 0.1628 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 413 | ANN: trainLoss: 0.1556 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 413 | ANN: trainLoss: 0.1468 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 413 | ANN: trainLoss: 0.1487 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 413 | ANN: trainLoss: 0.1490 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 413 | ANN: trainLoss: 0.1480 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 413 | ANN: trainLoss: 0.1521 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 413 | ANN: trainLoss: 0.1504 | trainAcc: 94.2708% (724/768)\n",
            "12 13 Epoch: 413 | ANN: trainLoss: 0.1426 | trainAcc: 94.3005% (728/772)\n",
            "0 4 Epoch: 413 | ANN: testLoss: 0.4403 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 413 | ANN: testLoss: 0.4705 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 413 | ANN: testLoss: 0.4818 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 413 | ANN: testLoss: 0.5763 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 414 | ANN: trainLoss: 0.1942 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 414 | ANN: trainLoss: 0.2028 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 414 | ANN: trainLoss: 0.1971 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 414 | ANN: trainLoss: 0.2014 | trainAcc: 92.1875% (236/256)\n",
            "4 13 Epoch: 414 | ANN: trainLoss: 0.1885 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 414 | ANN: trainLoss: 0.1897 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 414 | ANN: trainLoss: 0.1741 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 414 | ANN: trainLoss: 0.1745 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 414 | ANN: trainLoss: 0.1729 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 414 | ANN: trainLoss: 0.1720 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 414 | ANN: trainLoss: 0.1706 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 414 | ANN: trainLoss: 0.1733 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 414 | ANN: trainLoss: 0.1732 | trainAcc: 92.8756% (717/772)\n",
            "0 4 Epoch: 414 | ANN: testLoss: 0.4518 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 414 | ANN: testLoss: 0.4348 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 414 | ANN: testLoss: 0.4922 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 414 | ANN: testLoss: 0.3815 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 415 | ANN: trainLoss: 0.1418 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 415 | ANN: trainLoss: 0.1260 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 415 | ANN: trainLoss: 0.1445 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 415 | ANN: trainLoss: 0.1483 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 415 | ANN: trainLoss: 0.1601 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 415 | ANN: trainLoss: 0.1579 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 415 | ANN: trainLoss: 0.1651 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 415 | ANN: trainLoss: 0.1528 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 415 | ANN: trainLoss: 0.1475 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 415 | ANN: trainLoss: 0.1560 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 415 | ANN: trainLoss: 0.1501 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 415 | ANN: trainLoss: 0.1512 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 415 | ANN: trainLoss: 0.1746 | trainAcc: 94.4301% (729/772)\n",
            "0 4 Epoch: 415 | ANN: testLoss: 0.5108 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 415 | ANN: testLoss: 0.5053 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 415 | ANN: testLoss: 0.4908 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 415 | ANN: testLoss: 0.3744 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 416 | ANN: trainLoss: 0.2448 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 416 | ANN: trainLoss: 0.2010 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 416 | ANN: trainLoss: 0.2022 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 416 | ANN: trainLoss: 0.1733 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 416 | ANN: trainLoss: 0.1625 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 416 | ANN: trainLoss: 0.1582 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 416 | ANN: trainLoss: 0.1728 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 416 | ANN: trainLoss: 0.1745 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 416 | ANN: trainLoss: 0.1726 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 416 | ANN: trainLoss: 0.1720 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 416 | ANN: trainLoss: 0.1647 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 416 | ANN: trainLoss: 0.1562 | trainAcc: 94.0104% (722/768)\n",
            "12 13 Epoch: 416 | ANN: trainLoss: 0.1568 | trainAcc: 94.0415% (726/772)\n",
            "0 4 Epoch: 416 | ANN: testLoss: 0.5071 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 416 | ANN: testLoss: 0.5634 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 416 | ANN: testLoss: 0.4948 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 416 | ANN: testLoss: 0.4712 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 417 | ANN: trainLoss: 0.1919 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 417 | ANN: trainLoss: 0.1366 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 417 | ANN: trainLoss: 0.1852 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 417 | ANN: trainLoss: 0.1953 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 417 | ANN: trainLoss: 0.2005 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 417 | ANN: trainLoss: 0.1949 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 417 | ANN: trainLoss: 0.1971 | trainAcc: 91.7411% (411/448)\n",
            "7 13 Epoch: 417 | ANN: trainLoss: 0.1947 | trainAcc: 91.7969% (470/512)\n",
            "8 13 Epoch: 417 | ANN: trainLoss: 0.1943 | trainAcc: 92.0139% (530/576)\n",
            "9 13 Epoch: 417 | ANN: trainLoss: 0.1820 | trainAcc: 92.6562% (593/640)\n",
            "10 13 Epoch: 417 | ANN: trainLoss: 0.1756 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 417 | ANN: trainLoss: 0.1725 | trainAcc: 93.2292% (716/768)\n",
            "12 13 Epoch: 417 | ANN: trainLoss: 0.1684 | trainAcc: 93.2642% (720/772)\n",
            "0 4 Epoch: 417 | ANN: testLoss: 0.4175 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 417 | ANN: testLoss: 0.4362 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 417 | ANN: testLoss: 0.4876 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 417 | ANN: testLoss: 0.8226 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 418 | ANN: trainLoss: 0.1267 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 418 | ANN: trainLoss: 0.1757 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 418 | ANN: trainLoss: 0.1651 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 418 | ANN: trainLoss: 0.1790 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 418 | ANN: trainLoss: 0.1891 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 418 | ANN: trainLoss: 0.1783 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 418 | ANN: trainLoss: 0.1698 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 418 | ANN: trainLoss: 0.1742 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 418 | ANN: trainLoss: 0.1815 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 418 | ANN: trainLoss: 0.1750 | trainAcc: 93.7500% (600/640)\n",
            "10 13 Epoch: 418 | ANN: trainLoss: 0.1858 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 418 | ANN: trainLoss: 0.1811 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 418 | ANN: trainLoss: 0.2794 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 418 | ANN: testLoss: 0.3897 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 418 | ANN: testLoss: 0.4952 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 418 | ANN: testLoss: 0.4849 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 418 | ANN: testLoss: 0.9797 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 419 | ANN: trainLoss: 0.2115 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 419 | ANN: trainLoss: 0.1941 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 419 | ANN: trainLoss: 0.1753 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 419 | ANN: trainLoss: 0.1570 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 419 | ANN: trainLoss: 0.1534 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 419 | ANN: trainLoss: 0.1508 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 419 | ANN: trainLoss: 0.1481 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 419 | ANN: trainLoss: 0.1449 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 419 | ANN: trainLoss: 0.1529 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 419 | ANN: trainLoss: 0.1503 | trainAcc: 94.3750% (604/640)\n",
            "10 13 Epoch: 419 | ANN: trainLoss: 0.1526 | trainAcc: 94.3182% (664/704)\n",
            "11 13 Epoch: 419 | ANN: trainLoss: 0.1572 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 419 | ANN: trainLoss: 0.1771 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 419 | ANN: testLoss: 0.4832 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 419 | ANN: testLoss: 0.4656 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 419 | ANN: testLoss: 0.4887 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 419 | ANN: testLoss: 0.3678 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 420 | ANN: trainLoss: 0.1555 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 420 | ANN: trainLoss: 0.2184 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 420 | ANN: trainLoss: 0.2480 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 420 | ANN: trainLoss: 0.2339 | trainAcc: 90.6250% (232/256)\n",
            "4 13 Epoch: 420 | ANN: trainLoss: 0.2178 | trainAcc: 91.5625% (293/320)\n",
            "5 13 Epoch: 420 | ANN: trainLoss: 0.1932 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 420 | ANN: trainLoss: 0.1866 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 420 | ANN: trainLoss: 0.1907 | trainAcc: 93.1641% (477/512)\n",
            "8 13 Epoch: 420 | ANN: trainLoss: 0.1981 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 420 | ANN: trainLoss: 0.1965 | trainAcc: 92.5000% (592/640)\n",
            "10 13 Epoch: 420 | ANN: trainLoss: 0.1927 | trainAcc: 92.6136% (652/704)\n",
            "11 13 Epoch: 420 | ANN: trainLoss: 0.1904 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 420 | ANN: trainLoss: 0.1773 | trainAcc: 92.8756% (717/772)\n",
            "0 4 Epoch: 420 | ANN: testLoss: 0.4744 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 420 | ANN: testLoss: 0.4605 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 420 | ANN: testLoss: 0.4897 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 420 | ANN: testLoss: 0.3811 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 421 | ANN: trainLoss: 0.0876 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 421 | ANN: trainLoss: 0.1516 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 421 | ANN: trainLoss: 0.1482 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 421 | ANN: trainLoss: 0.1490 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 421 | ANN: trainLoss: 0.1538 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 421 | ANN: trainLoss: 0.1585 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 421 | ANN: trainLoss: 0.1536 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 421 | ANN: trainLoss: 0.1544 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 421 | ANN: trainLoss: 0.1661 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 421 | ANN: trainLoss: 0.1687 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 421 | ANN: trainLoss: 0.1691 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 421 | ANN: trainLoss: 0.1703 | trainAcc: 93.0990% (715/768)\n",
            "12 13 Epoch: 421 | ANN: trainLoss: 0.2813 | trainAcc: 93.0052% (718/772)\n",
            "0 4 Epoch: 421 | ANN: testLoss: 0.3638 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 421 | ANN: testLoss: 0.4482 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 421 | ANN: testLoss: 0.5040 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 421 | ANN: testLoss: 0.3793 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 422 | ANN: trainLoss: 0.1582 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 422 | ANN: trainLoss: 0.1517 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 422 | ANN: trainLoss: 0.1298 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 422 | ANN: trainLoss: 0.1224 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 422 | ANN: trainLoss: 0.1237 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 422 | ANN: trainLoss: 0.1250 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 422 | ANN: trainLoss: 0.1323 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 422 | ANN: trainLoss: 0.1439 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 422 | ANN: trainLoss: 0.1535 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 422 | ANN: trainLoss: 0.1596 | trainAcc: 95.0000% (608/640)\n",
            "10 13 Epoch: 422 | ANN: trainLoss: 0.1620 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 422 | ANN: trainLoss: 0.1591 | trainAcc: 94.9219% (729/768)\n",
            "12 13 Epoch: 422 | ANN: trainLoss: 0.1867 | trainAcc: 94.8187% (732/772)\n",
            "0 4 Epoch: 422 | ANN: testLoss: 0.4915 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 422 | ANN: testLoss: 0.4924 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 422 | ANN: testLoss: 0.4924 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 422 | ANN: testLoss: 0.3695 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 423 | ANN: trainLoss: 0.0629 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 423 | ANN: trainLoss: 0.1543 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 423 | ANN: trainLoss: 0.1250 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 423 | ANN: trainLoss: 0.1343 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 423 | ANN: trainLoss: 0.1360 | trainAcc: 95.9375% (307/320)\n",
            "5 13 Epoch: 423 | ANN: trainLoss: 0.1246 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 423 | ANN: trainLoss: 0.1307 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 423 | ANN: trainLoss: 0.1214 | trainAcc: 96.8750% (496/512)\n",
            "8 13 Epoch: 423 | ANN: trainLoss: 0.1372 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 423 | ANN: trainLoss: 0.1398 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 423 | ANN: trainLoss: 0.1354 | trainAcc: 96.0227% (676/704)\n",
            "11 13 Epoch: 423 | ANN: trainLoss: 0.1436 | trainAcc: 95.7031% (735/768)\n",
            "12 13 Epoch: 423 | ANN: trainLoss: 0.1707 | trainAcc: 95.5959% (738/772)\n",
            "0 4 Epoch: 423 | ANN: testLoss: 0.5014 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 423 | ANN: testLoss: 0.5305 | testAcc: 72.6562% (93/128)\n",
            "2 4 Epoch: 423 | ANN: testLoss: 0.4999 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 423 | ANN: testLoss: 0.3808 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 424 | ANN: trainLoss: 0.0744 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 424 | ANN: trainLoss: 0.1186 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 424 | ANN: trainLoss: 0.1497 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 424 | ANN: trainLoss: 0.1401 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 424 | ANN: trainLoss: 0.1405 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 424 | ANN: trainLoss: 0.1416 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 424 | ANN: trainLoss: 0.1404 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 424 | ANN: trainLoss: 0.1370 | trainAcc: 95.3125% (488/512)\n",
            "8 13 Epoch: 424 | ANN: trainLoss: 0.1490 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 424 | ANN: trainLoss: 0.1523 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 424 | ANN: trainLoss: 0.1463 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 424 | ANN: trainLoss: 0.1552 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 424 | ANN: trainLoss: 0.1831 | trainAcc: 94.5596% (730/772)\n",
            "0 4 Epoch: 424 | ANN: testLoss: 0.4822 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 424 | ANN: testLoss: 0.4985 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 424 | ANN: testLoss: 0.4942 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 424 | ANN: testLoss: 0.4254 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 425 | ANN: trainLoss: 0.2576 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 425 | ANN: trainLoss: 0.2812 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 425 | ANN: trainLoss: 0.2464 | trainAcc: 89.5833% (172/192)\n",
            "3 13 Epoch: 425 | ANN: trainLoss: 0.2523 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 425 | ANN: trainLoss: 0.2388 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 425 | ANN: trainLoss: 0.2316 | trainAcc: 90.8854% (349/384)\n",
            "6 13 Epoch: 425 | ANN: trainLoss: 0.2190 | trainAcc: 91.7411% (411/448)\n",
            "7 13 Epoch: 425 | ANN: trainLoss: 0.2128 | trainAcc: 92.3828% (473/512)\n",
            "8 13 Epoch: 425 | ANN: trainLoss: 0.2101 | trainAcc: 92.7083% (534/576)\n",
            "9 13 Epoch: 425 | ANN: trainLoss: 0.1988 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 425 | ANN: trainLoss: 0.1986 | trainAcc: 92.6136% (652/704)\n",
            "11 13 Epoch: 425 | ANN: trainLoss: 0.1932 | trainAcc: 92.7083% (712/768)\n",
            "12 13 Epoch: 425 | ANN: trainLoss: 0.1885 | trainAcc: 92.7461% (716/772)\n",
            "0 4 Epoch: 425 | ANN: testLoss: 0.5228 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 425 | ANN: testLoss: 0.5293 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 425 | ANN: testLoss: 0.4885 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 425 | ANN: testLoss: 0.3816 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 426 | ANN: trainLoss: 0.1020 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 426 | ANN: trainLoss: 0.1436 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 426 | ANN: trainLoss: 0.1917 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 426 | ANN: trainLoss: 0.1919 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 426 | ANN: trainLoss: 0.1853 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 426 | ANN: trainLoss: 0.1906 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 426 | ANN: trainLoss: 0.1941 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 426 | ANN: trainLoss: 0.1950 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 426 | ANN: trainLoss: 0.1826 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 426 | ANN: trainLoss: 0.1729 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 426 | ANN: trainLoss: 0.1673 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 426 | ANN: trainLoss: 0.1726 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 426 | ANN: trainLoss: 0.2145 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 426 | ANN: testLoss: 0.3122 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 426 | ANN: testLoss: 0.3848 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 426 | ANN: testLoss: 0.4920 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 426 | ANN: testLoss: 0.4352 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 427 | ANN: trainLoss: 0.1415 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 427 | ANN: trainLoss: 0.1295 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 427 | ANN: trainLoss: 0.1506 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 427 | ANN: trainLoss: 0.1666 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 427 | ANN: trainLoss: 0.1657 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 427 | ANN: trainLoss: 0.1559 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 427 | ANN: trainLoss: 0.1559 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 427 | ANN: trainLoss: 0.1564 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 427 | ANN: trainLoss: 0.1510 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 427 | ANN: trainLoss: 0.1597 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 427 | ANN: trainLoss: 0.1618 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 427 | ANN: trainLoss: 0.1634 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 427 | ANN: trainLoss: 0.1575 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 427 | ANN: testLoss: 0.6840 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 427 | ANN: testLoss: 0.5740 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 427 | ANN: testLoss: 0.5096 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 427 | ANN: testLoss: 0.3834 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 428 | ANN: trainLoss: 0.1317 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 428 | ANN: trainLoss: 0.1001 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 428 | ANN: trainLoss: 0.1602 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 428 | ANN: trainLoss: 0.1459 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 428 | ANN: trainLoss: 0.1502 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 428 | ANN: trainLoss: 0.1473 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 428 | ANN: trainLoss: 0.1580 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 428 | ANN: trainLoss: 0.1543 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 428 | ANN: trainLoss: 0.1552 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 428 | ANN: trainLoss: 0.1495 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 428 | ANN: trainLoss: 0.1564 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 428 | ANN: trainLoss: 0.1582 | trainAcc: 93.4896% (718/768)\n",
            "12 13 Epoch: 428 | ANN: trainLoss: 0.1478 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 428 | ANN: testLoss: 0.6046 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 428 | ANN: testLoss: 0.5926 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 428 | ANN: testLoss: 0.5065 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 428 | ANN: testLoss: 0.3798 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 429 | ANN: trainLoss: 0.1522 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 429 | ANN: trainLoss: 0.1237 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 429 | ANN: trainLoss: 0.1296 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 429 | ANN: trainLoss: 0.1411 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 429 | ANN: trainLoss: 0.1608 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 429 | ANN: trainLoss: 0.1661 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 429 | ANN: trainLoss: 0.1640 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 429 | ANN: trainLoss: 0.1741 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 429 | ANN: trainLoss: 0.1772 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 429 | ANN: trainLoss: 0.1855 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 429 | ANN: trainLoss: 0.1904 | trainAcc: 93.1818% (656/704)\n",
            "11 13 Epoch: 429 | ANN: trainLoss: 0.1817 | trainAcc: 93.4896% (718/768)\n",
            "12 13 Epoch: 429 | ANN: trainLoss: 0.4544 | trainAcc: 93.1347% (719/772)\n",
            "0 4 Epoch: 429 | ANN: testLoss: 0.6058 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 429 | ANN: testLoss: 0.5208 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 429 | ANN: testLoss: 0.5146 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 429 | ANN: testLoss: 0.4386 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 430 | ANN: trainLoss: 0.1454 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 430 | ANN: trainLoss: 0.1965 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 430 | ANN: trainLoss: 0.1681 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 430 | ANN: trainLoss: 0.1625 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 430 | ANN: trainLoss: 0.1538 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 430 | ANN: trainLoss: 0.1649 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 430 | ANN: trainLoss: 0.1697 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 430 | ANN: trainLoss: 0.1774 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 430 | ANN: trainLoss: 0.1688 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 430 | ANN: trainLoss: 0.1713 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 430 | ANN: trainLoss: 0.1719 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 430 | ANN: trainLoss: 0.1793 | trainAcc: 93.4896% (718/768)\n",
            "12 13 Epoch: 430 | ANN: trainLoss: 0.1726 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 430 | ANN: testLoss: 0.6224 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 430 | ANN: testLoss: 0.4877 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 430 | ANN: testLoss: 0.5353 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 430 | ANN: testLoss: 0.4593 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 431 | ANN: trainLoss: 0.1050 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 431 | ANN: trainLoss: 0.1206 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 431 | ANN: trainLoss: 0.1594 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 431 | ANN: trainLoss: 0.1717 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 431 | ANN: trainLoss: 0.1512 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 431 | ANN: trainLoss: 0.1742 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 431 | ANN: trainLoss: 0.1721 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 431 | ANN: trainLoss: 0.1690 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 431 | ANN: trainLoss: 0.1642 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 431 | ANN: trainLoss: 0.1562 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 431 | ANN: trainLoss: 0.1586 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 431 | ANN: trainLoss: 0.1643 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 431 | ANN: trainLoss: 0.1868 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 431 | ANN: testLoss: 0.5285 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 431 | ANN: testLoss: 0.4273 | testAcc: 82.0312% (105/128)\n",
            "2 4 Epoch: 431 | ANN: testLoss: 0.5255 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 431 | ANN: testLoss: 0.3967 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 432 | ANN: trainLoss: 0.1915 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 432 | ANN: trainLoss: 0.1339 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 432 | ANN: trainLoss: 0.1664 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 432 | ANN: trainLoss: 0.1625 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 432 | ANN: trainLoss: 0.1570 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 432 | ANN: trainLoss: 0.1462 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 432 | ANN: trainLoss: 0.1510 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 432 | ANN: trainLoss: 0.1503 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 432 | ANN: trainLoss: 0.1425 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 432 | ANN: trainLoss: 0.1422 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 432 | ANN: trainLoss: 0.1454 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 432 | ANN: trainLoss: 0.1431 | trainAcc: 94.9219% (729/768)\n",
            "12 13 Epoch: 432 | ANN: trainLoss: 0.1568 | trainAcc: 94.8187% (732/772)\n",
            "0 4 Epoch: 432 | ANN: testLoss: 0.3976 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 432 | ANN: testLoss: 0.4630 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 432 | ANN: testLoss: 0.5175 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 432 | ANN: testLoss: 0.3924 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 433 | ANN: trainLoss: 0.2790 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 433 | ANN: trainLoss: 0.2218 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 433 | ANN: trainLoss: 0.2116 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 433 | ANN: trainLoss: 0.1945 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 433 | ANN: trainLoss: 0.1819 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 433 | ANN: trainLoss: 0.1821 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 433 | ANN: trainLoss: 0.1697 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 433 | ANN: trainLoss: 0.1715 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 433 | ANN: trainLoss: 0.1687 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 433 | ANN: trainLoss: 0.1701 | trainAcc: 94.3750% (604/640)\n",
            "10 13 Epoch: 433 | ANN: trainLoss: 0.1672 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 433 | ANN: trainLoss: 0.1629 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 433 | ANN: trainLoss: 0.2130 | trainAcc: 94.5596% (730/772)\n",
            "0 4 Epoch: 433 | ANN: testLoss: 0.4335 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 433 | ANN: testLoss: 0.4291 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 433 | ANN: testLoss: 0.5028 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 433 | ANN: testLoss: 0.6173 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 434 | ANN: trainLoss: 0.1937 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 434 | ANN: trainLoss: 0.2106 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 434 | ANN: trainLoss: 0.1793 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 434 | ANN: trainLoss: 0.2047 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 434 | ANN: trainLoss: 0.1984 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 434 | ANN: trainLoss: 0.1864 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 434 | ANN: trainLoss: 0.1827 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 434 | ANN: trainLoss: 0.1833 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 434 | ANN: trainLoss: 0.1831 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 434 | ANN: trainLoss: 0.1736 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 434 | ANN: trainLoss: 0.1748 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 434 | ANN: trainLoss: 0.1686 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 434 | ANN: trainLoss: 0.1626 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 434 | ANN: testLoss: 0.5630 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 434 | ANN: testLoss: 0.4893 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 434 | ANN: testLoss: 0.4986 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 434 | ANN: testLoss: 0.6715 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 435 | ANN: trainLoss: 0.1097 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 435 | ANN: trainLoss: 0.1386 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 435 | ANN: trainLoss: 0.1625 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 435 | ANN: trainLoss: 0.1689 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 435 | ANN: trainLoss: 0.1719 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 435 | ANN: trainLoss: 0.1728 | trainAcc: 92.4479% (355/384)\n",
            "6 13 Epoch: 435 | ANN: trainLoss: 0.1733 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 435 | ANN: trainLoss: 0.1753 | trainAcc: 92.5781% (474/512)\n",
            "8 13 Epoch: 435 | ANN: trainLoss: 0.1680 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 435 | ANN: trainLoss: 0.1687 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 435 | ANN: trainLoss: 0.1670 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 435 | ANN: trainLoss: 0.1686 | trainAcc: 93.2292% (716/768)\n",
            "12 13 Epoch: 435 | ANN: trainLoss: 0.1938 | trainAcc: 93.1347% (719/772)\n",
            "0 4 Epoch: 435 | ANN: testLoss: 0.5559 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 435 | ANN: testLoss: 0.5001 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 435 | ANN: testLoss: 0.5013 | testAcc: 79.6875% (153/192)\n",
            "3 4 Epoch: 435 | ANN: testLoss: 0.4019 | testAcc: 79.7927% (154/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 436 | ANN: trainLoss: 0.1902 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 436 | ANN: trainLoss: 0.2181 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 436 | ANN: trainLoss: 0.1853 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 436 | ANN: trainLoss: 0.2019 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 436 | ANN: trainLoss: 0.1953 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 436 | ANN: trainLoss: 0.1854 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 436 | ANN: trainLoss: 0.1913 | trainAcc: 93.5268% (419/448)\n",
            "7 13 Epoch: 436 | ANN: trainLoss: 0.1871 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 436 | ANN: trainLoss: 0.1838 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 436 | ANN: trainLoss: 0.1850 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 436 | ANN: trainLoss: 0.1756 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 436 | ANN: trainLoss: 0.1787 | trainAcc: 94.0104% (722/768)\n",
            "12 13 Epoch: 436 | ANN: trainLoss: 0.1681 | trainAcc: 94.0415% (726/772)\n",
            "0 4 Epoch: 436 | ANN: testLoss: 0.2862 | testAcc: 89.0625% (57/64)\n",
            "1 4 Epoch: 436 | ANN: testLoss: 0.3642 | testAcc: 83.5938% (107/128)\n",
            "2 4 Epoch: 436 | ANN: testLoss: 0.4982 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 436 | ANN: testLoss: 0.6190 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 437 | ANN: trainLoss: 0.1324 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 437 | ANN: trainLoss: 0.1448 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 437 | ANN: trainLoss: 0.1154 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 437 | ANN: trainLoss: 0.1182 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 437 | ANN: trainLoss: 0.1304 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 437 | ANN: trainLoss: 0.1312 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 437 | ANN: trainLoss: 0.1300 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 437 | ANN: trainLoss: 0.1494 | trainAcc: 95.3125% (488/512)\n",
            "8 13 Epoch: 437 | ANN: trainLoss: 0.1595 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 437 | ANN: trainLoss: 0.1580 | trainAcc: 95.0000% (608/640)\n",
            "10 13 Epoch: 437 | ANN: trainLoss: 0.1606 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 437 | ANN: trainLoss: 0.1642 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 437 | ANN: trainLoss: 0.1528 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 437 | ANN: testLoss: 0.4477 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 437 | ANN: testLoss: 0.4775 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 437 | ANN: testLoss: 0.5087 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 437 | ANN: testLoss: 0.4225 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 438 | ANN: trainLoss: 0.2565 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 438 | ANN: trainLoss: 0.2226 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 438 | ANN: trainLoss: 0.1793 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 438 | ANN: trainLoss: 0.1628 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 438 | ANN: trainLoss: 0.1828 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 438 | ANN: trainLoss: 0.1639 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 438 | ANN: trainLoss: 0.1756 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 438 | ANN: trainLoss: 0.1758 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 438 | ANN: trainLoss: 0.1761 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 438 | ANN: trainLoss: 0.1727 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 438 | ANN: trainLoss: 0.1662 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 438 | ANN: trainLoss: 0.1618 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 438 | ANN: trainLoss: 0.1542 | trainAcc: 94.1710% (727/772)\n",
            "0 4 Epoch: 438 | ANN: testLoss: 0.4570 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 438 | ANN: testLoss: 0.5177 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 438 | ANN: testLoss: 0.5001 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 438 | ANN: testLoss: 0.3757 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 439 | ANN: trainLoss: 0.0910 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 439 | ANN: trainLoss: 0.1363 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 439 | ANN: trainLoss: 0.1863 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 439 | ANN: trainLoss: 0.1829 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 439 | ANN: trainLoss: 0.1663 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 439 | ANN: trainLoss: 0.1593 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 439 | ANN: trainLoss: 0.1553 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 439 | ANN: trainLoss: 0.1653 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 439 | ANN: trainLoss: 0.1577 | trainAcc: 94.0972% (542/576)\n",
            "9 13 Epoch: 439 | ANN: trainLoss: 0.1575 | trainAcc: 93.7500% (600/640)\n",
            "10 13 Epoch: 439 | ANN: trainLoss: 0.1653 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 439 | ANN: trainLoss: 0.1612 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 439 | ANN: trainLoss: 0.2879 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 439 | ANN: testLoss: 0.3750 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 439 | ANN: testLoss: 0.4154 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 439 | ANN: testLoss: 0.4862 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 439 | ANN: testLoss: 1.0486 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 440 | ANN: trainLoss: 0.1880 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 440 | ANN: trainLoss: 0.1843 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 440 | ANN: trainLoss: 0.1591 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 440 | ANN: trainLoss: 0.1546 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 440 | ANN: trainLoss: 0.1504 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 440 | ANN: trainLoss: 0.1443 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 440 | ANN: trainLoss: 0.1396 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 440 | ANN: trainLoss: 0.1514 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 440 | ANN: trainLoss: 0.1562 | trainAcc: 93.4028% (538/576)\n",
            "9 13 Epoch: 440 | ANN: trainLoss: 0.1615 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 440 | ANN: trainLoss: 0.1576 | trainAcc: 93.6080% (659/704)\n",
            "11 13 Epoch: 440 | ANN: trainLoss: 0.1484 | trainAcc: 94.0104% (722/768)\n",
            "12 13 Epoch: 440 | ANN: trainLoss: 0.1925 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 440 | ANN: testLoss: 0.5329 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 440 | ANN: testLoss: 0.4879 | testAcc: 82.0312% (105/128)\n",
            "2 4 Epoch: 440 | ANN: testLoss: 0.4903 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 440 | ANN: testLoss: 0.5944 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 441 | ANN: trainLoss: 0.0996 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 441 | ANN: trainLoss: 0.1035 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 441 | ANN: trainLoss: 0.1166 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 441 | ANN: trainLoss: 0.1263 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 441 | ANN: trainLoss: 0.1193 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 441 | ANN: trainLoss: 0.1194 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 441 | ANN: trainLoss: 0.1184 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 441 | ANN: trainLoss: 0.1271 | trainAcc: 95.3125% (488/512)\n",
            "8 13 Epoch: 441 | ANN: trainLoss: 0.1328 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 441 | ANN: trainLoss: 0.1308 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 441 | ANN: trainLoss: 0.1407 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 441 | ANN: trainLoss: 0.1455 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 441 | ANN: trainLoss: 0.1357 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 441 | ANN: testLoss: 0.5160 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 441 | ANN: testLoss: 0.4842 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 441 | ANN: testLoss: 0.5008 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 441 | ANN: testLoss: 0.3760 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 442 | ANN: trainLoss: 0.1850 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 442 | ANN: trainLoss: 0.1854 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 442 | ANN: trainLoss: 0.1712 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 442 | ANN: trainLoss: 0.1636 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 442 | ANN: trainLoss: 0.1691 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 442 | ANN: trainLoss: 0.1763 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 442 | ANN: trainLoss: 0.1647 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 442 | ANN: trainLoss: 0.1535 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 442 | ANN: trainLoss: 0.1576 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 442 | ANN: trainLoss: 0.1569 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 442 | ANN: trainLoss: 0.1525 | trainAcc: 95.0284% (669/704)\n",
            "11 13 Epoch: 442 | ANN: trainLoss: 0.1459 | trainAcc: 95.0521% (730/768)\n",
            "12 13 Epoch: 442 | ANN: trainLoss: 0.1580 | trainAcc: 94.9482% (733/772)\n",
            "0 4 Epoch: 442 | ANN: testLoss: 0.4782 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 442 | ANN: testLoss: 0.5718 | testAcc: 70.3125% (90/128)\n",
            "2 4 Epoch: 442 | ANN: testLoss: 0.5172 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 442 | ANN: testLoss: 0.3969 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 443 | ANN: trainLoss: 0.1503 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 443 | ANN: trainLoss: 0.1219 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 443 | ANN: trainLoss: 0.1180 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 443 | ANN: trainLoss: 0.1333 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 443 | ANN: trainLoss: 0.1199 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 443 | ANN: trainLoss: 0.1352 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 443 | ANN: trainLoss: 0.1309 | trainAcc: 96.6518% (433/448)\n",
            "7 13 Epoch: 443 | ANN: trainLoss: 0.1340 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 443 | ANN: trainLoss: 0.1331 | trainAcc: 96.3542% (555/576)\n",
            "9 13 Epoch: 443 | ANN: trainLoss: 0.1332 | trainAcc: 96.2500% (616/640)\n",
            "10 13 Epoch: 443 | ANN: trainLoss: 0.1293 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 443 | ANN: trainLoss: 0.1309 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 443 | ANN: trainLoss: 0.1234 | trainAcc: 96.2435% (743/772)\n",
            "0 4 Epoch: 443 | ANN: testLoss: 0.2929 | testAcc: 90.6250% (58/64)\n",
            "1 4 Epoch: 443 | ANN: testLoss: 0.4320 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 443 | ANN: testLoss: 0.4913 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 443 | ANN: testLoss: 1.0006 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 444 | ANN: trainLoss: 0.1023 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 444 | ANN: trainLoss: 0.1022 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 444 | ANN: trainLoss: 0.1573 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 444 | ANN: trainLoss: 0.1502 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 444 | ANN: trainLoss: 0.1605 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 444 | ANN: trainLoss: 0.1589 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 444 | ANN: trainLoss: 0.1683 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 444 | ANN: trainLoss: 0.1614 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 444 | ANN: trainLoss: 0.1552 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 444 | ANN: trainLoss: 0.1622 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 444 | ANN: trainLoss: 0.1601 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 444 | ANN: trainLoss: 0.1644 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 444 | ANN: trainLoss: 0.1688 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 444 | ANN: testLoss: 0.3626 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 444 | ANN: testLoss: 0.3998 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 444 | ANN: testLoss: 0.4966 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 444 | ANN: testLoss: 1.1301 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 445 | ANN: trainLoss: 0.2441 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 445 | ANN: trainLoss: 0.2385 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 445 | ANN: trainLoss: 0.2196 | trainAcc: 91.6667% (176/192)\n",
            "3 13 Epoch: 445 | ANN: trainLoss: 0.2103 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 445 | ANN: trainLoss: 0.2074 | trainAcc: 91.2500% (292/320)\n",
            "5 13 Epoch: 445 | ANN: trainLoss: 0.2023 | trainAcc: 91.4062% (351/384)\n",
            "6 13 Epoch: 445 | ANN: trainLoss: 0.2084 | trainAcc: 91.2946% (409/448)\n",
            "7 13 Epoch: 445 | ANN: trainLoss: 0.2020 | trainAcc: 91.4062% (468/512)\n",
            "8 13 Epoch: 445 | ANN: trainLoss: 0.1963 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 445 | ANN: trainLoss: 0.1894 | trainAcc: 92.0312% (589/640)\n",
            "10 13 Epoch: 445 | ANN: trainLoss: 0.1903 | trainAcc: 92.0455% (648/704)\n",
            "11 13 Epoch: 445 | ANN: trainLoss: 0.1999 | trainAcc: 92.0573% (707/768)\n",
            "12 13 Epoch: 445 | ANN: trainLoss: 0.1902 | trainAcc: 92.0984% (711/772)\n",
            "0 4 Epoch: 445 | ANN: testLoss: 0.7366 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 445 | ANN: testLoss: 0.5150 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 445 | ANN: testLoss: 0.5100 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 445 | ANN: testLoss: 0.4556 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 446 | ANN: trainLoss: 0.0950 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 446 | ANN: trainLoss: 0.1259 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 446 | ANN: trainLoss: 0.1387 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 446 | ANN: trainLoss: 0.1319 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 446 | ANN: trainLoss: 0.1563 | trainAcc: 92.1875% (295/320)\n",
            "5 13 Epoch: 446 | ANN: trainLoss: 0.1605 | trainAcc: 92.1875% (354/384)\n",
            "6 13 Epoch: 446 | ANN: trainLoss: 0.1676 | trainAcc: 92.6339% (415/448)\n",
            "7 13 Epoch: 446 | ANN: trainLoss: 0.1675 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 446 | ANN: trainLoss: 0.1625 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 446 | ANN: trainLoss: 0.1784 | trainAcc: 92.8125% (594/640)\n",
            "10 13 Epoch: 446 | ANN: trainLoss: 0.1785 | trainAcc: 93.0398% (655/704)\n",
            "11 13 Epoch: 446 | ANN: trainLoss: 0.1737 | trainAcc: 93.3594% (717/768)\n",
            "12 13 Epoch: 446 | ANN: trainLoss: 0.1754 | trainAcc: 93.3938% (721/772)\n",
            "0 4 Epoch: 446 | ANN: testLoss: 0.6007 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 446 | ANN: testLoss: 0.4766 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 446 | ANN: testLoss: 0.5058 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 446 | ANN: testLoss: 0.4080 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 447 | ANN: trainLoss: 0.0871 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 447 | ANN: trainLoss: 0.1009 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 447 | ANN: trainLoss: 0.1037 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 447 | ANN: trainLoss: 0.0969 | trainAcc: 97.6562% (250/256)\n",
            "4 13 Epoch: 447 | ANN: trainLoss: 0.0990 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 447 | ANN: trainLoss: 0.1163 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 447 | ANN: trainLoss: 0.1328 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 447 | ANN: trainLoss: 0.1332 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 447 | ANN: trainLoss: 0.1293 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 447 | ANN: trainLoss: 0.1245 | trainAcc: 95.9375% (614/640)\n",
            "10 13 Epoch: 447 | ANN: trainLoss: 0.1198 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 447 | ANN: trainLoss: 0.1197 | trainAcc: 96.0938% (738/768)\n",
            "12 13 Epoch: 447 | ANN: trainLoss: 0.2881 | trainAcc: 95.8549% (740/772)\n",
            "0 4 Epoch: 447 | ANN: testLoss: 0.3712 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 447 | ANN: testLoss: 0.4696 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 447 | ANN: testLoss: 0.5178 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 447 | ANN: testLoss: 0.4164 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 448 | ANN: trainLoss: 0.0896 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 448 | ANN: trainLoss: 0.1457 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 448 | ANN: trainLoss: 0.1583 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 448 | ANN: trainLoss: 0.1549 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 448 | ANN: trainLoss: 0.1481 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 448 | ANN: trainLoss: 0.1455 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 448 | ANN: trainLoss: 0.1505 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 448 | ANN: trainLoss: 0.1485 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 448 | ANN: trainLoss: 0.1439 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 448 | ANN: trainLoss: 0.1499 | trainAcc: 94.3750% (604/640)\n",
            "10 13 Epoch: 448 | ANN: trainLoss: 0.1482 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 448 | ANN: trainLoss: 0.1495 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 448 | ANN: trainLoss: 0.2505 | trainAcc: 94.5596% (730/772)\n",
            "0 4 Epoch: 448 | ANN: testLoss: 0.4643 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 448 | ANN: testLoss: 0.4811 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 448 | ANN: testLoss: 0.5323 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 448 | ANN: testLoss: 0.4558 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 449 | ANN: trainLoss: 0.1710 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 449 | ANN: trainLoss: 0.1277 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 449 | ANN: trainLoss: 0.1507 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 449 | ANN: trainLoss: 0.1422 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 449 | ANN: trainLoss: 0.1379 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 449 | ANN: trainLoss: 0.1551 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 449 | ANN: trainLoss: 0.1699 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 449 | ANN: trainLoss: 0.1675 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 449 | ANN: trainLoss: 0.1621 | trainAcc: 94.0972% (542/576)\n",
            "9 13 Epoch: 449 | ANN: trainLoss: 0.1559 | trainAcc: 94.3750% (604/640)\n",
            "10 13 Epoch: 449 | ANN: trainLoss: 0.1639 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 449 | ANN: trainLoss: 0.1647 | trainAcc: 94.0104% (722/768)\n",
            "12 13 Epoch: 449 | ANN: trainLoss: 0.1638 | trainAcc: 94.0415% (726/772)\n",
            "0 4 Epoch: 449 | ANN: testLoss: 0.4330 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 449 | ANN: testLoss: 0.4770 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 449 | ANN: testLoss: 0.5061 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 449 | ANN: testLoss: 0.3819 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 450 | ANN: trainLoss: 0.0642 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 450 | ANN: trainLoss: 0.0953 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 450 | ANN: trainLoss: 0.1140 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 450 | ANN: trainLoss: 0.1408 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 450 | ANN: trainLoss: 0.1388 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 450 | ANN: trainLoss: 0.1413 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 450 | ANN: trainLoss: 0.1513 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 450 | ANN: trainLoss: 0.1436 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 450 | ANN: trainLoss: 0.1465 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 450 | ANN: trainLoss: 0.1563 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 450 | ANN: trainLoss: 0.1574 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 450 | ANN: trainLoss: 0.1617 | trainAcc: 94.0104% (722/768)\n",
            "12 13 Epoch: 450 | ANN: trainLoss: 0.2217 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 450 | ANN: testLoss: 0.4544 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 450 | ANN: testLoss: 0.4744 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 450 | ANN: testLoss: 0.5070 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 450 | ANN: testLoss: 0.3821 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 451 | ANN: trainLoss: 0.3156 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 451 | ANN: trainLoss: 0.2249 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 451 | ANN: trainLoss: 0.1713 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 451 | ANN: trainLoss: 0.1724 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 451 | ANN: trainLoss: 0.1640 | trainAcc: 96.2500% (308/320)\n",
            "5 13 Epoch: 451 | ANN: trainLoss: 0.1599 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 451 | ANN: trainLoss: 0.1545 | trainAcc: 96.4286% (432/448)\n",
            "7 13 Epoch: 451 | ANN: trainLoss: 0.1618 | trainAcc: 96.0938% (492/512)\n",
            "8 13 Epoch: 451 | ANN: trainLoss: 0.1534 | trainAcc: 96.1806% (554/576)\n",
            "9 13 Epoch: 451 | ANN: trainLoss: 0.1561 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 451 | ANN: trainLoss: 0.1562 | trainAcc: 95.4545% (672/704)\n",
            "11 13 Epoch: 451 | ANN: trainLoss: 0.1531 | trainAcc: 95.3125% (732/768)\n",
            "12 13 Epoch: 451 | ANN: trainLoss: 0.1845 | trainAcc: 95.2073% (735/772)\n",
            "0 4 Epoch: 451 | ANN: testLoss: 0.5233 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 451 | ANN: testLoss: 0.4835 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 451 | ANN: testLoss: 0.5122 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 451 | ANN: testLoss: 0.5009 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 452 | ANN: trainLoss: 0.2213 | trainAcc: 87.5000% (56/64)\n",
            "1 13 Epoch: 452 | ANN: trainLoss: 0.2378 | trainAcc: 87.5000% (112/128)\n",
            "2 13 Epoch: 452 | ANN: trainLoss: 0.1910 | trainAcc: 90.1042% (173/192)\n",
            "3 13 Epoch: 452 | ANN: trainLoss: 0.1665 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 452 | ANN: trainLoss: 0.1704 | trainAcc: 91.8750% (294/320)\n",
            "5 13 Epoch: 452 | ANN: trainLoss: 0.1746 | trainAcc: 91.9271% (353/384)\n",
            "6 13 Epoch: 452 | ANN: trainLoss: 0.1868 | trainAcc: 91.2946% (409/448)\n",
            "7 13 Epoch: 452 | ANN: trainLoss: 0.1856 | trainAcc: 91.0156% (466/512)\n",
            "8 13 Epoch: 452 | ANN: trainLoss: 0.1828 | trainAcc: 91.6667% (528/576)\n",
            "9 13 Epoch: 452 | ANN: trainLoss: 0.1808 | trainAcc: 91.8750% (588/640)\n",
            "10 13 Epoch: 452 | ANN: trainLoss: 0.1754 | trainAcc: 92.1875% (649/704)\n",
            "11 13 Epoch: 452 | ANN: trainLoss: 0.1746 | trainAcc: 92.1875% (708/768)\n",
            "12 13 Epoch: 452 | ANN: trainLoss: 0.1650 | trainAcc: 92.2280% (712/772)\n",
            "0 4 Epoch: 452 | ANN: testLoss: 0.5022 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 452 | ANN: testLoss: 0.4917 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 452 | ANN: testLoss: 0.5092 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 452 | ANN: testLoss: 0.3869 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 453 | ANN: trainLoss: 0.1446 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 453 | ANN: trainLoss: 0.1850 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 453 | ANN: trainLoss: 0.1629 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 453 | ANN: trainLoss: 0.1801 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 453 | ANN: trainLoss: 0.1733 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 453 | ANN: trainLoss: 0.1691 | trainAcc: 92.7083% (356/384)\n",
            "6 13 Epoch: 453 | ANN: trainLoss: 0.1580 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 453 | ANN: trainLoss: 0.1589 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 453 | ANN: trainLoss: 0.1627 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 453 | ANN: trainLoss: 0.1639 | trainAcc: 93.2812% (597/640)\n",
            "10 13 Epoch: 453 | ANN: trainLoss: 0.1658 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 453 | ANN: trainLoss: 0.1725 | trainAcc: 92.7083% (712/768)\n",
            "12 13 Epoch: 453 | ANN: trainLoss: 0.2009 | trainAcc: 92.6166% (715/772)\n",
            "0 4 Epoch: 453 | ANN: testLoss: 0.3597 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 453 | ANN: testLoss: 0.5519 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 453 | ANN: testLoss: 0.5186 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 453 | ANN: testLoss: 0.7346 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 454 | ANN: trainLoss: 0.1427 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 454 | ANN: trainLoss: 0.1699 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 454 | ANN: trainLoss: 0.1996 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 454 | ANN: trainLoss: 0.1885 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 454 | ANN: trainLoss: 0.1788 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 454 | ANN: trainLoss: 0.1767 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 454 | ANN: trainLoss: 0.1678 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 454 | ANN: trainLoss: 0.1695 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 454 | ANN: trainLoss: 0.1724 | trainAcc: 94.0972% (542/576)\n",
            "9 13 Epoch: 454 | ANN: trainLoss: 0.1721 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 454 | ANN: trainLoss: 0.1658 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 454 | ANN: trainLoss: 0.1612 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 454 | ANN: trainLoss: 0.1625 | trainAcc: 94.5596% (730/772)\n",
            "0 4 Epoch: 454 | ANN: testLoss: 0.4964 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 454 | ANN: testLoss: 0.5694 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 454 | ANN: testLoss: 0.5023 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 454 | ANN: testLoss: 0.3767 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 455 | ANN: trainLoss: 0.1816 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 455 | ANN: trainLoss: 0.1566 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 455 | ANN: trainLoss: 0.1351 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 455 | ANN: trainLoss: 0.1340 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 455 | ANN: trainLoss: 0.1536 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 455 | ANN: trainLoss: 0.1567 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 455 | ANN: trainLoss: 0.1486 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 455 | ANN: trainLoss: 0.1515 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 455 | ANN: trainLoss: 0.1622 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 455 | ANN: trainLoss: 0.1585 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 455 | ANN: trainLoss: 0.1608 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 455 | ANN: trainLoss: 0.1569 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 455 | ANN: trainLoss: 0.1472 | trainAcc: 94.1710% (727/772)\n",
            "0 4 Epoch: 455 | ANN: testLoss: 0.3719 | testAcc: 87.5000% (56/64)\n",
            "1 4 Epoch: 455 | ANN: testLoss: 0.5135 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 455 | ANN: testLoss: 0.5013 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 455 | ANN: testLoss: 0.3796 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 456 | ANN: trainLoss: 0.1814 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 456 | ANN: trainLoss: 0.1565 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 456 | ANN: trainLoss: 0.1861 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 456 | ANN: trainLoss: 0.1721 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 456 | ANN: trainLoss: 0.1566 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 456 | ANN: trainLoss: 0.1404 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 456 | ANN: trainLoss: 0.1431 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 456 | ANN: trainLoss: 0.1400 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 456 | ANN: trainLoss: 0.1375 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 456 | ANN: trainLoss: 0.1412 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 456 | ANN: trainLoss: 0.1430 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 456 | ANN: trainLoss: 0.1441 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 456 | ANN: trainLoss: 0.1587 | trainAcc: 94.1710% (727/772)\n",
            "0 4 Epoch: 456 | ANN: testLoss: 0.4423 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 456 | ANN: testLoss: 0.4177 | testAcc: 82.8125% (106/128)\n",
            "2 4 Epoch: 456 | ANN: testLoss: 0.5044 | testAcc: 79.1667% (152/192)\n",
            "3 4 Epoch: 456 | ANN: testLoss: 0.3817 | testAcc: 79.2746% (153/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 457 | ANN: trainLoss: 0.1806 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 457 | ANN: trainLoss: 0.1926 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 457 | ANN: trainLoss: 0.1783 | trainAcc: 93.7500% (180/192)\n",
            "3 13 Epoch: 457 | ANN: trainLoss: 0.1740 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 457 | ANN: trainLoss: 0.1812 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 457 | ANN: trainLoss: 0.1887 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 457 | ANN: trainLoss: 0.1934 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 457 | ANN: trainLoss: 0.1927 | trainAcc: 92.7734% (475/512)\n",
            "8 13 Epoch: 457 | ANN: trainLoss: 0.1865 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 457 | ANN: trainLoss: 0.1787 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 457 | ANN: trainLoss: 0.1848 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 457 | ANN: trainLoss: 0.1797 | trainAcc: 93.7500% (720/768)\n",
            "12 13 Epoch: 457 | ANN: trainLoss: 0.1730 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 457 | ANN: testLoss: 0.5015 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 457 | ANN: testLoss: 0.5187 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 457 | ANN: testLoss: 0.4944 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 457 | ANN: testLoss: 0.4124 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 458 | ANN: trainLoss: 0.1831 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 458 | ANN: trainLoss: 0.1725 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 458 | ANN: trainLoss: 0.1652 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 458 | ANN: trainLoss: 0.1495 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 458 | ANN: trainLoss: 0.1575 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 458 | ANN: trainLoss: 0.1695 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 458 | ANN: trainLoss: 0.1565 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 458 | ANN: trainLoss: 0.1606 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 458 | ANN: trainLoss: 0.1599 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 458 | ANN: trainLoss: 0.1592 | trainAcc: 93.7500% (600/640)\n",
            "10 13 Epoch: 458 | ANN: trainLoss: 0.1592 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 458 | ANN: trainLoss: 0.1652 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 458 | ANN: trainLoss: 0.2329 | trainAcc: 93.3938% (721/772)\n",
            "0 4 Epoch: 458 | ANN: testLoss: 0.3428 | testAcc: 85.9375% (55/64)\n",
            "1 4 Epoch: 458 | ANN: testLoss: 0.4600 | testAcc: 82.0312% (105/128)\n",
            "2 4 Epoch: 458 | ANN: testLoss: 0.5060 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 458 | ANN: testLoss: 0.3929 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 459 | ANN: trainLoss: 0.0860 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 459 | ANN: trainLoss: 0.0783 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 459 | ANN: trainLoss: 0.0875 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 459 | ANN: trainLoss: 0.1052 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 459 | ANN: trainLoss: 0.1159 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 459 | ANN: trainLoss: 0.1215 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 459 | ANN: trainLoss: 0.1180 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 459 | ANN: trainLoss: 0.1274 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 459 | ANN: trainLoss: 0.1314 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 459 | ANN: trainLoss: 0.1349 | trainAcc: 95.0000% (608/640)\n",
            "10 13 Epoch: 459 | ANN: trainLoss: 0.1364 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 459 | ANN: trainLoss: 0.1406 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 459 | ANN: trainLoss: 0.1309 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 459 | ANN: testLoss: 0.3685 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 459 | ANN: testLoss: 0.4107 | testAcc: 81.2500% (104/128)\n",
            "2 4 Epoch: 459 | ANN: testLoss: 0.5013 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 459 | ANN: testLoss: 0.3859 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 460 | ANN: trainLoss: 0.1900 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 460 | ANN: trainLoss: 0.1673 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 460 | ANN: trainLoss: 0.1559 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 460 | ANN: trainLoss: 0.1355 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 460 | ANN: trainLoss: 0.1467 | trainAcc: 94.6875% (303/320)\n",
            "5 13 Epoch: 460 | ANN: trainLoss: 0.1550 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 460 | ANN: trainLoss: 0.1608 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 460 | ANN: trainLoss: 0.1562 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 460 | ANN: trainLoss: 0.1549 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 460 | ANN: trainLoss: 0.1593 | trainAcc: 94.3750% (604/640)\n",
            "10 13 Epoch: 460 | ANN: trainLoss: 0.1539 | trainAcc: 94.3182% (664/704)\n",
            "11 13 Epoch: 460 | ANN: trainLoss: 0.1539 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 460 | ANN: trainLoss: 0.1421 | trainAcc: 94.5596% (730/772)\n",
            "0 4 Epoch: 460 | ANN: testLoss: 0.5102 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 460 | ANN: testLoss: 0.4954 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 460 | ANN: testLoss: 0.4977 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 460 | ANN: testLoss: 0.3943 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 461 | ANN: trainLoss: 0.0595 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 461 | ANN: trainLoss: 0.1183 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 461 | ANN: trainLoss: 0.1231 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 461 | ANN: trainLoss: 0.1345 | trainAcc: 96.4844% (247/256)\n",
            "4 13 Epoch: 461 | ANN: trainLoss: 0.1298 | trainAcc: 96.5625% (309/320)\n",
            "5 13 Epoch: 461 | ANN: trainLoss: 0.1263 | trainAcc: 96.3542% (370/384)\n",
            "6 13 Epoch: 461 | ANN: trainLoss: 0.1266 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 461 | ANN: trainLoss: 0.1447 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 461 | ANN: trainLoss: 0.1422 | trainAcc: 95.8333% (552/576)\n",
            "9 13 Epoch: 461 | ANN: trainLoss: 0.1411 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 461 | ANN: trainLoss: 0.1411 | trainAcc: 95.7386% (674/704)\n",
            "11 13 Epoch: 461 | ANN: trainLoss: 0.1422 | trainAcc: 95.5729% (734/768)\n",
            "12 13 Epoch: 461 | ANN: trainLoss: 0.1661 | trainAcc: 95.4663% (737/772)\n",
            "0 4 Epoch: 461 | ANN: testLoss: 0.3640 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 461 | ANN: testLoss: 0.4591 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 461 | ANN: testLoss: 0.5002 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 461 | ANN: testLoss: 0.4682 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 462 | ANN: trainLoss: 0.1455 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 462 | ANN: trainLoss: 0.1123 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 462 | ANN: trainLoss: 0.1210 | trainAcc: 97.3958% (187/192)\n",
            "3 13 Epoch: 462 | ANN: trainLoss: 0.1316 | trainAcc: 96.8750% (248/256)\n",
            "4 13 Epoch: 462 | ANN: trainLoss: 0.1272 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 462 | ANN: trainLoss: 0.1296 | trainAcc: 96.6146% (371/384)\n",
            "6 13 Epoch: 462 | ANN: trainLoss: 0.1331 | trainAcc: 95.9821% (430/448)\n",
            "7 13 Epoch: 462 | ANN: trainLoss: 0.1494 | trainAcc: 95.5078% (489/512)\n",
            "8 13 Epoch: 462 | ANN: trainLoss: 0.1574 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 462 | ANN: trainLoss: 0.1638 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 462 | ANN: trainLoss: 0.1614 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 462 | ANN: trainLoss: 0.1609 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 462 | ANN: trainLoss: 0.1492 | trainAcc: 94.5596% (730/772)\n",
            "0 4 Epoch: 462 | ANN: testLoss: 0.5407 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 462 | ANN: testLoss: 0.4765 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 462 | ANN: testLoss: 0.4954 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 462 | ANN: testLoss: 0.4135 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 463 | ANN: trainLoss: 0.1960 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 463 | ANN: trainLoss: 0.1723 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 463 | ANN: trainLoss: 0.1708 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 463 | ANN: trainLoss: 0.1581 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 463 | ANN: trainLoss: 0.1584 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 463 | ANN: trainLoss: 0.1516 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 463 | ANN: trainLoss: 0.1432 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 463 | ANN: trainLoss: 0.1420 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 463 | ANN: trainLoss: 0.1544 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 463 | ANN: trainLoss: 0.1531 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 463 | ANN: trainLoss: 0.1510 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 463 | ANN: trainLoss: 0.1506 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 463 | ANN: trainLoss: 0.1488 | trainAcc: 94.1710% (727/772)\n",
            "0 4 Epoch: 463 | ANN: testLoss: 0.6023 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 463 | ANN: testLoss: 0.4876 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 463 | ANN: testLoss: 0.5012 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 463 | ANN: testLoss: 0.5440 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 464 | ANN: trainLoss: 0.1280 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 464 | ANN: trainLoss: 0.1366 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 464 | ANN: trainLoss: 0.1452 | trainAcc: 93.2292% (179/192)\n",
            "3 13 Epoch: 464 | ANN: trainLoss: 0.1533 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 464 | ANN: trainLoss: 0.1433 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 464 | ANN: trainLoss: 0.1355 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 464 | ANN: trainLoss: 0.1305 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 464 | ANN: trainLoss: 0.1443 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 464 | ANN: trainLoss: 0.1388 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 464 | ANN: trainLoss: 0.1365 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 464 | ANN: trainLoss: 0.1385 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 464 | ANN: trainLoss: 0.1425 | trainAcc: 94.2708% (724/768)\n",
            "12 13 Epoch: 464 | ANN: trainLoss: 0.2655 | trainAcc: 94.0415% (726/772)\n",
            "0 4 Epoch: 464 | ANN: testLoss: 0.4950 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 464 | ANN: testLoss: 0.5246 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 464 | ANN: testLoss: 0.4989 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 464 | ANN: testLoss: 0.4591 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 465 | ANN: trainLoss: 0.2195 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 465 | ANN: trainLoss: 0.2111 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 465 | ANN: trainLoss: 0.2018 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 465 | ANN: trainLoss: 0.1932 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 465 | ANN: trainLoss: 0.1764 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 465 | ANN: trainLoss: 0.1616 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 465 | ANN: trainLoss: 0.1527 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 465 | ANN: trainLoss: 0.1466 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 465 | ANN: trainLoss: 0.1410 | trainAcc: 95.1389% (548/576)\n",
            "9 13 Epoch: 465 | ANN: trainLoss: 0.1439 | trainAcc: 95.0000% (608/640)\n",
            "10 13 Epoch: 465 | ANN: trainLoss: 0.1460 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 465 | ANN: trainLoss: 0.1464 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 465 | ANN: trainLoss: 0.1474 | trainAcc: 94.5596% (730/772)\n",
            "0 4 Epoch: 465 | ANN: testLoss: 0.5268 | testAcc: 82.8125% (53/64)\n",
            "1 4 Epoch: 465 | ANN: testLoss: 0.5835 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 465 | ANN: testLoss: 0.4896 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 465 | ANN: testLoss: 0.7333 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 466 | ANN: trainLoss: 0.1573 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 466 | ANN: trainLoss: 0.1375 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 466 | ANN: trainLoss: 0.1783 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 466 | ANN: trainLoss: 0.1726 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 466 | ANN: trainLoss: 0.1714 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 466 | ANN: trainLoss: 0.1680 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 466 | ANN: trainLoss: 0.1647 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 466 | ANN: trainLoss: 0.1596 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 466 | ANN: trainLoss: 0.1616 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 466 | ANN: trainLoss: 0.1593 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 466 | ANN: trainLoss: 0.1585 | trainAcc: 93.6080% (659/704)\n",
            "11 13 Epoch: 466 | ANN: trainLoss: 0.1583 | trainAcc: 93.3594% (717/768)\n",
            "12 13 Epoch: 466 | ANN: trainLoss: 0.1804 | trainAcc: 93.2642% (720/772)\n",
            "0 4 Epoch: 466 | ANN: testLoss: 0.6034 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 466 | ANN: testLoss: 0.5183 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 466 | ANN: testLoss: 0.5051 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 466 | ANN: testLoss: 0.3803 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 467 | ANN: trainLoss: 0.1869 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 467 | ANN: trainLoss: 0.2018 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 467 | ANN: trainLoss: 0.1791 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 467 | ANN: trainLoss: 0.1532 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 467 | ANN: trainLoss: 0.1626 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 467 | ANN: trainLoss: 0.1571 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 467 | ANN: trainLoss: 0.1646 | trainAcc: 93.3036% (418/448)\n",
            "7 13 Epoch: 467 | ANN: trainLoss: 0.1646 | trainAcc: 92.9688% (476/512)\n",
            "8 13 Epoch: 467 | ANN: trainLoss: 0.1626 | trainAcc: 93.0556% (536/576)\n",
            "9 13 Epoch: 467 | ANN: trainLoss: 0.1592 | trainAcc: 92.9688% (595/640)\n",
            "10 13 Epoch: 467 | ANN: trainLoss: 0.1569 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 467 | ANN: trainLoss: 0.1569 | trainAcc: 93.6198% (719/768)\n",
            "12 13 Epoch: 467 | ANN: trainLoss: 0.2389 | trainAcc: 93.3938% (721/772)\n",
            "0 4 Epoch: 467 | ANN: testLoss: 0.4219 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 467 | ANN: testLoss: 0.4665 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 467 | ANN: testLoss: 0.5136 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 467 | ANN: testLoss: 0.3853 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 468 | ANN: trainLoss: 0.1066 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 468 | ANN: trainLoss: 0.1610 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 468 | ANN: trainLoss: 0.1445 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 468 | ANN: trainLoss: 0.1265 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 468 | ANN: trainLoss: 0.1399 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 468 | ANN: trainLoss: 0.1511 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 468 | ANN: trainLoss: 0.1483 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 468 | ANN: trainLoss: 0.1485 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 468 | ANN: trainLoss: 0.1442 | trainAcc: 94.0972% (542/576)\n",
            "9 13 Epoch: 468 | ANN: trainLoss: 0.1510 | trainAcc: 93.7500% (600/640)\n",
            "10 13 Epoch: 468 | ANN: trainLoss: 0.1464 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 468 | ANN: trainLoss: 0.1452 | trainAcc: 94.2708% (724/768)\n",
            "12 13 Epoch: 468 | ANN: trainLoss: 0.1462 | trainAcc: 94.3005% (728/772)\n",
            "0 4 Epoch: 468 | ANN: testLoss: 0.4689 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 468 | ANN: testLoss: 0.4926 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 468 | ANN: testLoss: 0.5136 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 468 | ANN: testLoss: 0.3972 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 469 | ANN: trainLoss: 0.0924 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 469 | ANN: trainLoss: 0.0926 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 469 | ANN: trainLoss: 0.0933 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 469 | ANN: trainLoss: 0.1117 | trainAcc: 96.0938% (246/256)\n",
            "4 13 Epoch: 469 | ANN: trainLoss: 0.1330 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 469 | ANN: trainLoss: 0.1451 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 469 | ANN: trainLoss: 0.1446 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 469 | ANN: trainLoss: 0.1376 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 469 | ANN: trainLoss: 0.1421 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 469 | ANN: trainLoss: 0.1378 | trainAcc: 95.1562% (609/640)\n",
            "10 13 Epoch: 469 | ANN: trainLoss: 0.1447 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 469 | ANN: trainLoss: 0.1472 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 469 | ANN: trainLoss: 0.1384 | trainAcc: 94.8187% (732/772)\n",
            "0 4 Epoch: 469 | ANN: testLoss: 0.5013 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 469 | ANN: testLoss: 0.4824 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 469 | ANN: testLoss: 0.5097 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 469 | ANN: testLoss: 0.3823 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 470 | ANN: trainLoss: 0.1473 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 470 | ANN: trainLoss: 0.1972 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 470 | ANN: trainLoss: 0.1791 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 470 | ANN: trainLoss: 0.1676 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 470 | ANN: trainLoss: 0.1635 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 470 | ANN: trainLoss: 0.1629 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 470 | ANN: trainLoss: 0.1589 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 470 | ANN: trainLoss: 0.1591 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 470 | ANN: trainLoss: 0.1793 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 470 | ANN: trainLoss: 0.1745 | trainAcc: 93.9062% (601/640)\n",
            "10 13 Epoch: 470 | ANN: trainLoss: 0.1697 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 470 | ANN: trainLoss: 0.1756 | trainAcc: 94.0104% (722/768)\n",
            "12 13 Epoch: 470 | ANN: trainLoss: 0.1683 | trainAcc: 94.0415% (726/772)\n",
            "0 4 Epoch: 470 | ANN: testLoss: 0.4736 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 470 | ANN: testLoss: 0.5043 | testAcc: 78.1250% (100/128)\n",
            "2 4 Epoch: 470 | ANN: testLoss: 0.4994 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 470 | ANN: testLoss: 0.4168 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 471 | ANN: trainLoss: 0.2167 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 471 | ANN: trainLoss: 0.1518 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 471 | ANN: trainLoss: 0.1420 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 471 | ANN: trainLoss: 0.1418 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 471 | ANN: trainLoss: 0.1382 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 471 | ANN: trainLoss: 0.1487 | trainAcc: 95.3125% (366/384)\n",
            "6 13 Epoch: 471 | ANN: trainLoss: 0.1482 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 471 | ANN: trainLoss: 0.1451 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 471 | ANN: trainLoss: 0.1387 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 471 | ANN: trainLoss: 0.1331 | trainAcc: 95.7812% (613/640)\n",
            "10 13 Epoch: 471 | ANN: trainLoss: 0.1473 | trainAcc: 95.3125% (671/704)\n",
            "11 13 Epoch: 471 | ANN: trainLoss: 0.1478 | trainAcc: 95.1823% (731/768)\n",
            "12 13 Epoch: 471 | ANN: trainLoss: 0.1848 | trainAcc: 95.0777% (734/772)\n",
            "0 4 Epoch: 471 | ANN: testLoss: 0.4332 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 471 | ANN: testLoss: 0.3981 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 471 | ANN: testLoss: 0.4909 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 471 | ANN: testLoss: 0.8034 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 472 | ANN: trainLoss: 0.0589 | trainAcc: 100.0000% (64/64)\n",
            "1 13 Epoch: 472 | ANN: trainLoss: 0.0948 | trainAcc: 98.4375% (126/128)\n",
            "2 13 Epoch: 472 | ANN: trainLoss: 0.1078 | trainAcc: 98.4375% (189/192)\n",
            "3 13 Epoch: 472 | ANN: trainLoss: 0.1036 | trainAcc: 98.0469% (251/256)\n",
            "4 13 Epoch: 472 | ANN: trainLoss: 0.1214 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 472 | ANN: trainLoss: 0.1272 | trainAcc: 96.0938% (369/384)\n",
            "6 13 Epoch: 472 | ANN: trainLoss: 0.1428 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 472 | ANN: trainLoss: 0.1418 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 472 | ANN: trainLoss: 0.1426 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 472 | ANN: trainLoss: 0.1470 | trainAcc: 94.3750% (604/640)\n",
            "10 13 Epoch: 472 | ANN: trainLoss: 0.1469 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 472 | ANN: trainLoss: 0.1535 | trainAcc: 94.4010% (725/768)\n",
            "12 13 Epoch: 472 | ANN: trainLoss: 0.1445 | trainAcc: 94.4301% (729/772)\n",
            "0 4 Epoch: 472 | ANN: testLoss: 0.3833 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 472 | ANN: testLoss: 0.3591 | testAcc: 82.0312% (105/128)\n",
            "2 4 Epoch: 472 | ANN: testLoss: 0.5140 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 472 | ANN: testLoss: 0.3860 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 473 | ANN: trainLoss: 0.1169 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 473 | ANN: trainLoss: 0.0944 | trainAcc: 96.8750% (124/128)\n",
            "2 13 Epoch: 473 | ANN: trainLoss: 0.1014 | trainAcc: 96.3542% (185/192)\n",
            "3 13 Epoch: 473 | ANN: trainLoss: 0.1179 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 473 | ANN: trainLoss: 0.1177 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 473 | ANN: trainLoss: 0.1265 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 473 | ANN: trainLoss: 0.1358 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 473 | ANN: trainLoss: 0.1308 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 473 | ANN: trainLoss: 0.1345 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 473 | ANN: trainLoss: 0.1379 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 473 | ANN: trainLoss: 0.1415 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 473 | ANN: trainLoss: 0.1375 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 473 | ANN: trainLoss: 0.1340 | trainAcc: 94.8187% (732/772)\n",
            "0 4 Epoch: 473 | ANN: testLoss: 0.5689 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 473 | ANN: testLoss: 0.5776 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 473 | ANN: testLoss: 0.5183 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 473 | ANN: testLoss: 0.3922 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 474 | ANN: trainLoss: 0.1396 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 474 | ANN: trainLoss: 0.1540 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 474 | ANN: trainLoss: 0.1458 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 474 | ANN: trainLoss: 0.1543 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 474 | ANN: trainLoss: 0.1652 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 474 | ANN: trainLoss: 0.1828 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 474 | ANN: trainLoss: 0.1686 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 474 | ANN: trainLoss: 0.1672 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 474 | ANN: trainLoss: 0.1669 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 474 | ANN: trainLoss: 0.1604 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 474 | ANN: trainLoss: 0.1693 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 474 | ANN: trainLoss: 0.1680 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 474 | ANN: trainLoss: 0.1552 | trainAcc: 94.8187% (732/772)\n",
            "0 4 Epoch: 474 | ANN: testLoss: 0.3405 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 474 | ANN: testLoss: 0.4397 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 474 | ANN: testLoss: 0.5152 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 474 | ANN: testLoss: 0.3864 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 475 | ANN: trainLoss: 0.1776 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 475 | ANN: trainLoss: 0.1557 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 475 | ANN: trainLoss: 0.1463 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 475 | ANN: trainLoss: 0.1836 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 475 | ANN: trainLoss: 0.1578 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 475 | ANN: trainLoss: 0.1777 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 475 | ANN: trainLoss: 0.1664 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 475 | ANN: trainLoss: 0.1567 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 475 | ANN: trainLoss: 0.1737 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 475 | ANN: trainLoss: 0.1736 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 475 | ANN: trainLoss: 0.1710 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 475 | ANN: trainLoss: 0.1715 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 475 | ANN: trainLoss: 0.1872 | trainAcc: 94.0415% (726/772)\n",
            "0 4 Epoch: 475 | ANN: testLoss: 0.5159 | testAcc: 70.3125% (45/64)\n",
            "1 4 Epoch: 475 | ANN: testLoss: 0.4668 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 475 | ANN: testLoss: 0.4997 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 475 | ANN: testLoss: 0.3846 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 476 | ANN: trainLoss: 0.1516 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 476 | ANN: trainLoss: 0.1636 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 476 | ANN: trainLoss: 0.1514 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 476 | ANN: trainLoss: 0.1393 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 476 | ANN: trainLoss: 0.1375 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 476 | ANN: trainLoss: 0.1402 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 476 | ANN: trainLoss: 0.1502 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 476 | ANN: trainLoss: 0.1503 | trainAcc: 94.9219% (486/512)\n",
            "8 13 Epoch: 476 | ANN: trainLoss: 0.1565 | trainAcc: 94.9653% (547/576)\n",
            "9 13 Epoch: 476 | ANN: trainLoss: 0.1628 | trainAcc: 94.3750% (604/640)\n",
            "10 13 Epoch: 476 | ANN: trainLoss: 0.1593 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 476 | ANN: trainLoss: 0.1591 | trainAcc: 94.4010% (725/768)\n",
            "12 13 Epoch: 476 | ANN: trainLoss: 0.2262 | trainAcc: 94.3005% (728/772)\n",
            "0 4 Epoch: 476 | ANN: testLoss: 0.5179 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 476 | ANN: testLoss: 0.6084 | testAcc: 71.8750% (92/128)\n",
            "2 4 Epoch: 476 | ANN: testLoss: 0.4966 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 476 | ANN: testLoss: 0.4018 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 477 | ANN: trainLoss: 0.2001 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 477 | ANN: trainLoss: 0.2129 | trainAcc: 89.8438% (115/128)\n",
            "2 13 Epoch: 477 | ANN: trainLoss: 0.1952 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 477 | ANN: trainLoss: 0.1808 | trainAcc: 92.5781% (237/256)\n",
            "4 13 Epoch: 477 | ANN: trainLoss: 0.1721 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 477 | ANN: trainLoss: 0.1690 | trainAcc: 93.2292% (358/384)\n",
            "6 13 Epoch: 477 | ANN: trainLoss: 0.1729 | trainAcc: 92.8571% (416/448)\n",
            "7 13 Epoch: 477 | ANN: trainLoss: 0.1644 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 477 | ANN: trainLoss: 0.1593 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 477 | ANN: trainLoss: 0.1569 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 477 | ANN: trainLoss: 0.1645 | trainAcc: 93.8920% (661/704)\n",
            "11 13 Epoch: 477 | ANN: trainLoss: 0.1674 | trainAcc: 93.4896% (718/768)\n",
            "12 13 Epoch: 477 | ANN: trainLoss: 0.1576 | trainAcc: 93.5233% (722/772)\n",
            "0 4 Epoch: 477 | ANN: testLoss: 0.6834 | testAcc: 60.9375% (39/64)\n",
            "1 4 Epoch: 477 | ANN: testLoss: 0.5203 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 477 | ANN: testLoss: 0.5053 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 477 | ANN: testLoss: 0.3806 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 478 | ANN: trainLoss: 0.2176 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 478 | ANN: trainLoss: 0.2078 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 478 | ANN: trainLoss: 0.1872 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 478 | ANN: trainLoss: 0.1739 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 478 | ANN: trainLoss: 0.1694 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 478 | ANN: trainLoss: 0.1683 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 478 | ANN: trainLoss: 0.1723 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 478 | ANN: trainLoss: 0.1623 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 478 | ANN: trainLoss: 0.1550 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 478 | ANN: trainLoss: 0.1513 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 478 | ANN: trainLoss: 0.1475 | trainAcc: 95.0284% (669/704)\n",
            "11 13 Epoch: 478 | ANN: trainLoss: 0.1534 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 478 | ANN: trainLoss: 0.1467 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 478 | ANN: testLoss: 0.5580 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 478 | ANN: testLoss: 0.5055 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 478 | ANN: testLoss: 0.5064 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 478 | ANN: testLoss: 0.4378 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 479 | ANN: trainLoss: 0.0964 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 479 | ANN: trainLoss: 0.1731 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 479 | ANN: trainLoss: 0.2168 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 479 | ANN: trainLoss: 0.2117 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 479 | ANN: trainLoss: 0.2091 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 479 | ANN: trainLoss: 0.2055 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 479 | ANN: trainLoss: 0.1989 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 479 | ANN: trainLoss: 0.2004 | trainAcc: 93.9453% (481/512)\n",
            "8 13 Epoch: 479 | ANN: trainLoss: 0.1989 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 479 | ANN: trainLoss: 0.1883 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 479 | ANN: trainLoss: 0.1868 | trainAcc: 94.3182% (664/704)\n",
            "11 13 Epoch: 479 | ANN: trainLoss: 0.1790 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 479 | ANN: trainLoss: 0.2129 | trainAcc: 94.5596% (730/772)\n",
            "0 4 Epoch: 479 | ANN: testLoss: 0.4182 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 479 | ANN: testLoss: 0.4979 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 479 | ANN: testLoss: 0.5002 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 479 | ANN: testLoss: 0.4833 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 480 | ANN: trainLoss: 0.1294 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 480 | ANN: trainLoss: 0.1166 | trainAcc: 97.6562% (125/128)\n",
            "2 13 Epoch: 480 | ANN: trainLoss: 0.1118 | trainAcc: 97.9167% (188/192)\n",
            "3 13 Epoch: 480 | ANN: trainLoss: 0.1298 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 480 | ANN: trainLoss: 0.1546 | trainAcc: 95.3125% (305/320)\n",
            "5 13 Epoch: 480 | ANN: trainLoss: 0.1531 | trainAcc: 95.0521% (365/384)\n",
            "6 13 Epoch: 480 | ANN: trainLoss: 0.1503 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 480 | ANN: trainLoss: 0.1647 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 480 | ANN: trainLoss: 0.1630 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 480 | ANN: trainLoss: 0.1663 | trainAcc: 93.7500% (600/640)\n",
            "10 13 Epoch: 480 | ANN: trainLoss: 0.1732 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 480 | ANN: trainLoss: 0.1685 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 480 | ANN: trainLoss: 0.2101 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 480 | ANN: testLoss: 0.5365 | testAcc: 65.6250% (42/64)\n",
            "1 4 Epoch: 480 | ANN: testLoss: 0.5361 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 480 | ANN: testLoss: 0.4992 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 480 | ANN: testLoss: 0.4047 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 481 | ANN: trainLoss: 0.1228 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 481 | ANN: trainLoss: 0.1074 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 481 | ANN: trainLoss: 0.1405 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 481 | ANN: trainLoss: 0.1484 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 481 | ANN: trainLoss: 0.1563 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 481 | ANN: trainLoss: 0.1529 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 481 | ANN: trainLoss: 0.1458 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 481 | ANN: trainLoss: 0.1480 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 481 | ANN: trainLoss: 0.1487 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 481 | ANN: trainLoss: 0.1415 | trainAcc: 94.6875% (606/640)\n",
            "10 13 Epoch: 481 | ANN: trainLoss: 0.1395 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 481 | ANN: trainLoss: 0.1372 | trainAcc: 94.9219% (729/768)\n",
            "12 13 Epoch: 481 | ANN: trainLoss: 0.1423 | trainAcc: 94.9482% (733/772)\n",
            "0 4 Epoch: 481 | ANN: testLoss: 0.6227 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 481 | ANN: testLoss: 0.4939 | testAcc: 78.9062% (101/128)\n",
            "2 4 Epoch: 481 | ANN: testLoss: 0.5073 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 481 | ANN: testLoss: 0.6758 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 482 | ANN: trainLoss: 0.1211 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 482 | ANN: trainLoss: 0.1834 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 482 | ANN: trainLoss: 0.1690 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 482 | ANN: trainLoss: 0.1632 | trainAcc: 93.7500% (240/256)\n",
            "4 13 Epoch: 482 | ANN: trainLoss: 0.1540 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 482 | ANN: trainLoss: 0.1480 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 482 | ANN: trainLoss: 0.1455 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 482 | ANN: trainLoss: 0.1545 | trainAcc: 93.5547% (479/512)\n",
            "8 13 Epoch: 482 | ANN: trainLoss: 0.1476 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 482 | ANN: trainLoss: 0.1489 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 482 | ANN: trainLoss: 0.1469 | trainAcc: 93.3239% (657/704)\n",
            "11 13 Epoch: 482 | ANN: trainLoss: 0.1581 | trainAcc: 93.0990% (715/768)\n",
            "12 13 Epoch: 482 | ANN: trainLoss: 0.3229 | trainAcc: 92.8756% (717/772)\n",
            "0 4 Epoch: 482 | ANN: testLoss: 0.4639 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 482 | ANN: testLoss: 0.5339 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 482 | ANN: testLoss: 0.5277 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 482 | ANN: testLoss: 0.3984 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 483 | ANN: trainLoss: 0.0991 | trainAcc: 98.4375% (63/64)\n",
            "1 13 Epoch: 483 | ANN: trainLoss: 0.1704 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 483 | ANN: trainLoss: 0.1760 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 483 | ANN: trainLoss: 0.1697 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 483 | ANN: trainLoss: 0.1653 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 483 | ANN: trainLoss: 0.1696 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 483 | ANN: trainLoss: 0.1593 | trainAcc: 94.6429% (424/448)\n",
            "7 13 Epoch: 483 | ANN: trainLoss: 0.1627 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 483 | ANN: trainLoss: 0.1663 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 483 | ANN: trainLoss: 0.1594 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 483 | ANN: trainLoss: 0.1549 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 483 | ANN: trainLoss: 0.1561 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 483 | ANN: trainLoss: 0.2806 | trainAcc: 94.4301% (729/772)\n",
            "0 4 Epoch: 483 | ANN: testLoss: 0.4741 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 483 | ANN: testLoss: 0.4332 | testAcc: 80.4688% (103/128)\n",
            "2 4 Epoch: 483 | ANN: testLoss: 0.5071 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 483 | ANN: testLoss: 0.3909 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 484 | ANN: trainLoss: 0.1042 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 484 | ANN: trainLoss: 0.1359 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 484 | ANN: trainLoss: 0.1468 | trainAcc: 94.2708% (181/192)\n",
            "3 13 Epoch: 484 | ANN: trainLoss: 0.1580 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 484 | ANN: trainLoss: 0.1808 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 484 | ANN: trainLoss: 0.1730 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 484 | ANN: trainLoss: 0.1607 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 484 | ANN: trainLoss: 0.1510 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 484 | ANN: trainLoss: 0.1418 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 484 | ANN: trainLoss: 0.1445 | trainAcc: 95.0000% (608/640)\n",
            "10 13 Epoch: 484 | ANN: trainLoss: 0.1466 | trainAcc: 95.0284% (669/704)\n",
            "11 13 Epoch: 484 | ANN: trainLoss: 0.1511 | trainAcc: 94.9219% (729/768)\n",
            "12 13 Epoch: 484 | ANN: trainLoss: 0.1438 | trainAcc: 94.9482% (733/772)\n",
            "0 4 Epoch: 484 | ANN: testLoss: 0.5656 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 484 | ANN: testLoss: 0.4839 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 484 | ANN: testLoss: 0.5043 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 484 | ANN: testLoss: 0.8835 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 485 | ANN: trainLoss: 0.1579 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 485 | ANN: trainLoss: 0.1318 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 485 | ANN: trainLoss: 0.1270 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 485 | ANN: trainLoss: 0.1305 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 485 | ANN: trainLoss: 0.1214 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 485 | ANN: trainLoss: 0.1364 | trainAcc: 94.5312% (363/384)\n",
            "6 13 Epoch: 485 | ANN: trainLoss: 0.1301 | trainAcc: 94.8661% (425/448)\n",
            "7 13 Epoch: 485 | ANN: trainLoss: 0.1246 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 485 | ANN: trainLoss: 0.1333 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 485 | ANN: trainLoss: 0.1300 | trainAcc: 95.0000% (608/640)\n",
            "10 13 Epoch: 485 | ANN: trainLoss: 0.1296 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 485 | ANN: trainLoss: 0.1327 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 485 | ANN: trainLoss: 0.1916 | trainAcc: 94.4301% (729/772)\n",
            "0 4 Epoch: 485 | ANN: testLoss: 0.4418 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 485 | ANN: testLoss: 0.4197 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 485 | ANN: testLoss: 0.5141 | testAcc: 75.0000% (144/192)\n",
            "3 4 Epoch: 485 | ANN: testLoss: 0.3873 | testAcc: 75.1295% (145/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 486 | ANN: trainLoss: 0.1978 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 486 | ANN: trainLoss: 0.1299 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 486 | ANN: trainLoss: 0.1190 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 486 | ANN: trainLoss: 0.1058 | trainAcc: 97.2656% (249/256)\n",
            "4 13 Epoch: 486 | ANN: trainLoss: 0.1160 | trainAcc: 96.8750% (310/320)\n",
            "5 13 Epoch: 486 | ANN: trainLoss: 0.1279 | trainAcc: 95.8333% (368/384)\n",
            "6 13 Epoch: 486 | ANN: trainLoss: 0.1213 | trainAcc: 96.2054% (431/448)\n",
            "7 13 Epoch: 486 | ANN: trainLoss: 0.1160 | trainAcc: 96.4844% (494/512)\n",
            "8 13 Epoch: 486 | ANN: trainLoss: 0.1167 | trainAcc: 96.5278% (556/576)\n",
            "9 13 Epoch: 486 | ANN: trainLoss: 0.1202 | trainAcc: 96.4062% (617/640)\n",
            "10 13 Epoch: 486 | ANN: trainLoss: 0.1215 | trainAcc: 96.1648% (677/704)\n",
            "11 13 Epoch: 486 | ANN: trainLoss: 0.1200 | trainAcc: 96.2240% (739/768)\n",
            "12 13 Epoch: 486 | ANN: trainLoss: 0.1697 | trainAcc: 95.9845% (741/772)\n",
            "0 4 Epoch: 486 | ANN: testLoss: 0.5761 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 486 | ANN: testLoss: 0.4853 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 486 | ANN: testLoss: 0.4975 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 486 | ANN: testLoss: 0.4165 | testAcc: 76.1658% (147/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 487 | ANN: trainLoss: 0.1899 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 487 | ANN: trainLoss: 0.1463 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 487 | ANN: trainLoss: 0.1447 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 487 | ANN: trainLoss: 0.1512 | trainAcc: 94.9219% (243/256)\n",
            "4 13 Epoch: 487 | ANN: trainLoss: 0.1550 | trainAcc: 94.0625% (301/320)\n",
            "5 13 Epoch: 487 | ANN: trainLoss: 0.1634 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 487 | ANN: trainLoss: 0.1664 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 487 | ANN: trainLoss: 0.1555 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 487 | ANN: trainLoss: 0.1479 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 487 | ANN: trainLoss: 0.1482 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 487 | ANN: trainLoss: 0.1559 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 487 | ANN: trainLoss: 0.1635 | trainAcc: 94.0104% (722/768)\n",
            "12 13 Epoch: 487 | ANN: trainLoss: 0.2472 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 487 | ANN: testLoss: 0.4773 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 487 | ANN: testLoss: 0.4265 | testAcc: 82.8125% (106/128)\n",
            "2 4 Epoch: 487 | ANN: testLoss: 0.4953 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 487 | ANN: testLoss: 0.4769 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 488 | ANN: trainLoss: 0.2573 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 488 | ANN: trainLoss: 0.1804 | trainAcc: 95.3125% (122/128)\n",
            "2 13 Epoch: 488 | ANN: trainLoss: 0.1575 | trainAcc: 96.8750% (186/192)\n",
            "3 13 Epoch: 488 | ANN: trainLoss: 0.1599 | trainAcc: 95.7031% (245/256)\n",
            "4 13 Epoch: 488 | ANN: trainLoss: 0.1540 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 488 | ANN: trainLoss: 0.1625 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 488 | ANN: trainLoss: 0.1537 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 488 | ANN: trainLoss: 0.1449 | trainAcc: 95.7031% (490/512)\n",
            "8 13 Epoch: 488 | ANN: trainLoss: 0.1472 | trainAcc: 95.4861% (550/576)\n",
            "9 13 Epoch: 488 | ANN: trainLoss: 0.1531 | trainAcc: 94.8438% (607/640)\n",
            "10 13 Epoch: 488 | ANN: trainLoss: 0.1587 | trainAcc: 94.4602% (665/704)\n",
            "11 13 Epoch: 488 | ANN: trainLoss: 0.1510 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 488 | ANN: trainLoss: 0.1394 | trainAcc: 94.8187% (732/772)\n",
            "0 4 Epoch: 488 | ANN: testLoss: 0.4904 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 488 | ANN: testLoss: 0.5905 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 488 | ANN: testLoss: 0.5050 | testAcc: 76.0417% (146/192)\n",
            "3 4 Epoch: 488 | ANN: testLoss: 0.5594 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 489 | ANN: trainLoss: 0.2049 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 489 | ANN: trainLoss: 0.1562 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 489 | ANN: trainLoss: 0.1335 | trainAcc: 95.3125% (183/192)\n",
            "3 13 Epoch: 489 | ANN: trainLoss: 0.1390 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 489 | ANN: trainLoss: 0.1352 | trainAcc: 95.0000% (304/320)\n",
            "5 13 Epoch: 489 | ANN: trainLoss: 0.1320 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 489 | ANN: trainLoss: 0.1424 | trainAcc: 95.0893% (426/448)\n",
            "7 13 Epoch: 489 | ANN: trainLoss: 0.1580 | trainAcc: 94.1406% (482/512)\n",
            "8 13 Epoch: 489 | ANN: trainLoss: 0.1537 | trainAcc: 94.4444% (544/576)\n",
            "9 13 Epoch: 489 | ANN: trainLoss: 0.1552 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 489 | ANN: trainLoss: 0.1542 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 489 | ANN: trainLoss: 0.1525 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 489 | ANN: trainLoss: 0.2269 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 489 | ANN: testLoss: 0.6849 | testAcc: 67.1875% (43/64)\n",
            "1 4 Epoch: 489 | ANN: testLoss: 0.5482 | testAcc: 73.4375% (94/128)\n",
            "2 4 Epoch: 489 | ANN: testLoss: 0.5211 | testAcc: 75.5208% (145/192)\n",
            "3 4 Epoch: 489 | ANN: testLoss: 0.4176 | testAcc: 75.6477% (146/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 490 | ANN: trainLoss: 0.1849 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 490 | ANN: trainLoss: 0.1723 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 490 | ANN: trainLoss: 0.1534 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 490 | ANN: trainLoss: 0.1458 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 490 | ANN: trainLoss: 0.1477 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 490 | ANN: trainLoss: 0.1314 | trainAcc: 94.2708% (362/384)\n",
            "6 13 Epoch: 490 | ANN: trainLoss: 0.1304 | trainAcc: 94.4196% (423/448)\n",
            "7 13 Epoch: 490 | ANN: trainLoss: 0.1307 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 490 | ANN: trainLoss: 0.1428 | trainAcc: 93.7500% (540/576)\n",
            "9 13 Epoch: 490 | ANN: trainLoss: 0.1457 | trainAcc: 93.4375% (598/640)\n",
            "10 13 Epoch: 490 | ANN: trainLoss: 0.1524 | trainAcc: 93.1818% (656/704)\n",
            "11 13 Epoch: 490 | ANN: trainLoss: 0.1632 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 490 | ANN: trainLoss: 0.1850 | trainAcc: 92.7461% (716/772)\n",
            "0 4 Epoch: 490 | ANN: testLoss: 0.6092 | testAcc: 76.5625% (49/64)\n",
            "1 4 Epoch: 490 | ANN: testLoss: 0.5565 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 490 | ANN: testLoss: 0.4956 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 490 | ANN: testLoss: 0.8023 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 491 | ANN: trainLoss: 0.2499 | trainAcc: 89.0625% (57/64)\n",
            "1 13 Epoch: 491 | ANN: trainLoss: 0.2168 | trainAcc: 91.4062% (117/128)\n",
            "2 13 Epoch: 491 | ANN: trainLoss: 0.2166 | trainAcc: 91.1458% (175/192)\n",
            "3 13 Epoch: 491 | ANN: trainLoss: 0.2131 | trainAcc: 91.0156% (233/256)\n",
            "4 13 Epoch: 491 | ANN: trainLoss: 0.2134 | trainAcc: 90.6250% (290/320)\n",
            "5 13 Epoch: 491 | ANN: trainLoss: 0.1993 | trainAcc: 91.1458% (350/384)\n",
            "6 13 Epoch: 491 | ANN: trainLoss: 0.2023 | trainAcc: 91.2946% (409/448)\n",
            "7 13 Epoch: 491 | ANN: trainLoss: 0.1968 | trainAcc: 91.4062% (468/512)\n",
            "8 13 Epoch: 491 | ANN: trainLoss: 0.1945 | trainAcc: 91.8403% (529/576)\n",
            "9 13 Epoch: 491 | ANN: trainLoss: 0.1824 | trainAcc: 92.5000% (592/640)\n",
            "10 13 Epoch: 491 | ANN: trainLoss: 0.1834 | trainAcc: 92.8977% (654/704)\n",
            "11 13 Epoch: 491 | ANN: trainLoss: 0.1854 | trainAcc: 92.8385% (713/768)\n",
            "12 13 Epoch: 491 | ANN: trainLoss: 0.1714 | trainAcc: 92.8756% (717/772)\n",
            "0 4 Epoch: 491 | ANN: testLoss: 0.5500 | testAcc: 73.4375% (47/64)\n",
            "1 4 Epoch: 491 | ANN: testLoss: 0.5172 | testAcc: 74.2188% (95/128)\n",
            "2 4 Epoch: 491 | ANN: testLoss: 0.5079 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 491 | ANN: testLoss: 0.3876 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 492 | ANN: trainLoss: 0.1455 | trainAcc: 96.8750% (62/64)\n",
            "1 13 Epoch: 492 | ANN: trainLoss: 0.1535 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 492 | ANN: trainLoss: 0.1666 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 492 | ANN: trainLoss: 0.1590 | trainAcc: 92.9688% (238/256)\n",
            "4 13 Epoch: 492 | ANN: trainLoss: 0.1586 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 492 | ANN: trainLoss: 0.1577 | trainAcc: 93.7500% (360/384)\n",
            "6 13 Epoch: 492 | ANN: trainLoss: 0.1524 | trainAcc: 93.9732% (421/448)\n",
            "7 13 Epoch: 492 | ANN: trainLoss: 0.1523 | trainAcc: 94.3359% (483/512)\n",
            "8 13 Epoch: 492 | ANN: trainLoss: 0.1541 | trainAcc: 94.0972% (542/576)\n",
            "9 13 Epoch: 492 | ANN: trainLoss: 0.1556 | trainAcc: 94.0625% (602/640)\n",
            "10 13 Epoch: 492 | ANN: trainLoss: 0.1562 | trainAcc: 94.1761% (663/704)\n",
            "11 13 Epoch: 492 | ANN: trainLoss: 0.1625 | trainAcc: 94.1406% (723/768)\n",
            "12 13 Epoch: 492 | ANN: trainLoss: 0.1872 | trainAcc: 94.0415% (726/772)\n",
            "0 4 Epoch: 492 | ANN: testLoss: 0.4311 | testAcc: 78.1250% (50/64)\n",
            "1 4 Epoch: 492 | ANN: testLoss: 0.5055 | testAcc: 75.7812% (97/128)\n",
            "2 4 Epoch: 492 | ANN: testLoss: 0.5096 | testAcc: 76.5625% (147/192)\n",
            "3 4 Epoch: 492 | ANN: testLoss: 0.4650 | testAcc: 76.6839% (148/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 493 | ANN: trainLoss: 0.1119 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 493 | ANN: trainLoss: 0.1541 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 493 | ANN: trainLoss: 0.1449 | trainAcc: 94.7917% (182/192)\n",
            "3 13 Epoch: 493 | ANN: trainLoss: 0.1620 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 493 | ANN: trainLoss: 0.1557 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 493 | ANN: trainLoss: 0.1598 | trainAcc: 93.4896% (359/384)\n",
            "6 13 Epoch: 493 | ANN: trainLoss: 0.1632 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 493 | ANN: trainLoss: 0.1538 | trainAcc: 94.5312% (484/512)\n",
            "8 13 Epoch: 493 | ANN: trainLoss: 0.1585 | trainAcc: 94.2708% (543/576)\n",
            "9 13 Epoch: 493 | ANN: trainLoss: 0.1595 | trainAcc: 94.2188% (603/640)\n",
            "10 13 Epoch: 493 | ANN: trainLoss: 0.1630 | trainAcc: 94.0341% (662/704)\n",
            "11 13 Epoch: 493 | ANN: trainLoss: 0.1664 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 493 | ANN: trainLoss: 0.1826 | trainAcc: 93.7824% (724/772)\n",
            "0 4 Epoch: 493 | ANN: testLoss: 0.5309 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 493 | ANN: testLoss: 0.4876 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 493 | ANN: testLoss: 0.5005 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 493 | ANN: testLoss: 0.3924 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 494 | ANN: trainLoss: 0.1276 | trainAcc: 92.1875% (59/64)\n",
            "1 13 Epoch: 494 | ANN: trainLoss: 0.1201 | trainAcc: 92.9688% (119/128)\n",
            "2 13 Epoch: 494 | ANN: trainLoss: 0.1292 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 494 | ANN: trainLoss: 0.1320 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 494 | ANN: trainLoss: 0.1430 | trainAcc: 92.8125% (297/320)\n",
            "5 13 Epoch: 494 | ANN: trainLoss: 0.1464 | trainAcc: 92.9688% (357/384)\n",
            "6 13 Epoch: 494 | ANN: trainLoss: 0.1431 | trainAcc: 93.7500% (420/448)\n",
            "7 13 Epoch: 494 | ANN: trainLoss: 0.1464 | trainAcc: 93.7500% (480/512)\n",
            "8 13 Epoch: 494 | ANN: trainLoss: 0.1488 | trainAcc: 93.9236% (541/576)\n",
            "9 13 Epoch: 494 | ANN: trainLoss: 0.1459 | trainAcc: 94.3750% (604/640)\n",
            "10 13 Epoch: 494 | ANN: trainLoss: 0.1436 | trainAcc: 94.6023% (666/704)\n",
            "11 13 Epoch: 494 | ANN: trainLoss: 0.1457 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 494 | ANN: trainLoss: 0.1586 | trainAcc: 94.5596% (730/772)\n",
            "0 4 Epoch: 494 | ANN: testLoss: 0.6377 | testAcc: 68.7500% (44/64)\n",
            "1 4 Epoch: 494 | ANN: testLoss: 0.5020 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 494 | ANN: testLoss: 0.5078 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 494 | ANN: testLoss: 0.3815 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 495 | ANN: trainLoss: 0.2313 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 495 | ANN: trainLoss: 0.1693 | trainAcc: 93.7500% (120/128)\n",
            "2 13 Epoch: 495 | ANN: trainLoss: 0.1772 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 495 | ANN: trainLoss: 0.1715 | trainAcc: 94.1406% (241/256)\n",
            "4 13 Epoch: 495 | ANN: trainLoss: 0.1590 | trainAcc: 94.3750% (302/320)\n",
            "5 13 Epoch: 495 | ANN: trainLoss: 0.1509 | trainAcc: 94.7917% (364/384)\n",
            "6 13 Epoch: 495 | ANN: trainLoss: 0.1424 | trainAcc: 95.3125% (427/448)\n",
            "7 13 Epoch: 495 | ANN: trainLoss: 0.1530 | trainAcc: 95.1172% (487/512)\n",
            "8 13 Epoch: 495 | ANN: trainLoss: 0.1552 | trainAcc: 94.7917% (546/576)\n",
            "9 13 Epoch: 495 | ANN: trainLoss: 0.1553 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 495 | ANN: trainLoss: 0.1477 | trainAcc: 94.7443% (667/704)\n",
            "11 13 Epoch: 495 | ANN: trainLoss: 0.1499 | trainAcc: 94.5312% (726/768)\n",
            "12 13 Epoch: 495 | ANN: trainLoss: 0.1773 | trainAcc: 94.3005% (728/772)\n",
            "0 4 Epoch: 495 | ANN: testLoss: 0.6025 | testAcc: 75.0000% (48/64)\n",
            "1 4 Epoch: 495 | ANN: testLoss: 0.4873 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 495 | ANN: testLoss: 0.5082 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 495 | ANN: testLoss: 0.4400 | testAcc: 78.2383% (151/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 496 | ANN: trainLoss: 0.1100 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 496 | ANN: trainLoss: 0.1161 | trainAcc: 96.0938% (123/128)\n",
            "2 13 Epoch: 496 | ANN: trainLoss: 0.1216 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 496 | ANN: trainLoss: 0.1391 | trainAcc: 94.5312% (242/256)\n",
            "4 13 Epoch: 496 | ANN: trainLoss: 0.1488 | trainAcc: 93.7500% (300/320)\n",
            "5 13 Epoch: 496 | ANN: trainLoss: 0.1489 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 496 | ANN: trainLoss: 0.1657 | trainAcc: 93.0804% (417/448)\n",
            "7 13 Epoch: 496 | ANN: trainLoss: 0.1613 | trainAcc: 93.3594% (478/512)\n",
            "8 13 Epoch: 496 | ANN: trainLoss: 0.1590 | trainAcc: 93.2292% (537/576)\n",
            "9 13 Epoch: 496 | ANN: trainLoss: 0.1549 | trainAcc: 93.5938% (599/640)\n",
            "10 13 Epoch: 496 | ANN: trainLoss: 0.1496 | trainAcc: 93.7500% (660/704)\n",
            "11 13 Epoch: 496 | ANN: trainLoss: 0.1508 | trainAcc: 93.8802% (721/768)\n",
            "12 13 Epoch: 496 | ANN: trainLoss: 0.1422 | trainAcc: 93.9119% (725/772)\n",
            "0 4 Epoch: 496 | ANN: testLoss: 0.4054 | testAcc: 79.6875% (51/64)\n",
            "1 4 Epoch: 496 | ANN: testLoss: 0.4971 | testAcc: 76.5625% (98/128)\n",
            "2 4 Epoch: 496 | ANN: testLoss: 0.5018 | testAcc: 77.0833% (148/192)\n",
            "3 4 Epoch: 496 | ANN: testLoss: 0.4581 | testAcc: 77.2021% (149/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 497 | ANN: trainLoss: 0.1229 | trainAcc: 93.7500% (60/64)\n",
            "1 13 Epoch: 497 | ANN: trainLoss: 0.1284 | trainAcc: 94.5312% (121/128)\n",
            "2 13 Epoch: 497 | ANN: trainLoss: 0.1485 | trainAcc: 95.8333% (184/192)\n",
            "3 13 Epoch: 497 | ANN: trainLoss: 0.1685 | trainAcc: 95.3125% (244/256)\n",
            "4 13 Epoch: 497 | ANN: trainLoss: 0.1522 | trainAcc: 95.6250% (306/320)\n",
            "5 13 Epoch: 497 | ANN: trainLoss: 0.1498 | trainAcc: 95.5729% (367/384)\n",
            "6 13 Epoch: 497 | ANN: trainLoss: 0.1501 | trainAcc: 95.5357% (428/448)\n",
            "7 13 Epoch: 497 | ANN: trainLoss: 0.1477 | trainAcc: 95.3125% (488/512)\n",
            "8 13 Epoch: 497 | ANN: trainLoss: 0.1525 | trainAcc: 95.3125% (549/576)\n",
            "9 13 Epoch: 497 | ANN: trainLoss: 0.1521 | trainAcc: 95.3125% (610/640)\n",
            "10 13 Epoch: 497 | ANN: trainLoss: 0.1544 | trainAcc: 95.0284% (669/704)\n",
            "11 13 Epoch: 497 | ANN: trainLoss: 0.1550 | trainAcc: 94.6615% (727/768)\n",
            "12 13 Epoch: 497 | ANN: trainLoss: 0.1533 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 497 | ANN: testLoss: 0.3795 | testAcc: 84.3750% (54/64)\n",
            "1 4 Epoch: 497 | ANN: testLoss: 0.5238 | testAcc: 79.6875% (102/128)\n",
            "2 4 Epoch: 497 | ANN: testLoss: 0.5135 | testAcc: 78.6458% (151/192)\n",
            "3 4 Epoch: 497 | ANN: testLoss: 0.3874 | testAcc: 78.7565% (152/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 498 | ANN: trainLoss: 0.2193 | trainAcc: 90.6250% (58/64)\n",
            "1 13 Epoch: 498 | ANN: trainLoss: 0.2411 | trainAcc: 90.6250% (116/128)\n",
            "2 13 Epoch: 498 | ANN: trainLoss: 0.1970 | trainAcc: 92.1875% (177/192)\n",
            "3 13 Epoch: 498 | ANN: trainLoss: 0.1932 | trainAcc: 91.7969% (235/256)\n",
            "4 13 Epoch: 498 | ANN: trainLoss: 0.1720 | trainAcc: 93.1250% (298/320)\n",
            "5 13 Epoch: 498 | ANN: trainLoss: 0.1836 | trainAcc: 92.4479% (355/384)\n",
            "6 13 Epoch: 498 | ANN: trainLoss: 0.1935 | trainAcc: 91.9643% (412/448)\n",
            "7 13 Epoch: 498 | ANN: trainLoss: 0.1867 | trainAcc: 92.5781% (474/512)\n",
            "8 13 Epoch: 498 | ANN: trainLoss: 0.1829 | trainAcc: 92.5347% (533/576)\n",
            "9 13 Epoch: 498 | ANN: trainLoss: 0.1729 | trainAcc: 93.1250% (596/640)\n",
            "10 13 Epoch: 498 | ANN: trainLoss: 0.1701 | trainAcc: 93.4659% (658/704)\n",
            "11 13 Epoch: 498 | ANN: trainLoss: 0.1751 | trainAcc: 93.4896% (718/768)\n",
            "12 13 Epoch: 498 | ANN: trainLoss: 0.2027 | trainAcc: 93.3938% (721/772)\n",
            "0 4 Epoch: 498 | ANN: testLoss: 0.5733 | testAcc: 71.8750% (46/64)\n",
            "1 4 Epoch: 498 | ANN: testLoss: 0.5610 | testAcc: 75.0000% (96/128)\n",
            "2 4 Epoch: 498 | ANN: testLoss: 0.5053 | testAcc: 77.6042% (149/192)\n",
            "3 4 Epoch: 498 | ANN: testLoss: 0.4424 | testAcc: 77.7202% (150/193)\n",
            "\n",
            "\n",
            "0 13 Epoch: 499 | ANN: trainLoss: 0.1369 | trainAcc: 95.3125% (61/64)\n",
            "1 13 Epoch: 499 | ANN: trainLoss: 0.1835 | trainAcc: 92.1875% (118/128)\n",
            "2 13 Epoch: 499 | ANN: trainLoss: 0.1722 | trainAcc: 92.7083% (178/192)\n",
            "3 13 Epoch: 499 | ANN: trainLoss: 0.1767 | trainAcc: 93.3594% (239/256)\n",
            "4 13 Epoch: 499 | ANN: trainLoss: 0.1766 | trainAcc: 93.4375% (299/320)\n",
            "5 13 Epoch: 499 | ANN: trainLoss: 0.1675 | trainAcc: 94.0104% (361/384)\n",
            "6 13 Epoch: 499 | ANN: trainLoss: 0.1635 | trainAcc: 94.1964% (422/448)\n",
            "7 13 Epoch: 499 | ANN: trainLoss: 0.1540 | trainAcc: 94.7266% (485/512)\n",
            "8 13 Epoch: 499 | ANN: trainLoss: 0.1588 | trainAcc: 94.6181% (545/576)\n",
            "9 13 Epoch: 499 | ANN: trainLoss: 0.1573 | trainAcc: 94.5312% (605/640)\n",
            "10 13 Epoch: 499 | ANN: trainLoss: 0.1508 | trainAcc: 94.8864% (668/704)\n",
            "11 13 Epoch: 499 | ANN: trainLoss: 0.1531 | trainAcc: 94.7917% (728/768)\n",
            "12 13 Epoch: 499 | ANN: trainLoss: 0.1596 | trainAcc: 94.6891% (731/772)\n",
            "0 4 Epoch: 499 | ANN: testLoss: 0.3507 | testAcc: 81.2500% (52/64)\n",
            "1 4 Epoch: 499 | ANN: testLoss: 0.4647 | testAcc: 77.3438% (99/128)\n",
            "2 4 Epoch: 499 | ANN: testLoss: 0.5132 | testAcc: 78.1250% (150/192)\n",
            "3 4 Epoch: 499 | ANN: testLoss: 0.3853 | testAcc: 78.2383% (151/193)\n",
            "---------------------------------------------\n",
            "Converting using MaxNorm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:00<00:00, 363.89it/s]\n",
            "100%|██████████| 13/13 [00:00<00:00, 420.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ANN accuracy: Test: 78.2400%\n",
            "SNN accuracy: max_norm: 77.2021%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "7o4VT4r0r-xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "outputId": "c47843cf-e29e-4307-e783-09009b1e60f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions for all models...\n",
            "Evaluating LENet CNN...\n",
            "Evaluating LENet SNN...\n",
            "Evaluating LENet_FCL CNN...\n",
            "Evaluating LENet_FCL SNN...\n",
            "All predictions generated.\n",
            "LENet CNN|LENet SNN|LENet_FCL CNN|LENet_FCL SNN|"
          ]
        }
      ],
      "source": [
        "# @title Evaluate models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib # For colormaps access\n",
        "\n",
        "# Ensure all necessary variables from previous cells are available:\n",
        "# cnn_model_lenet_ccb, snn_model_lenet, cnn_model_lenet_fcl, snn_model_lenet_fcl\n",
        "# test_data, test_label (from the last train_test_split)\n",
        "# BATCH_SIZE, TIME_STEPS, device\n",
        "# data_loader function\n",
        "\n",
        "# 0. Prepare data loader for evaluation (using test_data from the last split)\n",
        "eval_test_loader = data_loader(test_data, test_label, batch=BATCH_SIZE, shuffle=False, drop=False)\n",
        "\n",
        "true_labels_list = []\n",
        "for _, targets_batch in eval_test_loader:\n",
        "    true_labels_list.extend(targets_batch.cpu().numpy())\n",
        "true_labels_np = np.array(true_labels_list)\n",
        "\n",
        "# --- 1. Get predictions for all models ---\n",
        "print(\"Generating predictions for all models...\")\n",
        "all_model_predictions = {}\n",
        "model_objects = {\n",
        "    \"LENet CNN\": cnn_model_lenet_ccb,\n",
        "    \"LENet SNN\": snn_model_lenet,\n",
        "    \"LENet_FCL CNN\": cnn_model_lenet_fcl,\n",
        "    \"LENet_FCL SNN\": snn_model_lenet_fcl\n",
        "}\n",
        "\n",
        "for model_name, model_obj in model_objects.items():\n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    model_obj.eval().to(device)\n",
        "    current_preds = []\n",
        "    is_snn = \"SNN\" in model_name\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in eval_test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            if is_snn:\n",
        "                for m_module in model_obj.modules():\n",
        "                    if hasattr(m_module, 'reset'):\n",
        "                        m_module.reset()\n",
        "                accumulated_outputs = None\n",
        "                for t in range(TIME_STEPS):\n",
        "                    outputs_t = model_obj(inputs)\n",
        "                    if accumulated_outputs is None:\n",
        "                        accumulated_outputs = outputs_t.clone()\n",
        "                    else:\n",
        "                        accumulated_outputs += outputs_t\n",
        "                _, predicted = accumulated_outputs.max(1)\n",
        "            else: # ANN\n",
        "                outputs = model_obj(inputs)\n",
        "                _, predicted = outputs.max(1)\n",
        "            current_preds.extend(predicted.cpu().numpy())\n",
        "    all_model_predictions[model_name] = current_preds\n",
        "print(\"All predictions generated.\")\n",
        "\n",
        "# --- 2. Calculate Accuracies for all models ---\n",
        "accuracy_results = {}\n",
        "class_names = ['Rest', 'Elbow', 'Hand'] # Corresponds to labels 0, 1, 2\n",
        "\n",
        "for model_name, predictions in all_model_predictions.items():\n",
        "    overall_acc = accuracy_score(true_labels_np, predictions)\n",
        "    accuracy_results[model_name] = {\"Overall\": overall_acc}\n",
        "    for class_idx, class_name_key in enumerate(class_names):\n",
        "        class_indices = np.where(true_labels_np == class_idx)[0]\n",
        "        if len(class_indices) > 0:\n",
        "            class_true = true_labels_np[class_indices]\n",
        "            class_pred = np.array(predictions)[class_indices]\n",
        "            class_acc = accuracy_score(class_true, class_pred)\n",
        "            accuracy_results[model_name][class_name_key] = class_acc\n",
        "        else:\n",
        "            accuracy_results[model_name][class_name_key] = np.nan\n",
        "for model_name in all_model_predictions:\n",
        "  print(model_name, end=\"|\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title visualize results (Custom confusion matrix, green good, red bad)\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "import matplotlib # For colormaps access\n",
        "\n",
        "# Ensure all necessary variables from previous cells are available:\n",
        "# cnn_model_lenet_ccb, snn_model_lenet, cnn_model_lenet_fcl, snn_model_lenet_fcl\n",
        "# test_data, test_label (from the last train_test_split)\n",
        "# EPOCHS, DROP_OUT, TEST_SIZE, BATCH_SIZE, TIME_STEPS, device (from training blocks)\n",
        "# channel_count, num_classes, data_length (from data loading cell)\n",
        "# all_data, all_label (from data loading cell, for full dataset stats)\n",
        "# all_model_predictions, accuracy_results, true_labels_np (from previous evaluation steps)\n",
        "# data_loader function (defined in a previous cell)\n",
        "\n",
        "# --- 0. Report Header: Hyperparameters and Data Specifications (Table Format) ---\n",
        "print(\"--- Experiment Configuration & Data Specifications ---\")\n",
        "\n",
        "# Collect all parameters\n",
        "config_params = {\n",
        "    \"Dataset File\": \"sub-011_eeg.mat\",\n",
        "    \"EEG Channels\": channel_count if 'channel_count' in locals() else \"N/A\",\n",
        "    \"Data Shape (Full Dataset)\": (\n",
        "        f\"Samples: {all_data.shape[0]}, Channels: {all_data.shape[1]}, Length: {all_data.shape[2]}\"\n",
        "        if 'all_data' in locals() and hasattr(all_data, 'shape') and len(all_data.shape) == 3\n",
        "        else \"N/A\"),\n",
        "    \"Class Distribution (Full Dataset)\": (\n",
        "        f\"Rest: {np.sum(all_label == 0)}, \"\n",
        "        f\"Elbow: {np.sum(all_label == 1)}, \"\n",
        "        f\"Hand: {np.sum(all_label == 2)}\"\n",
        "    ) if 'all_label' in locals() and isinstance(all_label, np.ndarray) and all_label.ndim >=1 else \"N/A\",\n",
        "    \"Bandpass Filter (Reported)\": \"7-35Hz\", #Change in case other filtering technique was used\n",
        "    \"Sampling Ratio (Reported)\": \"90Hz\", #Change in case other sampling was used\n",
        "    \"TRAINING EPOCHS\": EPOCHS if 'EPOCHS' in locals() else \"N/A\",\n",
        "    \"Dropout\": DROP_OUT if 'DROP_OUT' in locals() else \"N/A\",\n",
        "    \"Test Split\": TEST_SIZE if 'TEST_SIZE' in locals() else \"N/A\",\n",
        "    \"Batch Size (Evaluation)\": BATCH_SIZE if 'BATCH_SIZE' in locals() else \"N/A\",\n",
        "    \"SNN Time Steps\": TIME_STEPS if 'TIME_STEPS' in locals() else \"N/A\"\n",
        "}\n",
        "\n",
        "# Determine column widths\n",
        "max_key_len = max(len(key) for key in config_params.keys())\n",
        "# Ensure values are strings for len() calculation\n",
        "str_config_values = [str(val) for val in config_params.values()]\n",
        "max_val_len = max(len(val) for val in str_config_values)\n",
        "\n",
        "key_col_width = max_key_len + 2 # Add some padding\n",
        "val_col_width = max_val_len + 2\n",
        "\n",
        "# Print table header\n",
        "print(\"+\" + \"-\" * (key_col_width) + \"+\" + \"-\" * (val_col_width) + \"+\")\n",
        "print(f\"| {'Parameter':<{key_col_width-1}} | {'Value':<{val_col_width-1}} |\")\n",
        "print(\"+\" + \"=\" * (key_col_width) + \"+\" + \"=\" * (val_col_width) + \"+\")\n",
        "\n",
        "# Print table rows\n",
        "for idx, (key, value) in enumerate(config_params.items()):\n",
        "    print(f\"| {key:<{key_col_width-1}} | {str(value):<{val_col_width-1}} |\")\n",
        "\n",
        "print(\"+\" + \"-\" * (key_col_width) + \"+\" + \"-\" * (val_col_width) + \"+\")\n",
        "print(\"\\n\") # Add a bit of space before the main output\n",
        "\n",
        "# --- 1. Plot Confusion Matrices and Print Descriptions ---\n",
        "# This section assumes all_model_predictions, true_labels_np, accuracy_results are pre-computed\n",
        "# from previous cells if this cell is run in isolation.\n",
        "print(\"\\n--- Confusion Matrices and Descriptions ---\")\n",
        "class_names_display = ['Rest', 'Elbow', 'Hand'] # For display purposes in CM\n",
        "figure_counter = 1\n",
        "\n",
        "try:\n",
        "    cmap_greens = matplotlib.colormaps['Greens']\n",
        "    cmap_reds = matplotlib.colormaps['Reds']\n",
        "except AttributeError: # Older matplotlib\n",
        "    cmap_greens = plt.cm.get_cmap('Greens')\n",
        "    cmap_reds = plt.cm.get_cmap('Reds')\n",
        "\n",
        "# Thresholds for color logic\n",
        "threshold_diagonal_good = 0.5  # Above this is green on diagonal\n",
        "threshold_off_diagonal_bad = 0.2 # Above this is red on off-diagonal (significant misclassification)\n",
        "\n",
        "# Check if necessary variables for plotting exist\n",
        "if 'all_model_predictions' in locals() and 'true_labels_np' in locals() and 'accuracy_results' in locals():\n",
        "    for model_name, predictions in all_model_predictions.items():\n",
        "        cm = confusion_matrix(true_labels_np, predictions, labels=range(len(class_names_display)))\n",
        "\n",
        "        # --- Custom Color Logic V2 ---\n",
        "        row_sums = cm.sum(axis=1, keepdims=True)\n",
        "        cm_normalized_row = np.zeros_like(cm, dtype=float)\n",
        "        for r_idx in range(cm.shape[0]):\n",
        "            if row_sums[r_idx, 0] > 0:\n",
        "                cm_normalized_row[r_idx, :] = cm[r_idx, :] / row_sums[r_idx, 0]\n",
        "\n",
        "        num_classes_cm = cm.shape[0]\n",
        "        color_matrix_rgb = np.zeros((num_classes_cm, num_classes_cm, 3))\n",
        "        colormap_input_values = np.zeros((num_classes_cm, num_classes_cm))\n",
        "\n",
        "\n",
        "        for i in range(num_classes_cm):\n",
        "            for j in range(num_classes_cm):\n",
        "                norm_value = cm_normalized_row[i, j]\n",
        "                color_val_for_cmap = 0.0\n",
        "\n",
        "                if row_sums[i, 0] == 0:\n",
        "                    color_matrix_rgb[i, j, :] = [0.95, 0.95, 0.95]\n",
        "                elif i == j:\n",
        "                    if norm_value > threshold_diagonal_good:\n",
        "                        color_val_for_cmap = norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_greens(color_val_for_cmap)[:3]\n",
        "                    else:\n",
        "                        color_val_for_cmap = 1.0 - norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_reds(color_val_for_cmap)[:3]\n",
        "                else:\n",
        "                    if norm_value > threshold_off_diagonal_bad:\n",
        "                        color_val_for_cmap = norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_reds(color_val_for_cmap)[:3]\n",
        "                    else:\n",
        "                        color_val_for_cmap = 1.0 - norm_value\n",
        "                        color_matrix_rgb[i, j, :] = cmap_greens(color_val_for_cmap)[:3]\n",
        "                colormap_input_values[i,j] = color_val_for_cmap\n",
        "        # --- End Custom Color Logic V2 ---\n",
        "\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names_display)\n",
        "        fig, ax = plt.subplots(figsize=(8, 7))\n",
        "        disp.plot(ax=ax, include_values=True, cmap='Greys', colorbar=False, values_format='d')\n",
        "\n",
        "        if ax.images:\n",
        "            ax.images[0].remove()\n",
        "        ax.imshow(color_matrix_rgb)\n",
        "\n",
        "        if disp.text_ is not None:\n",
        "            for i in range(num_classes_cm):\n",
        "                for j in range(num_classes_cm):\n",
        "                    if disp.text_[i, j] is not None:\n",
        "                        text_color = \"white\" if colormap_input_values[i,j] > 0.5 else \"black\"\n",
        "                        if row_sums[i,0] == 0:\n",
        "                            text_color = \"black\"\n",
        "                        disp.text_[i, j].set_color(text_color)\n",
        "\n",
        "        ax.set_title(f'Confusion Matrix - {model_name}\\n(Custom Good/Bad Row-Normalized Colors)')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"\\nFigure {figure_counter}: Confusion Matrix for {model_name}.\")\n",
        "        print(f\"This matrix visualizes classification performance with custom colors based on row-normalized values\")\n",
        "\n",
        "\n",
        "        # Ensure class_names is defined for accessing accuracy_results\n",
        "        class_names_for_acc = ['Rest', 'Elbow', 'Hand'] # Define or ensure it's passed from global scope\n",
        "\n",
        "        overall_acc_val = accuracy_results[model_name]['Overall'] * 100\n",
        "        print(f\"Overall Accuracy: {overall_acc_val:.2f}%.\")\n",
        "        for class_idx, class_name_label in enumerate(class_names_display):\n",
        "            # Use a consistent key for accuracy_results, assuming class_names_for_acc matches keys\n",
        "            current_class_key = class_names_for_acc[class_idx]\n",
        "            if current_class_key in accuracy_results[model_name]:\n",
        "                class_acc_val = accuracy_results[model_name][current_class_key]\n",
        "                if not np.isnan(class_acc_val):\n",
        "                    print(f\"Accuracy for {class_name_label}: {class_acc_val*100:.2f}%.\")\n",
        "                else:\n",
        "                    print(f\"Accuracy for {class_name_label}: N/A (value is NaN).\")\n",
        "            else:\n",
        "                print(f\"Accuracy for {class_name_label}: N/A (key not found).\")\n",
        "        print(\"-\" * 70)\n",
        "        figure_counter += 1\n",
        "else:\n",
        "    print(\"Required variables (all_model_predictions, true_labels_np, or accuracy_results) not found for plotting confusion matrices.\")\n",
        "\n",
        "\n",
        "# --- 2. Accuracy Table ---\n",
        "print(\"\\n\\n--- Model Performance Summary Table ---\")\n",
        "# Ensure class_names is defined for table header\n",
        "class_names_for_table = ['Rest', 'Elbow', 'Hand'] # Define or ensure it's passed\n",
        "\n",
        "if 'accuracy_results' in locals():\n",
        "    header = f\"| {'Model':<17} | {'Overall Acc.':<15} | {class_names_for_table[0]+' Acc.':<12} | {class_names_for_table[1]+' Acc.':<12} | {class_names_for_table[2]+' Acc.':<12} |\"\n",
        "    separator = \"|-------------------|-----------------|--------------|--------------|--------------|\"\n",
        "    print(header)\n",
        "    print(separator)\n",
        "    for model_name_key in accuracy_results:\n",
        "        overall_str = f\"{accuracy_results[model_name_key]['Overall']*100:.2f}%\"\n",
        "\n",
        "        rest_acc_val = accuracy_results[model_name_key].get(class_names_for_table[0], np.nan)\n",
        "        rest_str = f\"{rest_acc_val*100:.2f}%\" if not np.isnan(rest_acc_val) else \"N/A\"\n",
        "\n",
        "        elbow_acc_val = accuracy_results[model_name_key].get(class_names_for_table[1], np.nan)\n",
        "        elbow_str = f\"{elbow_acc_val*100:.2f}%\" if not np.isnan(elbow_acc_val) else \"N/A\"\n",
        "\n",
        "        hand_acc_val = accuracy_results[model_name_key].get(class_names_for_table[2], np.nan)\n",
        "        hand_str = f\"{hand_acc_val*100:.2f}%\" if not np.isnan(hand_acc_val) else \"N/A\"\n",
        "\n",
        "        row = f\"| {model_name_key:<17} | {overall_str:<15} | {rest_str:<12} | {elbow_str:<12} | {hand_str:<12} |\"\n",
        "        print(row)\n",
        "else:\n",
        "    print(\"Accuracy results not available for summary table.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "collapsed": true,
        "id": "sBup_2nVClAB",
        "outputId": "cd38aa9a-1b60-4052-b08d-24be0261ef4f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Experiment Configuration & Data Specifications ---\n",
            "+-----------------------------------+-----------------------------------------+\n",
            "| Parameter                          | Value                                    |\n",
            "+===================================+=========================================+\n",
            "| Dataset File                       | sub-011_eeg.mat                          |\n",
            "| EEG Channels                       | 62                                       |\n",
            "| Data Shape (Full Dataset)          | Samples: 965, Channels: 62, Length: 360  |\n",
            "| Class Distribution (Full Dataset)  | Rest: 365, Elbow: 300, Hand: 300         |\n",
            "| Bandpass Filter (Reported)         | 7-35Hz                                   |\n",
            "| Sampling Ratio (Reported)          | 90Hz                                     |\n",
            "| TRAINING EPOCHS                    | 500                                      |\n",
            "| Dropout                            | 0.35                                     |\n",
            "| Test Split                         | 0.2                                      |\n",
            "| Batch Size (Evaluation)            | 64                                       |\n",
            "| SNN Time Steps                     | 100                                      |\n",
            "+-----------------------------------+-----------------------------------------+\n",
            "\n",
            "\n",
            "\n",
            "--- Confusion Matrices and Descriptions ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 1: Confusion Matrix for LENet CNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 77.72%.\n",
            "Accuracy for Rest: 100.00%.\n",
            "Accuracy for Elbow: 61.22%.\n",
            "Accuracy for Hand: 60.66%.\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 2: Confusion Matrix for LENet SNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 77.72%.\n",
            "Accuracy for Rest: 100.00%.\n",
            "Accuracy for Elbow: 61.22%.\n",
            "Accuracy for Hand: 60.66%.\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 3: Confusion Matrix for LENet_FCL CNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 78.24%.\n",
            "Accuracy for Rest: 97.59%.\n",
            "Accuracy for Elbow: 55.10%.\n",
            "Accuracy for Hand: 70.49%.\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Figure 4: Confusion Matrix for LENet_FCL SNN.\n",
            "This matrix visualizes classification performance with custom colors based on row-normalized values\n",
            "Overall Accuracy: 76.17%.\n",
            "Accuracy for Rest: 97.59%.\n",
            "Accuracy for Elbow: 48.98%.\n",
            "Accuracy for Hand: 68.85%.\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "--- Model Performance Summary Table ---\n",
            "| Model             | Overall Acc.    | Rest Acc.    | Elbow Acc.   | Hand Acc.    |\n",
            "|-------------------|-----------------|--------------|--------------|--------------|\n",
            "| LENet CNN         | 77.72%          | 100.00%      | 61.22%       | 60.66%       |\n",
            "| LENet SNN         | 77.72%          | 100.00%      | 61.22%       | 60.66%       |\n",
            "| LENet_FCL CNN     | 78.24%          | 97.59%       | 55.10%       | 70.49%       |\n",
            "| LENet_FCL SNN     | 76.17%          | 97.59%       | 48.98%       | 68.85%       |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Energy consumption estimation and comparison\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# Note: Ensure that the LENet, LENet_FCL classes, and initialize_weights function\n",
        "# are defined in a previous cell. Also, 'device', 'channel_count',\n",
        "# and 'data_length' should be available from your data loading and setup cells.\n",
        "# If cnn_model_lenet_ccb and cnn_model_lenet_ccb_fcl (trained models) exist,\n",
        "# they will be used; otherwise, new instances are created for architecture analysis.\n",
        "\n",
        "# --- Energy Calculation Constants (original in pJ) ---\n",
        "E_MAC_PJ = 4.6  # pJ per MAC operation\n",
        "E_AC_PJ = 0.9   # pJ per AC operation (for SNN spike processing)\n",
        "\n",
        "# Conversion factor from pJ to µJ\n",
        "PJ_TO_UJ = 1e-6\n",
        "\n",
        "def get_macs_and_energy_final(\n",
        "    model, sample_input, model_name=\"Model\", T_snn_param=100, avg_firing_rate=0.02, conversion_factor=1.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates MACs and estimates energy consumption for ANN and SNN versions.\n",
        "    Returns energy in units determined by conversion_factor.\n",
        "    Formats energy values as floats with two decimal places.\n",
        "    Formats Total MACs as integers with commas.\n",
        "    \"\"\"\n",
        "    layer_flops_mac_list = []\n",
        "    hooks = []\n",
        "\n",
        "    def hook_fn_flops(module, input_tensor_tuple, output_tensor):\n",
        "        macs = 0\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            c_in = module.in_channels\n",
        "            c_out = module.out_channels\n",
        "            k_h, k_w = module.kernel_size\n",
        "            h_out, w_out = output_tensor.shape[2], output_tensor.shape[3]\n",
        "            groups = module.groups\n",
        "            macs = (c_in // groups) * k_h * k_w * c_out * h_out * w_out\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            n_in = module.in_features\n",
        "            n_out = module.out_features\n",
        "            macs = n_in * n_out\n",
        "        if macs > 0:\n",
        "            layer_flops_mac_list.append(macs)\n",
        "\n",
        "    for _, module_item in model.named_modules():\n",
        "        if isinstance(module_item, (nn.Conv2d, nn.Linear)):\n",
        "            hooks.append(module_item.register_forward_hook(hook_fn_flops))\n",
        "\n",
        "    model.eval()\n",
        "    model_device = next(model.parameters()).device\n",
        "    with torch.no_grad():\n",
        "        model(sample_input.to(model_device))\n",
        "\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    total_macs_all_layers = sum(layer_flops_mac_list) # Integer\n",
        "    energy_ann_pj = E_MAC_PJ * float(total_macs_all_layers) # Use float for energy calc\n",
        "\n",
        "    if not layer_flops_mac_list:\n",
        "        energy_snn_pj = 0.0\n",
        "    else:\n",
        "        macs_layer1_for_snn = float(layer_flops_mac_list[0])\n",
        "        energy_snn_term1_pj = E_MAC_PJ * macs_layer1_for_snn\n",
        "        sops_sum_layers_2_L = 0.0\n",
        "        for i in range(1, len(layer_flops_mac_list)):\n",
        "            macs_layer_i = float(layer_flops_mac_list[i])\n",
        "            sops_layer_i = avg_firing_rate * T_snn_param * macs_layer_i\n",
        "            sops_sum_layers_2_L += sops_layer_i\n",
        "        energy_snn_term2_pj = E_AC_PJ * sops_sum_layers_2_L\n",
        "        energy_snn_pj = energy_snn_term1_pj + energy_snn_term2_pj\n",
        "\n",
        "    energy_ann_converted = energy_ann_pj * conversion_factor\n",
        "    energy_snn_converted = energy_snn_pj * conversion_factor\n",
        "\n",
        "    output_unit_str = \"µJ\" if conversion_factor == PJ_TO_UJ else \"pJ\"\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Total MACs\": f\"{total_macs_all_layers:,}\", # Integer with commas\n",
        "        f\"ANN Energy ({output_unit_str})\": f\"{energy_ann_converted:,.2f}\",\n",
        "        f\"SNN Energy ({output_unit_str})\": f\"{energy_snn_converted:,.2f}\",\n",
        "    }\n",
        "\n",
        "# --- Prepare models and input for energy calculation ---\n",
        "if 'device' not in locals():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # print(f\"Warning: 'device' not found, defaulting to {device}\") # Silenced for cleaner output\n",
        "if 'channel_count' not in locals():\n",
        "    channel_count = 60\n",
        "    # print(f\"Warning: 'channel_count' not found, defaulting to {channel_count}\")\n",
        "if 'data_length' not in locals():\n",
        "    data_length = 1000\n",
        "    # print(f\"Warning: 'data_length' not found, defaulting to {data_length}\")\n",
        "if 'TIME_STEPS' not in locals():\n",
        "    TIME_STEPS = 100\n",
        "    # print(f\"Warning: 'TIME_STEPS' not found, defaulting to {TIME_STEPS}\")\n",
        "if 'DROP_OUT' not in locals():\n",
        "    DROP_OUT = 0.25\n",
        "    # print(f\"Warning: 'DROP_OUT' not found, defaulting to {DROP_OUT}\")\n",
        "\n",
        "sample_input_tensor = torch.randn(1, 1, channel_count, data_length)\n",
        "\n",
        "# Model 1: LENet (CCB)\n",
        "if 'cnn_model_lenet_ccb' in locals() and isinstance(cnn_model_lenet_ccb, nn.Module):\n",
        "    # print(\"Using existing 'cnn_model_lenet_ccb' for LENet (CCB) energy calculation.\") # Silenced\n",
        "    model_for_energy_lenet_ccb = cnn_model_lenet_ccb\n",
        "else:\n",
        "    print(\"Warning: 'cnn_model_lenet_ccb' not found. Initializing new LENet for energy calculation.\")\n",
        "    try:\n",
        "        model_for_energy_lenet_ccb = LENet(classes_num=3, channel_count=channel_count, drop_out=DROP_OUT).to(device)\n",
        "        if 'initialize_weights' in locals(): model_for_energy_lenet_ccb.apply(initialize_weights)\n",
        "    except NameError:\n",
        "        print(\"Error: LENet class definition not found.\")\n",
        "        model_for_energy_lenet_ccb = None\n",
        "\n",
        "# Model 2: LENet_FCL\n",
        "if 'cnn_model_lenet_ccb_fcl' in locals() and isinstance(cnn_model_lenet_ccb_fcl, nn.Module):\n",
        "    # print(\"Using existing 'cnn_model_lenet_ccb_fcl' for LENet_FCL energy calculation.\") # Silenced\n",
        "    model_for_energy_lenet_fcl = cnn_model_lenet_ccb_fcl\n",
        "    model_for_energy_lenet_fcl.eval()\n",
        "    with torch.no_grad():\n",
        "        model_for_energy_lenet_fcl(sample_input_tensor.to(device))\n",
        "else:\n",
        "    print(\"Warning: 'cnn_model_lenet_ccb_fcl' not found. Initializing new LENet_FCL for energy calculation.\")\n",
        "    try:\n",
        "        model_for_energy_lenet_fcl = LENet_FCL(classes_num=3, channel_count=channel_count, drop_out=DROP_OUT).to(device)\n",
        "        if 'initialize_weights' in locals(): model_for_energy_lenet_fcl.apply(initialize_weights)\n",
        "        model_for_energy_lenet_fcl.eval()\n",
        "        with torch.no_grad():\n",
        "            model_for_energy_lenet_fcl(sample_input_tensor.to(device))\n",
        "    except NameError:\n",
        "        print(\"Error: LENet_FCL class definition not found.\")\n",
        "        model_for_energy_lenet_fcl = None\n",
        "\n",
        "# --- Perform Energy Calculations ---\n",
        "AVG_FIRING_RATE = 0.02 # 2%\n",
        "energy_results_list_final = []\n",
        "\n",
        "CURRENT_CONVERSION_FACTOR = PJ_TO_UJ\n",
        "OUTPUT_UNIT_NAME = \"µJ\"\n",
        "\n",
        "if model_for_energy_lenet_ccb:\n",
        "    results_lenet_final = get_macs_and_energy_final(\n",
        "        model_for_energy_lenet_ccb,\n",
        "        sample_input_tensor,\n",
        "        model_name=\"LENet CCB\",\n",
        "        T_snn_param=TIME_STEPS,\n",
        "        avg_firing_rate=AVG_FIRING_RATE,\n",
        "        conversion_factor=CURRENT_CONVERSION_FACTOR\n",
        "    )\n",
        "    energy_results_list_final.append(results_lenet_final)\n",
        "# else: # Silence for cleaner output\n",
        "    # print(\"Skipping LENet_CCB energy calculation as model was not available.\")\n",
        "\n",
        "if model_for_energy_lenet_fcl:\n",
        "    results_lenet_fcl_final = get_macs_and_energy_final(\n",
        "        model_for_energy_lenet_fcl,\n",
        "        sample_input_tensor,\n",
        "        model_name=\"LENet FCL\",\n",
        "        T_snn_param=TIME_STEPS,\n",
        "        avg_firing_rate=AVG_FIRING_RATE,\n",
        "        conversion_factor=CURRENT_CONVERSION_FACTOR\n",
        "    )\n",
        "    energy_results_list_final.append(results_lenet_fcl_final)\n",
        "# else: # Silenced for cleaner output\n",
        "    # print(\"Skipping LENet FCL energy calculation as model was not available.\")\n",
        "\n",
        "# --- Generate Markdown Table ---\n",
        "if energy_results_list_final:\n",
        "    # Table title is now part of the initial print statements from your notebook\n",
        "    # markdown_table_final = f\"Table 2. Estimated Theoretical Energy Consumption per Inference.\\n\"\n",
        "    markdown_table_final = \"\" # Start with an empty string for the table itself\n",
        "\n",
        "    # Adjusted column widths and alignment to match the image\n",
        "    # | Model         | Total MACs   | ANN Energy (µJ) | SNN Energy (µJ) |\n",
        "    # |:--------------|:-------------|:----------------|:----------------|\n",
        "    # Widths: Model: 13, Total MACs: 12, ANN Energy: 15, SNN Energy: 15 (approx from image)\n",
        "\n",
        "    markdown_table_final += f\"| Model       | Total MACs   | ANN Energy ({OUTPUT_UNIT_NAME}) | SNN Energy ({OUTPUT_UNIT_NAME}) |\\n\"\n",
        "    markdown_table_final += f\"|:------------|:-------------|:----------------|:----------------|\\n\"\n",
        "\n",
        "    for result in energy_results_list_final:\n",
        "        markdown_table_final += (\n",
        "            f\"| {result['Model']:<11} | \"  # Adjusted width\n",
        "            f\"{result['Total MACs']:>12} | \"\n",
        "            f\"{result[f'ANN Energy ({OUTPUT_UNIT_NAME})']:>15} | \"\n",
        "            f\"{result[f'SNN Energy ({OUTPUT_UNIT_NAME})']:>15} |\\n\"\n",
        "        )\n",
        "\n",
        "    # The newline after the table is handled by the print statement for constants\n",
        "    # markdown_table_final += \"\\n\"\n",
        "\n",
        "    constants_line = (\n",
        "        f\"Constants used: E_MAC = ({E_MAC_PJ:.1f} pJ), \"\n",
        "        f\"E_AC = ({E_AC_PJ:.1f} pJ). \"\n",
        "        f\"SNN parameters: T = {TIME_STEPS}, \"\n",
        "        f\"Assumed Average Firing Rate (fr) = {AVG_FIRING_RATE*100:.0f}%.\"\n",
        "    )\n",
        "    # Print the table first, then the constants on a new line\n",
        "    print(markdown_table_final)\n",
        "    print(constants_line)\n",
        "else:\n",
        "    print(\"No energy consumption results to display. Please check model availability and definitions.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXy2ibddkwob",
        "outputId": "f37f55af-24b0-4d87-8ffe-12d700c2075e"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Model       | Total MACs   | ANN Energy (µJ) | SNN Energy (µJ) |\n",
            "|:------------|:-------------|:----------------|:----------------|\n",
            "| LENet CCB   |   33,973,008 |          156.28 |           93.15 |\n",
            "| LENet FCL   |   33,973,008 |          156.28 |           93.15 |\n",
            "\n",
            "Constants used: E_MAC = (4.6 pJ), E_AC = (0.9 pJ). SNN parameters: T = 100, Assumed Average Firing Rate (fr) = 2%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title NN Visualization\n",
        "\n",
        "# This block should be placed after the training of the CNN models\n",
        "# you wish to visualize. For example, after the\n",
        "# \"LENet to SNN Conversion Framework execution\" cell (which trains cnn_model_lenet_ccb) and\n",
        "# \"LENet_FCL to SNN Conversion Framework execution\" cell (which trains cnn_model_lenet_fcl).\n",
        "# The `cnn_model_lenet_ccb` and `cnn_model_lenet_fcl` variables from those cells\n",
        "# will be used for visualization.\n",
        "\n",
        "# Install nnviz if you haven't already (uncomment the line below if needed)\n",
        "# !pip install nnviz -q\n",
        "\n",
        "# Import necessary nnviz modules\n",
        "from nnviz import drawing, inspection\n",
        "import torch # Ensure torch is imported\n",
        "import os # For creating directories\n",
        "\n",
        "print(\"Starting Neural Network Visualization...\")\n",
        "\n",
        "# --- Create 'viz' subdirectory if it doesn't exist ---\n",
        "VIZ_SUBFOLDER = \"viz\"\n",
        "if not os.path.exists(VIZ_SUBFOLDER):\n",
        "    os.makedirs(VIZ_SUBFOLDER)\n",
        "    print(f\"Created subfolder: {VIZ_SUBFOLDER}\")\n",
        "else:\n",
        "    print(f\"Subfolder '{VIZ_SUBFOLDER}' already exists.\")\n",
        "\n",
        "# Define output formats\n",
        "OUTPUT_FORMATS = [\"png\", \"svg\", \"pdf\"]\n",
        "\n",
        "\n",
        "def visualize_and_save_model(\n",
        "    model_instance, model_base_name, subfolder, formats\n",
        "):\n",
        "    \"\"\"Helper function to inspect and save model visualization in multiple formats.\"\"\"\n",
        "    print(f\"\\nVisualizing {model_base_name} CNN model...\")\n",
        "    original_device = next(model_instance.parameters()).device\n",
        "    try:\n",
        "        # Move model to CPU for inspection (safer for fx tracing)\n",
        "        model_cpu = model_instance.to(\"cpu\")\n",
        "        print(f\"Moved {model_base_name} to CPU for inspection.\")\n",
        "\n",
        "        # Create an inspector\n",
        "        inspector = inspection.TorchFxInspector()\n",
        "\n",
        "        print(f\"Inspecting {model_base_name} on CPU...\")\n",
        "        graph = inspector.inspect(model_cpu)\n",
        "\n",
        "        for fmt in formats:\n",
        "            viz_filename = os.path.join(\n",
        "                subfolder, f\"{model_base_name}_cnn_architecture.{fmt}\"\n",
        "            )\n",
        "            # nnviz's GraphvizDrawer determines format by filename extension\n",
        "            drawer = drawing.GraphvizDrawer(viz_filename)\n",
        "            drawer.draw(graph)\n",
        "            print(\n",
        "                f\"{model_base_name} CNN model visualization saved to: {viz_filename}\"\n",
        "            )\n",
        "\n",
        "        print(\n",
        "            f\"Note: If output files are empty or show errors, ensure Graphviz \"\n",
        "            f\"(specifically the 'dot' command) is installed and accessible in your system's PATH.\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\n",
        "            f\"An error occurred during {model_base_name} CNN model visualization: {e}\"\n",
        "        )\n",
        "        print(\"Troubleshooting tips:\")\n",
        "        print(\"- Ensure 'nnviz' and 'graphviz' (dot executable) are installed and in PATH.\")\n",
        "        print(\n",
        "            \"- The model structure might contain operations not traceable by torch.fx.\"\n",
        "        )\n",
        "    finally:\n",
        "        # Attempt to move model back to its original device\n",
        "        try:\n",
        "            model_instance.to(original_device)\n",
        "            print(\n",
        "                f\"Moved {model_base_name} back to its original device: {original_device}.\"\n",
        "            )\n",
        "        except Exception as e_move:\n",
        "            print(\n",
        "                f\"Could not move {model_base_name} back to {original_device}: {e_move}\"\n",
        "            )\n",
        "\n",
        "\n",
        "# --- LENet CCB (Classification Convolution Block) CNN Model Visualization ---\n",
        "if \"cnn_model_lenet_ccb\" in locals() and isinstance(\n",
        "    cnn_model_lenet_ccb, torch.nn.Module\n",
        "):\n",
        "    visualize_and_save_model(\n",
        "        cnn_model_lenet_ccb, \"LENet_CCB\", VIZ_SUBFOLDER, OUTPUT_FORMATS\n",
        "    )\n",
        "else:\n",
        "    print(\n",
        "        \"\\nLENet CCB model ('cnn_model_lenet_ccb') not found or not a torch.nn.Module. \"\n",
        "        \"Skipping LENet CCB CNN visualization.\"\n",
        "    )\n",
        "\n",
        "# --- LENet FCL (Fully Connected Layer) CNN Model Visualization ---\n",
        "if \"cnn_model_lenet_fcl\" in locals() and isinstance(\n",
        "    cnn_model_lenet_fcl, torch.nn.Module\n",
        "):\n",
        "    visualize_and_save_model(\n",
        "        cnn_model_lenet_fcl,\n",
        "        \"LENet_FCL\",\n",
        "        VIZ_SUBFOLDER,\n",
        "        OUTPUT_FORMATS,\n",
        "    )\n",
        "else:\n",
        "    print(\n",
        "        \"\\nLENet FCL model ('cnn_model_lenet_fcl') not found or not a torch.nn.Module. \"\n",
        "        \"Skipping LENet FCL CNN visualization.\"\n",
        "    )\n",
        "\n",
        "print(\"\\nNeural Network Visualization process complete.\")\n",
        "\n",
        "# --- Move models back to GPU (if they were on GPU and exist) ---\n",
        "print(\"\\nAttempting to move models back to their original device (e.g., GPU)...\")\n",
        "\n",
        "# Check for the main 'device' variable from your training setup\n",
        "if \"device\" not in locals():\n",
        "    print(\n",
        "        \"Global 'device' variable not found. Cannot determine target device for models.\"\n",
        "    )\n",
        "    # Fallback or define device if necessary, e.g.:\n",
        "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    # print(f\"Using fallback device: {device}\")\n",
        "\n",
        "if \"device\" in locals(): # Proceed if 'device' is defined\n",
        "    if \"cnn_model_lenet_ccb\" in locals() and isinstance(\n",
        "        cnn_model_lenet_ccb, torch.nn.Module\n",
        "    ):\n",
        "        try:\n",
        "            current_model_device_ccb = next(\n",
        "                cnn_model_lenet_ccb.parameters()\n",
        "            ).device\n",
        "            if str(current_model_device_ccb) != str(device):\n",
        "                cnn_model_lenet_ccb.to(device)\n",
        "                print(\n",
        "                    f\"Moved 'cnn_model_lenet_ccb' from {current_model_device_ccb} to {device}.\"\n",
        "                )\n",
        "            else:\n",
        "                print(\n",
        "                    f\"'cnn_model_lenet_ccb' is already on the target device: {device}.\"\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                f\"Error moving 'cnn_model_lenet_ccb' to {device}: {e}\"\n",
        "            )\n",
        "    else:\n",
        "        print(\n",
        "            \"'cnn_model_lenet_ccb' not found or not a valid model, skipping move.\"\n",
        "        )\n",
        "\n",
        "    if \"cnn_model_lenet_fcl\" in locals() and isinstance(\n",
        "        cnn_model_lenet_fcl, torch.nn.Module\n",
        "    ):\n",
        "        try:\n",
        "            current_model_device_fcl = next(\n",
        "                cnn_model_lenet_fcl.parameters()\n",
        "            ).device\n",
        "            if str(current_model_device_fcl) != str(device):\n",
        "                cnn_model_lenet_fcl.to(device)\n",
        "                print(\n",
        "                    f\"Moved 'cnn_model_lenet_fcl' from {current_model_device_fcl} to {device}.\"\n",
        "                )\n",
        "            else:\n",
        "                print(\n",
        "                    f\"'cnn_model_lenet_fcl' is already on the target device: {device}.\"\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                f\"Error moving 'cnn_model_lenet_fcl' to {device}: {e}\"\n",
        "            )\n",
        "    else:\n",
        "        print(\n",
        "            \"'cnn_model_lenet_fcl' not found or not a valid model, skipping move.\"\n",
        "        )\n",
        "else:\n",
        "    print(\"Skipping final model device move as 'device' variable is not defined.\")\n"
      ],
      "metadata": {
        "id": "V8p4PokzI0HX",
        "cellView": "form",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b1b1f6-51c5-4a1d-ab2e-ad224866e45d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Neural Network Visualization...\n",
            "Subfolder 'viz' already exists.\n",
            "\n",
            "Visualizing LENet_CCB CNN model...\n",
            "Moved LENet_CCB to CPU for inspection.\n",
            "Inspecting LENet_CCB on CPU...\n",
            "LENet_CCB CNN model visualization saved to: viz/LENet_CCB_cnn_architecture.png\n",
            "LENet_CCB CNN model visualization saved to: viz/LENet_CCB_cnn_architecture.svg\n",
            "LENet_CCB CNN model visualization saved to: viz/LENet_CCB_cnn_architecture.pdf\n",
            "Note: If output files are empty or show errors, ensure Graphviz (specifically the 'dot' command) is installed and accessible in your system's PATH.\n",
            "Moved LENet_CCB back to its original device: cuda:0.\n",
            "\n",
            "Visualizing LENet_FCL CNN model...\n",
            "Moved LENet_FCL to CPU for inspection.\n",
            "Inspecting LENet_FCL on CPU...\n",
            "LENet_FCL CNN model visualization saved to: viz/LENet_FCL_cnn_architecture.png\n",
            "LENet_FCL CNN model visualization saved to: viz/LENet_FCL_cnn_architecture.svg\n",
            "LENet_FCL CNN model visualization saved to: viz/LENet_FCL_cnn_architecture.pdf\n",
            "Note: If output files are empty or show errors, ensure Graphviz (specifically the 'dot' command) is installed and accessible in your system's PATH.\n",
            "Moved LENet_FCL back to its original device: cuda:0.\n",
            "\n",
            "Neural Network Visualization process complete.\n",
            "\n",
            "Attempting to move models back to their original device (e.g., GPU)...\n",
            "Moved 'cnn_model_lenet_ccb' from cuda:0 to cuda.\n",
            "Moved 'cnn_model_lenet_fcl' from cuda:0 to cuda.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}